
@article{shao_universal_2018,
	title = {Universal {Word} {Segmentation}: {Implementation} and {Interpretation}},
	volume = {6},
	shorttitle = {Universal {Word} {Segmentation}},
	url = {https://doi.org/10.1162/tacl_a_00033},
	doi = {10.1162/tacl_a_00033},
	abstract = {Word segmentation is a low-level NLP task that is non-trivial for a considerable number of languages. In this paper, we present a sequence tagging framework and apply it to word segmentation for a wide range of languages with different writing systems and typological characteristics. Additionally, we investigate the correlations between various typological factors and word segmentation accuracy. The experimental results indicate that segmentation accuracy is positively related to word boundary markers and negatively to the number of unique non-segmental terms. Based on the analysis, we design a small set of language-specific settings and extensively evaluate the segmentation system on the Universal Dependencies datasets. Our model obtains state-of-the-art accuracies on all the UD languages. It performs substantially better on languages that are non-trivial to segment, such as Chinese, Japanese, Arabic and Hebrew, when compared to previous work.},
	urldate = {2019-09-30},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Shao, Yan and Hardmeier, Christian and Nivre, Joakim},
	month = aug,
	year = {2018},
	pages = {421--435},
	file = {2018_Shao et al_Universal Word Segmentation.pdf:/home/akira/Zotero/storage/5LMFJ5SJ/2018_Shao et al_Universal Word Segmentation.pdf:application/pdf;Snapshot:/home/akira/Zotero/storage/L5867H2I/tacl_a_00033.html:text/html}
}

@misc{noauthor_conditional_2019,
	title = {Conditional random field},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Conditional_random_field&oldid=907425484},
	abstract = {Conditional random fields (CRFs) are a class of statistical modeling method often applied in pattern recognition and machine learning and used for structured prediction. CRFs fall into the sequence modeling family. Whereas a discrete classifier predicts a label for a single sample without considering "neighboring" samples, a CRF can take context into account; e.g., the linear chain CRF (which is popular in natural language processing) predicts sequences of labels for sequences of input samples.
CRFs are a type of discriminative undirected probabilistic graphical model. They are used to encode known relationships between observations and construct consistent interpretations and are often used for labeling or parsing of sequential data, such as natural language processing or biological sequences
and in computer vision.
Specifically, CRFs find applications in POS tagging, shallow parsing,named entity recognition,gene finding and peptide critical functional region finding,
among other tasks, being an alternative to the related hidden Markov models (HMMs). In computer vision, CRFs are often used for object recognition and image segmentation.},
	language = {en},
	urldate = {2019-09-30},
	journal = {Wikipedia},
	month = jul,
	year = {2019},
	note = {Page Version ID: 907425484},
	file = {Snapshot:/home/akira/Zotero/storage/ZQ52JCHL/index.html:text/html}
}
@misc{noauthor_lexical_nodate,
	title = {Lexical analysis - {Wikipedia}},
	url = {https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization},
	urldate = {2019-09-30},
	file = {Lexical analysis - Wikipedia:/home/akira/Zotero/storage/9VCI7CM4/Lexical_analysis.html:text/html}
}
@misc{noauthor_text_2019,
	title = {Text segmentation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Text_segmentation&oldid=914378398},
	abstract = {Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics.  The term applies both to mental processes used by humans when reading text, and to artificial processes implemented in computers, which are the subject of natural language processing. The problem is non-trivial, because while some written languages have explicit word boundary markers, such as the word spaces of written English and the distinctive initial, medial and final letter shapes of Arabic, such signals are sometimes ambiguous and not present in all written languages.
Compare speech segmentation, the process of dividing speech into linguistically meaningful portions.},
	language = {en},
	urldate = {2019-09-30},
	journal = {Wikipedia},
	month = sep,
	year = {2019},
	note = {Page Version ID: 914378398},
	file = {Snapshot:/home/akira/Zotero/storage/4A2H3NN3/index.html:text/html}
}
@inproceedings{qian_transition-based_2015-2,
	title = {A transition-based model for joint segmentation, {POS}-tagging and normalization},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959905712&partnerID=40&md5=00d6ac7fdc06bd2b83d3dfb4ec314f4a},
	doi = {10/gf8qd7},
	abstract = {We propose a transition-based model for joint word segmentation, POS tagging and text normalization. Different from previous methods, the model can be trained on standard text corpora, overcoming the lack of annotated microblog corpora. To evaluate our model, we develop an annotated corpus based on microblogs. Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02\%, compared to the traditional approach. © 2015 Association for Computational Linguistics.},
	booktitle = {Conference {Proceedings} - {EMNLP} 2015: {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Qian, T. and Zhang, Y. and Zhang, M. and Ren, Y. and Ji, D.},
	year = {2015},
	note = {98},
	pages = {1837--1846},
	annote = {cited By 11},
	file = {Qian et al. - 2015 - A transition-based model for joint segmentation, P.pdf:/home/akira/Zotero/storage/5AFV9JGI/Qian et al. - 2015 - A transition-based model for joint segmentation, P.pdf:application/pdf}
}
@inproceedings{cho_novel_2009-2,
	address = {Stroudsburg, PA, USA},
	series = {{ACLShort} '09},
	title = {A {Novel} {Word} {Segmentation} {Approach} for {Written} {Languages} with {Word} {Boundary} {Markers}},
	copyright = {94},
	url = {http://dl.acm.org/citation.cfm?id=1667583.1667594},
	doi = {10/bqwx4g},
	booktitle = {Proceedings of the {ACL}-{IJCNLP} 2009 {Conference} {Short} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Han-Cheol and Lee, Do-Gil and Lee, Jung-Tae and Stenetorp, Pontus and Tsujii, Jun'ichi and Rim, Hae-Chang},
	year = {2009},
	note = {event-place: Suntec, Singapore},
	pages = {29--32},
	file = {Cho et al. - 2009 - A Novel Word Segmentation Approach for Written Lan.pdf:/home/akira/Zotero/storage/WXFWXQNS/Cho et al. - 2009 - A Novel Word Segmentation Approach for Written Lan.pdf:application/pdf}
}
@misc{noauthor_text_2018,
	title = {Text normalization},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Text_normalization&oldid=844464819},
	abstract = {Text normalization is the process of transforming text into a single canonical form that it might not have had before. Normalizing text before storing or processing it allows for separation of concerns, since input is guaranteed to be consistent before operations are performed on it. Text normalization requires being aware of what type of text is to be normalized and how it is to be processed afterwards; there is no all-purpose normalization procedure.},
	language = {en},
	urldate = {2019-10-01},
	journal = {Wikipedia},
	month = jun,
	year = {2018},
	note = {Page Version ID: 844464819},
	file = {Snapshot:/home/akira/Zotero/storage/3D845UA6/index.html:text/html}
}

@misc{noauthor_parallel_2019,
	title = {Parallel text},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Parallel_text&oldid=907822683},
	abstract = {A parallel text is a text placed alongside its translation or translations.  Parallel text alignment is the identification of the corresponding sentences in both halves of the parallel text. The Loeb Classical Library and the Clay Sanskrit Library are two examples of dual-language series of texts. Reference Bibles may contain the original languages and a translation, or several translations by themselves, for ease of comparison and study; Origen's Hexapla (Greek for "sixfold") placed six versions of the Old Testament side by side.  The most famous example is the Rosetta Stone.
Large collections of parallel texts are called parallel corpora (see text corpus). Alignments of parallel corpora at sentence level are prerequisite for many areas of linguistic research. 
During translation, sentences can be split, merged, deleted, inserted or reordered by the translator. This makes alignment a non-trivial task.},
	language = {en},
	urldate = {2019-10-01},
	journal = {Wikipedia},
	month = jul,
	year = {2019},
	note = {Page Version ID: 907822683},
	file = {Snapshot:/home/akira/Zotero/storage/Z4UVDEQN/index.html:text/html}
}

@incollection{rubin2013survey,
  title={A survey of feature location techniques},
  author={Rubin, Julia and Chechik, Marsha},
  booktitle={Domain Engineering},
  pages={29--58},
  year={2013},
  publisher={Springer}
}


@article{doval_comparing_2018-1,
	title = {Comparing neural- and {N}-gram-based language models for word segmentation},
	volume = {70},
	url = {https://doi.org/10.1002%2Fasi.24082},
	doi = {10/gfs6rd},
	number = {2},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Doval, Yerai and Gómez-Rodríguez, Carlos},
	month = dec,
	year = {2018},
	note = {105},
	pages = {187--197},
	file = {Full Text:/home/akira/Zotero/storage/KSU5WPLA/Doval e Gómez-Rodríguez - 2018 - Comparing neural- and N-gram-based language models.pdf:application/pdf}
}


@inproceedings{koehn_empirical_2003,
	address = {Stroudsburg, PA, USA},
	series = {{EACL} '03},
	title = {Empirical {Methods} for {Compound} {Splitting}},
	isbn = {1-333-56789-0},
	url = {https://doi.org/10.3115/1067807.1067833},
	doi = {10.3115/1067807.1067833},
	booktitle = {Proceedings of the {Tenth} {Conference} on {European} {Chapter} of the {Association} for {Computational} {Linguistics} - {Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Koehn, Philipp and Knight, Kevin},
	year = {2003},
	note = {event-place: Budapest, Hungary 28},
	pages = {187--193},
	file = {Koehn and Knight - 2003 - Empirical Methods for Compound Splitting.pdf:/home/akira/Zotero/storage/X53YTB2J/Koehn and Knight - 2003 - Empirical Methods for Compound Splitting.pdf:application/pdf}
}


@misc{noauthor_statistical_2019,
	title = {Statistical machine translation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Statistical_machine_translation&oldid=913331538},
	abstract = {Statistical machine translation (SMT) is a machine translation paradigm where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora.  The statistical approach contrasts with the rule-based approaches to machine translation as well as with example-based machine translation.The first ideas of statistical machine translation were introduced by Warren Weaver in 1949, including the ideas of applying Claude Shannon's information theory. Statistical machine translation was re-introduced in the late 1980s and early 1990s by researchers at IBM's Thomas J. Watson Research Center and has contributed to the significant resurgence in interest in machine translation in recent years. Nowadays it is by far the most widely studied machine translation method.},
	language = {en},
	urldate = {2019-10-01},
	journal = {Wikipedia},
	month = aug,
	year = {2019},
	note = {Page Version ID: 913331538},
	file = {Snapshot:/home/akira/Zotero/storage/6P52W7LQ/index.html:text/html}
}


@misc{noauthor_entailment_2019,
	title = {Entailment (linguistics)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Entailment_(linguistics)&oldid=912834137},
	abstract = {Linguistic entailments occur when one may draw necessary conclusions from a particular use of a word, phrase or sentence. Entailment phrases are relations between propositions, and are always worded as, "if A then B," meaning that if A is true, then B must also be true. Another way of phrasing this is, "if A is true, then B must necessarily be true."},
	language = {en},
	urldate = {2019-10-01},
	journal = {Wikipedia},
	month = aug,
	year = {2019},
	note = {Page Version ID: 912834137},
	file = {Snapshot:/home/akira/Zotero/storage/9CGXU453/index.html:text/html}
}


@misc{noauthor_lexeme_2019,
	title = {Lexeme},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Lexeme&oldid=916304284},
	abstract = {A lexeme ( (listen)) is a unit of lexical meaning that underlies a set of words that are related through inflection. It is a basic abstract unit of meaning, a unit of morphological analysis in linguistics that roughly corresponds to a set of forms taken by a single root word. For example, in English, run, runs, ran and running are forms of the same lexeme, which can be represented as RUN.One form, the lemma (or citation form), is chosen by convention as the canonical form of a lexeme. The lemma is the form used in dictionaries as an entry's headword. Other forms of a lexeme are often listed later in the entry if they are uncommon or irregularly-inflected.},
	language = {en},
	urldate = {2019-10-01},
	journal = {Wikipedia},
	month = sep,
	year = {2019},
	note = {Page Version ID: 916304284},
	file = {Snapshot:/home/akira/Zotero/storage/9GQT9ZZS/index.html:text/html}
}


@misc{noauthor_lexicon_2019,
	title = {Lexicon},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Lexicon&oldid=914623263},
	abstract = {A lexicon, word-hoard, wordbook, or word-stock is the vocabulary of a person, language, or branch of knowledge (such as nautical or medical).  In linguistics, a lexicon is a language's inventory of lexemes. The word "lexicon" derives from the Greek λεξικόν (lexicon), neuter of λεξικός (lexikos) meaning "of or for words."Linguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalogue of a language's words (its wordstock); and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. The lexicon is also thought to include bound morphemes, which cannot stand alone as words (such as most affixes). In some analyses, compound words and certain classes of idiomatic expressions and other collocations are also considered to be part of the lexicon. Dictionaries represent attempts at listing, in alphabetical order, the lexicon of a given language; usually, however, bound morphemes are not included.},
	language = {en},
	urldate = {2019-10-01},
	journal = {Wikipedia},
	month = sep,
	year = {2019},
	note = {Page Version ID: 914623263},
	file = {Snapshot:/home/akira/Zotero/storage/E72LGFZ7/index.html:text/html}
}