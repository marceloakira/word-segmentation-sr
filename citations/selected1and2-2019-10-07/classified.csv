BibliographyType,ISBN,Identifier,Author,Title,Journal,Volume,Number,Month,Pages,Year,Address,Note,URL,Booktitle,Chapter,Edition,Series,Editor,Publisher,ReportType,Howpublished,Institution,Organizations,School,Annote,Custom1,Custom2,Custom3,Custom4,Custom5
7,"","smith_82_2018-1","Smith, Aaron; Bohnet, Bernd; Lhoneux, Miryam de; Nivre, Joakim; Shao, Yan & Stymne, Sara","82 Treebanks, 34 Models: Universal Dependency Parsing with Multi-Treebank Models","arxiv:1809.02237",,,"","",2018,"","88","http://arxiv.org/abs/1809.02237v1","","","","","","","","","","","","","","","⛔ No DOI found","",""
6,"","binkley_dataset_2013","Binkley, D.; Lawrie, D.; Pollock, L.; Hill, E. & Vijay-Shanker, K.","A dataset for evaluating identifier splitters","",,,"","401--404",2013,"","02","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889056852&doi=10.1109%2fMSR.2013.6624055&partnerID=40&md5=20c554dc45e942680416227bea5a5bdf","{IEEE} {International} {Working} {Conference} on {Mining} {Software} {Repositories}","","","","","","","","","","","cited By 6","Software engineering and evolution techniques have recently started to exploit the natural language information in source code. A key step in doing so is splitting identifiers into their constituent words. While simple in concept, identifier splitting raises several challenging issues, leading to a range of splitting techniques. Consequently, the research community would benefit from a dataset (i.e., a gold set) that facilitates comparative studies of identifier splitting techniques. A gold set of 2,663 split identifiers was constructed from 8,522 individual human splitting judgements and can be obtained from www.cs.loyola.edu/∼binkley/ludiso. This set's construction and observations aimed at its effective use are described. © 2013 IEEE.","","","",""
7,"","aken_statistical_2011-1","Aken, Jerry R. Van","A statistical learning algorithm for word segmentation","arxiv:1105.6162",,,"","",2011,"","96","http://arxiv.org/abs/1105.6162v2","","","","","","","","","","","","","","","⛔ No DOI found","",""
7,"","hill_empirical_2014-1","Hill, Emily; Binkley, David; Lawrie, Dawn; Pollock, Lori & Vijay-Shanker, K.","An Empirical Study of Identifier Splitting Techniques","Empirical Softw. Engg.",19,6,"","1754--1780",2014,"","06","http://dx.doi.org/10.1007/s10664-013-9261-0","","","","","","","","","","","","","","","Program comprehension, Identifier names, Software engineering tools, Source code text analysis","",""
7,"","guerrouj_experimental_2014","Guerrouj, Latifa; Penta, Massimiliano; Guéhéneuc, Yann-Gaël & Antoniol, Giuliano","An Experimental Investigation on the Effects of Context on Source Code Identifiers Splitting and Expansion","Empirical Softw. Engg.",19,6,"","1706--1753",2014,"","07","http://dx.doi.org/10.1007/s10664-013-9260-1","","","","","","","","","","","","","","","Identifier splitting and expansion, Program understanding, Task context","",""
7,"","wang_improved_2015-2","Wang, H.; Han, X.; Liu, L.; Song, W. & Yuan, M.","An improved unsupervised approach to word segmentation","China Communications",12,7,"July","82--95",2015,"","100","","","","","","","","","","","","","","ESA is an unsupervised approach to word segmentation previously proposed by Wang, which is an iterative process consisting of three phases: Evaluation, Selection and Adjustment. In this article, we propose ExESA, the extension of ESA. In ExESA, the original approach is extended to a 2-pass process and the ratio of different word lengths is introduced as the third type of information combined with cohesion and separation. A maximum strategy is adopted to determine the best segmentation of a character sequence in the phrase of Selection. Besides, in Adjustment, ExESA re-evaluates separation information and individual information to overcome the overestimation frequencies. Additionally, a smoothing algorithm is applied to alleviate sparseness. The experiment results show that ExESA can further improve the performance and is time-saving by properly utilizing more information from un-annotated corpora. Moreover, the parameters of ExESA can be predicted by a set of empirical formulae or combined with the minimum description length principle.","","text analysis, natural language processing, Uncertainty, Accuracy, word segmentation, unsupervised learning, iterative methods, Entropy, Smoothing methods, character sequence, character sequence segmentation, cohesion, ExESA, Frequency measurement, improved unsupervised approach, iterative process, Length measurement, maximum strategy, minimum description length principle, overestimation frequency, Prediction algorithms, separation information, smoothing algorithm, smoothing methods, word length","",""
7,"","shishkova_annotated_nodate","Shishkova, Anna & Chernyak, Ekaterina","Annotated Suffix Tree Method for German Compound Splitting","",,,"","",,"","08","","","","","","","","","","","","","","","","","",""
6,"978-0-7695-4123-5","guerrouj_automatic_2010","Guerrouj, Latifa","Automatic Derivation of Concepts Based on the Analysis of Source Code Identifiers","",,,"","301--304",2010,"Washington, DC, USA","11","http://dx.doi.org/10.1109/WCRE.2010.45","Proceedings of the 2010 17th {Working} {Conference} on {Reverse} {Engineering}","","","{WCRE} '10","","IEEE Computer Society","","","","",""," Cited by 4","","","Program Comprehension, Linguistic Analysis, Identifier Splitting, Software Quality","",""
7,"","lee_automatic_2007-2","Lee, D.-G.; Rim, H.-C. & Yook, D.","Automatic word spacing using probabilistic models based on character n-grams","IEEE Intelligent Systems",22,1,"","28--35",2007,"","101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847611998&doi=10.1109%2fMIS.2007.4&partnerID=40&md5=f49f0e501e9b8b12dedfb8fee532c736","","","","","","","","","","","","cited By 16","Probabilistic models based on Hidden Markov models (HMM) for automatic word spacing that use characters n-grams, which is a sub-sequence of n characters in a given character sequence, are discussed. Automatic word spacing is a preprocessing techniques used for correcting boundaries between words in a sentence containing spacing errors. These model can be effectively applied to a natural language with a small character set, such as English, using character n-grams that are larger than trigrams. These models, which are language independent and can be effectively used for languages having word spacing, can also be used for word segmentation in the languages without explicit word spacing. These models, by generalizing the HMMs, can consider a broad context and estimate accurate probabilities.","","","",""
7,"","wang_breaking_2015-1","Wang, Wei & Shirley, Kenneth","Breaking Bad: Detecting malicious domains using word segmentation","arxiv:1506.04111",,,"","",2015,"","104","http://arxiv.org/abs/1506.04111v1","","","","","","","","","","","","","","","⛔ No DOI found","",""
6,"","dit_can_2011-1","Dit, B.; Guerrouj, L.; Poshyvanyk, D. & Antoniol, G.","Can Better Identifier Splitting Techniques Help Feature Location?","",,,"June","11--20",2011,"","14","","2011 {IEEE} 19th {International} {Conference} on {Program} {Comprehension}","","","","","","","","","","","","The paper presents an exploratory study of two feature location techniques utilizing three strategies for splitting identifiers: Camel Case, Samurai and manual splitting of identifiers. The main research question that we ask in this study is if we had a perfect technique for splitting identifiers, would it still help improve accuracy of feature location techniques applied in different scenarios and settings? In order to answer this research question we investigate two feature location techniques, one based on Information Retrieval and the other one based on the combination of Information Retrieval and dynamic analysis, for locating bugs and features using various configurations of preprocessing strategies on two open-source systems, Rhino and jEdit. The results of an extensive empirical evaluation reveal that feature location techniques using Information Retrieval can benefit from better preprocessing algorithms in some cases, and that their improvement in effectiveness while using manual splitting over state-of-the-art approaches is statistically significant in those cases. However, the results for feature location technique using the combination of Information Retrieval and dynamic analysis do not show any improvement while using manual splitting, indicating that any preprocessing technique will suffice if execution data is available. Overall, our findings outline potential benefits of putting additional research efforts into defining more sophisticated source code preprocessing techniques as they can still be useful in situations where execution information cannot be easily collected.","","information retrieval, Manuals, Dictionaries, Samurai, Software, dynamic analysis, feature location, identifier splitting algorithms, Algorithm design and analysis, Accuracy, CamelCase, feature location technique, Gold, identifier splitting technique, jEdit, Large scale integration, open source system, preprocessing strategies, Rhino","",""
7,"","doval_comparing_2018-1","Doval, Yerai & Gómez-Rodríguez, Carlos","Comparing neural- and N-gram-based language models for word segmentation","Journal of the Association for Information Science and Technology",70,2,"","187--197",2018,"","105","https://doi.org/10.1002%2Fasi.24082","","","","","","","","","","","","","","","","",""
6,"3-540-65101-2","kraaij_comparing_1998","Kraaij, Wessel & Pohlmann, Renée","Comparing the Effect of Syntactic vs. Statistical Phrase Indexing Strategies for Dutch","",,,"","605--617",1998,"London, UK, UK","15","http://dl.acm.org/citation.cfm?id=646631.696685","Proceedings of the {Second} {European} {Conference} on {Research} and {Advanced} {Technology} for {Digital} {Libraries}","","","{ECDL} '98","","Springer-Verlag","","","","","","","","","","",""
6,"","ordelman_compound_2003","Ordelman, R.; Van Hessen, A. & De Jong, F.","Compound decomposition in Dutch large vocabulary speech recognition","",,,"","225--228",2003,"","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009170957&partnerID=40&md5=9373eec435a757ed1ac5b17f6d94fcd8","{EUROSPEECH} 2003 - 8th {European} {Conference} on {Speech} {Communication} and {Technology}","","","","","","","","","","","cited By 23","This paper addresses compound splitting for Dutch in the context of broadcast news transcription. Language models were created using original text versions and text versions that were decomposed using a data-driven compound splitting algorithm. Language model performances were compared in terms of outof- vocabulary rates and word error rates in a real-world broadcast news transcription task. It was concluded that compound splitting does improve ASR performance. Best results were obtained when frequent compounds were not decomposed.","","","",""
7,"","shao_cross-lingual_2017-1","Shao, Yan","Cross-lingual Word Segmentation and Morpheme Segmentation as Sequence Labelling","arxiv:1709.03756",,,"","",2017,"","106","http://arxiv.org/abs/1709.03756v1","","","","","","","","","","","","","","","⛔ No DOI found","",""
6,"978-1-60558-512-3","khaitan_data-driven_2009","Khaitan, Sanjeet; Das, Arumay; Gain, Sandeep & Sampath, Adithi","Data-driven Compound Splitting Method for English Compounds in Domain Names","",,,"","207--214",2009,"New York, NY, USA","event-place: Hong Kong, China 21","http://doi.acm.org/10.1145/1645953.1645982","Proceedings of the 18th {ACM} {Conference} on {Information} and {Knowledge} {Management}","","","{CIKM} '09","","ACM","","","","","","","","","compound splitting, domain names","",""
7,"","liang_detecting_2014-1","Liang, Wang & KaiYong, Zhao","Detecting "protein words" through unsupervised word segmentation","arxiv:1404.6866",,,"","",2014,"","107","http://arxiv.org/abs/1404.6866v6","","","","","","","","","","","","","","","⛔ No DOI found","",""
6,"","henrich_determining_2011","Henrich, V. & Hinrichs, E.","Determining immediate constituents of compounds in GermaNet","",,,"","420--426",2011,"","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866877956&partnerID=40&md5=4e50f1d779c887840937a10cea5dcd74","International {Conference} {Recent} {Advances} in {Natural} {Language} {Processing}, {RANLP}","","","","","","","","","","","cited By 11","In order to be able to systematically link compounds in GermaNet to their constituent parts, compound splitting needs to be applied recursively and has to identify the immediate constituents at each level of analysis. Existing tools for compound splitting for German only offer an analysis of all component parts of a compound at once without any grouping of subconstituents. Thus, existing tools for splitting compounds were adapted to overcome this issue. Algorithms combining three heterogeneous kinds of compound splitters are developed to achieve better results. The best overall result with an accuracy of 92.42\% is achieved by a hybrid combined compound splitter that takes into account all knowledge provided by the individual compound splitters, and in addition some domain knowledge about German derivation morphology and compounding.","","","",""
6,"","rigouts_terryn_dutch_2016","Rigouts Terryn, Ayla; Macken, Lieve & Lefever, Els","Dutch hypernym detection: does decompounding help?","",,,"","74--78",2016,"","27","","Joint {Second} {Workshop} on {Language} and {Ontology} \& {Terminology} and {Knowledge} {Structures} ({LangOnto}2+ {TermiKS})","","","","","European Language Resources Association (ELRA)","","","","","","","","","","",""
1,"","baziotis_ekphrasis_nodate-2","Baziotis, Christos; Pelekis, Nikos & Doulkeridis, Christos","Ekphrasis","",,,"","",,"","108","https://github.com/cbaziotis/ekphrasis","","","","","","","","","","","","","Ekphrasis is a text processing tool, geared towards text from social networks, such as Twitter or Facebook. Ekphrasis performs tokenization, word normalization, word segmentation (for splitting hashtags) and spell correction, using word statistics from 2 big corpora (english Wikipedia, twitter - 330mil english tweets).","","","",""
6,"1-333-56789-0","koehn_empirical_2003","Koehn, Philipp & Knight, Kevin","Empirical Methods for Compound Splitting","",,,"","187--193",2003,"Stroudsburg, PA, USA","event-place: Budapest, Hungary 28","https://doi.org/10.3115/1067807.1067833","Proceedings of the {Tenth} {Conference} on {European} {Chapter} of the {Association} for {Computational} {Linguistics} - {Volume} 1","","","{EACL} '03","","Association for Computational Linguistics","","","","","","","","","","",""
6,"","jagfeld_evaluating_2017","Jagfeld, G.; Ziering, P. & Van Der Plas, L.","Evaluating compound splitters extrinsically with textual entailment","",2,,"","58--63",2017,"","29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040556164&doi=10.18653%2fv1%2fP17-2010&partnerID=40&md5=abad0047ea69c94b560a54599233b45f","{ACL} 2017 - 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}, {Proceedings} of the {Conference} ({Long} {Papers})","","","","","","","","","","","cited By 0","Traditionally, compound splitters are evaluated intrinsically on gold-standard data or extrinsically on the task of statistical machine translation. We explore a novel way for the extrinsic evaluation of compound splitters, namely recognizing textual entailment. Compound splitting has great potential for this novel task that is both transparent and well-defined. Moreover, we show that it addresses certain aspects that are either ignored in intrinsic evaluations or compensated for by task-internal mechanisms in statistical machine translation. We show significant improvements using different compound splitting methods on a German textual entailment dataset. © 2017 Association for Computational Linguistics.","","","",""
10,"","garbe_fast_nodate-2","Garbe, Wolf","Fast Word Segmentation of Noisy Text","",,,"","",,"","110","https://towardsdatascience.com/fast-word-segmentation-for-noisy-text-2c2c41f9e8da","","","","","","","","","","","","","A string can be divided in several ways. Each distinct segmentation variant is called a composition. This article evaluates different types of word segmentation and propose several algorithms for each one of the basics concepts","","","",""
7,"","carvalho_source_2015","Carvalho, Nuno Ramos; Almeida, José João; Henriques, Pedro Rangel & Varanda, Maria João","From Source Code Identifiers to Natural Language Terms","J. Syst. Softw.",100,C,"","117--128",2015,"","31","http://dx.doi.org/10.1016/j.jss.2014.10.013","","","","","","","","","","","","","","","Identifier splitting, Natural language processing, Program comprehension","",""
6,"978-1-932432-88-6","hewlett_fully_2011-2","Hewlett, Daniel & Cohen, Paul","Fully Unsupervised Word Segmentation with BVE and MDL","",,,"","540--545",2011,"Stroudsburg, PA, USA","111 event-place: Portland, Oregon","http://dl.acm.org/citation.cfm?id=2002736.2002843","Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Short} {Papers} - {Volume} 2","","","{HLT} '11","","Association for Computational Linguistics","","","","","","","","","","",""
6,"","sugisaki_german_2018","Sugisaki, K. & Tuggener, D.","German compound splitting using the compound productivity of morphemes","",,,"","141--147",2018,"","32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064207728&partnerID=40&md5=9ad88e6eff661a660d155b41ada1fbb9","{KONVENS} 2018 - {Conference} on {Natural} {Language} {Processing} / {Die} {Konferenz} zur {Verarbeitung} {Naturlicher} {Sprache}","","","","","","","","","","","cited By 0","In this work, we present a novel compound splitting method for German by capturing the compound productivity of morphemes. We use a giga web corpus to create a lexicon and decompose noun compounds by computing the probabilities of compound elements as bound and free morphemes. Furthermore, we provide a uniformed evaluation of several unsupervised approaches and morphological analysers for the task. Our method achieved a high F1 score of 0.92, which was a comparable result to state-of-the-art methods. © KONVENS 2018 - Conference on Natural Language Processing / Die Konferenz zur Verarbeitung Naturlicher Sprache.All right reserved.","","","",""
6,"","escartin_german_2014","Escartín, Carla Parra; Peitz, Stephan & Ney, Hermann","German Compounds and Statistical Machine Translation. Can they get along?","",,,"","48--56",2014,"","33","","Proceedings of the 10th {Workshop} on {Multiword} {Expressions} ({MWE})","","","","","","","","","","","","","","","",""
6,"978-3-540-78135-6","alfonseca_german_2008","Alfonseca, Enrique; Bilac, Slaven & Pharies, Stefan","German Decompounding in a Difficult Corpus","",,,"","128--139",2008,"Berlin, Heidelberg","35","","Computational {Linguistics} and {Intelligent} {Text} {Processing}","","","","Gelbukh, Alexander","Springer Berlin Heidelberg","","","","","","","Splitting compound words has proved to be useful in areas such as Machine Translation, Speech Recognition or Information Retrieval (IR). In the case of IR systems, they usually have to cope with noisy data, as user queries are usually written quickly and submitted without review. This work attempts at improving the current approaches for German decompounding when applied to query keywords. The results show an increase of more than 10\% in accuracy compared to other state-of-the-art methods.","","","",""
6,"","li_helpful_2018","Li, J.; Du, Q.; Shi, K.; He, Y.; Wang, X. & Xu, J.","Helpful or not? an investigation on the feasibility of identifier splitting via CNN-BiLSTM-CRF","",2018-July,,"","175--181",2018,"","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056829823&doi=10.18293%2fSEKE2018-167&partnerID=40&md5=da7e9e1bd5ebdb234108fb0e7e22f2f9","Proceedings of the {International} {Conference} on {Software} {Engineering} and {Knowledge} {Engineering}, {SEKE}","","","","","","","","","","","cited By 0","We recently introduced a new technique to handle source code identifier splitting. The proposed technique, denoted as CNN-BiLSTM-CRF[a neural network composed of a convolutional neural network(CNN), bidirectional long short-Term memory networks(BiLSTM) and conditional random fields(CRFs)] enables us to obtain a model that splits identifiers correctly and effectively. This technique combines the use of a CNN layer with the mature BiLSTM-CRF model. The experimental results indicate that CNN-BiLSTM-CRF delivers outstanding performance on all four of the evaluation oracles. More importantly, we endeavored to provide insight into the practical feasibility of this technique by considering the aspects of generality, data size in demand and construction cost, etc. Finally, we reasoned out that CNN-BiLSTM-CRF should be helpful and improvable for identifier splitting in practical works in terms of the accuracy and feasibility. This was validated by multifaceted experiments. Index Terms-identifier splitting, source code mining, program comprehension, CNN, BiLSTM-CRF, feasibility investigation. © 2018 Universitat zu Koln. All rights reserved.","","","",""
6,"978-1-932432-71-8","fritzinger_how_2010","Fritzinger, Fabienne & Fraser, Alexander","How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing","",,,"","224--234",2010,"Stroudsburg, PA, USA","event-place: Uppsala, Sweden 38","http://dl.acm.org/citation.cfm?id=1868850.1868884","Proceedings of the {Joint} {Fifth} {Workshop} on {Statistical} {Machine} {Translation} and {MetricsMATR}","","","{WMT} '10","","Association for Computational Linguistics","","","","","","","","","","",""
6,"978-1-932432-41-1","johnson_improving_2009-1","Johnson, Mark & Goldwater, Sharon","Improving Nonparameteric Bayesian Inference: Experiments on Unsupervised Word Segmentation with Adaptor Grammars","",,,"","317--325",2009,"Stroudsburg, PA, USA","113 event-place: Boulder, Colorado","http://dl.acm.org/citation.cfm?id=1620754.1620800","Proceedings of {Human} {Language} {Technologies}: {The} 2009 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}","","","{NAACL} '09","","Association for Computational Linguistics","","","","","","","","","","",""
7,"","chen_incremental_2016-1","Chen, Ruey-Cheng","Incremental Learning for Fully Unsupervised Word Segmentation Using Penalized Likelihood and Model Selection","arxiv:1607.05822",,,"","",2016,"","114","http://arxiv.org/abs/1607.05822v2","","","","","","","","","","","","","","","⛔ No DOI found","",""
7,"","paul_integration_2011-2","Paul, M.; Finch, A. & Sumita, E.","Integration of multiple bilingually-trained segmentation schemes into statistical machine translation","IEICE Transactions on Information and Systems",E94-D,3,"","690--697",2011,"","116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952129828&doi=10.1587%2ftransinf.E94.D.690&partnerID=40&md5=f02f8345f7fbdde304c3c697b3f3d408","","","","","","","","","","","","cited By 3","This paper proposes an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches. The method can be applied to any language pair in which the source language is unsegmented and the target language segmentation is known. In the first step, an iterative bootstrap method is applied to learn multiple segmentation schemes that are consistent with the phrasal segmentations of an SMT system trained on the resegmented bitext. In the second step, multiple segmentation schemes are integrated into a single SMT system by characterizing the source language side and merging identical translation pairs of differently segmented SMT models. Experimental results translating five Asian languages into English revealed that the proposed method of integrating multiple segmentation schemes outperforms SMT models trained on any of the learned word segmentations and performs comparably to available monolingually built segmentation tools. © 2011 The Institute of Electronics, Information and Communication Engineers.","","","",""
6,"978-1-60558-641-0","paul_language_2009-1","Paul, Michael; Finch, Andrew & Sumita, Eiichiro","Language Independent Word Segmentation for Statistical Machine Translation","",,,"","36--40",2009,"New York, NY, USA","event-place: Tokyo, Japan","http://doi.acm.org/10.1145/1667780.1667788","Proceedings of the 3rd {International} {Universal} {Communication} {Symposium}","","","{IUCS} '09","","ACM","","","","","","","","","","",""
6,"978-1-932432-87-9","macherey_language-independent_2011","Macherey, Klaus; Dai, Andrew M.; Talbot, David; Popat, Ashok C. & Och, Franz","Language-independent Compound Splitting with Morphological Operations","",,,"","1395--1404",2011,"Stroudsburg, PA, USA","event-place: Portland, Oregon","http://dl.acm.org/citation.cfm?id=2002472.2002644","Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} - {Volume} 1","","","{HLT} '11","","Association for Computational Linguistics","","","","","","","","","","",""
7,"","kawakami_learning_2018-1","Kawakami, Kazuya; Dyer, Chris & Blunsom, Phil","Learning to Discover, Ground and Use Words with Segmental Neural Language Models","arxiv:1811.09353",,,"","",2018,"","122","http://arxiv.org/abs/1811.09353v2","","","","","","","","","","","","","","","⛔ No DOI found","",""
6,"","ma_letter_2016","Ma, Jianqiang; Henrich, Verena & Hinrichs, Erhard","Letter sequence labeling for compound splitting","",,,"","76--81",2016,"","41","","Proceedings of the 14th {SIGMORPHON} {Workshop} on {Computational} {Research} in {Phonetics}, {Phonology}, and {Morphology}","","","","","","","","","","","","","","","",""
6,"","corazza_linsen_2012","Corazza, A.; Martino, S. Di & Maggio, V.","LINSEN: An efficient approach to split identifiers and expand abbreviations","",,,"September","233--242",2012,"","43","","2012 28th {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM})","","","","","","","","","","","","Information Retrieval (IR) techniques are being exploited by an increasing number of tools supporting Software Maintenance activities. Indeed the lexical information embedded in the source code can be valuable for tasks such as concept location, clustering or recovery of traceability links. The application of such IR-based techniques relies on the consistency of the lexicon available in the different artifacts, and their effectiveness can worsen if programmers introduce abbreviations (e.g: rect) and/or do not strictly follow naming conventions such as Camel Case (e.g: UTFtoASCII). In this paper we propose an approach to automatically split identifiers in their composing words, and expand abbreviations. The solution is based on a graph model and performs in linear time with respect to the size of the dictionary, taking advantage of an approximate string matching technique. The proposed technique exploits a number of different dictionaries, referring to increasingly broader contexts, in order to achieve a disambiguation strategy based on the knowledge gathered from the most appropriate domain. The approach has been compared to other splitting and expansion techniques, using freely available oracles for the identifiers extracted from 24 C/C++ and Java open source systems. Results show an improvement in both splitting and expanding performance, in addition to a strong enhancement in the computational efficiency.","","Context, information retrieval, abbreviation expansion, approximate string matching technique, Approximation algorithms, C-C++, C++ language, camel case, concept location, Conferences, Dictionaries, dictionary, disambiguation strategy, Expansion, identifier splitting, information retrieval techniques, IR-based techniques, Java, Java open source systems, lexical information, LINSEN, oracles, pattern clustering, Program Comprehension, public domain software, Software algorithms, software maintenance, Software maintenance, software maintenance activities, Source Code Identifiers, Splitting, traceability links clustering, traceability links recovery","",""
6,"","enslen_mining_2009","Enslen, E.; Hill, E.; Pollock, L. & Vijay-Shanker, K.","Mining source code to automatically split identifiers for software analysis","",,,"","71--80",2009,"","46","","2009 6th {IEEE} {International} {Working} {Conference} on {Mining} {Software} {Repositories}","","","","","","","","","","","","Automated software engineering tools (e.g., program search, concern location, code reuse, quality assessment, etc.) increasingly rely on natural language information from comments and identifiers in code. The first step in analyzing words from identifiers requires splitting identifiers into their constituent words. Unlike natural languages, where space and punctuation are used to delineate words, identifiers cannot contain spaces. One common way to split identifiers is to follow programming language naming conventions. For example, Java programmers often use camel case, where words are delineated by uppercase letters or non-alphabetic characters. However, programmers also create identifiers by concatenating sequences of words together with no discernible delineation, which poses challenges to automatic identifier splitting. In this paper, we present an algorithm to automatically split identifiers into sequences of words by mining word frequencies in source code. With these word frequencies, our identifier splitter uses a scoring technique to automatically select the most appropriate partitioning for an identifier. In an evaluation of over 8000 identifiers from open source Java programs, our Samurai approach outperforms the existing state of the art techniques.","","Natural languages, data mining, Java, software maintenance, Software maintenance, program diagnostics, Frequency, automatic split identifier, Information analysis, Open source software, Programming profession, Quality assessment, software analysis, Software quality, Software tools, source code mining, word frequency mining","",""
7,"","machacek_morphological_2018-1","Macháček, Dominik; Vidra, Jonáš & Bojar, Ondřej","Morphological and Language-Agnostic Word Segmentation for NMT","arxiv:1806.05482",,,"","",2018,"","124","http://arxiv.org/abs/1806.05482v1","","","","","","","","","","","","","","","⛔ No DOI found","",""
6,"","moreau_multilingual_2019-2","Moreau, E. & Vogel, C.","Multilingual word segmentation: Training many language-specific tokenizers smoothly thanks to the universal dependencies corpus","",,,"","1119--1127",2019,"","125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059903761&partnerID=40&md5=0d85521a7ec28ece3d5e35165d639dda","{LREC} 2018 - 11th {International} {Conference} on {Language} {Resources} and {Evaluation}","","","","","","","","","","","cited By 1","This paper describes how a tokenizer can be trained from any dataset in the Universal Dependencies 2.1 corpus (UD2) (Nivre et al., 2017). A software tool, which relies on Elephant (Evang et al., 2013) to perform the training, is also made available. Beyond providing the community with a large choice of language-specific tokenizers, we argue in this paper that: (1) tokenization should be considered as a supervised task; (2) language scalability requires a streamlined software engineering process across languages. © LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.","","⛔ No DOI found","",""
7,"","sennrich_neural_2015-1","Sennrich, Rico; Haddow, Barry & Birch, Alexandra","Neural Machine Translation of Rare Words with Subword Units","arxiv:1508.07909",,,"","",2015,"","127","http://arxiv.org/abs/1508.07909v5","","","","","","","","","","","","","","","⛔ No DOI found","",""
7,"","yang_neural_2017-1","Yang, Jie; Zhang, Yue & Dong, Fei","Neural Word Segmentation with Rich Pretraining","arxiv:1704.08960",,,"","",2017,"","128","http://arxiv.org/abs/1704.08960v1","","","","","","","","","","","","","","","⛔ No DOI found","",""
1,"","jenks_python_nodate-2","Jenks, Grant","Python Word Segmentation","",,,"","",,"","130","https://github.com/grantjenks/python-wordsegment","","","","","","","","","","","","","Based on code from the chapter "Natural Language Corpus Data" by Peter Norvig from the book "Beautiful Data" (Segaran and Hammerbacher, 2009). Data files are derived from the Google Web Trillion Word Corpus, as described by Thorsten Brants and Alex Franz, and distributed by the Linguistic Data Consortium. This module contains only a subset of that data. The unigram data includes only the most common 333,000 words. Similarly, bigram data includes only the most common 250,000 phrases. Every word and phrase is lowercased with punctuation removed.","","","",""
7,"","reuter_segmenting_2016","Reuter, Jack; Pereira-Martins, Jhonata & Kalita, Jugal","Segmenting twitter hashtags","International Journal on Natural Language Computing",5,,"","23--36",2016,"","57","","","","","","","","","","","","","","","","","",""
6,"","srinivasan_segmenting_2012","Srinivasan, S.; Bhattacharya, S. & Chakraborty, R.","Segmenting web-domains and hashtags using length specific models","",,,"","1113--1122",2012,"","58","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871061019&doi=10.1145%2f2396761.2398410&partnerID=40&md5=4a4ef08843f61ac76f3dbc0b0b0a71a5","{ACM} {International} {Conference} {Proceeding} {Series}","","","","","","","","","","","cited By 13","Segmentation of a string of English language characters into a sequence of words has many applications. Here, we study two applications in the internet domain. First application is the web domain segmentation which is crucial for monetization of broken URLs. Secondly, we propose and study a novel application of twitter hashtag segmentation for increasing recall on twitter searches. Existing methods for word segmentation use unsupervised language models. We find that when using multiple corpora, the joint probability model from multiple corpora performs significantly better than the individual corpora. Motivated by this, we propose weighted joint probability model, with weights specific to each corpus. We propose to train the weights in a supervised manner using max-margin methods. The supervised probability models improve segmentation accuracy over joint probability models. Finally, we observe that length of segments is an important parameter for word segmentation, and incorporate length-specific weights into our model. The length specific models further improve segmentation accuracy over supervised probability models. For all models proposed here, inference problem can be solved using the dynamic programming algorithm. We test our methods on five different datasets, two from web domains data, and three from news headlines data from an LDC dataset. The supervised length specific models show significant improvements over unsupervised single corpus and joint probability models. Cross-testing between the datasets confirm that supervised probability models trained on all datasets, and length specific models trained on news headlines data, generalize well. Segmentation of hashtags result in significant improvement in recall on searches for twitter trends. © 2012 ACM.","","","",""
7,"","coster_selective_2004","Cöster, R.; Sahlgren, M. & Karlgren, J.","Selective compound splitting of swedish queries for boolean combinations of truncated terms","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3237,,"","337--344",2004,"","59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048871540&partnerID=40&md5=f5b2b1834f7cad7abe59755b8a103d03","","","","","","","","","","","","cited By 3","In languages that use compound words such as Swedish, it is often neccessary to split compound words when indexing documents or queries. One of the problems is that it is difficult to find constituents that express a concept similar to that expressed by the compound. The approach taken here is to expand a query with the leading constituents of the compound words. Every query term is truncated so as to increase recall by hopefully finding other compounds with the leading constituent as prefix. This approach increases recall in a rather uncontrolled way, so we use a Boolean quorum-level search method to rank documents both according to a tf-idf factor but also to the number of matching Boolean combinations. The Boolean combinations performed relatively well, taking into consideration that the queries were very short (maximum of five search terms). Also included in this paper are the results of two other methods we are currently working on in our lab; one for re-ranking search results on the basis of stylistic analysis of documents, and one for dimensionality reduction using Random Indexing. © Springer-Verlag 2004.","","","",""
6,"","wu_sembler_2012-2","Wu, X.; Fan, W. & Yu, Y.","Sembler: Ensembling crowd sequential labeling for improved quality","",2,,"","1713--1719",2012,"","133","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868274778&partnerID=40&md5=056a981811f122859f050ce2dfaa4ab1","Proceedings of the {National} {Conference} on {Artificial} {Intelligence}","","","","","","","","","","","cited By 7","Many natural language processing tasks, such as named entity recognition (NER), part of speech (POS) tagging, word segmentation, and etc., can be formulated as sequential data labeling problems. Building a sound labeler requires very large number of correctly labeled training examples, which may not always be possible. On the other hand, crowdsourcing provides an inexpensive yet efficient alternative to collect manual sequential labeling from non-experts. However the quality of crowd labeling cannot be guaranteed, and three kinds of errors are typical: (1) incorrect annotations due to lack of expertise (e.g., labeling gene names from plain text requires corresponding domain knowledge); (2) ignored or omitted annotations due to carelessness or low confidence; (3) noisy annotations due to cheating or vandalism. To correct these mistakes, we present Sembler, a statistical model for ensembling crowd sequential labelings. Sembler considers three types of statistical information: (1) the majority agreement that proves the correctness of an annotation; (2) correct annotation that improves the credibility of the corresponding annotator; (3) correct annotation that enhances the correctness of other annotations which share similar linguistic or contextual features. We evaluate the proposed model on a real Twitter and a synthetical biological data set, and find that Sembler is particularly accurate when more than half of annotators make mistakes. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.","","⛔ No DOI found","",""
6,"","weller-di_marco_simple_2017","Weller-Di Marco, Marion","Simple compound splitting for German","",,,"","161--166",2017,"","61","","Proceedings of the 13th {Workshop} on {Multiword} {Expressions} ({MWE} 2017)","","","","","","","","","","","","","","","",""
7,"","hucka_spiral_2018","Hucka, Michael","Spiral: splitters for identifiers in source code files.","J. Open Source Software",3,24,"","653",2018,"","66","","","","","","","","","","","","","","","","","",""
7,"","daiber_splitting_2015","Daiber, Joachim; Quiroz, Lautaro; Wechsler, Roger & Frank, Stella","Splitting compounds by semantic analogy","arXiv preprint arXiv:1509.04473",,,"","",2015,"","67","","","","","","","","","","","","","","","","","",""
6,"","shapiro_splitting_2016","Shapiro, Naomi Tachikawa","Splitting compounds with ngrams","",,,"","630--640",2016,"","68","","Proceedings of {COLING} 2016, the 26th {International} {Conference} on {Computational} {Linguistics}: {Technical} {Papers}","","","","","","","","","","","","","","","",""
6,"","clouet_splitting_2014-1","Clouet, Elizaveta Loginova & Daille, Béatrice","Splitting of compound terms in non-prototypical compounding languages","",,,"","",2014,"","136","","","","","","","","","","","","","","","","","",""
7,"","markovtsev_splitting_2018","Markovtsev, Vadim; Long, Waren; Bulychev, Egor; Keramitas, Romain; Slavnov, Konstantin & Markowski, Gabor","Splitting source code identifiers using Bidirectional LSTM Recurrent Neural Network","arXiv preprint arXiv:1805.11651",,,"","",2018,"","69","","","","","","","","","","","","","","","","","",""
7,"","popovic_statistical_2006-1","Popović, M.; Stein, D. & Ney, H.","Statistical machine translation of german compound words","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4139 LNAI,,"","616--624",2006,"","70","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749674758&partnerID=40&md5=5a2ba01a1897f9b2a062138b3e9f05d3","","","","","","","","","","","","cited By 20","German compound words pose special problems to statistical machine translation systems: the occurence of each of the components in the training data is not sufficient for successful translation. Even if the compound itself has been seen during training, the system may not be capable of translating it properly into two or more words. If German is the target language, the system might generate only separated components or may not be capable of choosing the correct compound. In this work, we investigate and compare different strategies for the treatment of German compound words in statistical machine translation systems. For translation from German, we compare linguistic-based and corpusbased compound splitting. For translation into German, we investigate splitting and rejoining German compounds, as well as joining English potential components. Additionaly, we investigate word alignments enhanced with knowledge about the splitting points of German compounds. The translation quality is consistently improved by all methods for both translation directions. © Springer-Verlag Berlin Heidelberg 2006.","","","",""
13,"","norvig_statistical_nodate-2","Norvig, Peter","Statistical Natural Language Processing in Python. or How To Do Things With Words. And Counters. or Everything I Needed to Know About NLP I learned From Sesame Street. Except Kneser-Ney Smoothing. The Count Didn't Cover That.","",,,"","",,"","137","https://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb","","","","","","","","","","","","","In this notebook Peter Norvig show us how to work with statistical natural language processing. With a theorical/pratical approach, Norvig shows the fundamentals about word segmentation using python.","","","",""
7,"","guerrouj_tidier_2013","Guerrouj, L.; Di Penta, M.; Antoniol, G. & Guéh́eneuc, Y.-G.","TIDIER: An identifier splitting approach using speech recognition techniques","Journal of software: Evolution and Process",25,6,"","575--599",2013,"","74","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883654687&doi=10.1002%2fsmr.539&partnerID=40&md5=79227717373f850527ad7601246a17bb","","","","","","","","","","","","cited By 21","The software engineering literature reports empirical evidence on the relation between various characteristics of a software system and its quality. Among other factors, recent studies have shown that a proper choice of identifiers influences understandability and maintainability. Indeed, identifiers are developers' main source of information and guide their cognitive processes during program comprehension when high-level documentation is scarce or outdated and when source code is not sufficiently commented. This paper proposes a novel approach to recognize words composing source code identifiers. The approach is based on an adaptation of Dynamic Time Warping used to recognize words in continuous speech. The approach overcomes the limitations of existing identifier-splitting approaches when naming conventions (e.g., Camel Case) are not used or when identifiers contain abbreviations. We apply the approach on a sample of more than 1000 identifiers extracted from 340 C programs and compare its results with a simple Camel Case splitter and with an implementation of an alternative identifier splitting approach, Samurai. Results indicate the capability of the novel approach: (i) to outperform the alternative ones, when using a dictionary augmented with domain knowledge or a contextual dictionary and (ii) to expand 48\% of a set of selected abbreviations into dictionary words. Copyright © 2011 John Wiley \& Sons, Ltd.","","","",""
6,"","ziering_towards_2016","Ziering, P. & Van Der Plas, L.","Towards unsupervised and language-independent compound splitting using inflectional morphological transformations","",,,"","644--653",2016,"","75","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994128765&partnerID=40&md5=5b0b9b0941575eb4aac9f0edfea61654","2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL} {HLT} 2016 - {Proceedings} of the {Conference}","","","","","","","","","","","cited By 7","In this paper, we address the task of language-independent, knowledge-lean and unsupervised compound splitting, which is an essential component for many natural language processing tasks such as machine translation. Previous methods on statistical compound splitting either include language-specific knowledge (e.g., linking elements) or rely on parallel data, which results in limited applicability. We aim to overcome these limitations by learning compounding morphology from inflectional information derived from lemmatized monolingual corpora. In experiments for Germanic languages, we show that our approach significantly outperforms language-dependent stateof-the-art methods in finding the correct split point and that word inflection is a good approximation for compounding morphology. ©2016 Association for Computational Linguistics.","","","",""
6,"","guerrouj_tris_2012","Guerrouj, L.; Galinier, P.; Guéhéneuc, Y.-G.; Antoniol, G. & Di Penta, M.","TRIS: A fast and accurate identifiers splitting and expansion algorithm","",,,"","103--112",2012,"","76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872297690&doi=10.1109%2fWCRE.2012.20&partnerID=40&md5=cde8887fb5e9ca9bd4f353e3c9bd9ad8","Proceedings - {Working} {Conference} on {Reverse} {Engineering}, {WCRE}","","","","","","","","","","","cited By 4","Understanding source code identifiers, by identifying words composing them, is a necessary step for many program comprehension, reverse engineering, or redocumentation tasks. To this aim, researchers have proposed several identifier splitting and expansion approaches such as Samurai, TIDIER and more recently GenTest. The ultimate goal of such approaches is to help disambiguating conceptual information encoded in compound (or abbreviated) identifiers. This paper presents TRIS, TRee-based Identifier Splitter, a two-phases approach to split and expand program identifiers. First, TRIS pre-compiles transformed dictionary words into a tree representation, associating a cost to each transformation. In a second phase, it maps the identifier splitting/expansion problem into a minimization problem, i.e., the search of the shortest path (optimal split/expansion) in a weighted graph. We apply TRIS to a sample of 974 identifiers extracted from JHotDraw, 3,085 from Lynx, and to a sample of 489 identifiers extracted from 340 C programs. Also, we compare TRIS with GenTest on a set of 2,663 mixed Java, C and C++ identifiers. We report evidence that TRIS split (and expansion) is more accurate than state-of-the-art approaches and that it is also efficient in terms of computation time. © 2012 IEEE.","","","",""
7,"","shao_universal_2018-1","Shao, Yan; Hardmeier, Christian & Nivre, Joakim","Universal Word Segmentation: Implementation and Interpretation","arxiv:1807.02974",,,"","",2018,"","143","http://arxiv.org/abs/1807.02974v1","","","","","","","","","","","","","","","⛔ No DOI found","",""
7,"","kazakov_unsupervised_2001-2","Kazakov, D. & Manandhar, S.","Unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming","Machine Learning",43,1-2,"","121--162",2001,"","144","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035312598&doi=10.1023%2fA%3a1007629103294&partnerID=40&md5=eaae5dc95f7c91cc97525afdf2bb2c17","","","","","","","","","","","","cited By 21","This article presents a combination of unsupervised and supervised learning techniques for the generation of word segmentation rules from a raw list of words. First, a language bias for word segmentation is introduced and a simple genetic algorithm is used in the search for a segmentation that corresponds to the best bias value. In the second phase, the words segmented by the genetic algorithm are used as an input for the first order decision list learner CLOG. The result is a set of first order rules which can be used for segmentation of unseen words. When applied on either the training data or unseen data, these rules produce segmentation which are linguistically meaningful, and to a large degree conforming to the annotation provided.","","","",""
7,"","roshani_unsupervised_2014-1","Roshani, Asra","Unsupervised segmentation of sequences using harmony search and hierarchical clustering techniques","",,,"","",2014,"","146","","","","","","","","","","","","","","","","⛔ No DOI found","",""
7,"","tambouratzis_using_2009-2","Tambouratzis, G.","Using an Ant Colony Metaheuristic to Optimize Automatic Word Segmentation for Ancient Greek","IEEE Transactions on Evolutionary Computation",13,4,"","742--753",2009,"","148","","","","","","","","","","","","","","Given a text or collection of texts involving unconstrained language, a basic task in a multitude of applications is the identification of stems and endings for each word form, which is termed morphological analysis. In this paper, the use of an ant colony optimization (ACO) metaheuristic is proposed for a linguistic task that involves the automated morphological segmentation of Ancient Greek word forms into stem and ending. The task of morphological analysis is essential for implementing text-processing applications such as semantic analysis and information retrieval. The difficulty of the morphological analysis task differs depending on the language chosen, being hardest in the case of highly-inflectional languages, where each stem may be associated with a large number of different endings. In this paper, focus is placed on the morphological analysis of ancient Greek, which has been shown to be a particularly hard task. To perform this task, a system for the automated morphological processing has been proposed, which implements the morphological analysis of words by coupling an iterative pattern-recognition algorithm with a modest amount of linguistic knowledge, expressed via a set of interactions associated with weights. In an earlier version of the system, these weights were determined by combining the input from specialized scientists with a lengthy manual optimization process. In this paper, the ACO metaheuristic is applied to the task of defining near-optimal system weights using an automated process based on a set of training data. The experiments performed indicate that the segmentation quality achieved by ACO is equivalent to or in several cases substantially higher than that achieved using manually optimized weights.","","information retrieval, Information retrieval, Data mining, text analysis, optimisation, Performance analysis, natural language processing, Algorithm design and analysis, Information analysis, ancient Greek, Ancient Greek, ant colony metaheuristic, Ant colony optimization, ant colony optimization (ACO) metaheuristic, automated morphological analysis, automated morphological processing, heuristic function, highly-inflectional languages, Iterative algorithms, iterative methods, iterative pattern-recognition algorithm, linguistic knowledge, linguistics, manual optimization process, morphological analysis, near-optimal system, optimize automatic word segmentation, Pattern analysis, pattern recognition, segmentation quality, semantic analysis, text processing, Text processing, text processing application, Training data","",""
6,"","sun_using_2013-2","Sun, R.; Zhou, W. & Liu, Z.","Using language rules to improve the performance of word segmentation","",03,,"","1665--1669",2013,"","149","","2013 6th {International} {Congress} on {Image} and {Signal} {Processing} ({CISP})","","","","","","","","","","","","Due to the disadvantage of the word segmentation tool in the aspect of the accuracy of word segmentation and speech tagging, it has harmful effects to the further research work. So this paper proposes a method based on the results of word segmentation to utilize language rules, which has been defined and described in Event Ontology, to verify and correct them. Experimental results show that compared with the method of only using word segmentation tool, the method of using language rules has a better performance.","","Ontologies, text analysis, natural language processing, Dictionaries, Word Segmentation, ontologies (artificial intelligence), Accuracy, Computers, Event, event ontology, Event Ontology, Hidden Markov models, Language Rule, language rules, Performance, speech tagging, Sun, Tagging, word segmentation tool","",""
6,"","gabay_using_2008-2","Gabay, D.; Ziv, B. E. & Elhadad, M.","Using wikipedia links to construct word segmentation corpora","",WS-08-15,,"","61--63",2008,"","150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449775446&partnerID=40&md5=ad7bc10a1c3ed07e582931513ab8ae88","{AAAI} {Workshop} - {Technical} {Report}","","","","","","","","","","","cited By 3","Tagged corpora are essential for evaluating and training natural language processing tools. The cost of constructing large enough manually tagged corpora is high, even when the annotation level is shallow. This article describes a simple method to automatically create a partially tagged corpus, using Wikipedia hyperlinks. The resulting corpus contains information about the correct segmentation of 523,599 non-consecutive words in 363,090 sentences. We used our method to construct a corpus of Modern Hebrew (which we have made available at http://www.cs.bgu.ac.il/-nlpproj). The method can also be applied to other languages where word segmentation is difficult to determine, such as East and South-East Asian languages. Copyright © 2008.","","⛔ No DOI found","",""
6,"","wang_web_2011","Wang, K.; Thrasher, C. & Hsu, B.-J.","Web scale NLP: A case study on URL word breaking","",,,"","357--366",2011,"","84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052123854&doi=10.1145%2f1963405.1963457&partnerID=40&md5=f0884c46e20c2b2cf89b5c9a1bb92dd2","Proceedings of the 20th {International} {Conference} on {World} {Wide} {Web}, {WWW} 2011","","","","","","","","","","","cited By 32","This paper uses the URL word breaking task as an example to elaborate what we identify as crucialin designingstatistical natural language processing (NLP) algorithmsfor Web scale applications: (1) rudimentary multilingual capabilities to cope with the global nature of the Web, (2) multi-style modeling to handle diverse language styles seen in the Web contents, (3) fast adaptation to keep pace with the dynamic changes of the Web, (4) minimal heuristic assumptions for generalizability and robustness, and (5) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost. We first show that the state-of-the-art word breaking techniquescan be unified and generalized under the Bayesian minimum risk (BMR) framework that, using a Web scale N-gram, can meet the first three requirements. We discuss how the existing techniques can be viewed asintroducing additional assumptions to the basic BMR framework, and describe a generic yet efficient implementation called word synchronous beam search.Testing the framework and its implementation on a series of large scale experiments reveals the following. First, the language style used to build the modelplays a critical role in the word breaking task, and the most suitable for the URL word breaking task appears to be that ofthe document title where the best performance is obtained. Models created from other language styles, such as from document body, anchor text, and even queries, exhibit varying degrees of mismatch. Although all styles benefit from increasing modeling power which, in our experiments, corresponds tothe use of a higher order N-gram, the gain is most recognizable for the title model. The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models, but are less effective and, in many cases,lead to poorer performance than the matched model with minimal assumptions. For the matched model based on document titles, an accuracy rate of 97.18\% can already be achieved using simple trigram without any heuristics. Copyright © 2011 by the Association for Computing Machinery, Inc. (ACM).","","","",""
6,"978-1-932432-92-3","hewlett_word_2011-2","Hewlett, Daniel & Cohen, Paul","Word Segmentation As General Chunking","",,,"","39--47",2011,"Stroudsburg, PA, USA","151 event-place: Portland, Oregon","http://dl.acm.org/citation.cfm?id=2018936.2018941","Proceedings of the {Fifteenth} {Conference} on {Computational} {Natural} {Language} {Learning}","","","{CoNLL} '11","","Association for Computational Linguistics","","","","","","","","","","",""
6,"","stahlberg_word_2012-2","Stahlberg, F.; Schlippe, T.; Vogel, S. & Schultz, T.","Word segmentation through cross-lingual word-to-phoneme alignment","",,,"","85--90",2012,"","153","","2012 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})","","","","","","","","","","","","We present our new alignment model Model 3P for cross-lingual word-to-phoneme alignment, and show that unsupervised learning of word segmentation is more accurate when information of another language is used. Word segmentation with cross-lingual information is highly relevant to bootstrap pronunciation dictionaries from audio data for Automatic Speech Recognition, bypass the written form in Speech-to-Speech Translation or build the vocabulary of an unseen language, particularly in the context of under-resourced languages. Using Model 3P for the alignment between English words and Spanish phonemes outperforms a state-of-the-art monolingual word segmentation approach [1] on the BTEC corpus [2] by up to 42\% absolute in F-Score on the phoneme level and a GIZA++ alignment based on IBM Model 3 by up to 17\%.","","natural language processing, Dictionaries, word segmentation, Vocabulary, unsupervised learning, Hidden Markov models, Training data, Grammar, speech recognition, automatic speech recognition, Vectors, Error analysis, alignment model, bootstrap pronunciation dictionaries, cross lingual information, cross lingual word-to-phoneme alignment, English words, Spanish phonemes, speech-to-speech translation, under-resourced language","",""
6,"","owczarzak_wordsyoudontknow_2014","Owczarzak, Karolina; de Haan, Ferdinand; Krupka, George & Hindle, Don","Wordsyoudontknow: Evaluation of lexicon-based decompounding with unknown handling","",,,"","63--71",2014,"","87","","Proceedings of the {First} {Workshop} on {Computational} {Approaches} to {Compound} {Analysis} ({ComAComA} 2014)","","","","","","","","","","","","","","","",""
