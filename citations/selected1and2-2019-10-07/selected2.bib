
@book{fibla_bilingual_nodate-2,
	title = {Bilingual {Word} {Segmentation}},
	url = {https://github.com/laiafr/bilingual_wordseg},
	abstract = {This project aims to model balanced bilinguals and monolinguals of 3 different languages, with language switching happening every other sentence, or every 100 sentences. We test the performance of different algorithms on word segmentation that represent different coginitve strategies that infants could be brining into the word segmentation task. This repository contains the collected corpora from English, Catalan and Spanish which are also freely available in CHILDES https://childes.talkbank.org As well as all the steps to process the data wich are descrived in the recipes of each language.},
	urldate = {2019-09-19},
	author = {Fibla, Laia},
	note = {102}
}

@book{baziotis_ekphrasis_nodate-2,
	title = {Ekphrasis},
	url = {https://github.com/cbaziotis/ekphrasis},
	abstract = {Ekphrasis is a text processing tool, geared towards text from social networks, such as Twitter or Facebook. Ekphrasis performs tokenization, word normalization, word segmentation (for splitting hashtags) and spell correction, using word statistics from 2 big corpora (english Wikipedia, twitter - 330mil english tweets).},
	urldate = {2019-09-19},
	author = {Baziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos},
	note = {108}
}

@misc{garbe_fast_nodate-2,
	title = {Fast {Word} {Segmentation} of {Noisy} {Text}},
	url = {https://towardsdatascience.com/fast-word-segmentation-for-noisy-text-2c2c41f9e8da},
	abstract = {A string can be divided in several ways. Each distinct segmentation variant is called a composition. This article evaluates different types of word segmentation and propose several algorithms for each one of the basics concepts},
	language = {Inglês},
	urldate = {2019-09-19},
	author = {Garbe, Wolf},
	note = {110}
}

@book{lanxiaowei_ik-analyzer_nodate-2,
	title = {{IK}-{Analyzer}},
	url = {https://github.com/yida-lxw/IK},
	abstract = {The source code of IK-Analyzer,Supported Arabic numerals and Chinese characters, Chinese figures and Chinese characters and Arabic Numbers and English letters of the word segmentation},
	urldate = {2019-09-19},
	author = {{Lanxiaowei}},
	note = {112}
}

@book{jenks_python_nodate-2,
	title = {Python {Word} {Segmentation}},
	url = {https://github.com/grantjenks/python-wordsegment},
	abstract = {Based on code from the chapter "Natural Language Corpus Data" by Peter Norvig from the book "Beautiful Data" (Segaran and Hammerbacher, 2009). Data files are derived from the Google Web Trillion Word Corpus, as described by Thorsten Brants and Alex Franz, and distributed by the Linguistic Data Consortium. This module contains only a subset of that data. The unigram data includes only the most common 333,000 words. Similarly, bigram data includes only the most common 250,000 phrases. Every word and phrase is lowercased with punctuation removed.},
	urldate = {2019-09-19},
	author = {Jenks, Grant},
	note = {130}
}

@techreport{norvig_statistical_nodate-2,
	type = {Relaçtório {Técnico}},
	title = {Statistical {Natural} {Language} {Processing} in {Python}. or {How} {To} {Do} {Things} {With} {Words}. {And} {Counters}. or {Everything} {I} {Needed} to {Know} {About} {NLP} {I} learned {From} {Sesame} {Street}. {Except} {Kneser}-{Ney} {Smoothing}. {The} {Count} {Didn}'t {Cover} {That}.},
	url = {https://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb},
	abstract = {In this notebook Peter Norvig show us how to work with statistical natural language processing. With a theorical/pratical approach, Norvig shows the fundamentals about word segmentation using python.},
	language = {Inglês},
	urldate = {2019-09-19},
	author = {Norvig, Peter},
	note = {137}
}

@book{vittal_text-reconstruction_nodate-2,
	title = {Text-{Reconstruction}},
	url = {https://github.com/sujay-vittal/Text-Reconstruction},
	abstract = {This project involves two tasks - word segmentation and vowel insertion. Word segmentation often comes up when processing many non-English languages, in which words might not be flanked by spaces on either end, such as written Chinese or long compound German words. Vowel insertion is relevant for languages like Arabic or Hebrew, where modern script eschews notations for vowel sounds and the human reader infers them from context. The goal of Vowel Insertion is to insert vowels back into segmented words in a way that maximizes sentence fluency (i.e., minimizes sentence cost). A bigram cost function is used.},
	urldate = {2019-09-19},
	author = {Vittal, Sujay},
	note = {140}
}

@book{frcchang_zpar_nodate-2,
	title = {{ZPar}},
	url = {https://github.com/frcchang/zpar},
	abstract = {ZPar statistical parser. Universal language support (depending on the availability of training data), with language-specific features for Chinese and English. Currently support word segmentation, POS tagging, dependency and phrase-structure parsing.},
	urldate = {2019-09-19},
	author = {{frcchang}},
	note = {154}
}

@article{zhu_systematic_2019-1,
	title = {A {Systematic} {Study} of {Leveraging} {Subword} {Information} for {Learning} {Word} {Representations}},
	url = {http://arxiv.org/abs/1904.07994v2},
	journal = {arxiv:1904.07994},
	author = {Zhu, Yi and Vulić, Ivan and Korhonen, Anna},
	year = {2019},
	note = {97},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/3SR5YGZX/Zhu et al. - 2019 - A Systematic Study of Leveraging Subword Informati.pdf:application/pdf}
}

@article{nguyen_learning_2019-3,
	title = {Learning short-text semantic similarity with word embeddings and external knowledge sources},
	volume = {182},
	issn = {0950-7051},
	url = {http://www.sciencedirect.com/science/article/pii/S095070511930317X},
	doi = {10/gf8qds},
	abstract = {We present a novel method based on interdependent representations of short texts for determining their degree of semantic similarity. The method represents each short text as two dense vectors: the former is built using the word-to-word similarity based on pre-trained word vectors, the latter is built using the word-to-word similarity based on external sources of knowledge. We also developed a preprocessing algorithm that chains coreferential named entities together and performs word segmentation to preserve the meaning of phrasal verbs and idioms. We evaluated the proposed method on three popular datasets, namely Microsoft Research Paraphrase Corpus, STS2015 and P4PIN, and obtained state-of-the-art results on all three without using prior knowledge of natural language, e.g., part-of-speech tags or parse tree, which indicates the interdependent representations of short text pairs are effective and efficient for semantic textual similarity tasks.},
	journal = {Knowledge-Based Systems},
	author = {Nguyen, Hien T. and Duong, Phuc H. and Cambria, Erik},
	year = {2019},
	note = {121},
	keywords = {Paraphrase identification, Semantic textual similarity, Sentence similarity, Short text similarity},
	pages = {104842}
}

@article{nguyen_learning_2019-4,
	title = {Learning short-text semantic similarity with word embeddings and external knowledge sources},
	volume = {182},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069875471&doi=10.1016%2fj.knosys.2019.07.013&partnerID=40&md5=79164e89f20b11d4c4cf48b954deaae2},
	doi = {10/gf8qds},
	abstract = {We present a novel method based on interdependent representations of short texts for determining their degree of semantic similarity. The method represents each short text as two dense vectors: the former is built using the word-to-word similarity based on pre-trained word vectors, the latter is built using the word-to-word similarity based on external sources of knowledge. We also developed a preprocessing algorithm that chains coreferential named entities together and performs word segmentation to preserve the meaning of phrasal verbs and idioms. We evaluated the proposed method on three popular datasets, namely Microsoft Research Paraphrase Corpus, STS2015 and P4PIN, and obtained state-of-the-art results on all three without using prior knowledge of natural language, e.g., part-of-speech tags or parse tree, which indicates the interdependent representations of short text pairs are effective and efficient for semantic textual similarity tasks. © 2019 Elsevier B.V.},
	journal = {Knowledge-Based Systems},
	author = {Nguyen, H.T. and Duong, P.H. and Cambria, E.},
	year = {2019},
	note = {120},
	annote = {cited By 0},
	file = {Nguyen et al. - 2019 - Learning short-text semantic similarity with word .pdf:/home/akira/Zotero/storage/8295KYQ5/Nguyen et al. - 2019 - Learning short-text semantic similarity with word .pdf:application/pdf}
}

@inproceedings{moreau_multilingual_2019-2,
	title = {Multilingual word segmentation: {Training} many language-specific tokenizers smoothly thanks to the universal dependencies corpus},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059903761&partnerID=40&md5=0d85521a7ec28ece3d5e35165d639dda},
	abstract = {This paper describes how a tokenizer can be trained from any dataset in the Universal Dependencies 2.1 corpus (UD2) (Nivre et al., 2017). A software tool, which relies on Elephant (Evang et al., 2013) to perform the training, is also made available. Beyond providing the community with a large choice of language-specific tokenizers, we argue in this paper that: (1) tokenization should be considered as a supervised task; (2) language scalability requires a streamlined software engineering process across languages. © LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.},
	booktitle = {{LREC} 2018 - 11th {International} {Conference} on {Language} {Resources} and {Evaluation}},
	author = {Moreau, E. and Vogel, C.},
	year = {2019},
	note = {125},
	keywords = {⛔ No DOI found},
	pages = {1119--1127},
	annote = {cited By 1},
	file = {Moreau and Vogel - 2019 - Multilingual word segmentation Training many lang.pdf:/home/akira/Zotero/storage/2I4HQFVQ/Moreau and Vogel - 2019 - Multilingual word segmentation Training many lang.pdf:application/pdf}
}

@article{smith_82_2018-1,
	title = {82 {Treebanks}, 34 {Models}: {Universal} {Dependency} {Parsing} with {Multi}-{Treebank} {Models}},
	url = {http://arxiv.org/abs/1809.02237v1},
	journal = {arxiv:1809.02237},
	author = {Smith, Aaron and Bohnet, Bernd and Lhoneux, Miryam de and Nivre, Joakim and Shao, Yan and Stymne, Sara},
	year = {2018},
	note = {88},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/LQQQKTE7/Smith et al. - 2018 - 82 Treebanks, 34 Models Universal Dependency Pars.pdf:application/pdf}
}

@article{wang_altas_2018-2,
	title = {{ALTAS}: {An} intelligent text analysis system based on knowledge graphs},
	volume = {10987 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050537718&doi=10.1007%2f978-3-319-96890-2_40&partnerID=40&md5=671150c35b5218bda2fe429115d446d7},
	doi = {10/gf8qdz},
	abstract = {This paper presents an intelligent text analysis system, called ALTAS, to support various text analysis tasks such as statistics analysis, sentiment analysis, text classification, and text clustering. The system contains four main components: knowledge graphs, text processing, text analysis and intelligent report. First, the system has built a semantic-rich knowledge base using several knowledge graph resources. A novel text processing and analysis framework based on knowledge graphs is developed and implemented. Given a text dataset, the text processing phase will do data cleaning, word segmentation and feature extraction for it. With the extracted features, the text analysis phase allows users to select a text mining task. We have implemented the proposed novel algorithm and several typical algorithms for each task. If users select multiple algorithms for the task, the intelligent report phase will automatically generate comparison results for users. Especially, the intelligent report phase also provides users a paper summary generating function on text mining problems. © Springer International Publishing AG, part of Springer Nature 2018.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Wang, X. and Gao, C. and Cao, J. and Lin, K. and Du, W. and Yang, Z.},
	year = {2018},
	note = {99},
	pages = {466--470},
	annote = {cited By 0}
}

@article{doval_comparing_2018-1,
	title = {Comparing neural- and {N}-gram-based language models for word segmentation},
	volume = {70},
	url = {https://doi.org/10.1002%2Fasi.24082},
	doi = {10/gfs6rd},
	number = {2},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Doval, Yerai and Gómez-Rodríguez, Carlos},
	year = {2018},
	note = {105},
	pages = {187--197},
	file = {Full Text:/home/akira/Zotero/storage/KSU5WPLA/Doval e Gómez-Rodríguez - 2018 - Comparing neural- and N-gram-based language models.pdf:application/pdf}
}

@article{kawakami_learning_2018-1,
	title = {Learning to {Discover}, {Ground} and {Use} {Words} with {Segmental} {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/1811.09353v2},
	journal = {arxiv:1811.09353},
	author = {Kawakami, Kazuya and Dyer, Chris and Blunsom, Phil},
	year = {2018},
	note = {122},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/RALC8BPS/Kawakami et al. - 2018 - Learning to Discover, Ground and Use Words with Se.pdf:application/pdf}
}

@article{machacek_morphological_2018-1,
	title = {Morphological and {Language}-{Agnostic} {Word} {Segmentation} for {NMT}},
	url = {http://arxiv.org/abs/1806.05482v1},
	journal = {arxiv:1806.05482},
	author = {Macháček, Dominik and Vidra, Jonáš and Bojar, Ondřej},
	year = {2018},
	note = {124},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/8UX99ETR/Macháček et al. - 2018 - Morphological and Language-Agnostic Word Segmentat.pdf:application/pdf}
}

@article{zhang_research_2018-3,
	title = {Research and {Experiment} of {Intelligent} {Natural} {Language} {Processing} {Algorithms}},
	volume = {102},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040677725&doi=10.1007%2fs11277-018-5316-2&partnerID=40&md5=73b8734ca4c3841c9c0e63013dc69c05},
	doi = {10/gf8qdp},
	abstract = {Natural language processing is mainly divided into two parts: speech processing and word processing. The level of word processing is mainly studied. Natural language processing is divided into lexical analysis, syntax analysis and semantic analysis. Aiming at the scope of the language ambiguity and thesaurus in the field of smart home, the maximal matching algorithm is used to segment the natural language. Then, through the way of template matching, semantic comprehension finally forms the code form that can control the home node. In the system applied in this paper, the speech is processed into words through the existing voice input function of the mobile terminal. Then, the control instruction is obtained through the language processing method. The processed data is communicated to the server via socket. The server sends the data to the home node through the Zigbee protocol. Finally, control of home appliances is achieved. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
	number = {4},
	journal = {Wireless Personal Communications},
	author = {Zhang, Z. and Bi, X.},
	year = {2018},
	note = {131},
	pages = {2927--2939},
	annote = {cited By 0},
	file = {Zhang and Bi - 2018 - Research and Experiment of Intelligent Natural Lan.pdf:/home/akira/Zotero/storage/NLQBEYVD/Zhang and Bi - 2018 - Research and Experiment of Intelligent Natural Lan.pdf:application/pdf}
}

@inproceedings{brito_self-organizing_2018-1,
	title = {Self-{Organizing} {Maps} with {Variable} {Input} {Length} for {Motif} {Discovery} and {Word} {Segmentation}},
	url = {https://doi.org/10.1109%2Fijcnn.2018.8489090},
	doi = {10/gf8qdn},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Brito, Raphael C. and Bassani, Hansenclever F.},
	month = jul,
	year = {2018},
	note = {132},
	file = {Brito e Bassani - 2018 - Self-Organizing Maps with Variable Input Length fo.pdf:/home/akira/Zotero/storage/FEGR2L2Y/Brito e Bassani - 2018 - Self-Organizing Maps with Variable Input Length fo.pdf:application/pdf}
}

@article{shao_universal_2018-1,
	title = {Universal {Word} {Segmentation}: {Implementation} and {Interpretation}},
	url = {http://arxiv.org/abs/1807.02974v1},
	journal = {arxiv:1807.02974},
	author = {Shao, Yan and Hardmeier, Christian and Nivre, Joakim},
	year = {2018},
	note = {143},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/NH2QJWD7/Shao et al. - 2018 - Universal Word Segmentation Implementation and In.pdf:application/pdf}
}

@article{godard_unsupervised_2018-1,
	title = {Unsupervised {Word} {Segmentation} from {Speech} with {Attention}},
	url = {http://arxiv.org/abs/1806.06734v1},
	journal = {arxiv:1806.06734},
	author = {Godard, Pierre and Zanon-Boito, Marcely and Ondel, Lucas and Berard, Alexandre and Yvon, François and Villavicencio, Aline and Besacier, Laurent},
	year = {2018},
	note = {147},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/CICSYAQQ/Godard et al. - 2018 - Unsupervised Word Segmentation from Speech with At.pdf:application/pdf}
}

@article{shao_cross-lingual_2017-1,
	title = {Cross-lingual {Word} {Segmentation} and {Morpheme} {Segmentation} as {Sequence} {Labelling}},
	url = {http://arxiv.org/abs/1709.03756v1},
	journal = {arxiv:1709.03756},
	author = {Shao, Yan},
	year = {2017},
	note = {106},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/9LDU4SXF/Shao - 2017 - Cross-lingual Word Segmentation and Morpheme Segme.pdf:application/pdf}
}

@article{williams_is_2017-1,
	title = {Is space a word, too?},
	url = {http://arxiv.org/abs/1710.07729v1},
	journal = {arxiv:1710.07729},
	author = {Williams, Jake Ryland and Santia, Giovanni C.},
	year = {2017},
	note = {117},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/ZRG26WUV/Williams e Santia - 2017 - Is space a word, too.pdf:application/pdf}
}

@inproceedings{gao_modeling_2017-2,
	title = {Modeling on evaluation object extraction in e-commerce corpus based on semantic feature},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011290552&doi=10.1109%2fICMIC.2016.7804151&partnerID=40&md5=65db6606157eeccd05cb6708dd9e10d9},
	doi = {10/gf8qdt},
	abstract = {As a newly shopping tool, electronic commerce has been drawing more and more attention of researchers. According to the characteristics of comments diversity, it is necessary to extract evaluation object which is an important component of sentiment information. This paper explores Conditional Random Field (CRF) to do evaluation objects extraction. After observing generally used features in sentiment extraction, this paper conclude all the features into four categories, i.e. word Segmentation, Part-of-speech Tagging (POS), Dependency Parsing, Semantic Dependency Parsing. What's more, focusing on the introduction of new feature semantic dependency is a very vital item in our research. In the experiment, we examine the various features and combinations in the extraction task performance, and make a detailed comparative study. The experimental results confirm that adding the feature of semantic dependency has better performance in terms of the evaluation object extraction. © 2016 University of MEDEA, Algeria.},
	booktitle = {Proceedings of 2016 8th {International} {Conference} on {Modelling}, {Identification} and {Control}, {ICMIC} 2016},
	author = {Gao, K. and Zhang, S.-S. and Su, S. and Li, M.},
	year = {2017},
	note = {123},
	pages = {434--438},
	annote = {cited By 0},
	file = {Gao et al. - 2017 - Modeling on evaluation object extraction in e-comm.pdf:/home/akira/Zotero/storage/TWYI7WVT/Gao et al. - 2017 - Modeling on evaluation object extraction in e-comm.pdf:application/pdf}
}

@article{song_named_2017-2,
	title = {Named entity recognition based on conditional random fields},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029009712&doi=10.1007%2fs10586-017-1146-3&partnerID=40&md5=8cf127b4e560b9faaf5042dc71ed7239},
	doi = {10/gf8qdr},
	abstract = {Named entity recognition (NER) is one of the fundamental problems in many natural language processing applications and the study on NER has great significance. Combining words segmentation and parts of speech analysis, the paper proposes a new NER method based on conditional random fields considering the graininess of candidate entities. The recognition granularity can be divided into two levels: word-based and character-based. We use segmented text to extract characteristics according to the characteristic templates which had been trained in the training phase, and then calculate (Formula presented.) to get the best result from the input sequence. The paper valuates the algorithm for different graininess on large-scale corpus experimentally, and the results show that this method has high research value and feasibility. © 2017 Springer Science+Business Media, LLC},
	journal = {Cluster Computing},
	author = {Song, S. and Zhang, N. and Huang, H.},
	year = {2017},
	note = {126},
	pages = {1--12},
	annote = {cited By 3; Article in Press},
	file = {Song et al. - 2017 - Named entity recognition based on conditional rand.pdf:/home/akira/Zotero/storage/P4Y8HTXI/Song et al. - 2017 - Named entity recognition based on conditional rand.pdf:application/pdf}
}

@article{yang_neural_2017-1,
	title = {Neural {Word} {Segmentation} with {Rich} {Pretraining}},
	url = {http://arxiv.org/abs/1704.08960v1},
	journal = {arxiv:1704.08960},
	author = {Yang, Jie and Zhang, Yue and Dong, Fei},
	year = {2017},
	note = {128},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/L4N2EHEJ/Yang et al. - 2017 - Neural Word Segmentation with Rich Pretraining.pdf:application/pdf}
}

@article{chen_incremental_2016-1,
	title = {Incremental {Learning} for {Fully} {Unsupervised} {Word} {Segmentation} {Using} {Penalized} {Likelihood} and {Model} {Selection}},
	url = {http://arxiv.org/abs/1607.05822v2},
	journal = {arxiv:1607.05822},
	author = {Chen, Ruey-Cheng},
	year = {2016},
	note = {114},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/JFMPUAYM/Chen - 2016 - Incremental Learning for Fully Unsupervised Word S.pdf:application/pdf}
}

@inproceedings{matsumoto_sensibility_2016-2,
	title = {Sensibility estimation method for youth slang by using sensibility co-occurrence feature vector obtained from microblog},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963939887&doi=10.1109%2fCompComm.2015.7387618&partnerID=40&md5=4a1d165f66f914a0b90d4bd3b4765816},
	doi = {10/gf8qdm},
	abstract = {Social networking sites such as Twitter provide more opportunities to express what people think or intend in short text. In short text, abbreviations such as "ASAP" or "joinus" and emoticons are often used. Because these expressions are not registered into the existing dictionaries, these are analyzed as unknown expressions. That can be a bottleneck for improving accuracy of reputation analysis in text mining. To use context for unknown word clustering is a major method, however, it usually requires word segmentation process and it has weakness for split errors of unknown expressions such as youth slang. In this paper, we proposed a method to obtain the appropriate context even though unknown expressions cause split errors and estimate sensibility expressed in the text. Because the dimensions of the obtained context vector were enormous, we also proposed a method to create a feature vector based on the co-occurrence of the sensibility words as simple expression with low dimension. As an evaluation experiment, the proposed method showed certain accuracy even with the small training data. © 2015 IEEE.},
	booktitle = {Proceedings of 2015 {IEEE} {International} {Conference} on {Computer} and {Communications}, {ICCC} 2015},
	author = {Matsumoto, K. and Yoshida, M. and Kita, K.},
	year = {2016},
	note = {134},
	pages = {473--478},
	annote = {cited By 0},
	file = {Matsumoto et al. - 2016 - Sensibility estimation method for youth slang by u.pdf:/home/akira/Zotero/storage/D5FACJLN/Matsumoto et al. - 2016 - Sensibility estimation method for youth slang by u.pdf:application/pdf}
}

@article{xia_word_2016-1,
	title = {Word {Segmentation} on {Micro}-blog {Texts} with {External} {Lexicon} and {Heterogeneous} {Data}},
	url = {http://arxiv.org/abs/1608.01448v2},
	journal = {arxiv:1608.01448},
	author = {Xia, Qingrong and Li, Zhenghua and Chao, Jiayuan and Zhang, Min},
	year = {2016},
	note = {152},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/MLA5NMUT/Xia et al. - 2016 - Word Segmentation on Micro-blog Texts with Externa.pdf:application/pdf}
}

@inproceedings{qian_transition-based_2015-2,
	title = {A transition-based model for joint segmentation, {POS}-tagging and normalization},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959905712&partnerID=40&md5=00d6ac7fdc06bd2b83d3dfb4ec314f4a},
	doi = {10/gf8qd7},
	abstract = {We propose a transition-based model for joint word segmentation, POS tagging and text normalization. Different from previous methods, the model can be trained on standard text corpora, overcoming the lack of annotated microblog corpora. To evaluate our model, we develop an annotated corpus based on microblogs. Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02\%, compared to the traditional approach. © 2015 Association for Computational Linguistics.},
	booktitle = {Conference {Proceedings} - {EMNLP} 2015: {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Qian, T. and Zhang, Y. and Zhang, M. and Ren, Y. and Ji, D.},
	year = {2015},
	note = {98},
	pages = {1837--1846},
	annote = {cited By 11},
	file = {Qian et al. - 2015 - A transition-based model for joint segmentation, P.pdf:/home/akira/Zotero/storage/5AFV9JGI/Qian et al. - 2015 - A transition-based model for joint segmentation, P.pdf:application/pdf}
}

@article{wang_improved_2015-2,
	title = {An improved unsupervised approach to word segmentation},
	volume = {12},
	doi = {10/gf8qd5},
	abstract = {ESA is an unsupervised approach to word segmentation previously proposed by Wang, which is an iterative process consisting of three phases: Evaluation, Selection and Adjustment. In this article, we propose ExESA, the extension of ESA. In ExESA, the original approach is extended to a 2-pass process and the ratio of different word lengths is introduced as the third type of information combined with cohesion and separation. A maximum strategy is adopted to determine the best segmentation of a character sequence in the phrase of Selection. Besides, in Adjustment, ExESA re-evaluates separation information and individual information to overcome the overestimation frequencies. Additionally, a smoothing algorithm is applied to alleviate sparseness. The experiment results show that ExESA can further improve the performance and is time-saving by properly utilizing more information from un-annotated corpora. Moreover, the parameters of ExESA can be predicted by a set of empirical formulae or combined with the minimum description length principle.},
	number = {7},
	journal = {China Communications},
	author = {Wang, H. and Han, X. and Liu, L. and Song, W. and Yuan, M.},
	month = jul,
	year = {2015},
	note = {100},
	keywords = {text analysis, natural language processing, Uncertainty, Accuracy, word segmentation, unsupervised learning, iterative methods, Entropy, Smoothing methods, character sequence, character sequence segmentation, cohesion, ExESA, Frequency measurement, improved unsupervised approach, iterative process, Length measurement, maximum strategy, minimum description length principle, overestimation frequency, Prediction algorithms, separation information, smoothing algorithm, smoothing methods, word length},
	pages = {82--95},
	file = {Wang et al. - 2015 - An improved unsupervised approach to word segmenta.pdf:/home/akira/Zotero/storage/YMJEGQIS/Wang et al. - 2015 - An improved unsupervised approach to word segmenta.pdf:application/pdf}
}

@article{wang_breaking_2015-1,
	title = {Breaking {Bad}: {Detecting} malicious domains using word segmentation},
	url = {http://arxiv.org/abs/1506.04111v1},
	journal = {arxiv:1506.04111},
	author = {Wang, Wei and Shirley, Kenneth},
	year = {2015},
	note = {104},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/NC3L4SL3/Wang e Shirley - 2015 - Breaking Bad Detecting malicious domains using wo.pdf:application/pdf}
}

@inproceedings{takahashi_keyboard_2015-2,
	title = {Keyboard logs as natural annotations for word segmentation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959884109&partnerID=40&md5=b951aa277fd249c48fa0691e5b63d495},
	doi = {10/gf8qd6},
	abstract = {In this paper we propose a framework to improve word segmentation accuracy using input method logs. An input method is software used to type sentences in languages which have far more characters than the number of keys on a keyboard. The main contributions of this paper are: 1) an input method server that proposes word candidates which are not included in the vocabulary, 2) a publicly usable input method that logs user behavior (like typing and selection of word candidates), and 3) a method for improving word segmentation by using these logs. We conducted word segmentation experiments on tweets from Twitter, and showed that our method improves accuracy in this domain. Our method itself is domain-independent and only needs logs from the target domain. © 2015 Association for Computational Linguistics.},
	booktitle = {Conference {Proceedings} - {EMNLP} 2015: {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Takahashi, F. and Mori, S.},
	year = {2015},
	note = {118},
	pages = {1186--1196},
	annote = {cited By 2},
	file = {Takahashi and Mori - 2015 - Keyboard logs as natural annotations for word segm.pdf:/home/akira/Zotero/storage/I7RWW6WT/Takahashi and Mori - 2015 - Keyboard logs as natural annotations for word segm.pdf:application/pdf}
}

@article{sennrich_neural_2015-1,
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {http://arxiv.org/abs/1508.07909v5},
	journal = {arxiv:1508.07909},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	year = {2015},
	note = {127},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/IHCFSY4E/Sennrich et al. - 2015 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf}
}

@article{nivre_towards_2015-3,
	title = {Towards a universal grammar for natural language processing},
	volume = {9041},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942673144&doi=10.1007%2f978-3-319-18111-0_1&partnerID=40&md5=0c36bdb69403326297326f303bf15cc2},
	doi = {10/f3mzxj},
	abstract = {Universal Dependencies is a recent initiative to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. In this paper, I outline the motivation behind the initiative and explain how the basic design principles follow from these requirements. I then discuss the different components of the annotation standard, including principles for word segmentation, morphological annotation, and syntactic annotation. I conclude with some thoughts on the challenges that lie ahead. © Springer International Publishing Switzerland 2015.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Nivre, J.},
	year = {2015},
	note = {142},
	pages = {3--16},
	annote = {cited By 26},
	file = {Nivre - 2015 - Towards a universal grammar for natural language p.pdf:/home/akira/Zotero/storage/DUAJFYVL/Nivre - 2015 - Towards a universal grammar for natural language p.pdf:application/pdf}
}

@article{liang_detecting_2014-1,
	title = {Detecting "protein words" through unsupervised word segmentation},
	url = {http://arxiv.org/abs/1404.6866v6},
	journal = {arxiv:1404.6866},
	author = {Liang, Wang and KaiYong, Zhao},
	year = {2014},
	note = {107},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/SQY5UMD9/Liang e KaiYong - 2014 - Detecting protein words through unsupervised wor.pdf:application/pdf}
}

@inproceedings{zhu_exploration_2014-2,
	title = {Exploration and development of text knowledge extraction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900429048&partnerID=40&md5=2427896a84c7d70a080a58e189d138f1},
	abstract = {Text knowledge extraction technology has been applied in many fields, but few practices in the field of petroleum exploration and development. In this paper,we comprehensively utilize a statistical and natural language understanding technology to extract knowledge from the articles in the field of petroleum exploration and development. First of all, we get the key words and the core sentences containing article process model. Then after the word segmentation and phrase recognition, we use semantic templates to match and extract semantic information from the key sentences. The experimental results show that this method achieves 70\% accuracy rate in keywords spotting and process model extraction. © 2014 Taylor \& Francis Group, London.},
	booktitle = {Information {Technology} and {Computer} {Application} {Engineering} - {Proceedings} of the 2013 {International} {Conference} on {Information} {Technology} and {Computer} {Application} {Engineering}, {ITCAE} 2013},
	author = {Zhu, L. and Li, H. and Wang, S. and Li, C.},
	year = {2014},
	note = {109},
	keywords = {⛔ No DOI found},
	pages = {79--83},
	annote = {cited By 0}
}

@inproceedings{clouet_splitting_2014-1,
	title = {Splitting of compound terms in non-prototypical compounding languages},
	doi = {10/gf8qdv},
	author = {Clouet, Elizaveta Loginova and Daille, Béatrice},
	year = {2014},
	note = {136},
	file = {Clouet and Daille - 2014 - Splitting of compound terms in non-prototypical co.pdf:/home/akira/Zotero/storage/6N3S96V2/Clouet and Daille - 2014 - Splitting of compound terms in non-prototypical co.pdf:application/pdf}
}

@inproceedings{pate_syllable_2014-2,
	title = {Syllable weight encodes mostly the same information for english word segmentation as dictionary stress},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961296056&partnerID=40&md5=cf2aa54e54fbb32050bfd4d0c6349bd0},
	doi = {10/gf8qdw},
	abstract = {Stress is a useful cue for English word segmentation. A wide range of computational models have found that stress cues enable a 2-10\% improvement in segmentation accuracy, depending on the kind of model, by using input that has been annotated with stress using a pronouncing dictionary. However, stress is neither invariably produced nor unambiguously identifiable in real speech. Heavy syllables, i.e. those with long vowels or syllable codas, attract stress in English. We devise Adaptor Grammar word segmentation models that exploit either stress, or syllable weight, or both, and evaluate the utility of syllable weight as a cue to word boundaries. Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does. © 2014 Association for Computational Linguistics.},
	booktitle = {{EMNLP} 2014 - 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {Proceedings} of the {Conference}},
	author = {Pate, J.K. and Johnson, M.},
	year = {2014},
	note = {138},
	pages = {844--853},
	annote = {cited By 0},
	file = {Pate and Johnson - 2014 - Syllable weight encodes mostly the same informatio.pdf:/home/akira/Zotero/storage/HJ6EXAAJ/Pate and Johnson - 2014 - Syllable weight encodes mostly the same informatio.pdf:application/pdf}
}

@article{roshani_unsupervised_2014-1,
	title = {Unsupervised segmentation of sequences using harmony search and hierarchical clustering techniques},
	author = {Roshani, Asra},
	year = {2014},
	note = {146},
	keywords = {⛔ No DOI found},
	file = {Roshani - 2014 - Unsupervised segmentation of sequences using harmo.pdf:/home/akira/Zotero/storage/Y65UNZMF/Roshani - 2014 - Unsupervised segmentation of sequences using harmo.pdf:application/pdf}
}

@article{zeng_classification-based_2013-2,
	title = {A classification-based approach for implicit feature identification},
	volume = {8202 LNAI},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893031086&doi=10.1007%2f978-3-642-41491-6_18&partnerID=40&md5=77380a4f99a8805341450d35c070b926},
	doi = {10/gf8qd4},
	abstract = {In recent years, sentiment analysis and opinion mining has grown to be one of the most active research areas. Most of the existing researches on feature-level opinion mining are dedicated to extract explicitly appeared features and opinion words. However, among the numerous kinds of reviews on the web, there are a significant number of reviews that contain only opinion words which imply some product features. The identification of such implicit features is still one of the most challenge tasks in opinion mining. In this paper, we propose a classification-based approach to deal with the task of implicit feature identification. Firstly, by exploiting the word segmentation, part-of-speech(POS) tagging and dependency parsing, a rule based method to extract the explicit feature-opinion pairs is presented. Secondly, the feature-opinion pairs for each opinion word are clustered and the training documents for each clustered feature-opinion pair are then constructed. Finally, the identification of implicit features is formulated into a classification-based feature selection. Experiments demonstrate that our approach outperforms the existing methods significantly. © Springer-Verlag 2013.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Zeng, L. and Li, F.},
	year = {2013},
	note = {90},
	pages = {190--202},
	annote = {cited By 16},
	file = {Zeng and Li - 2013 - A classification-based approach for implicit featu.pdf:/home/akira/Zotero/storage/GET7UVZ5/Zeng and Li - 2013 - A classification-based approach for implicit featu.pdf:application/pdf}
}

@inproceedings{elsner_joint_2013-2,
	title = {A joint learning model of word segmentation, lexical acquisition, and phonetic variability},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926303216&partnerID=40&md5=1bb60c0df570ef1356d6b59e11aae487},
	abstract = {We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We define the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed first, our model is qualitatively more similar to human learners. On data with variable pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence. © 2013 Association for Computational Linguistics.},
	booktitle = {{EMNLP} 2013 - 2013 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {Proceedings} of the {Conference}},
	author = {Elsner, M. and Goldwater, S. and Feldman, N.H. and Wood, F.},
	year = {2013},
	note = {92},
	keywords = {⛔ No DOI found},
	pages = {42--54},
	annote = {cited By 19},
	file = {Elsner et al. - 2013 - A joint learning model of word segmentation, lexic.pdf:/home/akira/Zotero/storage/UN3PGX45/Elsner et al. - 2013 - A joint learning model of word segmentation, lexic.pdf:application/pdf}
}

@article{namer_rule-based_2013-3,
	title = {A {Rule}-{Based} {Morphosemantic} {Analyzer} for {French} for a {Fine}-{Grained} {Semantic} {Annotation} of {Texts}},
	volume = {380 CCIS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904608636&doi=10.1007%2f978-3-642-40486-3_6&partnerID=40&md5=39c06edc8d7ffbce4c228d77c841004d},
	doi = {10/gf8qdx},
	abstract = {We describe DériF, a rule-based morphosemantic analyzer developed for French. Unlike existing word segmentation tools, DériF provides derived and compound words with various sorts of semantic information: (1) a definition, computed from both the base meaning and the specificities of the morphological rule; (2) lexical-semantic features, inferred from general linguistic properties of derivation rules; (3) lexical relations (synonymy, (co-)hyponymy) with other, morphologically unrelated, words belonging to the same analyzed corpus. © Springer-Verlag Berlin Heidelberg 2013.},
	journal = {Communications in Computer and Information Science},
	author = {Namer, F.},
	year = {2013},
	note = {95},
	pages = {92--114},
	annote = {cited By 0},
	file = {Namer - 2013 - A Rule-Based Morphosemantic Analyzer for French fo.pdf:/home/akira/Zotero/storage/6D7L84UJ/Namer - 2013 - A Rule-Based Morphosemantic Analyzer for French fo.pdf:application/pdf}
}

@inproceedings{sun_using_2013-2,
	title = {Using language rules to improve the performance of word segmentation},
	volume = {03},
	doi = {10/gf8qdk},
	abstract = {Due to the disadvantage of the word segmentation tool in the aspect of the accuracy of word segmentation and speech tagging, it has harmful effects to the further research work. So this paper proposes a method based on the results of word segmentation to utilize language rules, which has been defined and described in Event Ontology, to verify and correct them. Experimental results show that compared with the method of only using word segmentation tool, the method of using language rules has a better performance.},
	booktitle = {2013 6th {International} {Congress} on {Image} and {Signal} {Processing} ({CISP})},
	author = {Sun, R. and Zhou, W. and Liu, Z.},
	year = {2013},
	note = {149},
	keywords = {Ontologies, text analysis, natural language processing, Dictionaries, Word Segmentation, ontologies (artificial intelligence), Accuracy, Computers, Event, event ontology, Event Ontology, Hidden Markov models, Language Rule, language rules, Performance, speech tagging, Sun, Tagging, word segmentation tool},
	pages = {1665--1669},
	file = {Sun et al. - 2013 - Using language rules to improve the performance of.pdf:/home/akira/Zotero/storage/CY645BDE/Sun et al. - 2013 - Using language rules to improve the performance of.pdf:application/pdf}
}

@inproceedings{dai_hybrid_2012-2,
	title = {A hybrid method to segment words},
	doi = {10/gf8qd3},
	abstract = {Word segmentation is the foundations of machine translation, text classification and information searching. A method is proposed which combines word segmentation based on dictionary with reverse maximum matching and word segmentation based on statistic with suffix array. The input texts are segmented using the reserve maximum matching method based on dictionary, and a two-way suffix arrays are constructed, longest common prefix are computed, candidate words are filtered out by setting the threshold, the candidate words are filtered using mutual information in order to the true words. The texts that are ambiguity are filtered using information entropy. It is showed that the accuracy of word segmentation may achieve above 97\% in the experiment.},
	booktitle = {2012 {International} {Conference} on {Audio}, {Language} and {Image} {Processing}},
	author = {Dai, Y. and Ren, X.},
	month = jul,
	year = {2012},
	note = {91},
	keywords = {text analysis, natural language processing, Dictionaries, Arrays, pattern classification, Accuracy, text classification, word segmentation, language translation, machine translation, Information filters, common prefix, hybrid method, information entropy, information searching, input texts, Matched filters, Sorting, suffix array},
	pages = {1131--1134},
	file = {Dai and Ren - 2012 - A hybrid method to segment words.pdf:/home/akira/Zotero/storage/KATP7DN3/Dai and Ren - 2012 - A hybrid method to segment words.pdf:application/pdf}
}

@article{chang_multi-function_2012-2,
	title = {A multi-function {MSN} robot for campus information seeking},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862902958&doi=10.1166%2fasl.2012.2643&partnerID=40&md5=4582590a11bfce38b51c8e993aca82e6},
	doi = {10/gf8qd2},
	abstract = {An MSN robot is an intelligent system that can chat with users to accomplish a specific task. In this paper, we present a framework to build an MSN robot that can chat with students to provide campus information. The MSN robot has two major functions; that is it can provide two kinds of information: curriculum information and living information. The curriculum information includes a road map of all courses so that students can register courses according to their interests and future plans. The living information includes the information of restaurants, houses, transportation, and scenic spots around the campus. The MSN robot consists of three components: word segmentation and POS tagging, natural language understanding, and response text generation. The chat between the robot and students starts with an input of free-text question about campus information. Once the question is received, the natural language understanding component determines the meaning of the input question using classification methods. Finally, the output component generates a text response using a set of predefined templates. The MSN robot is implemented using the Dot MSN, an open source package that can connect the MSN Messenger service. The experimental results show that among several classification methods the support vector machine (SVM) achieves the best performance in classifying users' input questions into curriculum information or living information. © 2012 American Scientific Publishers.},
	journal = {Advanced Science Letters},
	author = {Chang, S.-F. and Yu, L.-C.},
	year = {2012},
	note = {93},
	pages = {828--832},
	annote = {cited By 0}
}

@inproceedings{araki_online_2012-2,
	title = {Online learning of concepts and words using multimodal {LDA} and hierarchical {Pitman}-{Yor} {Language} {Model}},
	doi = {10/gf8qdq},
	abstract = {In this paper, we propose an online algorithm for multimodal categorization based on the autonomously acquired multimodal information and partial words given by human users. For multimodal concept formation, multimodal latent Dirichlet allocation (MLDA) using Gibbs sampling is extended to an online version. We introduce a particle filter, which significantly improve the performance of the online MLDA, to keep tracking good models among various models with different parameters. We also introduce an unsupervised word segmentation method based on hierarchical Pitman-Yor Language Model (HPYLM). Since the HPYLM requires no predefined lexicon, we can make the robot system that learns concepts and words in completely unsupervised manner. The proposed algorithms are implemented on a real robot and tested using real everyday objects to show the validity of the proposed system.},
	booktitle = {2012 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Araki, T. and Nakamura, T. and Nagai, T. and Nagasaka, S. and Taniguchi, T. and Iwahashi, N.},
	year = {2012},
	note = {129},
	keywords = {Data models, Predictive models, natural language processing, human-robot interaction, Humans, unsupervised word segmentation, Robot sensing systems, educational robots, Gibbs sampling, Haptic interfaces, hierarchical Pitman-Yor language model, learning systems, multimodal categorization, multimodal concept formation, multimodal information, multimodal latent Dirichlet allocation, multimodal LDA, online algorithm, online learning, partial words, particle filter, predefined lexicon, robot system, Vectors},
	pages = {1623--1630},
	file = {Araki et al. - 2012 - Online learning of concepts and words using multim.pdf:/home/akira/Zotero/storage/ELCUQP2K/Araki et al. - 2012 - Online learning of concepts and words using multim.pdf:application/pdf}
}

@inproceedings{wu_sembler_2012-2,
	title = {Sembler: {Ensembling} crowd sequential labeling for improved quality},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868274778&partnerID=40&md5=056a981811f122859f050ce2dfaa4ab1},
	abstract = {Many natural language processing tasks, such as named entity recognition (NER), part of speech (POS) tagging, word segmentation, and etc., can be formulated as sequential data labeling problems. Building a sound labeler requires very large number of correctly labeled training examples, which may not always be possible. On the other hand, crowdsourcing provides an inexpensive yet efficient alternative to collect manual sequential labeling from non-experts. However the quality of crowd labeling cannot be guaranteed, and three kinds of errors are typical: (1) incorrect annotations due to lack of expertise (e.g., labeling gene names from plain text requires corresponding domain knowledge); (2) ignored or omitted annotations due to carelessness or low confidence; (3) noisy annotations due to cheating or vandalism. To correct these mistakes, we present Sembler, a statistical model for ensembling crowd sequential labelings. Sembler considers three types of statistical information: (1) the majority agreement that proves the correctness of an annotation; (2) correct annotation that improves the credibility of the corresponding annotator; (3) correct annotation that enhances the correctness of other annotations which share similar linguistic or contextual features. We evaluate the proposed model on a real Twitter and a synthetical biological data set, and find that Sembler is particularly accurate when more than half of annotators make mistakes. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.},
	booktitle = {Proceedings of the {National} {Conference} on {Artificial} {Intelligence}},
	author = {Wu, X. and Fan, W. and Yu, Y.},
	year = {2012},
	note = {133},
	keywords = {⛔ No DOI found},
	pages = {1713--1719},
	annote = {cited By 7},
	file = {Wu et al. - 2012 - Sembler Ensembling crowd sequential labeling for .pdf:/home/akira/Zotero/storage/A2B775IV/Wu et al. - 2012 - Sembler Ensembling crowd sequential labeling for .pdf:application/pdf}
}

@inproceedings{stahlberg_word_2012-2,
	title = {Word segmentation through cross-lingual word-to-phoneme alignment},
	doi = {10/gf8qdj},
	abstract = {We present our new alignment model Model 3P for cross-lingual word-to-phoneme alignment, and show that unsupervised learning of word segmentation is more accurate when information of another language is used. Word segmentation with cross-lingual information is highly relevant to bootstrap pronunciation dictionaries from audio data for Automatic Speech Recognition, bypass the written form in Speech-to-Speech Translation or build the vocabulary of an unseen language, particularly in the context of under-resourced languages. Using Model 3P for the alignment between English words and Spanish phonemes outperforms a state-of-the-art monolingual word segmentation approach [1] on the BTEC corpus [2] by up to 42\% absolute in F-Score on the phoneme level and a GIZA++ alignment based on IBM Model 3 by up to 17\%.},
	booktitle = {2012 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	author = {Stahlberg, F. and Schlippe, T. and Vogel, S. and Schultz, T.},
	year = {2012},
	note = {153},
	keywords = {natural language processing, Dictionaries, word segmentation, Vocabulary, unsupervised learning, Hidden Markov models, Training data, Grammar, speech recognition, automatic speech recognition, Vectors, Error analysis, alignment model, bootstrap pronunciation dictionaries, cross lingual information, cross lingual word-to-phoneme alignment, English words, Spanish phonemes, speech-to-speech translation, under-resourced language},
	pages = {85--90},
	file = {Stahlberg et al. - 2012 - Word segmentation through cross-lingual word-to-ph.pdf:/home/akira/Zotero/storage/3QJFPCZD/Stahlberg et al. - 2012 - Word segmentation through cross-lingual word-to-ph.pdf:application/pdf}
}

@article{aken_statistical_2011-1,
	title = {A statistical learning algorithm for word segmentation},
	url = {http://arxiv.org/abs/1105.6162v2},
	journal = {arxiv:1105.6162},
	author = {Aken, Jerry R. Van},
	year = {2011},
	note = {96},
	keywords = {⛔ No DOI found},
	file = {Full Text:/home/akira/Zotero/storage/AR72IWPS/Aken - 2011 - A statistical learning algorithm for word segmenta.pdf:application/pdf}
}

@inproceedings{hewlett_fully_2011-2,
	address = {Stroudsburg, PA, USA},
	series = {{HLT} '11},
	title = {Fully {Unsupervised} {Word} {Segmentation} with {BVE} and {MDL}},
	isbn = {978-1-932432-88-6},
	url = {http://dl.acm.org/citation.cfm?id=2002736.2002843},
	booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Short} {Papers} - {Volume} 2},
	publisher = {Association for Computational Linguistics},
	author = {Hewlett, Daniel and Cohen, Paul},
	year = {2011},
	note = {111 event-place: Portland, Oregon},
	pages = {540--545},
	file = {Full Text:/home/akira/Zotero/storage/X5H6EVSX/Hewlett e Cohen - 2011 - Fully Unsupervised Word Segmentation with BVE and .pdf:application/pdf}
}

@article{paul_integration_2011-2,
	title = {Integration of multiple bilingually-trained segmentation schemes into statistical machine translation},
	volume = {E94-D},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952129828&doi=10.1587%2ftransinf.E94.D.690&partnerID=40&md5=f02f8345f7fbdde304c3c697b3f3d408},
	doi = {10/bsbh8n},
	abstract = {This paper proposes an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches. The method can be applied to any language pair in which the source language is unsegmented and the target language segmentation is known. In the first step, an iterative bootstrap method is applied to learn multiple segmentation schemes that are consistent with the phrasal segmentations of an SMT system trained on the resegmented bitext. In the second step, multiple segmentation schemes are integrated into a single SMT system by characterizing the source language side and merging identical translation pairs of differently segmented SMT models. Experimental results translating five Asian languages into English revealed that the proposed method of integrating multiple segmentation schemes outperforms SMT models trained on any of the learned word segmentations and performs comparably to available monolingually built segmentation tools. © 2011 The Institute of Electronics, Information and Communication Engineers.},
	number = {3},
	journal = {IEICE Transactions on Information and Systems},
	author = {Paul, M. and Finch, A. and Sumita, E.},
	year = {2011},
	note = {116},
	pages = {690--697},
	annote = {cited By 3},
	file = {Full Text:/home/akira/Zotero/storage/YTEN8TNS/Paul et al. - 2011 - Integration of multiple bilingually-trained segmen.pdf:application/pdf}
}

@inproceedings{hewlett_word_2011-2,
	address = {Stroudsburg, PA, USA},
	series = {{CoNLL} '11},
	title = {Word {Segmentation} {As} {General} {Chunking}},
	isbn = {978-1-932432-92-3},
	url = {http://dl.acm.org/citation.cfm?id=2018936.2018941},
	booktitle = {Proceedings of the {Fifteenth} {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Hewlett, Daniel and Cohen, Paul},
	year = {2011},
	note = {151 event-place: Portland, Oregon},
	pages = {39--47},
	file = {Full Text:/home/akira/Zotero/storage/NXKP7UHW/Hewlett e Cohen - 2011 - Word Segmentation As General Chunking.pdf:application/pdf}
}

@inproceedings{weber_base-form_2010-2,
	title = {A base-form lexicon of content words for correct word segmentation and syntactic-semantic annotation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894110931&partnerID=40&md5=f6254b4cf0ee2665f59a6be8a53db773},
	abstract = {One issue in natural language processing is the interaction between a rule-based computational morphology and a syntactic-semantic analysis system. This is because derivational and compound word forms raise the question of how to deal with ambiguities caused by the rule-based analyser, and how to add additional information like valency to a derivational or compound word form if its valency frames differ from those of its root word. In this paper we propose a lexicon design addressing both of these issues. We evaluate our design in the context of a large-scale morphological analysis system for German in which the lexicon serves as an interface between morphology and syntax. In doing so, we aim at enriching the wellformed analysis results with additional information so that an adequate syntactic-semantic analysis can be ensured.},
	booktitle = {Semantic {Approaches} in {Natural} {Language} {Processing} - {Proceedings} of the {Conference} on {Natural} {Language} {Processing} 2010, {KONVENS}},
	author = {Weber, C. and Handl, J.},
	year = {2010},
	note = {89},
	keywords = {⛔ No DOI found},
	annote = {cited By 0},
	annote = {page 175
 },
	file = {Weber and Handl - 2010 - A base-form lexicon of content words for correct w.pdf:/home/akira/Zotero/storage/RXIRA7PG/Weber and Handl - 2010 - A base-form lexicon of content words for correct w.pdf:application/pdf}
}

@inproceedings{paul_integration_2010-1,
	address = {Stroudsburg, PA, USA},
	series = {{WMT} '10},
	title = {Integration of {Multiple} {Bilingually}-learned {Segmentation} {Schemes} into {Statistical} {Machine} {Translation}},
	isbn = {978-1-932432-71-8},
	url = {http://dl.acm.org/citation.cfm?id=1868850.1868910},
	booktitle = {Proceedings of the {Joint} {Fifth} {Workshop} on {Statistical} {Machine} {Translation} and {MetricsMATR}},
	publisher = {Association for Computational Linguistics},
	author = {Paul, Michael and Finch, Andrew and Sumita, Eiichiro},
	year = {2010},
	note = {115 event-place: Uppsala, Sweden},
	pages = {400--408},
	file = {Full Text:/home/akira/Zotero/storage/9BHTDMQY/Paul et al. - 2010 - Integration of Multiple Bilingually-learned Segmen.pdf:application/pdf}
}

@inproceedings{cho_novel_2009-2,
	address = {Stroudsburg, PA, USA},
	series = {{ACLShort} '09},
	title = {A {Novel} {Word} {Segmentation} {Approach} for {Written} {Languages} with {Word} {Boundary} {Markers}},
	copyright = {94},
	url = {http://dl.acm.org/citation.cfm?id=1667583.1667594},
	doi = {10/bqwx4g},
	booktitle = {Proceedings of the {ACL}-{IJCNLP} 2009 {Conference} {Short} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Han-Cheol and Lee, Do-Gil and Lee, Jung-Tae and Stenetorp, Pontus and Tsujii, Jun'ichi and Rim, Hae-Chang},
	year = {2009},
	note = {event-place: Suntec, Singapore},
	pages = {29--32},
	file = {Cho et al. - 2009 - A Novel Word Segmentation Approach for Written Lan.pdf:/home/akira/Zotero/storage/WXFWXQNS/Cho et al. - 2009 - A Novel Word Segmentation Approach for Written Lan.pdf:application/pdf}
}

@inproceedings{ma_bilingually_2009-1,
	address = {Stroudsburg, PA, USA},
	series = {{EACL} '09},
	title = {Bilingually {Motivated} {Domain}-adapted {Word} {Segmentation} for {Statistical} {Machine} {Translation}},
	url = {http://dl.acm.org/citation.cfm?id=1609067.1609128},
	booktitle = {Proceedings of the 12th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ma, Yanjun and Way, Andy},
	year = {2009},
	note = {103 event-place: Athens, Greece},
	keywords = {⛔ No DOI found},
	pages = {549--557},
	file = {Full Text:/home/akira/Zotero/storage/IIIGXMST/Ma e Way - 2009 - Bilingually Motivated Domain-adapted Word Segmenta.pdf:application/pdf}
}

@inproceedings{johnson_improving_2009-1,
	address = {Stroudsburg, PA, USA},
	series = {{NAACL} '09},
	title = {Improving {Nonparameteric} {Bayesian} {Inference}: {Experiments} on {Unsupervised} {Word} {Segmentation} with {Adaptor} {Grammars}},
	isbn = {978-1-932432-41-1},
	url = {http://dl.acm.org/citation.cfm?id=1620754.1620800},
	booktitle = {Proceedings of {Human} {Language} {Technologies}: {The} 2009 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Johnson, Mark and Goldwater, Sharon},
	year = {2009},
	note = {113 event-place: Boulder, Colorado},
	pages = {317--325},
	file = {Full Text:/home/akira/Zotero/storage/9GDKPWN3/Johnson e Goldwater - 2009 - Improving Nonparameteric Bayesian Inference Exper.pdf:application/pdf}
}

@inproceedings{paul_language_2009-1,
	address = {New York, NY, USA},
	series = {{IUCS} '09},
	title = {Language {Independent} {Word} {Segmentation} for {Statistical} {Machine} {Translation}},
	copyright = {119},
	isbn = {978-1-60558-641-0},
	url = {http://doi.acm.org/10.1145/1667780.1667788},
	doi = {10/fqpwcc},
	booktitle = {Proceedings of the 3rd {International} {Universal} {Communication} {Symposium}},
	publisher = {ACM},
	author = {Paul, Michael and Finch, Andrew and Sumita, Eiichiro},
	year = {2009},
	note = {event-place: Tokyo, Japan},
	pages = {36--40},
	file = {Paul et al. - 2009 - Language Independent Word Segmentation for Statist.pdf:/home/akira/Zotero/storage/9PXBNIG3/Paul et al. - 2009 - Language Independent Word Segmentation for Statist.pdf:application/pdf}
}

@inproceedings{noauthor_session_2009-1,
	address = {Stroudsburg, PA, USA},
	series = {{ACL} '09},
	title = {Session {Details}: {Word} {Segmentation} and {POS} {Tagging}},
	isbn = {978-1-932432-45-9},
	url = {http://dl.acm.org/citation.cfm?id=1687878.3252572},
	booktitle = {Proceedings of the {Joint} {Conference} of the 47th {Annual} {Meeting} of the {ACL} and the 4th {International} {Joint} {Conference} on {Natural} {Language} {Processing} of the {AFNLP}: {Volume} 1 - {Volume} 1},
	publisher = {Association for Computational Linguistics},
	year = {2009},
	note = {135 event-place: Suntec, Singapore},
	pages = {--},
	annote = {Session Chair-Ng, Hwee Tou}
}

@article{tambouratzis_using_2009-2,
	title = {Using an {Ant} {Colony} {Metaheuristic} to {Optimize} {Automatic} {Word} {Segmentation} for {Ancient} {Greek}},
	volume = {13},
	doi = {10/b2q8xs},
	abstract = {Given a text or collection of texts involving unconstrained language, a basic task in a multitude of applications is the identification of stems and endings for each word form, which is termed morphological analysis. In this paper, the use of an ant colony optimization (ACO) metaheuristic is proposed for a linguistic task that involves the automated morphological segmentation of Ancient Greek word forms into stem and ending. The task of morphological analysis is essential for implementing text-processing applications such as semantic analysis and information retrieval. The difficulty of the morphological analysis task differs depending on the language chosen, being hardest in the case of highly-inflectional languages, where each stem may be associated with a large number of different endings. In this paper, focus is placed on the morphological analysis of ancient Greek, which has been shown to be a particularly hard task. To perform this task, a system for the automated morphological processing has been proposed, which implements the morphological analysis of words by coupling an iterative pattern-recognition algorithm with a modest amount of linguistic knowledge, expressed via a set of interactions associated with weights. In an earlier version of the system, these weights were determined by combining the input from specialized scientists with a lengthy manual optimization process. In this paper, the ACO metaheuristic is applied to the task of defining near-optimal system weights using an automated process based on a set of training data. The experiments performed indicate that the segmentation quality achieved by ACO is equivalent to or in several cases substantially higher than that achieved using manually optimized weights.},
	number = {4},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Tambouratzis, G.},
	year = {2009},
	note = {148},
	keywords = {information retrieval, Information retrieval, Data mining, text analysis, optimisation, Performance analysis, natural language processing, Algorithm design and analysis, Information analysis, ancient Greek, Ancient Greek, ant colony metaheuristic, Ant colony optimization, ant colony optimization (ACO) metaheuristic, automated morphological analysis, automated morphological processing, heuristic function, highly-inflectional languages, Iterative algorithms, iterative methods, iterative pattern-recognition algorithm, linguistic knowledge, linguistics, manual optimization process, morphological analysis, near-optimal system, optimize automatic word segmentation, Pattern analysis, pattern recognition, segmentation quality, semantic analysis, text processing, Text processing, text processing application, Training data},
	pages = {742--753},
	file = {Tambouratzis - 2009 - Using an Ant Colony Metaheuristic to Optimize Auto.pdf:/home/akira/Zotero/storage/T9VHSHSX/Tambouratzis - 2009 - Using an Ant Colony Metaheuristic to Optimize Auto.pdf:application/pdf}
}

@inproceedings{gabay_using_2008-2,
	title = {Using wikipedia links to construct word segmentation corpora},
	volume = {WS-08-15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449775446&partnerID=40&md5=ad7bc10a1c3ed07e582931513ab8ae88},
	abstract = {Tagged corpora are essential for evaluating and training natural language processing tools. The cost of constructing large enough manually tagged corpora is high, even when the annotation level is shallow. This article describes a simple method to automatically create a partially tagged corpus, using Wikipedia hyperlinks. The resulting corpus contains information about the correct segmentation of 523,599 non-consecutive words in 363,090 sentences. We used our method to construct a corpus of Modern Hebrew (which we have made available at http://www.cs.bgu.ac.il/-nlpproj). The method can also be applied to other languages where word segmentation is difficult to determine, such as East and South-East Asian languages. Copyright © 2008.},
	booktitle = {{AAAI} {Workshop} - {Technical} {Report}},
	author = {Gabay, D. and Ziv, B.E. and Elhadad, M.},
	year = {2008},
	note = {150},
	keywords = {⛔ No DOI found},
	pages = {61--63},
	annote = {cited By 3},
	file = {Gabay et al. - 2008 - Using wikipedia links to construct word segmentati.pdf:/home/akira/Zotero/storage/JVMZ39DY/Gabay et al. - 2008 - Using wikipedia links to construct word segmentati.pdf:application/pdf}
}

@article{lee_automatic_2007-2,
	title = {Automatic word spacing using probabilistic models based on character n-grams},
	volume = {22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847611998&doi=10.1109%2fMIS.2007.4&partnerID=40&md5=f49f0e501e9b8b12dedfb8fee532c736},
	doi = {10/c6b6z9},
	abstract = {Probabilistic models based on Hidden Markov models (HMM) for automatic word spacing that use characters n-grams, which is a sub-sequence of n characters in a given character sequence, are discussed. Automatic word spacing is a preprocessing techniques used for correcting boundaries between words in a sentence containing spacing errors. These model can be effectively applied to a natural language with a small character set, such as English, using character n-grams that are larger than trigrams. These models, which are language independent and can be effectively used for languages having word spacing, can also be used for word segmentation in the languages without explicit word spacing. These models, by generalizing the HMMs, can consider a broad context and estimate accurate probabilities.},
	number = {1},
	journal = {IEEE Intelligent Systems},
	author = {Lee, D.-G. and Rim, H.-C. and Yook, D.},
	year = {2007},
	note = {101},
	pages = {28--35},
	annote = {cited By 16},
	file = {Lee et al. - 2007 - Automatic word spacing using probabilistic models .pdf:/home/akira/Zotero/storage/SQF3783E/Lee et al. - 2007 - Automatic word spacing using probabilistic models .pdf:application/pdf}
}

@inproceedings{guo_research_2007-3,
	title = {The research on the application of text clustering and natural language understanding in automatic abstracting},
	volume = {4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-44049094329&doi=10.1109%2fFSKD.2007.584&partnerID=40&md5=1786ee4735dd963c58059a2f015c195a},
	doi = {10/cf5vpg},
	abstract = {A method of realization of Automatic Abstracting based on Text Clustering and Natural Language Understanding is brought forward, aimed at overcoming shortages of some current methods. The method makes use of text Clustering and can realize Automatic Abstracting of multi- documents. The algorithm of twice Word Segmentation based on the Title and FirstSentences in Paragraphs is brought forward. Its precision and recall is above 95\% For a specific domain on plastics, an Automatic Abstracting system named TCAAS is implemented. The precision and recall of multidocument's Automatic Abstracting is above 75\% And experiments do prove that it is feasible to use the method to develop a domain Automatic Abstracting System, which is valuable for further study in more depth. © 2007 IEEE.},
	booktitle = {Proceedings - {Fourth} {International} {Conference} on {Fuzzy} {Systems} and {Knowledge} {Discovery}, {FSKD} 2007},
	author = {Guo, Q. and Li, C.},
	year = {2007},
	note = {141},
	pages = {92--96},
	annote = {cited By 4},
	file = {Guo and Li - 2007 - The research on the application of text clustering.pdf:/home/akira/Zotero/storage/YAZAQ8WQ/Guo and Li - 2007 - The research on the application of text clustering.pdf:application/pdf}
}

@article{creutz_unsupervised_2007-1,
	title = {Unsupervised {Models} for {Morpheme} {Segmentation} and {Morphology} {Learning}},
	volume = {4},
	issn = {1550-4875},
	url = {http://doi.acm.org/10.1145/1187415.1187418},
	doi = {10/bx99qh},
	number = {1},
	journal = {ACM Trans. Speech Lang. Process.},
	author = {Creutz, Mathias and Lagus, Krista},
	year = {2007},
	note = {145},
	keywords = {Efficient storage, highly inflecting and compounding languages, language independent methods, maximum a posteriori (MAP) estimation, morpheme lexicon and segmentation, unsupervised learning},
	pages = {3:1--3:34},
	file = {Creutz and Lagus - 2007 - Unsupervised Models for Morpheme Segmentation and .pdf:/home/akira/Zotero/storage/GFXGSMQ2/Creutz and Lagus - 2007 - Unsupervised Models for Morpheme Segmentation and .pdf:application/pdf}
}

@inproceedings{geyken_tagh_2006-2,
	address = {Berlin, Heidelberg},
	title = {{TAGH}: {A} {Complete} {Morphology} for {German} {Based} on {Weighted} {Finite} {State} {Automata}},
	isbn = {978-3-540-35469-7},
	abstract = {TAGH is a system for automatic recognition of German word forms. It is based on a stem lexicon with allomorphs and a concatenative mechanism for inflection and word formation. Weighted FSA and a cost function are used in order to determine the correct segmentation of complex forms: the correct segmentation for a given compound is supposed to be the one with the least cost. TAGH is based on a large stem lexicon of almost 80.000 stems that was compiled within 5 years on the basis of large newspaper corpora and literary texts. The number of analyzable word forms is increased considerably by more than 1000 different rules for derivational and compositional word formation. The recognition rate of TAGH is more than 99\% for modern newspaper text and approximately 98.5\% for literary texts.},
	booktitle = {Finite-{State} {Methods} and {Natural} {Language} {Processing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Geyken, Alexander and Hanneforth, Thomas},
	editor = {Yli-Jyrä, Anssi and Karttunen, Lauri and Karhumäki, Juhani},
	year = {2006},
	note = {139},
	pages = {55--66},
	file = {Geyken and Hanneforth - 2006 - TAGH A Complete Morphology for German Based on We.pdf:/home/akira/Zotero/storage/C5ISUSPX/Geyken and Hanneforth - 2006 - TAGH A Complete Morphology for German Based on We.pdf:application/pdf}
}

@article{kazakov_unsupervised_2001-2,
	title = {Unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming},
	volume = {43},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035312598&doi=10.1023%2fA%3a1007629103294&partnerID=40&md5=eaae5dc95f7c91cc97525afdf2bb2c17},
	doi = {10/fng8qb},
	abstract = {This article presents a combination of unsupervised and supervised learning techniques for the generation of word segmentation rules from a raw list of words. First, a language bias for word segmentation is introduced and a simple genetic algorithm is used in the search for a segmentation that corresponds to the best bias value. In the second phase, the words segmented by the genetic algorithm are used as an input for the first order decision list learner CLOG. The result is a set of first order rules which can be used for segmentation of unseen words. When applied on either the training data or unseen data, these rules produce segmentation which are linguistically meaningful, and to a large degree conforming to the annotation provided.},
	number = {1-2},
	journal = {Machine Learning},
	author = {Kazakov, D. and Manandhar, S.},
	year = {2001},
	note = {144},
	pages = {121--162},
	annote = {cited By 21},
	file = {Full Text:/home/akira/Zotero/storage/WQU2LPGV/Kazakov e Manandhar - 2001 - Unsupervised learning of word segmentation rules w.pdf:application/pdf}
}