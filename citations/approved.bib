% Encoding: UTF-8

@Article{shishkovaannotated,
  author = {Shishkova, Anna and Chernyak, Ekaterina},
  title  = {Annotated Suffix Tree Method for German Compound Splitting},
}

@Article{volkcompounds,
  author = {Volk, Martin and Mascarell, Laura and Fishel, Mark},
  title  = {Compounds, Coreferences and Multiword Translations},
}

@Article{ullmanparaphrasing,
  author = {Ullman, Edvin},
  title  = {Paraphrasing of Swedish Compound Nouns},
}

@Article{Patel2019437,
  author          = {Patel, R.N. and Pimpale, P.B. and Sasikumar, M.},
  title           = {Machine translation in Indian languages: Challenges and resolution},
  journal         = {Journal of Intelligent Systems},
  year            = {2019},
  volume          = {28},
  number          = {3},
  pages           = {437-445},
  note            = {cited By 1},
  abstract        = {English to Indian language machine translation poses the challenge of structural and morphological divergence. This paper describes English to Indian language statistical machine translation using preordering and suffix separation. The preordering uses rules to transfer the structure of the source sentences prior to training and translation. This syntactic restructuring helps statistical machine translation to tackle the structural divergence and hence provides better translation quality. The suffix separation is used to tackle the morphological divergence between English and highly agglutinative Indian languages. We demonstrate that the use of preordering and suffix separation helps in improving the quality of English to Indian language machine translation. © 2019 Walter de Gruyter GmbH, Berlin/Boston.},
  affiliation     = {KBCS Division, Centre for Development of Advanced Computing, Gulmohar Cross Road No. 9, Opp Juhu Shopping Center, Juhu, Mumbai, 400049, India},
  author_keywords = {reordering; Statistical machine translation; suffix and compound splitting; transliteration},
  document_type   = {Article},
  doi             = {10.1515/jisys-2018-0014},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053124049&doi=10.1515%2fjisys-2018-0014&partnerID=40&md5=5b1e11decdb5614448ceda0e259a43a4},
}

@Article{altinok2018demorphy,
  author  = {Altinok, Duygu},
  title   = {DEMorphy, German Language Morphological Analyzer},
  journal = {arXiv preprint arXiv:1803.00902},
  year    = {2018},
}

@Conference{Sugisaki2018141,
  author        = {Sugisaki, K. and Tuggener, D.},
  title         = {German compound splitting using the compound productivity of morphemes},
  year          = {2018},
  pages         = {141-147},
  note          = {cited By 0},
  abstract      = {In this work, we present a novel compound splitting method for German by capturing the compound productivity of morphemes. We use a giga web corpus to create a lexicon and decompose noun compounds by computing the probabilities of compound elements as bound and free morphemes. Furthermore, we provide a uniformed evaluation of several unsupervised approaches and morphological analysers for the task. Our method achieved a high F1 score of 0.92, which was a comparable result to state-of-the-art methods. © KONVENS 2018 - Conference on Natural Language Processing / Die Konferenz zur Verarbeitung Naturlicher Sprache.All right reserved.},
  affiliation   = {German Department, University of Zurich, Schönberggasse 9, Zurich, 8001, Switzerland; School of Engineering, Zurich University of Applied Sciences, Steinberggasse 13, Winterthur, 8400, Switzerland},
  document_type = {Conference Paper},
  journal       = {KONVENS 2018 - Conference on Natural Language Processing / Die Konferenz zur Verarbeitung Naturlicher Sprache},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064207728&partnerID=40&md5=9ad88e6eff661a660d155b41ada1fbb9},
}

@Conference{Li2018175,
  author        = {Li, J. and Du, Q. and Shi, K. and He, Y. and Wang, X. and Xu, J.},
  title         = {Helpful or not? an investigation on the feasibility of identifier splitting via CNN-BiLSTM-CRF},
  year          = {2018},
  volume        = {2018-July},
  pages         = {175-181},
  note          = {cited By 0},
  abstract      = {We recently introduced a new technique to handle source code identifier splitting. The proposed technique, denoted as CNN-BiLSTM-CRF[a neural network composed of a convolutional neural network(CNN), bidirectional long short-Term memory networks(BiLSTM) and conditional random fields(CRFs)] enables us to obtain a model that splits identifiers correctly and effectively. This technique combines the use of a CNN layer with the mature BiLSTM-CRF model. The experimental results indicate that CNN-BiLSTM-CRF delivers outstanding performance on all four of the evaluation oracles. More importantly, we endeavored to provide insight into the practical feasibility of this technique by considering the aspects of generality, data size in demand and construction cost, etc. Finally, we reasoned out that CNN-BiLSTM-CRF should be helpful and improvable for identifier splitting in practical works in terms of the accuracy and feasibility. This was validated by multifaceted experiments. Index Terms-identifier splitting, source code mining, program comprehension, CNN, BiLSTM-CRF, feasibility investigation. © 2018 Universitat zu Koln. All rights reserved.},
  affiliation   = {School of Software Engineering, Tongji University, China; Software Engineering RandD Centre, Jishi Building, Tongji University, China; Smart City Labotary, Jishi Building, Tongji University, China; Shanghai Research and Development Center, Baidu Inc, China},
  document_type = {Conference Paper},
  doi           = {10.18293/SEKE2018-167},
  journal       = {Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056829823&doi=10.18293%2fSEKE2018-167&partnerID=40&md5=da7e9e1bd5ebdb234108fb0e7e22f2f9},
}

@InProceedings{pinter2018investigating,
  author       = {Pint{\'e}r, G{\'a}bor and Schielke, Mira and Petrick, Rico},
  title        = {Investigating Word Segmentation Techniques for German Using Finite-State Transducers},
  booktitle    = {International Conference on Speech and Computer},
  year         = {2018},
  pages        = {511--521},
  organization = {Springer},
}

@Article{hucka2018nostril,
  author  = {Hucka, Michael},
  title   = {Nostril: A nonsense string evaluator written in Python.},
  journal = {J. Open Source Software},
  year    = {2018},
  volume  = {3},
  number  = {25},
  pages   = {596},
}

@Article{hucka2018spiral,
  author  = {Hucka, Michael},
  title   = {Spiral: splitters for identifiers in source code files.},
  journal = {J. Open Source Software},
  year    = {2018},
  volume  = {3},
  number  = {24},
  pages   = {653},
}

@Article{markovtsev2018splitting,
  author  = {Markovtsev, Vadim and Long, Waren and Bulychev, Egor and Keramitas, Romain and Slavnov, Konstantin and Markowski, Gabor},
  title   = {Splitting source code identifiers using Bidirectional LSTM Recurrent Neural Network},
  journal = {arXiv preprint arXiv:1805.11651},
  year    = {2018},
}

@Conference{Jagfeld201758,
  author        = {Jagfeld, G. and Ziering, P. and Van Der Plas, L.},
  title         = {Evaluating compound splitters extrinsically with textual entailment},
  year          = {2017},
  volume        = {2},
  pages         = {58-63},
  note          = {cited By 0},
  abstract      = {Traditionally, compound splitters are evaluated intrinsically on gold-standard data or extrinsically on the task of statistical machine translation. We explore a novel way for the extrinsic evaluation of compound splitters, namely recognizing textual entailment. Compound splitting has great potential for this novel task that is both transparent and well-defined. Moreover, we show that it addresses certain aspects that are either ignored in intrinsic evaluations or compensated for by task-internal mechanisms in statistical machine translation. We show significant improvements using different compound splitting methods on a German textual entailment dataset. © 2017 Association for Computational Linguistics.},
  affiliation   = {Institute for Natural Language Processing, University of Stuttgart, Germany; Institute of Linguistics, University of Malta, Malta},
  document_type = {Conference Paper},
  doi           = {10.18653/v1/P17-2010},
  journal       = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040556164&doi=10.18653%2fv1%2fP17-2010&partnerID=40&md5=abad0047ea69c94b560a54599233b45f},
}

@InProceedings{weller2017simple,
  author    = {Weller-Di Marco, Marion},
  title     = {Simple compound splitting for German},
  booktitle = {Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)},
  year      = {2017},
  pages     = {161--166},
}

@InProceedings{fujinuma2017substring,
  author    = {Fujinuma, Yoshinari and Grissom II, Alvin},
  title     = {Substring Frequency Features for Segmentation of Japanese Katakana Words with Unlabeled Corpora},
  booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  year      = {2017},
  pages     = {74--79},
}

@MastersThesis{nordal2016cross,
  author = {Nordal, Martin},
  title  = {Cross-lingual information retrieval using compound word splitting},
  school = {NTNU},
  year   = {2016},
}

@InProceedings{rigouts2016dutch,
  author       = {Rigouts Terryn, Ayla and Macken, Lieve and Lefever, Els},
  title        = {Dutch hypernym detection: does decompounding help?},
  booktitle    = {Joint Second Workshop on Language and Ontology \& Terminology and Knowledge Structures (LangOnto2+ TermiKS)},
  year         = {2016},
  pages        = {74--78},
  organization = {European Language Resources Association (ELRA)},
}

@InProceedings{ma2016letter,
  author    = {Ma, Jianqiang and Henrich, Verena and Hinrichs, Erhard},
  title     = {Letter sequence labeling for compound splitting},
  booktitle = {Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  year      = {2016},
  pages     = {76--81},
}

@Article{reuter2016segmenting,
  author  = {Reuter, Jack and Pereira-Martins, Jhonata and Kalita, Jugal},
  title   = {Segmenting twitter hashtags},
  journal = {International Journal on Natural Language Computing},
  year    = {2016},
  volume  = {5},
  pages   = {23--36},
}

@InProceedings{shapiro2016splitting,
  author    = {Shapiro, Naomi Tachikawa},
  title     = {Splitting compounds with ngrams},
  booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
  year      = {2016},
  pages     = {630--640},
}

@Conference{Ziering2016644,
  author        = {Ziering, P. and Van Der Plas, L.},
  title         = {Towards unsupervised and language-independent compound splitting using inflectional morphological transformations},
  year          = {2016},
  pages         = {644-653},
  note          = {cited By 4},
  abstract      = {In this paper, we address the task of language-independent, knowledge-lean and unsupervised compound splitting, which is an essential component for many natural language processing tasks such as machine translation. Previous methods on statistical compound splitting either include language-specific knowledge (e.g., linking elements) or rely on parallel data, which results in limited applicability. We aim to overcome these limitations by learning compounding morphology from inflectional information derived from lemmatized monolingual corpora. In experiments for Germanic languages, we show that our approach significantly outperforms language-dependent stateof-the-art methods in finding the correct split point and that word inflection is a good approximation for compounding morphology. ©2016 Association for Computational Linguistics.},
  affiliation   = {Institute for Natural Language Processing, University of Stuttgart, Germany; Institute of Linguistics, University of Malta, Malta},
  document_type = {Conference Paper},
  journal       = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994128765&partnerID=40&md5=5b0b9b0941575eb4aac9f0edfea61654},
}

@Conference{Riedl2016617,
  author        = {Riedl, M. and Biemann, C.},
  title         = {Unsupervised compound splitting with distributional semantics rivals supervised methods},
  year          = {2016},
  pages         = {617-622},
  note          = {cited By 8},
  abstract      = {In this paper we present a word decompounding method that is based on distributional semantics. Our method does not require any linguistic knowledge and is initialized using a large monolingual corpus. The core idea of our approach is that parts of compounds (like "candle" and "stick") are semantically similar to the entire compound, which helps to exclude spurious splits (like "candles" and "tick"). We report results for German and Dutch: For German, our unsupervised method comes on par with the performance of a rule-based and a supervised method and significantly outperforms two unsupervised baselines. For Dutch, our method performs only slightly below a rule-based optimized compound splitter. ©2016 Association for Computational Linguistics.},
  affiliation   = {Language Technology, Computer Science Department, Technische Universität Darmstadt, Hochschulstrasse 10, Darmstadt, D-64289, Germany},
  document_type = {Conference Paper},
  journal       = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994120942&partnerID=40&md5=907e2881fa0d72e3de877cd94f519d2d},
}

@Conference{Hirschmann20163199,
  author        = {Hirschmann, F. and Nam, J. and Fürnkranz, J.},
  title         = {What makes word-level neural machine translation hard: A case study on English-German translation},
  year          = {2016},
  pages         = {3199-3208},
  note          = {cited By 3},
  abstract      = {Traditional machine translation systems often require heavy feature engineering and the combination of multiple techniques for solving different subproblems. In recent years, several end-to-end learning architectures based on recurrent neural networks have been proposed. Unlike traditional systems, Neural Machine Translation (NMT) systems learn the parameters of the model and require only minimal preprocessing. Memory and time constraints allow to take only a fixed number of words into account, which leads to the out-of-vocabulary (OOV) problem. In this work, we analyze why the OOV problem arises and why it is considered a serious problem in German. We study the effectiveness of compound word splitters for alleviating the OOV problem, resulting in a 2.5+ BLEU points improvement over a baseline on the WMT' 14 German-to-English translation task. For English-to-German translation, we use target-side compound splitting through a special syntax during training that allows the model to merge compound words and gain 0.2 BLEU points. © 1963-2018 ACL.},
  affiliation   = {Knowledge Engineering Group, Technische Universität Darmstadt, Darmstadt, Germany},
  document_type = {Conference Paper},
  journal       = {COLING 2016 - 26th International Conference on Computational Linguistics, Proceedings of COLING 2016: Technical Papers},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039147994&partnerID=40&md5=92b1d72c2067f0a3005d477c56d828af},
}

@Article{escartin2015assessing,
  author  = {Escart{\'\i}n, Carla Parra and Alonso, H{\'e}ctor Mart{\'\i}nez},
  title   = {Assessing WordNet for bilingual compound dictionary extraction},
  journal = {MULTI-WORD UNITS IN MACHINE TRANSLATION AND TRANSLATION TECHNOLOGIES MUMTTT2015},
  year    = {2015},
  pages   = {19},
}

@Article{prestes2015extraccao,
  author = {Prestes, Kassius Vargas},
  title  = {Extra{\c{c}}{\~a}o multil{\'\i}ngue de termos multipalavra em corpora compar{\'a}veis},
  year   = {2015},
}

@Article{Carvalho:2015:SCI:2948288.2948331,
  author     = {Carvalho, Nuno Ramos and Almeida, Jos{\'e} Jo\~{a}o and Henriques, Pedro Rangel and Varanda, Maria Jo\~{a}o},
  title      = {From Source Code Identifiers to Natural Language Terms},
  journal    = {J. Syst. Softw.},
  year       = {2015},
  volume     = {100},
  number     = {C},
  pages      = {117--128},
  month      = feb,
  issn       = {0164-1212},
  acmid      = {2948331},
  address    = {New York, NY, USA},
  doi        = {10.1016/j.jss.2014.10.013},
  issue_date = {February 2015},
  keywords   = {Identifier splitting, Natural language processing, Program comprehension},
  numpages   = {12},
  publisher  = {Elsevier Science Inc.},
  url        = {http://dx.doi.org/10.1016/j.jss.2014.10.013},
}

@InProceedings{tiedemann2015morphological,
  author    = {Tiedemann, J{\"o}rg and Ginter, Filip and Kanerva, Jenna},
  title     = {Morphological segmentation and OPUS for Finnish-English machine translation},
  booktitle = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
  year      = {2015},
  pages     = {177--183},
}

@Article{Bretschneider2015207,
  author          = {Bretschneider, C. and Zillner, S.},
  title           = {Semantic splitting of German medical compounds},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2015},
  volume          = {9302},
  pages           = {207-215},
  note            = {cited By 0},
  abstract        = {Compounding is widespread in highly inflectional languages with a quarter of all nouns created by composition. In our field of study, the German medical language, the amount of compounds significantly outnumbers this figure with 64%. Thus, their correct splitting is a high-impact preprocessing step for any NLP-based application. In this work we address two challenges of medical decomposition: First, we introduce the consideration of unknown constituents in order to split compounds that were not recognized as such so far. Second, our approach builds on the corpus-based approach of Koehn and Knight and adds semantic knowledge from domain ontologies to increase the accuracy during disambiguation of the various split options. Using this first-of-a-kind semantic approach in a study on decomposition of German medical compounds, we outperform the existing approaches by far. © Springer International Publishing Switzerland 2015.},
  affiliation     = {Siemens AG, Corporate Technology, Munich, Germany; Center for Information and Language Processing, University Munich, Munich, Germany; School of International Business and Entrepreneurship, Steinbeis University, Berlin, Germany},
  author_keywords = {Compound splitting; Medical NLP; Ontology; Semantics},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-319-24033-6_24},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951778429&doi=10.1007%2f978-3-319-24033-6_24&partnerID=40&md5=b80f85ba2c958dce5bb50b1ba2869b01},
}

@Article{daiber2015splitting,
  author  = {Daiber, Joachim and Quiroz, Lautaro and Wechsler, Roger and Frank, Stella},
  title   = {Splitting compounds by semantic analogy},
  journal = {arXiv preprint arXiv:1509.04473},
  year    = {2015},
}

@Article{hellwig2015using,
  author  = {Hellwig, Oliver},
  title   = {Using Recurrent Neural Networks for joint compound splitting and Sandhi resolution in Sanskrit},
  journal = {Proceedings of the 7th LTC},
  year    = {2015},
  pages   = {289--293},
}

@InProceedings{van2014taxonomy,
  author    = {Van Huyssteen, Gerhard and Verhoeven, Ben},
  title     = {A Taxonomy for Afrikaans and Dutch compounds},
  booktitle = {Proceedings of the First Workshop on Computational Approaches to Compound Analysis (ComAComA 2014)},
  year      = {2014},
  pages     = {31--40},
}

@Article{Pecina:2014:AMT:2657512.2657842,
  author     = {Pecina, Pavel and Du\v{s}ek, Ond\v{r}ej and Goeuriot, Lorraine and Haji\v{c}, Jan and Hlav\'{a}\v{c}ov\'{a}, Jaroslava and Jones, Gareth J. F. and Kelly, Liadh and Leveling, Johannes and Mare\v{c}ek, David and Nov\'{a}k, Michal and Popel, Martin and Rosa, Rudolf and Tamchyna, Ale\v{s} and Ure\v{s}ov\'{a}, Zde\v{n}ka},
  title      = {Adaptation of Machine Translation for Multilingual Information Retrieval in the Medical Domain},
  journal    = {Artif. Intell. Med.},
  year       = {2014},
  volume     = {61},
  number     = {3},
  pages      = {165--185},
  month      = jul,
  issn       = {0933-3657},
  acmid      = {2657842},
  address    = {Essex, UK},
  doi        = {10.1016/j.artmed.2014.01.004},
  issue_date = {July, 2014},
  keywords   = {Compound splitting, Cross-language information retrieval, Domain adaptation of statistical machine translation, Intelligent training data selection for machine translation, Medical query translation, Statistical machine translation},
  numpages   = {21},
  publisher  = {Elsevier Science Publishers Ltd.},
  url        = {http://dx.doi.org/10.1016/j.artmed.2014.01.004},
}

@Article{Hill:2014:ESI:2683115.2683127,
  author     = {Hill, Emily and Binkley, David and Lawrie, Dawn and Pollock, Lori and Vijay-Shanker, K.},
  title      = {An Empirical Study of Identifier Splitting Techniques},
  journal    = {Empirical Softw. Engg.},
  year       = {2014},
  volume     = {19},
  number     = {6},
  pages      = {1754--1780},
  month      = dec,
  issn       = {1382-3256},
  acmid      = {2683127},
  address    = {Hingham, MA, USA},
  doi        = {10.1007/s10664-013-9261-0},
  issue_date = {December 2014},
  keywords   = {Identifier names, Program comprehension, Software engineering tools, Source code text analysis},
  numpages   = {27},
  publisher  = {Kluwer Academic Publishers},
  url        = {http://dx.doi.org/10.1007/s10664-013-9261-0},
}

@Article{Guerrouj:2014:EIE:2683115.2683136,
  author     = {Guerrouj, Latifa and Penta, Massimiliano and Gu{\'e}h{\'e}neuc, Yann-Ga\"{e}l and Antoniol, Giuliano},
  title      = {An Experimental Investigation on the Effects of Context on Source Code Identifiers Splitting and Expansion},
  journal    = {Empirical Softw. Engg.},
  year       = {2014},
  volume     = {19},
  number     = {6},
  pages      = {1706--1753},
  month      = dec,
  issn       = {1382-3256},
  acmid      = {2683136},
  address    = {Hingham, MA, USA},
  doi        = {10.1007/s10664-013-9260-1},
  issue_date = {December 2014},
  keywords   = {Identifier splitting and expansion, Program understanding, Task context},
  numpages   = {48},
  publisher  = {Kluwer Academic Publishers},
  url        = {http://dx.doi.org/10.1007/s10664-013-9260-1},
}

@Article{kirkedal2014automatic,
  author = {Kirkedal, Andreas S{\o}eborg},
  title  = {Automatic Phonetic Transcription for Danish Speech Recognition},
  year   = {2014},
}

@InProceedings{10.1007/978-3-319-10888-9_23,
  author    = {Bick, Eckhard},
  title     = {Constraint Grammar-Based Swedish-Danish Machine Translation},
  booktitle = {Advances in Natural Language Processing},
  year      = {2014},
  editor    = {Przepi{\'o}rkowski, Adam and Ogrodniczuk, Maciej},
  pages     = {216--227},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {This paper describes and evaluates a grammar-based machine translation system for the Swedish-Danish language pair. Source-language structural analysis, polysemy resolution, syntactic movement rules and target-language agreement are based on Constraint Grammar morphosyntactic tags and dependency trees. Lexical transfer rules exploit dependency links to access contextual information, such as syntactic argument function, semantic type and quantifiers, or to integrate verbal features, e.g. diathesis and auxiliaries. Out-of-vocabulary words are handled by derivational and compound analysis with a combined coverage of 99.3{\%}, as well as systematic morpho-phonemic transliterations for the remaining cases. The system achieved BLEU scores of 0.65-0.8 depending on references and outperformed both STMT and RBMT competitors by a large margin.},
  isbn      = {978-3-319-10888-9},
}

@InProceedings{brun2014decomposing,
  author    = {Brun, Caroline and Roux, Claude},
  title     = {Decomposing Hashtags to Improve Tweet Polarity Classification (D{\'e}composition des {\u{G}} hash tags {\u{g}} pour l’am{\'e}lioration de la classification en polarit{\'e} des {\u{G}} tweets {\u{g}})[in French]},
  booktitle = {Proceedings of TALN 2014 (Volume 2: Short Papers)},
  year      = {2014},
  pages     = {473--478},
}

@InProceedings{weller2014distinguishing,
  author    = {Weller, Marion and Cap, Fabienne and M{\"u}ller, Stefan and im Walde, Sabine Schulte and Fraser, Alexander},
  title     = {Distinguishing degrees of compositionality in compound splitting for statistical machine translation},
  booktitle = {Proceedings of the First Workshop on Computational Approaches to Compound Analysis (ComAComA 2014)},
  year      = {2014},
  pages     = {81--90},
}

@InProceedings{escartin2014german,
  author    = {Escart{\'\i}n, Carla Parra and Peitz, Stephan and Ney, Hermann},
  title     = {German Compounds and Statistical Machine Translation. Can they get along?},
  booktitle = {Proceedings of the 10th Workshop on Multiword Expressions (MWE)},
  year      = {2014},
  pages     = {48--56},
}

@Conference{Fishel2014159,
  author        = {Fishel, M. and Sennrich, R.},
  title         = {Handling technical OOVs in SMT},
  year          = {2014},
  pages         = {159-162},
  note          = {cited By 0},
  abstract      = {We present a project on machine translation of software help desk tickets, a highly technical text domain. The main source of translation errors were out-of-vocabulary tokens (OOVs), most of which were either in-domain German compounds or technical token sequences that must be preserved verbatim in the output. We describe our efforts on compound splitting and treatment of non-translatable tokens, which lead to a significant translation quality gain. © 2014 The authors.},
  affiliation   = {Institute of Computational Linguistics, University of Zurich, Binzmühlestr. 14, Zürich, CH-8050, Switzerland},
  document_type = {Conference Paper},
  journal       = {Proceedings of the 17th Annual Conference of the European Association for Machine Translation, EAMT 2014},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000716797&partnerID=40&md5=3634713662e062110d558fbc30f94db4},
}

@Article{ablimit2014lexicon,
  author    = {Ablimit, Mijit and Kawahara, Tatsuya and Hamdulla, Askar},
  title     = {Lexicon optimization based on discriminative learning for automatic speech recognition of agglutinative language},
  journal   = {Speech communication},
  year      = {2014},
  volume    = {60},
  pages     = {78--87},
  publisher = {Elsevier},
}

@Article{verhoeven2014proceedings,
  author = {Verhoeven, Ben and Daelemans, Walter and van Zaanen, Menno and van Huyssteen, Gerhard},
  title  = {Proceedings of the First Workshop on Computational Approaches to Compound Analysis held at the 25th International Conference on Computational Linguistics (COLING 2014)},
  year   = {2014},
}

@Article{Wołk2014107,
  author          = {Wołk, K. and Marasek, K.},
  title           = {Real-time statistical speech translation},
  journal         = {Advances in Intelligent Systems and Computing},
  year            = {2014},
  volume          = {275 AISC},
  number          = {VOLUME 1},
  pages           = {107-113},
  note            = {cited By 8},
  abstract        = {This research investigates the Statistical Machine Translation approaches to translate speech in real time automatically. Such systems can be used in a pipeline with speech recognition and synthesis software in order to produce a real-time voice communication system between foreigners. We obtained three main data sets from spoken proceedings that represent three different types of human speech. TED, Europarl, and OPUS parallel text corpora were used as the basis for training of language models, for developmental tuning and testing of the translation system. We also conducted experiments involving part of speech tagging, compound splitting, linear language model interpolation, TrueCasing and morphosyntactic analysis. We evaluated the effects of variety of data preparations on the translation results using the BLEU, NIST, METEOR and TER metrics and tried to give answer which metric is most suitable for PL-EN language pair. © Springer International Publishing Switzerland 2014.},
  affiliation     = {Department of Multimedia, Polish Japanese Institute of Information Technology, Koszykowa 86, 02-008 Warsaw, Poland},
  author_keywords = {Knowledge-free learning; Machine learning; Machine translation; NLP; Speech translation},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-319-05951-8_11},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904602811&doi=10.1007%2f978-3-319-05951-8_11&partnerID=40&md5=d9d3d16efc7102d26081575e8d464b8b},
}

@PhdThesis{leidig2014single,
  author = {Leidig, Sebastian and Schlippe, Dipl-Inform Tim and Schultz, Ing Tanja},
  title  = {Single and Combined Features for the Detection of Anglicisms in German and Afrikaans},
  school = {Bachelor’s Thesis, Karlsruhe Institute of Technology},
  year   = {2014},
}

@InProceedings{pimpale2014smt,
  author    = {Pimpale, Prakash B and Patel, Raj Nath and Sasikumar, M},
  title     = {SMT from Agglutinative Languages: Use of Suffix Separation and Word Splitting},
  booktitle = {Proceedings of the 11th International Conference on Natural Language Processing},
  year      = {2014},
  pages     = {2--10},
}

@InProceedings{junczys2014smt,
  author    = {Junczys-Dowmunt, Marcin and Pouliquen, Bruno},
  title     = {SMT of German Patents at WIPO: Decompounding and Verb Structure Pre-reordering},
  booktitle = {Proceedings of the 17th Annual Conference of the European Association for Machine Translation (EAMT2014)},
  year      = {2014},
  pages     = {217--220},
}

@InProceedings{santos2014using,
  author    = {Santos, Pedro Bispo},
  title     = {Using compound lists for german decompounding in a back-off scenario},
  booktitle = {Workshop on Computational, Cognitive, and Linguistic Approaches to the Analysis of Complex Words and Collocations (CCLCC 2014)},
  year      = {2014},
  pages     = {51--55},
}

@InProceedings{owczarzak2014wordsyoudontknow,
  author    = {Owczarzak, Karolina and de Haan, Ferdinand and Krupka, George and Hindle, Don},
  title     = {Wordsyoudontknow: Evaluation of lexicon-based decompounding with unknown handling},
  booktitle = {Proceedings of the First Workshop on Computational Approaches to Compound Analysis (ComAComA 2014)},
  year      = {2014},
  pages     = {63--71},
}

@InProceedings{10.1007/978-3-642-40802-1_14,
  author    = {Ganguly, Debasis and Leveling, Johannes and Jones, Gareth J. F.},
  title     = {A Case Study in Decompounding for Bengali Information Retrieval},
  booktitle = {Information Access Evaluation. Multilinguality, Multimodality, and Visualization},
  year      = {2013},
  editor    = {Forner, Pamela and M{\"u}ller, Henning and Paredes, Roberto and Rosso, Paolo and Stein, Benno},
  pages     = {108--119},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Decompounding has been found to improve information retrieval (IR) effectiveness for compounding languages such as Dutch, German, or Finnish. No previous studies, however, exist on the effect of decomposition of compounds in IR for Indian languages. In this case study, we investigate the effect of decompounding for Bengali, a highly agglutinative Indian language. The standard approach of decompounding for IR, i.e. indexing compound parts (constituents) in addition to compound words, has proven beneficial for European languages. Our experiments reported in this paper show that such a standard approach does not work particularly well for Bengali IR. Some unique characteristics of Bengali compounds are: i) only one compound constituent may be a valid word in contrast to the stricter requirement of both being so; and ii) the first character of the right constituent can be modified by the rules of Sandhi in contrast to simple concatenation. As a solution, we firstly propose a more relaxed decompounding where a compound word is decomposed into only one constituent if the other constituent is not a valid word, and secondly we perform selective decompounding by ensuring that constituents often co-occur with the compound word, which indicates how related the constituents and the compound are. We perform experiments on Bengali ad-hoc IR collections from FIRE 2008 to 2012. Our experiments show that both the relaxed decomposition and the co-occurrence-based constituent selection proves more effective than the standard frequency-based decomposition method, improving mean average precision (MAP) up to 2.72{\%} and recall up to 1.8{\%}, compared to not decompounding words.},
  isbn      = {978-3-642-40802-1},
}

@Conference{Binkley2013401,
  author        = {Binkley, D. and Lawrie, D. and Pollock, L. and Hill, E. and Vijay-Shanker, K.},
  title         = {A dataset for evaluating identifier splitters},
  year          = {2013},
  pages         = {401-404},
  note          = {cited By 6},
  abstract      = {Software engineering and evolution techniques have recently started to exploit the natural language information in source code. A key step in doing so is splitting identifiers into their constituent words. While simple in concept, identifier splitting raises several challenging issues, leading to a range of splitting techniques. Consequently, the research community would benefit from a dataset (i.e., a gold set) that facilitates comparative studies of identifier splitting techniques. A gold set of 2,663 split identifiers was constructed from 8,522 individual human splitting judgements and can be obtained from www.cs.loyola.edu/∼binkley/ludiso. This set's construction and observations aimed at its effective use are described. © 2013 IEEE.},
  affiliation   = {Loyola University Maryland, Baltimore, MD 21210, United States; University of Delaware, Newark, DE 19716, United States; Montclair State University, Montclair, NJ, 07043, United States},
  art_number    = {6624055},
  document_type = {Conference Paper},
  doi           = {10.1109/MSR.2013.6624055},
  journal       = {IEEE International Working Conference on Mining Software Repositories},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889056852&doi=10.1109%2fMSR.2013.6624055&partnerID=40&md5=20c554dc45e942680416227bea5a5bdf},
}

@Article{Kpodjedo20131090,
  author          = {Kpodjedo, S. and Ricca, F. and Galinier, P. and Antoniol, G. and Gueheneuc, Y.-G.},
  title           = {Madmatch: Many-to-many approximate diagram matching for design comparison},
  journal         = {IEEE Transactions on Software Engineering},
  year            = {2013},
  volume          = {39},
  number          = {8},
  pages           = {1090-1111},
  note            = {cited By 13},
  abstract        = {Matching algorithms play a fundamental role in many important but difficult software engineering activities, especially design evolution analysis and model comparison. We present MADMatch, a fast and scalable many-to-many approximate diagram matching approach based on an error-tolerant graph matching (ETGM) formulation. Diagrams are represented as graphs, costs are assigned to possible differences between two given graphs, and the goal is to retrieve the cheapest matching. We address the resulting optimization problem with a tabu search enhanced by the novel use of lexical and structural information. Through several case studies with different types of diagrams and tasks, we show that our generic approach obtains better results than dedicated state-of-the-art algorithms, such as AURA, PLTSDiff, or UMLDiff, on the exact same datasets used to introduce (and evaluate) these algorithms. © 1976-2012 IEEE.},
  affiliation     = {Ecole Polytechnique of Montreal, DGIGL, 2900 - Edouard Montpetit, Montreal, QC H3C 3A7, Canada; Universita di Genova, Genova 16126, Italy},
  art_number      = {6568862},
  author_keywords = {approximate graph matching; Diagram differencing; identifier splitting; search-based software engineering},
  document_type   = {Article},
  doi             = {10.1109/TSE.2013.9},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881038069&doi=10.1109%2fTSE.2013.9&partnerID=40&md5=9a548cfb32b924a02c660b079f2e1e6b},
}

@Article{Guerrouj2013575,
  author          = {Guerrouj, L. and Di Penta, M. and Antoniol, G. and Guéh́eneuc, Y.-G.},
  title           = {TIDIER: An identifier splitting approach using speech recognition techniques},
  journal         = {Journal of software: Evolution and Process},
  year            = {2013},
  volume          = {25},
  number          = {6},
  pages           = {575-599},
  note            = {cited By 21},
  abstract        = {The software engineering literature reports empirical evidence on the relation between various characteristics of a software system and its quality. Among other factors, recent studies have shown that a proper choice of identifiers influences understandability and maintainability. Indeed, identifiers are developers' main source of information and guide their cognitive processes during program comprehension when high-level documentation is scarce or outdated and when source code is not sufficiently commented. This paper proposes a novel approach to recognize words composing source code identifiers. The approach is based on an adaptation of Dynamic Time Warping used to recognize words in continuous speech. The approach overcomes the limitations of existing identifier-splitting approaches when naming conventions (e.g., Camel Case) are not used or when identifiers contain abbreviations. We apply the approach on a sample of more than 1000 identifiers extracted from 340 C programs and compare its results with a simple Camel Case splitter and with an implementation of an alternative identifier splitting approach, Samurai. Results indicate the capability of the novel approach: (i) to outperform the alternative ones, when using a dictionary augmented with domain knowledge or a contextual dictionary and (ii) to expand 48% of a set of selected abbreviations into dictionary words. Copyright © 2011 John Wiley & Sons, Ltd.},
  affiliation     = {DGIGL/P TIDEJ Team/SOCCER Lab., École Polytechnique de Montréal, Québec, Canada; RCOST - Department of Engineering, University of Sannio, Italy},
  author_keywords = {Identifier splitting; Linguistic analysis; Program comprehension},
  document_type   = {Article},
  doi             = {10.1002/smr.539},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883654687&doi=10.1002%2fsmr.539&partnerID=40&md5=79227717373f850527ad7601246a17bb},
}

@InProceedings{6405277,
  author    = {A. {Corazza} and S. {Di Martino} and V. {Maggio}},
  title     = {LINSEN: An efficient approach to split identifiers and expand abbreviations},
  booktitle = {2012 28th IEEE International Conference on Software Maintenance (ICSM)},
  year      = {2012},
  pages     = {233-242},
  month     = {Sep.},
  abstract  = {Information Retrieval (IR) techniques are being exploited by an increasing number of tools supporting Software Maintenance activities. Indeed the lexical information embedded in the source code can be valuable for tasks such as concept location, clustering or recovery of traceability links. The application of such IR-based techniques relies on the consistency of the lexicon available in the different artifacts, and their effectiveness can worsen if programmers introduce abbreviations (e.g: rect) and/or do not strictly follow naming conventions such as Camel Case (e.g: UTFtoASCII). In this paper we propose an approach to automatically split identifiers in their composing words, and expand abbreviations. The solution is based on a graph model and performs in linear time with respect to the size of the dictionary, taking advantage of an approximate string matching technique. The proposed technique exploits a number of different dictionaries, referring to increasingly broader contexts, in order to achieve a disambiguation strategy based on the knowledge gathered from the most appropriate domain. The approach has been compared to other splitting and expansion techniques, using freely available oracles for the identifiers extracted from 24 C/C++ and Java open source systems. Results show an improvement in both splitting and expanding performance, in addition to a strong enhancement in the computational efficiency.},
  doi       = {10.1109/ICSM.2012.6405277},
  keywords  = {C++ language;information retrieval;Java;pattern clustering;public domain software;software maintenance;LINSEN;identifier splitting;abbreviation expansion;information retrieval techniques;software maintenance activities;lexical information;concept location;traceability links clustering;traceability links recovery;IR-based techniques;camel case;dictionary;approximate string matching technique;disambiguation strategy;C-C++;Java open source systems;oracles;Dictionaries;Software algorithms;Approximation algorithms;Context;Software maintenance;Conferences;Source Code Identifiers;Program Comprehension;Splitting;Expansion},
}

@Conference{Srinivasan20121113,
  author          = {Srinivasan, S. and Bhattacharya, S. and Chakraborty, R.},
  title           = {Segmenting web-domains and hashtags using length specific models},
  year            = {2012},
  pages           = {1113-1122},
  note            = {cited By 13},
  abstract        = {Segmentation of a string of English language characters into a sequence of words has many applications. Here, we study two applications in the internet domain. First application is the web domain segmentation which is crucial for monetization of broken URLs. Secondly, we propose and study a novel application of twitter hashtag segmentation for increasing recall on twitter searches. Existing methods for word segmentation use unsupervised language models. We find that when using multiple corpora, the joint probability model from multiple corpora performs significantly better than the individual corpora. Motivated by this, we propose weighted joint probability model, with weights specific to each corpus. We propose to train the weights in a supervised manner using max-margin methods. The supervised probability models improve segmentation accuracy over joint probability models. Finally, we observe that length of segments is an important parameter for word segmentation, and incorporate length-specific weights into our model. The length specific models further improve segmentation accuracy over supervised probability models. For all models proposed here, inference problem can be solved using the dynamic programming algorithm. We test our methods on five different datasets, two from web domains data, and three from news headlines data from an LDC dataset. The supervised length specific models show significant improvements over unsupervised single corpus and joint probability models. Cross-testing between the datasets confirm that supervised probability models trained on all datasets, and length specific models trained on news headlines data, generalize well. Segmentation of hashtags result in significant improvement in recall on searches for twitter trends. © 2012 ACM.},
  affiliation     = {Yahoo SDC, Bangalore, India; Yahoo Labs., Bangalore, India; Indian Statistical Institute, Kolkata, India},
  author_keywords = {compound splitting; hashtag segmentation; structured learning; web domain segmentation; word segmentation},
  document_type   = {Conference Paper},
  doi             = {10.1145/2396761.2398410},
  journal         = {ACM International Conference Proceeding Series},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871061019&doi=10.1145%2f2396761.2398410&partnerID=40&md5=4a4ef08843f61ac76f3dbc0b0b0a71a5},
}

@Conference{Sureka20121,
  author          = {Sureka, A.},
  title           = {Source code identifier splitting using yahoo image and web search engine},
  year            = {2012},
  pages           = {1-8},
  note            = {cited By 1},
  abstract        = {Source-code or program identifiers are sequence of characters consisting of one or more tokens representing domain concepts. Splitting or tokenizing identifiers that does not contain explicit markers or clues such as came-casing or using underscore as a token separatoris a technically challenging problem. In this paper, we present a technique for automatic tokenization and splitting of source-code identifiers using Yahoo web search and image search similarity distance. We present an algorithm that decides the split position based on various factors such as conceptual correlations and semantic relatedness between the left and right splits strings of a given identifier, popularity of the token and its length. The number of hits or search results returned by the web and image search engine serves as a proxy to measures such as term popularity and correlation. We perform a series of experiments to validate the proposed approach and present performance results.},
  affiliation     = {Indraprastha Institute of Information Technology, Delhi IIIT-D, New Delhi, India},
  author_keywords = {Identifier Splitting; Identifier Tokenization; Mining Software Repositories; Program Comprehension; Yahoo Similarity Distance},
  document_type   = {Conference Paper},
  doi             = {10.1145/2384416.2384417},
  journal         = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869760971&doi=10.1145%2f2384416.2384417&partnerID=40&md5=042160e97cea76d60ed1709ec697fb7f},
}

@Conference{Biggers2012164,
  author          = {Biggers, L.R.},
  title           = {The effects of identifier retention and stop word removal on a latent Dirichlet allocation based feature location technique},
  year            = {2012},
  pages           = {164-169},
  note            = {cited By 1},
  abstract        = {Feature location, an important task in program comprehension, occurs when the developer identifies the source code entity or entities responsible for implementing a functionality. Researchers have applied static analysis techniques to multiple software maintenance tasks, including feature localization. Static analysis techniques operate on a document corpus. Configuration and preprocessing decisions are required to build a suitable source code corpus for a static analysis technique. Currently, there is little guidance in the software engineering literature for making such configuration decisions. This paper focuses on two preprocessing methods for source code corpora, identifier splitting and stop word lists. We experiment on three open source Java test suites, i.e. Mylyn 1.0.1, Rhino 1.5R5, and Rhino 1.6R5. Our results indicate that identifier splitting and stop word list decisions do not significantly affect the performance of the LDA based feature location technique. © 2012 ACM.},
  affiliation     = {Department of Computer Science, University of Alabama, Tuscaloosa, AL 35487-0290, United States},
  author_keywords = {feature location; information retrieval; program comprehension; software evolution and maintenance; static analysis},
  document_type   = {Conference Paper},
  doi             = {10.1145/2184512.2184551},
  journal         = {Proceedings of the Annual Southeast Conference},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862667183&doi=10.1145%2f2184512.2184551&partnerID=40&md5=596eddec20f80762f4b8b0c7f84166e1},
}

@Conference{Guerrouj2012103,
  author          = {Guerrouj, L. and Galinier, P. and Guéhéneuc, Y.-G. and Antoniol, G. and Di Penta, M.},
  title           = {TRIS: A fast and accurate identifiers splitting and expansion algorithm},
  year            = {2012},
  pages           = {103-112},
  note            = {cited By 7},
  abstract        = {Understanding source code identifiers, by identifying words composing them, is a necessary step for many program comprehension, reverse engineering, or redocumentation tasks. To this aim, researchers have proposed several identifier splitting and expansion approaches such as Samurai, TIDIER and more recently GenTest. The ultimate goal of such approaches is to help disambiguating conceptual information encoded in compound (or abbreviated) identifiers. This paper presents TRIS, TRee-based Identifier Splitter, a two-phases approach to split and expand program identifiers. First, TRIS pre-compiles transformed dictionary words into a tree representation, associating a cost to each transformation. In a second phase, it maps the identifier splitting/expansion problem into a minimization problem, i.e., the search of the shortest path (optimal split/expansion) in a weighted graph. We apply TRIS to a sample of 974 identifiers extracted from JHotDraw, 3,085 from Lynx, and to a sample of 489 identifiers extracted from 340 C programs. Also, we compare TRIS with GenTest on a set of 2,663 mixed Java, C and C++ identifiers. We report evidence that TRIS split (and expansion) is more accurate than state-of-the-art approaches and that it is also efficient in terms of computation time. © 2012 IEEE.},
  affiliation     = {DGIGL, École Polytechnique de Montréal, Canada; Dept. of Engineering, University of Sannio, Italy},
  art_number      = {6385106},
  author_keywords = {Identifier Splitting/Expansion; Linguistic Analysis; Optimal Path; Program Comprehension; Weighted Acyclic Graph},
  document_type   = {Conference Paper},
  doi             = {10.1109/WCRE.2012.20},
  journal         = {Proceedings - Working Conference on Reverse Engineering, WCRE},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872297690&doi=10.1109%2fWCRE.2012.20&partnerID=40&md5=cde8887fb5e9ca9bd4f353e3c9bd9ad8},
}

@InProceedings{5970159,
  author    = {B. {Dit} and L. {Guerrouj} and D. {Poshyvanyk} and G. {Antoniol}},
  title     = {Can Better Identifier Splitting Techniques Help Feature Location?},
  booktitle = {2011 IEEE 19th International Conference on Program Comprehension},
  year      = {2011},
  pages     = {11-20},
  month     = {June},
  abstract  = {The paper presents an exploratory study of two feature location techniques utilizing three strategies for splitting identifiers: Camel Case, Samurai and manual splitting of identifiers. The main research question that we ask in this study is if we had a perfect technique for splitting identifiers, would it still help improve accuracy of feature location techniques applied in different scenarios and settings? In order to answer this research question we investigate two feature location techniques, one based on Information Retrieval and the other one based on the combination of Information Retrieval and dynamic analysis, for locating bugs and features using various configurations of preprocessing strategies on two open-source systems, Rhino and jEdit. The results of an extensive empirical evaluation reveal that feature location techniques using Information Retrieval can benefit from better preprocessing algorithms in some cases, and that their improvement in effectiveness while using manual splitting over state-of-the-art approaches is statistically significant in those cases. However, the results for feature location technique using the combination of Information Retrieval and dynamic analysis do not show any improvement while using manual splitting, indicating that any preprocessing technique will suffice if execution data is available. Overall, our findings outline potential benefits of putting additional research efforts into defining more sophisticated source code preprocessing techniques as they can still be useful in situations where execution information cannot be easily collected.},
  doi       = {10.1109/ICPC.2011.47},
  keywords  = {information retrieval;identifier splitting technique;CamelCase;Samurai;information retrieval;preprocessing strategies;open source system;Rhino;jEdit;feature location technique;Software;Dictionaries;Gold;Manuals;Accuracy;Algorithm design and analysis;Large scale integration;feature location;information retrieval;dynamic analysis;identifier splitting algorithms},
}

@Conference{Henrich2011420,
  author        = {Henrich, V. and Hinrichs, E.},
  title         = {Determining immediate constituents of compounds in GermaNet},
  year          = {2011},
  pages         = {420-426},
  note          = {cited By 11},
  abstract      = {In order to be able to systematically link compounds in GermaNet to their constituent parts, compound splitting needs to be applied recursively and has to identify the immediate constituents at each level of analysis. Existing tools for compound splitting for German only offer an analysis of all component parts of a compound at once without any grouping of subconstituents. Thus, existing tools for splitting compounds were adapted to overcome this issue. Algorithms combining three heterogeneous kinds of compound splitters are developed to achieve better results. The best overall result with an accuracy of 92.42% is achieved by a hybrid combined compound splitter that takes into account all knowledge provided by the individual compound splitters, and in addition some domain knowledge about German derivation morphology and compounding.},
  affiliation   = {University of Tübingen, Germany},
  document_type = {Conference Paper},
  journal       = {International Conference Recent Advances in Natural Language Processing, RANLP},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866877956&partnerID=40&md5=4e50f1d779c887840937a10cea5dcd74},
}

@InProceedings{Macherey:2011:LCS:2002472.2002644,
  author    = {Macherey, Klaus and Dai, Andrew M. and Talbot, David and Popat, Ashok C. and Och, Franz},
  title     = {Language-independent Compound Splitting with Morphological Operations},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
  year      = {2011},
  series    = {HLT '11},
  pages     = {1395--1404},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {2002644},
  isbn      = {978-1-932432-87-9},
  location  = {Portland, Oregon},
  numpages  = {10},
  url       = {http://dl.acm.org/citation.cfm?id=2002472.2002644},
}

@Conference{Wang2011357,
  author          = {Wang, K. and Thrasher, C. and Hsu, B.-J.},
  title           = {Web scale NLP: A case study on URL word breaking},
  year            = {2011},
  pages           = {357-366},
  note            = {cited By 32},
  abstract        = {This paper uses the URL word breaking task as an example to elaborate what we identify as crucialin designingstatistical natural language processing (NLP) algorithmsfor Web scale applications: (1) rudimentary multilingual capabilities to cope with the global nature of the Web, (2) multi-style modeling to handle diverse language styles seen in the Web contents, (3) fast adaptation to keep pace with the dynamic changes of the Web, (4) minimal heuristic assumptions for generalizability and robustness, and (5) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost. We first show that the state-of-the-art word breaking techniquescan be unified and generalized under the Bayesian minimum risk (BMR) framework that, using a Web scale N-gram, can meet the first three requirements. We discuss how the existing techniques can be viewed asintroducing additional assumptions to the basic BMR framework, and describe a generic yet efficient implementation called word synchronous beam search.Testing the framework and its implementation on a series of large scale experiments reveals the following. First, the language style used to build the modelplays a critical role in the word breaking task, and the most suitable for the URL word breaking task appears to be that ofthe document title where the best performance is obtained. Models created from other language styles, such as from document body, anchor text, and even queries, exhibit varying degrees of mismatch. Although all styles benefit from increasing modeling power which, in our experiments, corresponds tothe use of a higher order N-gram, the gain is most recognizable for the title model. The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models, but are less effective and, in many cases,lead to poorer performance than the matched model with minimal assumptions. For the matched model based on document titles, an accuracy rate of 97.18% can already be achieved using simple trigram without any heuristics. Copyright © 2011 by the Association for Computing Machinery, Inc. (ACM).},
  affiliation     = {Microsoft Research, ISRC, One Microsoft Way, Redmond, WA 98052, United States},
  author_keywords = {Compound splitting; Multi-style language model; URL segmentation; Web scale word breaking; Word segmentation},
  document_type   = {Conference Paper},
  doi             = {10.1145/1963405.1963457},
  journal         = {Proceedings of the 20th International Conference on World Wide Web, WWW 2011},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052123854&doi=10.1145%2f1963405.1963457&partnerID=40&md5=f0884c46e20c2b2cf89b5c9a1bb92dd2},
}

@InProceedings{Guerrouj:2010:ADC:1919284.1919610,
  author    = {Guerrouj, Latifa},
  title     = {Automatic Derivation of Concepts Based on the Analysis of Source Code Identifiers},
  booktitle = {Proceedings of the 2010 17th Working Conference on Reverse Engineering},
  year      = {2010},
  series    = {WCRE '10},
  pages     = {301--304},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid     = {1919610},
  doi       = {10.1109/WCRE.2010.45},
  isbn      = {978-0-7695-4123-5},
  keywords  = {Identifier Splitting, Program Comprehension, Linguistic Analysis, Software Quality},
  numpages  = {4},
  url       = {http://dx.doi.org/10.1109/WCRE.2010.45},
}

@InProceedings{Fritzinger:2010:ABD:1868850.1868884,
  author    = {Fritzinger, Fabienne and Fraser, Alexander},
  title     = {How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing},
  booktitle = {Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR},
  year      = {2010},
  series    = {WMT '10},
  pages     = {224--234},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1868884},
  isbn      = {978-1-932432-71-8},
  location  = {Uppsala, Sweden},
  numpages  = {11},
  url       = {http://dl.acm.org/citation.cfm?id=1868850.1868884},
}

@Article{Kumar201057,
  author          = {Kumar, A. and Mittal, V. and Kulkarni, A.},
  title           = {Sanskrit compound processor},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2010},
  volume          = {6465 LNAI},
  pages           = {57-69},
  note            = {cited By 3},
  abstract        = {Sanskrit is very rich in compound formation. Typically a compound does not code the relation between its components explicitly. To understand the meaning of a compound, it is necessary to identify its components, discover the relations between them and finally generate a paraphrase of the compound. In this paper, we discuss the automatic segmentation and type identification of a compound using simple statistics that results from the manually annotated data. © 2010 Springer-Verlag Berlin Heidelberg.},
  affiliation     = {Department of Sanskrit Studies, University of Hyderabad, India; Language Technologies Research Centre, IIIT, Hyderabad, India},
  author_keywords = {Optimality Theory; Sanskrit Compound Splitter; Sanskrit Compound Type Identifier; Sanskrit Compounds},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-17528-2_5},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651063736&doi=10.1007%2f978-3-642-17528-2_5&partnerID=40&md5=ba8a099b10cf2e63862a66f42bd24e2f},
}

@Article{Zeman2010216,
  author          = {Zeman, D.},
  title           = {Using TectoMT as a preprocessing tool for phrase-based statistical machine translation},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2010},
  volume          = {6231 LNAI},
  pages           = {216-223},
  note            = {cited By 0},
  abstract        = {We present a systematic comparison of preprocessing techniques for two language pairs: English-Czech and English-Hindi. The two target languages, although both belonging to the Indo-European language family, show significant differences in morphology, syntax and word order. We describe how TectoMT, a successful framework for analysis and generation of language, can be used as preprocessor for a phrase-based MT system.We compare the two language pairs and the optimal sets of source-language transformations applied to them. The following transformations are examples of possible preprocessing steps: lemmatization; retokenization, compound splitting; removing/adding words lacking counterparts in the other language; phrase reordering to resemble the target word order; marking syntactic functions. TectoMT, as well as all other tools and data sets we use, are freely available on the Web. © 2010 Springer-Verlag Berlin Heidelberg.},
  affiliation     = {Univerzita Karlova v Praze, ÚFAL, Malostranské náměstí 25, 11800 Prague, Czech Republic},
  author_keywords = {phrase-based translation; preprocessing; reordering},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-15760-8_28},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049263442&doi=10.1007%2f978-3-642-15760-8_28&partnerID=40&md5=d8df80e3e3bbfd901f36029213a636fe},
}

@InProceedings{Khaitan:2009:DCS:1645953.1645982,
  author    = {Khaitan, Sanjeet and Das, Arumay and Gain, Sandeep and Sampath, Adithi},
  title     = {Data-driven Compound Splitting Method for English Compounds in Domain Names},
  booktitle = {Proceedings of the 18th ACM Conference on Information and Knowledge Management},
  year      = {2009},
  series    = {CIKM '09},
  pages     = {207--214},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1645982},
  doi       = {10.1145/1645953.1645982},
  isbn      = {978-1-60558-512-3},
  keywords  = {compound splitting, domain names},
  location  = {Hong Kong, China},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/1645953.1645982},
}

@InProceedings{5069482,
  author    = {E. {Enslen} and E. {Hill} and L. {Pollock} and K. {Vijay-Shanker}},
  title     = {Mining source code to automatically split identifiers for software analysis},
  booktitle = {2009 6th IEEE International Working Conference on Mining Software Repositories},
  year      = {2009},
  pages     = {71-80},
  month     = {May},
  abstract  = {Automated software engineering tools (e.g., program search, concern location, code reuse, quality assessment, etc.) increasingly rely on natural language information from comments and identifiers in code. The first step in analyzing words from identifiers requires splitting identifiers into their constituent words. Unlike natural languages, where space and punctuation are used to delineate words, identifiers cannot contain spaces. One common way to split identifiers is to follow programming language naming conventions. For example, Java programmers often use camel case, where words are delineated by uppercase letters or non-alphabetic characters. However, programmers also create identifiers by concatenating sequences of words together with no discernible delineation, which poses challenges to automatic identifier splitting. In this paper, we present an algorithm to automatically split identifiers into sequences of words by mining word frequencies in source code. With these word frequencies, our identifier splitter uses a scoring technique to automatically select the most appropriate partitioning for an identifier. In an evaluation of over 8000 identifiers from open source Java programs, our Samurai approach outperforms the existing state of the art techniques.},
  doi       = {10.1109/MSR.2009.5069482},
  keywords  = {data mining;program diagnostics;software maintenance;source code mining;automatic split identifier;software analysis;word frequency mining;software maintenance;Software maintenance;Natural languages;Programming profession;Software tools;Java;Frequency;Open source software;Software quality;Information analysis;Quality assessment},
}

@Article{braga2008algoritmos,
  author = {Braga, Daniela Filipa Macedo Moreira da and others},
  title  = {Algoritmos de processamento da linguagem natural para sistemas de conversao texto-fala em portugu{\^e}s},
  year   = {2008},
}

@InProceedings{Stymne:2008:GCF:1432430.1432474,
  author    = {Stymne, Sara},
  title     = {German Compounds in Factored Statistical Machine Translation},
  booktitle = {Proceedings of the 6th International Conference on Advances in Natural Language Processing},
  year      = {2008},
  series    = {GoTAL '08},
  pages     = {464--475},
  address   = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  acmid     = {1432474},
  doi       = {10.1007/978-3-540-85287-2_44},
  isbn      = {978-3-540-85286-5},
  location  = {Gothenburg, Sweden},
  numpages  = {12},
  url       = {http://dx.doi.org/10.1007/978-3-540-85287-2_44},
}

@InProceedings{10.1007/978-3-540-78135-6_12,
  author    = {Alfonseca, Enrique and Bilac, Slaven and Pharies, Stefan},
  title     = {German Decompounding in a Difficult Corpus},
  booktitle = {Computational Linguistics and Intelligent Text Processing},
  year      = {2008},
  editor    = {Gelbukh, Alexander},
  pages     = {128--139},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Splitting compound words has proved to be useful in areas such as Machine Translation, Speech Recognition or Information Retrieval (IR). In the case of IR systems, they usually have to cope with noisy data, as user queries are usually written quickly and submitted without review. This work attempts at improving the current approaches for German decompounding when applied to query keywords. The results show an increase of more than 10{\%} in accuracy compared to other state-of-the-art methods.},
  isbn      = {978-3-540-78135-6},
}

@Conference{Stymne2008182,
  author        = {Stymne, S. and Holmqvist, M.},
  title         = {Processing of Swedish compounds for phrase-based statistical machine translation},
  year          = {2008},
  pages         = {182-191},
  note          = {cited By 6},
  abstract      = {We investigated the effects of processing Swedish compounds for phrase-based SMT between Swedish and English. Compounds were split in a pre-processing step using an unsupervised empirical method. After translation into Swedish, compounds were merged, using a novel merging algorithm. We investigated two ways of handling compound parts, by marking them as compound parts or by normalizing them to a canonical form. We found that compound splitting did improve translation into Swedish, according to automatic metrics. For translation into English the results were not consistent across automatic metrics. However, error analysis of compound translation showed a small improvement in the systems that used splitting. The number of untranslated words in the English output was reduced by 50%.},
  affiliation   = {Department of Computer and Information Science, Linköping University, Sweden},
  document_type = {Conference Paper},
  journal       = {Proceedings of the 12th European Association for Machine Translation Conference, EAMT 2008},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857535918&partnerID=40&md5=11ef39e004bb3623703d713985652303},
}

@InProceedings{Holz:2008:UKL:1787578.1787592,
  author    = {Holz, Florian and Biemann, Chris},
  title     = {Unsupervised and Knowledge-free Learning of Compound Splits and Periphrases},
  booktitle = {Proceedings of the 9th International Conference on Computational Linguistics and Intelligent Text Processing},
  year      = {2008},
  series    = {CICLing'08},
  pages     = {117--127},
  address   = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  acmid     = {1787592},
  isbn      = {3-540-78134-X, 978-3-540-78134-9},
  location  = {Haifa, Israel},
  numpages  = {11},
  url       = {http://dl.acm.org/citation.cfm?id=1787578.1787592},
}

@Conference{Bordag2007,
  author          = {Bordag, S.},
  title           = {Unsupervised and knowledge-free morpheme segmentation and analysis},
  year            = {2007},
  volume          = {1173},
  note            = {cited By 0},
  abstract        = {This paper presents a revised version of an unsupervised and knowledge-free morpheme boundary detection algorithm based on letter successor variety (LSV) and a trie classifier [5]. Additionally a morphemic analysis based on contextual similarity provides knowledge about relatedness of the found morphs. For the boundary detection the challenge of increasing recall of found morphs while retaining a high precision is tackled by adding a compound splitter, iterating the LSV analysis and dividing the trie classifier into two distinctly applied clasifiers. The result is a significantly improved overall performance and a decreased reliance on corpus size. Further possible improvements and analyses are discussed.},
  affiliation     = {University of Leipzig, Germany},
  author_keywords = {Distributed similarity; Letter successor variety; Morpheme analysis; Morpheme boundary detection},
  document_type   = {Conference Paper},
  journal         = {CEUR Workshop Proceedings},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921960169&partnerID=40&md5=af8fc25f3c4af981dbe962e6e760ad24},
}

@Article{Pedersen200775,
  author        = {Pedersen, B.S.},
  title         = {Using shallow linguistic analysis to improve search on Danish compounds},
  journal       = {Natural Language Engineering},
  year          = {2007},
  volume        = {13},
  number        = {1},
  pages         = {75-90},
  note          = {cited By 0},
  abstract      = {In this paper we focus on a specific search-related query expansion topic, namely search on Danish compounds and expansion to some of their synonymous phrases. Compounds constitute a specific issue in search, in particular in languages where they are written in one word, as is the case for Danish and the other Scandinavian languages. For such languages, expansion of the query compound into separate lemmas is a way of finding the often frequent alternative synonymous phrases in which the content of a compound can also be expressed. However, it is crucial to note that the number of irrelevant hits is generally very high when using this expansion strategy. The aim of this paper is therefore to examine how we can obtain better search results on split compounds, partly by looking at the internal structure of the original compound, partly by analyzing the context in which the split compound occurs. In this context, we pursue two hypotheses: (1) that some categories of compounds are more likely to have synonymous 'split' counterparts than others; and (2) that search results where both the search words (obtained by splitting the compound) occur in the same noun phrase, are more likely to contain a synonymous phrase to the original compound query. The search results from 410 enhanced compound queries are used as a test bed for our experiments. On these search results, we perform a shallow linguistic analysis and introduce a new, linguistically based threshold for retrieved hits. The results obtained by using this strategy demonstrate that compound splitting combined with a shallow linguistic analysis focusing on the argument structure of the compound head as well as on the recognition of NPs, can improve search by substantially bringing down the number of irrelevant hits. © 2006 Cambridge University Press.},
  affiliation   = {Center for Sprogteknologi, University of Copenhagen, Njalsgade 80, DK-2300 Copenhagen S, Denmark},
  document_type = {Article},
  doi           = {10.1017/S1351324906004256},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847289690&doi=10.1017%2fS1351324906004256&partnerID=40&md5=b20e47119434e911c56c063423f9c5b1},
}

@Article{Popović2006616,
  author        = {Popović, M. and Stein, D. and Ney, H.},
  title         = {Statistical machine translation of german compound words},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2006},
  volume        = {4139 LNAI},
  pages         = {616-624},
  note          = {cited By 20},
  abstract      = {German compound words pose special problems to statistical machine translation systems: the occurence of each of the components in the training data is not sufficient for successful translation. Even if the compound itself has been seen during training, the system may not be capable of translating it properly into two or more words. If German is the target language, the system might generate only separated components or may not be capable of choosing the correct compound. In this work, we investigate and compare different strategies for the treatment of German compound words in statistical machine translation systems. For translation from German, we compare linguistic-based and corpusbased compound splitting. For translation into German, we investigate splitting and rejoining German compounds, as well as joining English potential components. Additionaly, we investigate word alignments enhanced with knowledge about the splitting points of German compounds. The translation quality is consistently improved by all methods for both translation directions. © Springer-Verlag Berlin Heidelberg 2006.},
  affiliation   = {Informatik VI, Computer Science Department, RWTH Aachen University, Ahornstrasse 55, 52056 Aachen, Germany},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749674758&partnerID=40&md5=5a2ba01a1897f9b2a062138b3e9f05d3},
}

@Article{Airio2006,
  author   = {Airio, Eija},
  title    = {Word normalization and decompounding in mono- and bilingual IR},
  journal  = {Information Retrieval},
  year     = {2006},
  volume   = {9},
  number   = {3},
  pages    = {249--271},
  month    = {Jun},
  issn     = {1573-7659},
  abstract = {The present research studies the impact of decompounding and two different word normalization methods, stemming and lemmatization, on monolingual and bilingual retrieval. The languages in the monolingual runs are English, Finnish, German and Swedish. The source language of the bilingual runs is English, and the target languages are Finnish, German and Swedish. In the monolingual runs, retrieval in a lemmatized compound index gives almost as good results as retrieval in a decompounded index, but in the bilingual runs differences are found: retrieval in a lemmatized decompounded index performs better than retrieval in a lemmatized compound index. The reason for the poorer performance of indexes without decompounding in bilingual retrieval is the difference between the source language and target languages: phrases are used in English, while compounds are used instead of phrases in Finnish, German and Swedish. No remarkable performance differences could be found between stemming and lemmatization.},
  day      = {01},
  doi      = {10.1007/s10791-006-0884-2},
  url      = {https://doi.org/10.1007/s10791-006-0884-2},
}

@Article{Hedlund2004,
  author   = {Hedlund, Turid and Airio, Eija and Keskustalo, Heikki and Lehtokangas, Raija and Pirkola, Ari and J{\"a}rvelin, Kalervo},
  title    = {Dictionary-Based Cross-Language Information Retrieval: Learning Experiences from CLEF 2000--2002},
  journal  = {Information Retrieval},
  year     = {2004},
  volume   = {7},
  number   = {1},
  pages    = {99--119},
  month    = {Jan},
  issn     = {1573-7659},
  abstract = {In this study the basic framework and performance analysis results are presented for the three year long development process of the dictionary-based UTACLIR system. The tests expand from bilingual CLIR for three language pairs Swedish, Finnish and German to English, to six language pairs, from English to French, German, Spanish, Italian, Dutch and Finnish, and from bilingual to multilingual. In addition, transitive translation tests are reported. The development process of the UTACLIR query translation system will be regarded from the point of view of a learning process. The contribution of the individual components, the effectiveness of compound handling, proper name matching and structuring of queries are analyzed. The results and the fault analysis have been valuable in the development process. Overall the results indicate that the process is robust and can be extended to other languages. The individual effects of the different components are in general positive. However, performance also depends on the topic set and the number of compounds and proper names in the topic, and to some extent on the source and target language. The dictionaries used affect the performance significantly.},
  day      = {01},
  doi      = {10.1023/B:INRT.0000009442.34054.55},
  url      = {https://doi.org/10.1023/B:INRT.0000009442.34054.55},
}

@Article{Hollink200433,
  author          = {Hollink, V. and Kamps, J. and Monz, C. and De Rijke, M.},
  title           = {Monolingual document retrieval for European languages},
  journal         = {Information Retrieval},
  year            = {2004},
  volume          = {7},
  number          = {1-2},
  pages           = {33-52},
  note            = {cited By 54},
  abstract        = {Recent years have witnessed considerable advances in information retrieval for European languages other than English. We give an overview of commonly used techniques and we analyze them with respect to their impact on retrieval effectiveness. The techniques considered range from linguistically motivated techniques, such as morphological normalization and compound splitting, to knowledge-free approaches, such as n-gram indexing. Evaluations are carried out against data from the CLEF campaign, covering eight European languages. Our results show that for many of these languages a modicum of linguistic techniques may lead to improvements in retrieval effectiveness, as can the use of language independent techniques.},
  affiliation     = {Lang. and Inference Technology Group, ILLC, University of Amsterdam, Nieuwe Achtergracht 166, 1018 WV, Amsterdam, Netherlands; Social Science Informatics (SWI), Department of Psychology, University of Amsterdam, Amsterdam, Netherlands},
  author_keywords = {Cross-lingual information retrieval; European languages; Monolingual document retrieval; Morphological normalization; Tokenization},
  document_type   = {Review},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-3843105784&partnerID=40&md5=f9c951e94a8a59c020fd5a0c4b3bbf46},
}

@Article{Cöster2004337,
  author        = {Cöster, R. and Sahlgren, M. and Karlgren, J.},
  title         = {Selective compound splitting of swedish queries for boolean combinations of truncated terms},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2004},
  volume        = {3237},
  pages         = {337-344},
  note          = {cited By 3},
  abstract      = {In languages that use compound words such as Swedish, it is often neccessary to split compound words when indexing documents or queries. One of the problems is that it is difficult to find constituents that express a concept similar to that expressed by the compound. The approach taken here is to expand a query with the leading constituents of the compound words. Every query term is truncated so as to increase recall by hopefully finding other compounds with the leading constituent as prefix. This approach increases recall in a rather uncontrolled way, so we use a Boolean quorum-level search method to rank documents both according to a tf-idf factor but also to the number of matching Boolean combinations. The Boolean combinations performed relatively well, taking into consideration that the queries were very short (maximum of five search terms). Also included in this paper are the results of two other methods we are currently working on in our lab; one for re-ranking search results on the basis of stylistic analysis of documents, and one for dimensionality reduction using Random Indexing. © Springer-Verlag 2004.},
  affiliation   = {Swedish Institute of Computer Science, SICS, Box 1263, SE-164 29 Kista, Sweden},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048871540&partnerID=40&md5=f5b2b1834f7cad7abe59755b8a103d03},
}

@Conference{Ordelman2003225,
  author        = {Ordelman, R. and Van Hessen, A. and De Jong, F.},
  title         = {Compound decomposition in Dutch large vocabulary speech recognition},
  year          = {2003},
  pages         = {225-228},
  note          = {cited By 23},
  abstract      = {This paper addresses compound splitting for Dutch in the context of broadcast news transcription. Language models were created using original text versions and text versions that were decomposed using a data-driven compound splitting algorithm. Language model performances were compared in terms of outof- vocabulary rates and word error rates in a real-world broadcast news transcription task. It was concluded that compound splitting does improve ASR performance. Best results were obtained when frequent compounds were not decomposed.},
  affiliation   = {Department of Computer Science, University of Twente, Netherlands},
  document_type = {Conference Paper},
  journal       = {EUROSPEECH 2003 - 8th European Conference on Speech Communication and Technology},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009170957&partnerID=40&md5=9373eec435a757ed1ac5b17f6d94fcd8},
}

@Article{LIBBEN200350,
  author   = {Gary Libben and Martha Gibson and Yeo Bom Yoon and Dominiek Sandra},
  title    = {Compound fracture: The role of semantic transparency and morphological headedness},
  journal  = {Brain and Language},
  year     = {2003},
  volume   = {84},
  number   = {1},
  pages    = {50 - 64},
  issn     = {0093-934X},
  note     = {Brain and Language Special Issue},
  abstract = {This paper explores the role of semantic transparency in the representation and processing of English compounds. We focus on the question of whether semantic transparency is best viewed as a property of the entire multimorphemic string or as a property of constituent morphemes. Accordingly, we investigated the processing of English compound nouns that were categorized in terms of the semantic transparency of each of their constituents. Fully transparent such as bedroom are those in which the meanings of each of the constituents are transparently represented in the meaning of the compound as a whole. These compounds were contrasted with compounds such as strawberry, in which only the second constituent is semantically transparent, jailbird, in which only the first constituent is transparent, and hogwash, in which neither constituent is semantically transparent. We propose that significant insights can be achieved through such analysis of the transparency of individual morphemes. The two experiments that we report present evidence that both semantically transparent compounds and semantically opaque compounds show morphological constituency. The semantic transparency of the morphological head (the second constituent in a morphologically right-headed language such as English) was found to play a significant role in overall lexical decision latencies, in patterns of decomposition, and in the effects of stimulus repetition within the experiment.},
  doi      = {https://doi.org/10.1016/S0093-934X(02)00520-5},
  url      = {http://www.sciencedirect.com/science/article/pii/S0093934X02005205},
}

@InProceedings{Koehn:2003:EMC:1067807.1067833,
  author    = {Koehn, Philipp and Knight, Kevin},
  title     = {Empirical Methods for Compound Splitting},
  booktitle = {Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics - Volume 1},
  year      = {2003},
  series    = {EACL '03},
  pages     = {187--193},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1067833},
  doi       = {10.3115/1067807.1067833},
  isbn      = {1-333-56789-0},
  location  = {Budapest, Hungary},
  numpages  = {7},
  url       = {https://doi.org/10.3115/1067807.1067833},
}

@Article{HEDLUND2001147,
  author   = {Turid Hedlund and Ari Pirkola and Kalervo Järvelin},
  title    = {Aspects of Swedish morphology and semantics from the perspective of mono- and cross-language information retrieval},
  journal  = {Information Processing \& Management},
  year     = {2001},
  volume   = {37},
  number   = {1},
  pages    = {147 - 161},
  issn     = {0306-4573},
  abstract = {This paper analyzes the features of the Swedish language from the viewpoint of mono- and cross-language information retrieval (CLIR). The study was motivated by the fact that Swedish is known poorly from the IR perspective. This paper shows that Swedish has unique features, in particular gender features, the use of fogemorphemes in the formation of compound words, and a high frequency of homographic words. Especially in dictionary-based CLIR, correct word normalization and compound splitting are essential. It was shown in this study, however, that publicly available morphological analysis tools used for normalization and compound splitting have pitfalls that might decrease the effectiveness of IR and CLIR. A comparative study was performed to test the degree of lexical ambiguity in Swedish, Finnish and English. The results suggest that part-of-speech tagging might be useful in Swedish IR due to the high frequency of homographic words.},
  doi      = {https://doi.org/10.1016/S0306-4573(00)00024-8},
  keywords = {Text retrieval, Cross-language information retrieval, Swedish language, Natural language processing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306457300000248},
}

@Article{Hedlund2001147,
  author        = {Hedlund, T. and Pirkola, A. and Järvelin, K.},
  title         = {Aspects of Swedish morphology and semantics from the perspective of mono- and cross-language information retrieval},
  journal       = {Information Processing and Management},
  year          = {2001},
  volume        = {37},
  number        = {1},
  pages         = {147-161},
  note          = {cited By 30},
  abstract      = {This paper analyzes the features of the Swedish language from the viewpoint of mono- and cross-language information retrieval (CLIR). The study was motivated by the fact that Swedish is known poorly from the IR perspective. This paper shows that Swedish has unique features, in particular gender features, the use of fogemorphemes in the formation of compound words, and a high frequency of homographic words. Especially in dictionary-based CLIR, correct word normalization and compound splitting are essential. It was shown in this study, however, that publicly available morphological analysis tools used for normalization and compound splitting have pitfalls that might decrease the effectiveness of IR and CLIR. A comparative study was performed to test the degree of lexical ambiguity in Swedish, Finnish and English. The results suggest that part-of-speech tagging might be useful in Swedish IR due to the high frequency of homographic words.},
  affiliation   = {Department of Information Studies, Univ. Tampere, PO Box 607, FIN-33101, Tampere, Finland},
  document_type = {Article},
  doi           = {10.1016/S0306-4573(00)00024-8},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035152106&doi=10.1016%2fS0306-4573%2800%2900024-8&partnerID=40&md5=7a27a4aaed911888c61fa3e179606b73},
}

@InProceedings{10.1007/3-540-44645-1_20,
  author    = {Hedlund, Turid and Keskustalo, Heikki and Pirkola, Ari and Sepponen, Mikko and J{\"a}rvelin, Kalervo},
  title     = {Bilingual Tests with Swedish, Finnish, and German Queries: Dealing with Morphology, Compound Words, and Query Structure},
  booktitle = {Cross-Language Information Retrieval and Evaluation},
  year      = {2001},
  editor    = {Peters, Carol},
  pages     = {210--223},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We designed, implemented and evaluated an automated method for query construction for CLIR from Finnish, Swedish and German to English. This method seeks to automatically extract topical information from request sentences written in one of the source languages and to create a target language query, based on translations given by a translation dictionary. We paid particular attention to morphology, compound words and query structure. we tested this approach in the bilingual track of CLEF. All the source languages are compound languages, i.e., languages rich in compound words. A compound word refers to a multi-word expression where the component words are written together. Because source language request words may appear in various inflected forms not included in a translation dictionary, morphological normalization was used to aid dictionary translation. The query resulting from this process may be structured according to the translation alternatives of each source language word or remain as an unstructured word list.},
  isbn      = {978-3-540-44645-3},
}

@Misc{greghi2001processo,
  author    = {Greghi, Juliana Galvani and Martins, Ronaldo Teixeira and Nunes, Maria das Gra{\c{c}}as Volpe},
  title     = {O Processo de Desenvolvimento da BDL-NILC},
  year      = {2001},
  publisher = {S{\'e}rie de Relat{\'o}rios do NILC, NILC-TR-01-7. S{\~a}o Carlos, Outubro, 57p},
}

@Article{ranchhod2001uso,
  author  = {Ranchhod, Elisabete Marques},
  title   = {O uso de dicion{\'a}rios e de aut{\'o}matos finitos na representa{\c{c}}{\~a}o lexical das l{\'\i}nguas naturais},
  journal = {Tratamento das L{\'\i}nguas por Computador. Uma Introdu{\c{c}}{\~a}o {\`a} Lingu{\'\i}stica Computacional e suas Aplica{\c{c}}{\~o}es},
  year    = {2001},
  pages   = {13--47},
}

@Conference{Monz2001,
  author        = {Monz, C. and De Rijke, M.},
  title         = {The university of Amsterdam at CLEF 2001},
  year          = {2001},
  volume        = {1167},
  note          = {cited By 1},
  abstract      = {This paper describes the official runs of our team for CLEF-2001. We took part in the monolingual task, for Dutch, German, and Italian. The focus of our experiments was on the effects of morphological analyses such as stemming and compound splitting on retrieval effectiveness. Confirming earlier reports on retrieval in compound splitting languages such as Dutch and German, we found improvements to be around 25% for German and as much as 55% for Dutch. For Italian, lexicon-based stemming resulted in gains of up to 25%. Copyright © 2001 for the individual papers by the papers' authors.},
  affiliation   = {Institute for Logic, Language and Computation (ILLC), University of Amsterdam, Amsterdam, 1018 TV, Netherlands},
  document_type = {Conference Paper},
  journal       = {CEUR Workshop Proceedings},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921953688&partnerID=40&md5=85d483b1ce4dbd8f4a9881095044adb8},
}

@InProceedings{Kraaij:1998:CES:646631.696685,
  author    = {Kraaij, Wessel and Pohlmann, Ren{\'e}e},
  title     = {Comparing the Effect of Syntactic vs. Statistical Phrase Indexing Strategies for Dutch},
  booktitle = {Proceedings of the Second European Conference on Research and Advanced Technology for Digital Libraries},
  year      = {1998},
  series    = {ECDL '98},
  pages     = {605--617},
  address   = {London, UK, UK},
  publisher = {Springer-Verlag},
  acmid     = {696685},
  isbn      = {3-540-65101-2},
  numpages  = {13},
  url       = {http://dl.acm.org/citation.cfm?id=646631.696685},
}

@Comment{jabref-meta: databaseType:bibtex;}
