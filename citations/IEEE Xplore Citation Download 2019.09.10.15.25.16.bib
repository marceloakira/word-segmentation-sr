@ARTICLE{895039,
author={B. {Van Houdt} and C. {Blondia}},
journal={IEEE Journal on Selected Areas in Communications},
title={Analysis of an identifier splitting algorithm combined with polling (ISAP) for contention resolution in a wireless access network},
year={2000},
volume={18},
number={11},
pages={2345-2355},
abstract={A contention resolution scheme for an uplink contention channel in a wireless access network is presented. The scheme consists of a tree algorithm, namely the identifier splitting algorithm (ISA), combined with a polling scheme. Initially, ISA is used, but at a certain level of the tree, the scheme switches to polling of the stations. This scheme is further enhanced by skipping a few levels in the tree when starting the algorithm (both in a static and a dynamic way) and by allowing multiple instants simultaneously. An analytical model of the system and its variants leads to the evaluation of its performance, by means of the delay density function and the throughput characteristics. This model is used to investigate the influence of the packet arrival rate, the instant at which the ISA scheme switches to polling, the starting level of the ISA scheme, and the use of multiple instances on the mean delay, the delay quantiles, and the throughput.},
keywords={radio access networks;telecommunication congestion control;trees (mathematics);delays;packet radio networks;wireless LAN;cellular radio;time division multiple access;access protocols;identifier splitting algorithm;polling;contention resolution;wireless access network;uplink contention channel;tree algorithm;analytical model;performance evaluation;delay density function;throughput characteristics;packet arrival rate;ISA scheme starting level;mean delay;delay quantiles;throughput;wireless LAN;cellular radio;TDMA;ISAP protocol;Algorithm design and analysis;Instruction sets;Delay;Media Access Protocol;Throughput;Access protocols;Downlink;Wireless networks;Switches;Density functional theory},
doi={10.1109/49.895039},
ISSN={},
month={Nov},}
@INPROCEEDINGS{5970159,
author={B. {Dit} and L. {Guerrouj} and D. {Poshyvanyk} and G. {Antoniol}},
booktitle={2011 IEEE 19th International Conference on Program Comprehension},
title={Can Better Identifier Splitting Techniques Help Feature Location?},
year={2011},
volume={},
number={},
pages={11-20},
abstract={The paper presents an exploratory study of two feature location techniques utilizing three strategies for splitting identifiers: Camel Case, Samurai and manual splitting of identifiers. The main research question that we ask in this study is if we had a perfect technique for splitting identifiers, would it still help improve accuracy of feature location techniques applied in different scenarios and settings? In order to answer this research question we investigate two feature location techniques, one based on Information Retrieval and the other one based on the combination of Information Retrieval and dynamic analysis, for locating bugs and features using various configurations of preprocessing strategies on two open-source systems, Rhino and jEdit. The results of an extensive empirical evaluation reveal that feature location techniques using Information Retrieval can benefit from better preprocessing algorithms in some cases, and that their improvement in effectiveness while using manual splitting over state-of-the-art approaches is statistically significant in those cases. However, the results for feature location technique using the combination of Information Retrieval and dynamic analysis do not show any improvement while using manual splitting, indicating that any preprocessing technique will suffice if execution data is available. Overall, our findings outline potential benefits of putting additional research efforts into defining more sophisticated source code preprocessing techniques as they can still be useful in situations where execution information cannot be easily collected.},
keywords={information retrieval;identifier splitting technique;CamelCase;Samurai;information retrieval;preprocessing strategies;open source system;Rhino;jEdit;feature location technique;Software;Dictionaries;Gold;Manuals;Accuracy;Algorithm design and analysis;Large scale integration;feature location;information retrieval;dynamic analysis;identifier splitting algorithms},
doi={10.1109/ICPC.2011.47},
ISSN={},
month={June},}
@INPROCEEDINGS{6385106,
author={L. {Guerrouj} and P. {Galinier} and Y. {Guéhéneuc} and G. {Antoniol} and M. {Di Penta}},
booktitle={2012 19th Working Conference on Reverse Engineering},
title={TRIS: A Fast and Accurate Identifiers Splitting and Expansion Algorithm},
year={2012},
volume={},
number={},
pages={103-112},
abstract={Understanding source code identifiers, by identifying words composing them, is a necessary step for many program comprehension, reverse engineering, or redocumentation tasks. To this aim, researchers have proposed several identifier splitting and expansion approaches such as Samurai, TIDIER and more recently GenTest. The ultimate goal of such approaches is to help disambiguating conceptual information encoded in compound (or abbreviated) identifiers. This paper presents TRIS, TRee-based Identifier Splitter, a two-phases approach to split and expand program identifiers. First, TRIS pre-compiles transformed dictionary words into a tree representation, associating a cost to each transformation. In a second phase, it maps the identifier splitting/expansion problem into a minimization problem, i.e., the search of the shortest path (optimal split/expansion) in a weighted graph. We apply TRIS to a sample of 974 identifiers extracted from JHotDraw, 3,085 from Lynx, and to a sample of 489 identifiers extracted from 340 C programs. Also, we compare TRIS with GenTest on a set of 2,663 mixed Java, C and C++ identifiers. We report evidence that TRIS split (and expansion) is more accurate than state-of-the-art approaches and that it is also efficient in terms of computation time.},
keywords={identification;program diagnostics;reverse engineering;source coding;trees (mathematics);identifier splitting algorithm;identifier expansion algorithm;source code identifiers;program comprehension;reverse engineering;redocumentation tasks;Samurai;TIDIER;GenTest;TRIS;tree-based identifier splitter;two-phase approach;transformed dictionary words;tree representation;minimization problem;shortest path;weighted graph;JHotDraw;Lynx;C programs;C++ identifiers;C identifiers;Java identifiers;state-of-the-art approaches;Dictionaries;Java;Complexity theory;Buildings;Software;Context;Identifier Splitting/Expansion;Program Comprehension;Linguistic Analysis;Optimal Path;Weighted Acyclic Graph},
doi={10.1109/WCRE.2012.20},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7503746,
author={S. {Khatiwada} and M. {Kelly} and A. {Mahmoud}},
booktitle={2016 IEEE 24th International Conference on Program Comprehension (ICPC)},
title={STAC: A tool for Static Textual Analysis of Code},
year={2016},
volume={},
number={},
pages={1-3},
abstract={Static textual analysis techniques have been recently applied to process and synthesize source code. The underlying tenet is that important information is embedded in code identifiers and internal code comments. Such information can be analyzed to provide automatic aid for several software engineering activities. To facilitate this line of work, we present STAC, a tool for supporting Static Textual Analysis of Code. STAC is designed as a light-weight stand-alone tool that provides a practical one-stop solution for code indexing. Code indexing is the process of extracting important textual information from source code. Accurate indexing has been found to significantly influence the performance of code retrieval and analysis methods. STAC provides features for extracting and processing textual patterns found in Java, C++, and C# code artifacts. These features include identifier splitting, stemming, lemmatization, and spell-checking. STAC is also provided as an API to help researchers to integrate basic code indexing features into their code.},
keywords={application program interfaces;C# language;C++ language;indexing;information retrieval;Java;program diagnostics;source code (software);text analysis;static textual analysis of code;source code processing;source code synthesis;code identifiers;internal code aid;internal code comments;software engineering activities;STAC tool;light-weight stand-alone tool;code indexing;textual information extraction;code retrieval methods;code analysis methods;textual pattern processing;textual pattern extraction;Java;C++;C# code;identifier;splitting;stemming;lemmatization;spell-checking;API;Dictionaries;Feature extraction;Indexing;Natural languages;Ice;Data mining;Java},
doi={10.1109/ICPC.2016.7503746},
ISSN={},
month={May},}
@INPROCEEDINGS{801991,
author={B. {Van Houdt} and C. {Blondia} and O. {Casals} and J. {Garcia}},
booktitle={Proceedings 24th Conference on Local Computer Networks. LCN'99},
title={Packet level performance characteristics of a MAC protocol for wireless ATM LANs},
year={1999},
volume={},
number={},
pages={14-23},
abstract={This paper determines packet level performance measures of a MAC protocol for a wireless ATM local area network. A key characteristic of the MAC protocol is the identifier splitting algorithm with polling, a contention resolution scheme used to inform the base station about the bandwidth needs of a mobile station when no piggybacking can be used. We consider higher layer packets that are generated at the mobile station and investigate the influence of the traffic characteristics of the packet arrival process on the efficiency of the protocol and on the delay that packets experience to access the shared medium.},
keywords={asynchronous transfer mode;wireless LAN;access protocols;packet radio networks;telecommunication traffic;land mobile radio;time division multiple access;queueing theory;packet level performance characteristics;MAC protocol;wireless ATM LAN;performance measures;local area network;identifier splitting algorithm with polling;contention resolution;base station;bandwidth;mobile station;higher layer packets;traffic characteristics;packet arrival process;protocol efficiency;packet delay;shared medium access;time division multiplexing access;TDMA;frequency division duplex;queueing model;Media Access Protocol;Wireless application protocol;Wireless LAN;Local area networks;Asynchronous transfer mode;Bandwidth;Access protocols;Delay;Electronic mail;Switches},
doi={10.1109/LCN.1999.801991},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6624055,
author={D. {Binkley} and D. {Lawrie} and L. {Pollock} and E. {Hill} and K. {Vijay-Shanker}},
booktitle={2013 10th Working Conference on Mining Software Repositories (MSR)},
title={A dataset for evaluating identifier splitters},
year={2013},
volume={},
number={},
pages={401-404},
abstract={Software engineering and evolution techniques have recently started to exploit the natural language information in source code. A key step in doing so is splitting identifiers into their constituent words. While simple in concept, identifier splitting raises several challenging issues, leading to a range of splitting techniques. Consequently, the research community would benefit from a dataset (i.e., a gold set) that facilitates comparative studies of identifier splitting techniques. A gold set of 2,663 split identifiers was constructed from 8,522 individual human splitting judgements and can be obtained from www.cs.loyola.edu/~binkley/ludiso. This set's construction and observations aimed at its effective use are described.},
keywords={computational linguistics;program interpreters;software engineering;source coding;identifier splitter evaluation dataset;software engineering;software evolution techniques;natural language information;source code;constituent words;identifier splitting techniques;human splitting judgements;Gold;Java;Data mining;Software;Speech recognition;Educational institutions},
doi={10.1109/MSR.2013.6624055},
ISSN={},
month={May},}
@ARTICLE{6464271,
author={S. {Kpodjedo} and F. {Ricca} and P. {Galinier} and G. {Antoniol} and Y. {Guéhéneuc}},
journal={IEEE Transactions on Software Engineering},
title={MADMatch: Many-to-Many Approximate Diagram Matching for Design Comparison},
year={2013},
volume={39},
number={8},
pages={1090-1111},
abstract={Matching algorithms play a fundamental role in many important but difficult software engineering activities, especially design evolution analysis and model comparison. We present MADMatch, a fast and scalable many-to-many approximate diagram matching approach based on an error-tolerant graph matching (ETGM) formulation. Diagrams are represented as graphs, costs are assigned to possible differences between two given graphs, and the goal is to retrieve the cheapest matching. We address the resulting optimization problem with a tabu search enhanced by the novel use of lexical and structural information. Through several case studies with different types of diagrams and tasks, we show that our generic approach obtains better results than dedicated state-of-the-art algorithms, such as AURA, PLTSDiff, or UMLDiff, on the exact same datasets used to introduce (and evaluate) these algorithms.},
keywords={graph theory;optimisation;search problems;software engineering;MADMatch approach;many-to-many approximate diagram matching approach;error-tolerant graph matching;ETGM;software engineering;design evolution analysis;model comparison;design comparison;optimization problem;tabu search;lexical information;structural information;AURA algorithm;PLTSDiff algorithm;UMLDiff algorithm;Unified modeling language;Algorithm design and analysis;Software;Scalability;Software algorithms;Software engineering;Optimization;Diagram differencing;search-based software engineering;approximate graph matching;identifier splitting},
doi={10.1109/TSE.2013.9},
ISSN={},
month={Aug},}
@INPROCEEDINGS{5645490,
author={L. {Guerrouj}},
booktitle={2010 17th Working Conference on Reverse Engineering},
title={Automatic Derivation of Concepts Based on the Analysis of Source Code Identifiers},
year={2010},
volume={},
number={},
pages={301-304},
abstract={The existing software engineering literature has empirically shown that a proper choice of identifiers influences software understandability and maintainability. Indeed, identifiers are developers' main up-to-date source of information and guide their cognitive processes during program understanding when the high-level documentation is scarce or outdated and when the source code is not sufficiently commented. Deriving domain terms from identifiers using high-level and domain concepts is not an easy task when naming conventions (e.g., Camel Case) are not used or strictly followed and-or when these words have been abbreviated or otherwise transformed. Our thesis is to develop an approach that overcomes the shortcomings of the existing approaches and maps identifiers to domain concepts even in the absence of naming conventions and-or the presence of abbreviations. Our approach uses a thesaurus of words and abbreviations to map terms or transformed words composing identifiers to dictionary words. It relies on an oracle that we manually build for the validation of our results. To evaluate our technique, we apply it to derive concepts from identifiers of different systems and open source projects. We also enrich it by the use of domain knowledge and context-aware dictionaries to analyze how sensitive are its performances to the use of contextual information and specialized knowledge.},
keywords={software maintenance;system documentation;automatic derivation;source code identifiers;software engineering;software understandability;software maintainability;cognitive processes;program understanding;high-level documentation;maps identifiers;domain concepts;naming conventions;Dictionaries;Software;Conferences;Thesauri;Presses;Buildings;Speech recognition;Identifier Splitting;Program Comprehension;Linguistic Analysis;Software Quality},
doi={10.1109/WCRE.2010.45},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6405277,
author={A. {Corazza} and S. {Di Martino} and V. {Maggio}},
booktitle={2012 28th IEEE International Conference on Software Maintenance (ICSM)},
title={LINSEN: An efficient approach to split identifiers and expand abbreviations},
year={2012},
volume={},
number={},
pages={233-242},
abstract={Information Retrieval (IR) techniques are being exploited by an increasing number of tools supporting Software Maintenance activities. Indeed the lexical information embedded in the source code can be valuable for tasks such as concept location, clustering or recovery of traceability links. The application of such IR-based techniques relies on the consistency of the lexicon available in the different artifacts, and their effectiveness can worsen if programmers introduce abbreviations (e.g: rect) and/or do not strictly follow naming conventions such as Camel Case (e.g: UTFtoASCII). In this paper we propose an approach to automatically split identifiers in their composing words, and expand abbreviations. The solution is based on a graph model and performs in linear time with respect to the size of the dictionary, taking advantage of an approximate string matching technique. The proposed technique exploits a number of different dictionaries, referring to increasingly broader contexts, in order to achieve a disambiguation strategy based on the knowledge gathered from the most appropriate domain. The approach has been compared to other splitting and expansion techniques, using freely available oracles for the identifiers extracted from 24 C/C++ and Java open source systems. Results show an improvement in both splitting and expanding performance, in addition to a strong enhancement in the computational efficiency.},
keywords={C++ language;information retrieval;Java;pattern clustering;public domain software;software maintenance;LINSEN;identifier splitting;abbreviation expansion;information retrieval techniques;software maintenance activities;lexical information;concept location;traceability links clustering;traceability links recovery;IR-based techniques;camel case;dictionary;approximate string matching technique;disambiguation strategy;C-C++;Java open source systems;oracles;Dictionaries;Software algorithms;Approximation algorithms;Context;Software maintenance;Conferences;Source Code Identifiers;Program Comprehension;Splitting;Expansion},
doi={10.1109/ICSM.2012.6405277},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{5069482,
author={E. {Enslen} and E. {Hill} and L. {Pollock} and K. {Vijay-Shanker}},
booktitle={2009 6th IEEE International Working Conference on Mining Software Repositories},
title={Mining source code to automatically split identifiers for software analysis},
year={2009},
volume={},
number={},
pages={71-80},
abstract={Automated software engineering tools (e.g., program search, concern location, code reuse, quality assessment, etc.) increasingly rely on natural language information from comments and identifiers in code. The first step in analyzing words from identifiers requires splitting identifiers into their constituent words. Unlike natural languages, where space and punctuation are used to delineate words, identifiers cannot contain spaces. One common way to split identifiers is to follow programming language naming conventions. For example, Java programmers often use camel case, where words are delineated by uppercase letters or non-alphabetic characters. However, programmers also create identifiers by concatenating sequences of words together with no discernible delineation, which poses challenges to automatic identifier splitting. In this paper, we present an algorithm to automatically split identifiers into sequences of words by mining word frequencies in source code. With these word frequencies, our identifier splitter uses a scoring technique to automatically select the most appropriate partitioning for an identifier. In an evaluation of over 8000 identifiers from open source Java programs, our Samurai approach outperforms the existing state of the art techniques.},
keywords={data mining;program diagnostics;software maintenance;source code mining;automatic split identifier;software analysis;word frequency mining;software maintenance;Software maintenance;Natural languages;Programming profession;Software tools;Java;Frequency;Open source software;Software quality;Information analysis;Quality assessment},
doi={10.1109/MSR.2009.5069482},
ISSN={},
month={May},}
@INPROCEEDINGS{6405328,
author={D. {Binkley} and D. {Lawrie} and C. {Uehlinger}},
booktitle={2012 28th IEEE International Conference on Software Maintenance (ICSM)},
title={Vocabulary normalization improves IR-based concept location},
year={2012},
volume={},
number={},
pages={588-591},
abstract={Tool support is crucial to modern software development, evolution, and maintenance. Early tools reused the static analysis performed by the compiler. These were followed by dynamic analysis tools and more recently tools that exploit natural language. This later class has the advantage that it can incorporate not only the code, but artifacts from all phases of software construction and its subsequent evolution. Unfortunately, the natural language found in source code often uses a vocabulary different from that used in other software artifacts and thus increases the vocabulary mismatch problem. This problem exists because many natural-language tools imported from Information Retrieval (IR) and Natural Language Processing (NLP) implicitly assume the use of a single natural language vocabulary. Vocabulary normalization, which goes well beyond simple identifier splitting, brings the vocabulary of the source into line with other artifacts. Consequently, it is expected to improve the performance of existing and future IR and NLP based tools. As a case study, an experiment with an LSI-based feature locator is replicated. Normalization universally improves performance. For the tersest queries, this improvement is over 180% (p <; 0.0001).},
keywords={computational linguistics;information retrieval;natural language processing;program compilers;program diagnostics;software maintenance;software tools;system monitoring;vocabulary normalization;IR-based concept location;software development;software evolution;software maintenance;static analysis;compiler;dynamic analysis tools;natural language processing;source code;vocabulary mismatch;NLP;information retrieval;natural language vocabulary;LSI-based feature locator;tool support;Vocabulary;Software maintenance;Conferences;Natural language processing;Educational institutions;vocabulary normalization;information retrieval;concept location},
doi={10.1109/ICSM.2012.6405328},
ISSN={},
month={Sep.},}
