% Encoding: UTF-8

@Article{Carvalho:2015:SCI:2948288.2948331,
  author     = {Carvalho, Nuno Ramos and Almeida, Jos{\'e} Jo\~{a}o and Henriques, Pedro Rangel and Varanda, Maria Jo\~{a}o},
  title      = {From Source Code Identifiers to Natural Language Terms},
  journal    = {J. Syst. Softw.},
  year       = {2015},
  volume     = {100},
  number     = {C},
  pages      = {117--128},
  month      = feb,
  issn       = {0164-1212},
  acmid      = {2948331},
  address    = {New York, NY, USA},
  doi        = {10.1016/j.jss.2014.10.013},
  issue_date = {February 2015},
  keywords   = {Identifier splitting, Natural language processing, Program comprehension},
  numpages   = {12},
  publisher  = {Elsevier Science Inc.},
  url        = {http://dx.doi.org/10.1016/j.jss.2014.10.013},
}

@InProceedings{Bretschneider:2015:SSG:2955018.2955044,
  author    = {Bretschneider, Claudia and Zillner, Sonja},
  title     = {Semantic Splitting of German Medical Compounds},
  booktitle = {Proceedings of the 18th International Conference on Text, Speech, and Dialogue - Volume 9302},
  year      = {2015},
  series    = {TSD 2015},
  pages     = {207--215},
  address   = {New York, NY, USA},
  publisher = {Springer-Verlag New York, Inc.},
  acmid     = {2955044},
  doi       = {10.1007/978-3-319-24033-6_24},
  isbn      = {978-3-319-24032-9},
  keywords  = {Compound splitting, Medical NLP, Ontology, Semantics},
  location  = {Pilsen, Czech Republic},
  numpages  = {9},
  url       = {http://dx.doi.org/10.1007/978-3-319-24033-6_24},
}

@Article{Pecina:2014:AMT:2657512.2657842,
  author     = {Pecina, Pavel and Du\v{s}ek, Ond\v{r}ej and Goeuriot, Lorraine and Haji\v{c}, Jan and Hlav\'{a}\v{c}ov\'{a}, Jaroslava and Jones, Gareth J. F. and Kelly, Liadh and Leveling, Johannes and Mare\v{c}ek, David and Nov\'{a}k, Michal and Popel, Martin and Rosa, Rudolf and Tamchyna, Ale\v{s} and Ure\v{s}ov\'{a}, Zde\v{n}ka},
  title      = {Adaptation of Machine Translation for Multilingual Information Retrieval in the Medical Domain},
  journal    = {Artif. Intell. Med.},
  year       = {2014},
  volume     = {61},
  number     = {3},
  pages      = {165--185},
  month      = jul,
  issn       = {0933-3657},
  acmid      = {2657842},
  address    = {Essex, UK},
  doi        = {10.1016/j.artmed.2014.01.004},
  issue_date = {July, 2014},
  keywords   = {Compound splitting, Cross-language information retrieval, Domain adaptation of statistical machine translation, Intelligent training data selection for machine translation, Medical query translation, Statistical machine translation},
  numpages   = {21},
  publisher  = {Elsevier Science Publishers Ltd.},
  url        = {http://dx.doi.org/10.1016/j.artmed.2014.01.004},
}

@Article{Hill:2014:ESI:2683115.2683127,
  author     = {Hill, Emily and Binkley, David and Lawrie, Dawn and Pollock, Lori and Vijay-Shanker, K.},
  title      = {An Empirical Study of Identifier Splitting Techniques},
  journal    = {Empirical Softw. Engg.},
  year       = {2014},
  volume     = {19},
  number     = {6},
  pages      = {1754--1780},
  month      = dec,
  issn       = {1382-3256},
  acmid      = {2683127},
  address    = {Hingham, MA, USA},
  doi        = {10.1007/s10664-013-9261-0},
  issue_date = {December 2014},
  keywords   = {Identifier names, Program comprehension, Software engineering tools, Source code text analysis},
  numpages   = {27},
  publisher  = {Kluwer Academic Publishers},
  url        = {http://dx.doi.org/10.1007/s10664-013-9261-0},
}

@Article{Guerrouj:2014:EIE:2683115.2683136,
  author     = {Guerrouj, Latifa and Penta, Massimiliano and Gu{\'e}h{\'e}neuc, Yann-Ga\"{e}l and Antoniol, Giuliano},
  title      = {An Experimental Investigation on the Effects of Context on Source Code Identifiers Splitting and Expansion},
  journal    = {Empirical Softw. Engg.},
  year       = {2014},
  volume     = {19},
  number     = {6},
  pages      = {1706--1753},
  month      = dec,
  issn       = {1382-3256},
  acmid      = {2683136},
  address    = {Hingham, MA, USA},
  doi        = {10.1007/s10664-013-9260-1},
  issue_date = {December 2014},
  keywords   = {Identifier splitting and expansion, Program understanding, Task context},
  numpages   = {48},
  publisher  = {Kluwer Academic Publishers},
  url        = {http://dx.doi.org/10.1007/s10664-013-9260-1},
}

@Proceedings{Aguiar:2014:2645892,
  title     = {MobiArch '14: Proceedings of the 9th ACM Workshop on Mobility in the Evolving Internet Architecture},
  year      = {2014},
  address   = {New York, NY, USA},
  publisher = {ACM},
  isbn      = {978-1-4503-3074-9},
  location  = {Maui, Hawaii, USA},
}

@InProceedings{Kalyanakrishnan:2014:BDT:2661829.2662044,
  author    = {Kalyanakrishnan, Shivaram and Singh, Deepthi and Kant, Ravi},
  title     = {On Building Decision Trees from Large-scale Data in Applications of On-line Advertising},
  booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
  year      = {2014},
  series    = {CIKM '14},
  pages     = {669--678},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2662044},
  doi       = {10.1145/2661829.2662044},
  isbn      = {978-1-4503-2598-1},
  keywords  = {categorical features, clustering, cross-validation, decision trees, on-line advertising},
  location  = {Shanghai, China},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/2661829.2662044},
}

@InProceedings{Binkley:2013:DEI:2487085.2487158,
  author    = {Binkley, David and Lawrie, Dawn and Pollock, Lori and Hill, Emily and Vijay-Shanker, K.},
  title     = {A Dataset for Evaluating Identifier Splitters},
  booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
  year      = {2013},
  series    = {MSR '13},
  pages     = {401--404},
  address   = {Piscataway, NJ, USA},
  publisher = {IEEE Press},
  acmid     = {2487158},
  isbn      = {978-1-4673-2936-1},
  location  = {San Francisco, CA, USA},
  numpages  = {4},
  url       = {http://dl.acm.org/citation.cfm?id=2487085.2487158},
}

@Article{Kpodjedo:2013:MMA:2516408.2516487,
  author     = {Kpodjedo, Segla and Ricca, Filippo and Galinier, Philippe and Antoniol, Giuliano and Gueheneuc, Yann-Gael},
  title      = {MADMatch: Many-to-Many Approximate Diagram Matching for Design Comparison},
  journal    = {IEEE Trans. Softw. Eng.},
  year       = {2013},
  volume     = {39},
  number     = {8},
  pages      = {1090--1111},
  month      = aug,
  issn       = {0098-5589},
  acmid      = {2516487},
  address    = {Piscataway, NJ, USA},
  doi        = {10.1109/TSE.2013.9},
  issue_date = {August 2013},
  keywords   = {Unified modeling language,Algorithm design and analysis,Software,Scalability,Software algorithms,Software engineering,Optimization,identifier splitting,Diagram differencing,search-based software engineering,approximate graph matching},
  numpages   = {22},
  publisher  = {IEEE Press},
  url        = {http://dx.doi.org/10.1109/TSE.2013.9},
}

@Proceedings{Jana:2013:2505906,
  title     = {MobiArch '13: Proceedings of the Eighth ACM International Workshop on Mobility in the Evolving Internet Architecture},
  year      = {2013},
  address   = {New York, NY, USA},
  publisher = {ACM},
  isbn      = {978-1-4503-2366-6},
  note      = {533133},
  location  = {Miami, Florida, USA},
}

@PhdThesis{Biggers:2012:IEC:2520238,
  author    = {Biggers, Lauren R.},
  title     = {Investigating the Effect of Corpus Construction on Latent Dirichlet Allocation Based Feature Location},
  year      = {2012},
  address   = {Tuscaloosa, AL, USA},
  note      = {AAI3539960},
  advisor   = {Kraft, Nicholas A.},
  isbn      = {978-1-267-64503-6},
  publisher = {University of Alabama},
}

@InProceedings{Srinivasan:2012:SWH:2396761.2398410,
  author    = {Srinivasan, Sriram and Bhattacharya, Sourangshu and Chakraborty, Rudrasis},
  title     = {Segmenting Web-domains and Hashtags Using Length Specific Models},
  booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
  year      = {2012},
  series    = {CIKM '12},
  pages     = {1113--1122},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2398410},
  doi       = {10.1145/2396761.2398410},
  isbn      = {978-1-4503-1156-4},
  keywords  = {compound splitting, hashtag segmentation, structured learning, web domain segmentation, word segmentation},
  location  = {Maui, Hawaii, USA},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/2396761.2398410},
}

@InProceedings{Sureka:2012:SCI:2384416.2384417,
  author    = {Sureka, Ashish},
  title     = {Source Code Identifier Splitting Using Yahoo Image and Web Search Engine},
  booktitle = {Proceedings of the First International Workshop on Software Mining},
  year      = {2012},
  series    = {SoftwareMining '12},
  pages     = {1--8},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2384417},
  doi       = {10.1145/2384416.2384417},
  isbn      = {978-1-4503-1560-9},
  keywords  = {Yahoo similarity distance, identifier splitting, identifier tokenization, mining software repositories, program comprehension},
  location  = {Beijing, China},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/2384416.2384417},
}

@InProceedings{Biggers:2012:EIR:2184512.2184551,
  author    = {Biggers, Lauren R.},
  title     = {The Effects of Identifier Retention and Stop Word Removal on a Latent Dirichlet Allocation Based Feature Location Technique},
  booktitle = {Proceedings of the 50th Annual Southeast Regional Conference},
  year      = {2012},
  series    = {ACM-SE '12},
  pages     = {164--169},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2184551},
  doi       = {10.1145/2184512.2184551},
  isbn      = {978-1-4503-1203-5},
  keywords  = {feature location, information retrieval, program comprehension, software evolution and maintenance, static analysis},
  location  = {Tuscaloosa, Alabama},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/2184512.2184551},
}

@InProceedings{Guerrouj:2012:TFA:2420240.2420450,
  author    = {Guerrouj, Latifa and Galinier, Philippe and Gueheneuc, Yann-Gael and Antoniol, Giuliano and Di Penta, Massimiliano},
  title     = {TRIS: A Fast and Accurate Identifiers Splitting and Expansion Algorithm},
  booktitle = {Proceedings of the 2012 19th Working Conference on Reverse Engineering},
  year      = {2012},
  series    = {WCRE '12},
  pages     = {103--112},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid     = {2420450},
  doi       = {10.1109/WCRE.2012.20},
  isbn      = {978-0-7695-4891-3},
  keywords  = {Identifier Splitting/Expansion, Program Comprehension, Linguistic Analysis, Optimal Path, Weighted Acyclic Graph},
  numpages  = {10},
  url       = {http://dx.doi.org/10.1109/WCRE.2012.20},
}

@InProceedings{Lawrie:2012:VNI:2473496.2473614,
  author    = {Lawrie, Dawn and Uehlinger, Christopher and Binkley, Dave},
  title     = {Vocabulary Normalization Improves IR-based Concept Location},
  booktitle = {Proceedings of the 2012 IEEE International Conference on Software Maintenance (ICSM)},
  year      = {2012},
  series    = {ICSM '12},
  pages     = {588--591},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid     = {2473614},
  doi       = {10.1109/ICSM.2012.6405328},
  isbn      = {978-1-4673-2313-0},
  keywords  = {Vocabulary,Software maintenance,Conferences,Natural language processing,Educational institutions,concept location,vocabulary normalization,information retrieval},
  numpages  = {4},
  url       = {http://dx.doi.org/10.1109/ICSM.2012.6405328},
}

@InProceedings{Dit:2011:BIS:2057176.2057205,
  author    = {Dit, Bogdan and Guerrouj, Latifa and Poshyvanyk, Denys and Antoniol, Giuliano},
  title     = {Can Better Identifier Splitting Techniques Help Feature Location?},
  booktitle = {Proceedings of the 2011 IEEE 19th International Conference on Program Comprehension},
  year      = {2011},
  series    = {ICPC '11},
  pages     = {11--20},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid     = {2057205},
  doi       = {10.1109/ICPC.2011.47},
  isbn      = {978-0-7695-4398-7},
  keywords  = {feature location, information retrieval, dynamic analysis, identifier splitting algorithms},
  numpages  = {10},
  url       = {https://doi.org/10.1109/ICPC.2011.47},
}

@InProceedings{Macherey:2011:LCS:2002472.2002644,
  author    = {Macherey, Klaus and Dai, Andrew M. and Talbot, David and Popat, Ashok C. and Och, Franz},
  title     = {Language-independent Compound Splitting with Morphological Operations},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
  year      = {2011},
  series    = {HLT '11},
  pages     = {1395--1404},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {2002644},
  isbn      = {978-1-932432-87-9},
  location  = {Portland, Oregon},
  numpages  = {10},
  url       = {http://dl.acm.org/citation.cfm?id=2002472.2002644},
}

@InProceedings{Wang:2011:WSN:1963405.1963457,
  author    = {Wang, Kuansan and Thrasher, Christopher and Hsu, Bo-June Paul},
  title     = {Web Scale NLP: A Case Study on Url Word Breaking},
  booktitle = {Proceedings of the 20th International Conference on World Wide Web},
  year      = {2011},
  series    = {WWW '11},
  pages     = {357--366},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1963457},
  doi       = {10.1145/1963405.1963457},
  isbn      = {978-1-4503-0632-4},
  keywords  = {compound splitting, multi-style language model, url segmentation, web scale word breaking, word segmentation},
  location  = {Hyderabad, India},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/1963405.1963457},
}

@InProceedings{Guerrouj:2010:ADC:1919284.1919610,
  author    = {Guerrouj, Latifa},
  title     = {Automatic Derivation of Concepts Based on the Analysis of Source Code Identifiers},
  booktitle = {Proceedings of the 2010 17th Working Conference on Reverse Engineering},
  year      = {2010},
  series    = {WCRE '10},
  pages     = {301--304},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid     = {1919610},
  doi       = {10.1109/WCRE.2010.45},
  isbn      = {978-0-7695-4123-5},
  keywords  = {Identifier Splitting, Program Comprehension, Linguistic Analysis, Software Quality},
  numpages  = {4},
  url       = {http://dx.doi.org/10.1109/WCRE.2010.45},
}

@InProceedings{Fritzinger:2010:ABD:1868850.1868884,
  author    = {Fritzinger, Fabienne and Fraser, Alexander},
  title     = {How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing},
  booktitle = {Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR},
  year      = {2010},
  series    = {WMT '10},
  pages     = {224--234},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1868884},
  isbn      = {978-1-932432-71-8},
  location  = {Uppsala, Sweden},
  numpages  = {11},
  url       = {http://dl.acm.org/citation.cfm?id=1868850.1868884},
}

@Proceedings{Venkataramani:2010:1859983,
  title     = {MobiArch '10: Proceedings of the Fifth ACM International Workshop on Mobility in the Evolving Internet Architecture},
  year      = {2010},
  address   = {New York, NY, USA},
  publisher = {ACM},
  isbn      = {978-1-4503-0143-5},
  note      = {533104},
  location  = {Chicago, Illinois, USA},
}

@InProceedings{Yang:2010:RMM:1909632.1911836,
  author    = {Yang, Xin and Li, Chong and Ji, Xin-sheng},
  title     = {Research on the Mobility Management Scheme in Heterogeneous Network},
  booktitle = {Proceedings of the 2010 International Conference on Electrical and Control Engineering},
  year      = {2010},
  series    = {ICECE '10},
  pages     = {4759--4763},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid     = {1911836},
  doi       = {10.1109/iCECE.2010.1152},
  isbn      = {978-0-7695-4031-3},
  keywords  = {Heterogeneous Networks, Integration, Mobile management, ISMSTS},
  numpages  = {5},
  url       = {https://doi.org/10.1109/iCECE.2010.1152},
}

@InProceedings{Zeman:2010:UTP:1887176.1887206,
  author    = {Zeman, Daniel},
  title     = {Using TectoMT As a Preprocessing Tool for Phrase-based Statistical Machine Translation},
  booktitle = {Proceedings of the 13th International Conference on Text, Speech and Dialogue},
  year      = {2010},
  series    = {TSD'10},
  pages     = {216--223},
  address   = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  acmid     = {1887206},
  isbn      = {3-642-15759-9, 978-3-642-15759-2},
  keywords  = {phrase-based translation, preprocessing, reordering},
  location  = {Brno, Czech Republic},
  numpages  = {8},
  url       = {http://dl.acm.org/citation.cfm?id=1887176.1887206},
}

@InProceedings{Hanka:2009:NDN:1547552.1547818,
  author    = {Hanka, Oliver and Splei{\$\beta\$}, Christoph and Kunzmann, Gerald and Ebersp\"{a}cher, J\"{o}rg},
  title     = {A Novel DHT-Based Network Architecture for the Next Generation Internet},
  booktitle = {Proceedings of the 2009 Eighth International Conference on Networks},
  year      = {2009},
  series    = {ICN '09},
  pages     = {332--341},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid     = {1547818},
  doi       = {10.1109/ICN.2009.12},
  isbn      = {978-0-7695-3552-4},
  keywords  = {NGI, DHT, DNS, Routing, Locator/Identifier Split},
  numpages  = {10},
  url       = {https://doi.org/10.1109/ICN.2009.12},
}

@InProceedings{Khaitan:2009:DCS:1645953.1645982,
  author    = {Khaitan, Sanjeet and Das, Arumay and Gain, Sandeep and Sampath, Adithi},
  title     = {Data-driven Compound Splitting Method for English Compounds in Domain Names},
  booktitle = {Proceedings of the 18th ACM Conference on Information and Knowledge Management},
  year      = {2009},
  series    = {CIKM '09},
  pages     = {207--214},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1645982},
  doi       = {10.1145/1645953.1645982},
  isbn      = {978-1-60558-512-3},
  keywords  = {compound splitting, domain names},
  location  = {Hong Kong, China},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/1645953.1645982},
}

@InProceedings{Enslen:2009:MSC:1590955.1591139,
  author    = {Enslen, Eric and Hill, Emily and Pollock, Lori and Vijay-Shanker, K.},
  title     = {Mining Source Code to Automatically Split Identifiers for Software Analysis},
  booktitle = {Proceedings of the 2009 6th IEEE International Working Conference on Mining Software Repositories},
  year      = {2009},
  series    = {MSR '09},
  pages     = {71--80},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid     = {1591139},
  doi       = {10.1109/MSR.2009.5069482},
  isbn      = {978-1-4244-3493-0},
  numpages  = {10},
  url       = {http://dx.doi.org/10.1109/MSR.2009.5069482},
}

@InProceedings{Stymne:2008:GCF:1432430.1432474,
  author    = {Stymne, Sara},
  title     = {German Compounds in Factored Statistical Machine Translation},
  booktitle = {Proceedings of the 6th International Conference on Advances in Natural Language Processing},
  year      = {2008},
  series    = {GoTAL '08},
  pages     = {464--475},
  address   = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  acmid     = {1432474},
  doi       = {10.1007/978-3-540-85287-2_44},
  isbn      = {978-3-540-85286-5},
  location  = {Gothenburg, Sweden},
  numpages  = {12},
  url       = {http://dx.doi.org/10.1007/978-3-540-85287-2_44},
}

@InProceedings{Holz:2008:UKL:1787578.1787592,
  author    = {Holz, Florian and Biemann, Chris},
  title     = {Unsupervised and Knowledge-free Learning of Compound Splits and Periphrases},
  booktitle = {Proceedings of the 9th International Conference on Computational Linguistics and Intelligent Text Processing},
  year      = {2008},
  series    = {CICLing'08},
  pages     = {117--127},
  address   = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  acmid     = {1787592},
  isbn      = {3-540-78134-X, 978-3-540-78134-9},
  location  = {Haifa, Israel},
  numpages  = {11},
  url       = {http://dl.acm.org/citation.cfm?id=1787578.1787592},
}

@Article{Ahlgren:2007:ISS:1276104.1276111,
  author     = {Ahlgren, Per and Kek\"{a}l\"{a}inen, Jaana},
  title      = {Indexing Strategies for Swedish Full Text Retrieval Under Different User Scenarios},
  journal    = {Inf. Process. Manage.},
  year       = {2007},
  volume     = {43},
  number     = {1},
  pages      = {81--102},
  month      = jan,
  issn       = {0306-4573},
  acmid      = {1276111},
  address    = {Tarrytown, NY, USA},
  doi        = {10.1016/j.ipm.2006.03.003},
  issue_date = {January 2007},
  keywords   = {base word form index, discounted cumulated gain, indexing strategy, inflected word form index, truncation, user scenario},
  numpages   = {22},
  publisher  = {Pergamon Press, Inc.},
  url        = {http://dx.doi.org/10.1016/j.ipm.2006.03.003},
}

@Proceedings{Fu:2007:1366919,
  title     = {MobiArch '07: Proceedings of 2Nd ACM/IEEE International Workshop on Mobility in the Evolving Internet Architecture},
  year      = {2007},
  address   = {New York, NY, USA},
  publisher = {ACM},
  isbn      = {978-1-59593-784-1},
  location  = {Kyoto, Japan},
}

@Article{Krioukov:2007:CRI:1273445.1273450,
  author     = {Krioukov, Dmitri and claffy, k c and Fall, Kevin and Brady, Arthur},
  title      = {On Compact Routing for the Internet},
  journal    = {SIGCOMM Comput. Commun. Rev.},
  year       = {2007},
  volume     = {37},
  number     = {3},
  pages      = {41--52},
  month      = jul,
  issn       = {0146-4833},
  acmid      = {1273450},
  address    = {New York, NY, USA},
  doi        = {10.1145/1273445.1273450},
  issue_date = {July 2007},
  keywords   = {compact routing, internet routing, routing scalability},
  numpages   = {12},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1273445.1273450},
}

@Article{Pedersen:2007:USL:1230769.1230773,
  author     = {Pedersen, Bolette Sandford},
  title      = {Using Shallow Linguistic Analysis to Improve Search on Danish Compounds},
  journal    = {Nat. Lang. Eng.},
  year       = {2007},
  volume     = {13},
  number     = {1},
  pages      = {75--90},
  month      = mar,
  issn       = {1351-3249},
  acmid      = {1230773},
  address    = {New York, NY, USA},
  doi        = {10.1017/S1351324906004256},
  issue_date = {March 2007},
  numpages   = {16},
  publisher  = {Cambridge University Press},
  url        = {http://dx.doi.org/10.1017/S1351324906004256},
}

@Article{VanHoudt:2006:AIS:2312123.2313377,
  author     = {Van Houdt, B. and Blondia, C.},
  title      = {Analysis of an Identifier Splitting Algorithm Combined with Polling (ISAP) for Contention Resolution in a Wireless Access Network},
  journal    = {IEEE J.Sel. A. Commun.},
  year       = {2006},
  volume     = {18},
  number     = {11},
  pages      = {2345--2355},
  month      = sep,
  issn       = {0733-8716},
  acmid      = {2313377},
  address    = {Piscataway, NJ, USA},
  doi        = {10.1109/49.895039},
  issue_date = {September 2006},
  numpages   = {11},
  publisher  = {IEEE Press},
  url        = {http://dx.doi.org/10.1109/49.895039},
}

@Proceedings{Fu:2006:1186699,
  title     = {MobiArch '06: Proceedings of First ACM/IEEE International Workshop on Mobility in the Evolving Internet Architecture},
  year      = {2006},
  address   = {New York, NY, USA},
  publisher = {ACM},
  isbn      = {1-59593-566-5},
  location  = {San Francisco, California},
}

@InProceedings{Popovic:2006:SMT:2091189.2091252,
  author    = {Popovi\'{c}, Maja and Stein, Daniel and Ney, Hermann},
  title     = {Statistical Machine Translation of German Compound Words},
  booktitle = {Proceedings of the 5th International Conference on Advances in Natural Language Processing},
  year      = {2006},
  series    = {FinTAL'06},
  pages     = {616--624},
  address   = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  acmid     = {2091252},
  doi       = {10.1007/11816508_61},
  isbn      = {3-540-37334-9, 978-3-540-37334-6},
  location  = {Turku, Finland},
  numpages  = {9},
  url       = {http://dx.doi.org/10.1007/11816508_61},
}

@Article{Ahlgren:2006:SFT:1164975.1164991,
  author     = {Ahlgren, Per and Kek\"{a}l\"{a}inen, Jaana},
  title      = {Swedish Full Text Retrieval: Effectiveness of Different Combinations of Indexing Strategies with Query Terms},
  journal    = {Inf. Retr.},
  year       = {2006},
  volume     = {9},
  number     = {6},
  pages      = {681--697},
  month      = dec,
  issn       = {1386-4564},
  acmid      = {1164991},
  address    = {Hingham, MA, USA},
  doi        = {10.1007/s10791-006-9009-1},
  issue_date = {December 2006},
  keywords   = {Indexing strategies, Morphological analysis, Stemming, Swedish, Truncation},
  numpages   = {17},
  publisher  = {Kluwer Academic Publishers},
  url        = {http://dx.doi.org/10.1007/s10791-006-9009-1},
}

@Article{Hollink:2004:MDR:961294.961310,
  author     = {Hollink, Vera and Kamps, Jaap and Monz, Christof and De Rijke, Maarten},
  title      = {Monolingual Document Retrieval for European Languages},
  journal    = {Inf. Retr.},
  year       = {2004},
  volume     = {7},
  number     = {1-2},
  pages      = {33--52},
  month      = jan,
  issn       = {1386-4564},
  acmid      = {961310},
  address    = {Norwell, MA, USA},
  doi        = {10.1023/B:INRT.0000009439.19151.4c},
  issue_date = {January-April 2004},
  keywords   = {European languages, cross-lingual information retrieval, monolingual document retrieval, morphological normalization, tokenization},
  numpages   = {20},
  publisher  = {Kluwer Academic Publishers},
  url        = {https://doi.org/10.1023/B:INRT.0000009439.19151.4c},
}

@InProceedings{Koehn:2003:EMC:1067807.1067833,
  author    = {Koehn, Philipp and Knight, Kevin},
  title     = {Empirical Methods for Compound Splitting},
  booktitle = {Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics - Volume 1},
  year      = {2003},
  series    = {EACL '03},
  pages     = {187--193},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1067833},
  doi       = {10.3115/1067807.1067833},
  isbn      = {1-333-56789-0},
  location  = {Budapest, Hungary},
  numpages  = {7},
  url       = {https://doi.org/10.3115/1067807.1067833},
}

@InProceedings{Hedlund:2001:UCE:648264.753394,
  author    = {Hedlund, Turid and Keskustalo, Heikki and Pirkola, Ari and Airio, Eija and J\"{a}rvelin, Kalervo},
  title     = {Utaclir @ CLEF 2001 - Effects of Compound Splitting and N-Gram Techniques},
  booktitle = {Revised Papers from the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems},
  year      = {2002},
  series    = {CLEF '01},
  pages     = {118--136},
  address   = {London, UK, UK},
  publisher = {Springer-Verlag},
  acmid     = {753394},
  isbn      = {3-540-44042-9},
  numpages  = {19},
  url       = {http://dl.acm.org/citation.cfm?id=648264.753394},
}

@InProceedings{Vries:2000:PMA:648263.753372,
  author    = {Vries, Arjen P. de},
  title     = {A Poor Man's Approach to CLEF},
  booktitle = {Revised Papers from the Workshop of Cross-Language Evaluation Forum on Cross-Language Information Retrieval and Evaluation},
  year      = {2001},
  series    = {CLEF '00},
  pages     = {149--155},
  address   = {London, UK, UK},
  publisher = {Springer-Verlag},
  acmid     = {753372},
  isbn      = {3-540-42446-6},
  numpages  = {7},
  url       = {http://dl.acm.org/citation.cfm?id=648263.753372},
}

@InProceedings{VanHoudt:1999:PLP:788014.788445,
  author    = {Van Houdt, B. and Blondia, C. and Casals, O. and Garcia, J.},
  title     = {Packet Level Performance Characteristics of a MAC Protocol for Wireless ATM LANs},
  booktitle = {Proceedings of the 24th Annual IEEE Conference on Local Computer Networks},
  year      = {1999},
  series    = {LCN '99},
  pages     = {14--},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid     = {788445},
  isbn      = {0-7695-0309-8},
  keywords  = {Wireless ATM, MAC protocol, packet level performance},
  url       = {http://dl.acm.org/citation.cfm?id=788014.788445},
}

@InProceedings{Kraaij:1998:CES:646631.696685,
  author    = {Kraaij, Wessel and Pohlmann, Ren{\'e}e},
  title     = {Comparing the Effect of Syntactic vs. Statistical Phrase Indexing Strategies for Dutch},
  booktitle = {Proceedings of the Second European Conference on Research and Advanced Technology for Digital Libraries},
  year      = {1998},
  series    = {ECDL '98},
  pages     = {605--617},
  address   = {London, UK, UK},
  publisher = {Springer-Verlag},
  acmid     = {696685},
  isbn      = {3-540-65101-2},
  numpages  = {13},
  url       = {http://dl.acm.org/citation.cfm?id=646631.696685},
}

@InProceedings{7503746,
  author    = {S. {Khatiwada} and M. {Kelly} and A. {Mahmoud}},
  title     = {STAC: A tool for Static Textual Analysis of Code},
  booktitle = {2016 IEEE 24th International Conference on Program Comprehension (ICPC)},
  year      = {2016},
  pages     = {1-3},
  month     = {May},
  abstract  = {Static textual analysis techniques have been recently applied to process and synthesize source code. The underlying tenet is that important information is embedded in code identifiers and internal code comments. Such information can be analyzed to provide automatic aid for several software engineering activities. To facilitate this line of work, we present STAC, a tool for supporting Static Textual Analysis of Code. STAC is designed as a light-weight stand-alone tool that provides a practical one-stop solution for code indexing. Code indexing is the process of extracting important textual information from source code. Accurate indexing has been found to significantly influence the performance of code retrieval and analysis methods. STAC provides features for extracting and processing textual patterns found in Java, C++, and C# code artifacts. These features include identifier splitting, stemming, lemmatization, and spell-checking. STAC is also provided as an API to help researchers to integrate basic code indexing features into their code.},
  doi       = {10.1109/ICPC.2016.7503746},
  keywords  = {application program interfaces;C# language;C++ language;indexing;information retrieval;Java;program diagnostics;source code (software);text analysis;static textual analysis of code;source code processing;source code synthesis;code identifiers;internal code aid;internal code comments;software engineering activities;STAC tool;light-weight stand-alone tool;code indexing;textual information extraction;code retrieval methods;code analysis methods;textual pattern processing;textual pattern extraction;Java;C++;C# code;identifier;splitting;stemming;lemmatization;spell-checking;API;Dictionaries;Feature extraction;Indexing;Natural languages;Ice;Data mining;Java},
}

@InProceedings{6624055,
  author    = {D. {Binkley} and D. {Lawrie} and L. {Pollock} and E. {Hill} and K. {Vijay-Shanker}},
  title     = {A dataset for evaluating identifier splitters},
  booktitle = {2013 10th Working Conference on Mining Software Repositories (MSR)},
  year      = {2013},
  pages     = {401-404},
  month     = {May},
  abstract  = {Software engineering and evolution techniques have recently started to exploit the natural language information in source code. A key step in doing so is splitting identifiers into their constituent words. While simple in concept, identifier splitting raises several challenging issues, leading to a range of splitting techniques. Consequently, the research community would benefit from a dataset (i.e., a gold set) that facilitates comparative studies of identifier splitting techniques. A gold set of 2,663 split identifiers was constructed from 8,522 individual human splitting judgements and can be obtained from www.cs.loyola.edu/~binkley/ludiso. This set's construction and observations aimed at its effective use are described.},
  doi       = {10.1109/MSR.2013.6624055},
  keywords  = {computational linguistics;program interpreters;software engineering;source coding;identifier splitter evaluation dataset;software engineering;software evolution techniques;natural language information;source code;constituent words;identifier splitting techniques;human splitting judgements;Gold;Java;Data mining;Software;Speech recognition;Educational institutions},
}

@Article{6464271,
  author   = {S. {Kpodjedo} and F. {Ricca} and P. {Galinier} and G. {Antoniol} and Y. {Guéhéneuc}},
  title    = {MADMatch: Many-to-Many Approximate Diagram Matching for Design Comparison},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2013},
  volume   = {39},
  number   = {8},
  pages    = {1090-1111},
  month    = {Aug},
  abstract = {Matching algorithms play a fundamental role in many important but difficult software engineering activities, especially design evolution analysis and model comparison. We present MADMatch, a fast and scalable many-to-many approximate diagram matching approach based on an error-tolerant graph matching (ETGM) formulation. Diagrams are represented as graphs, costs are assigned to possible differences between two given graphs, and the goal is to retrieve the cheapest matching. We address the resulting optimization problem with a tabu search enhanced by the novel use of lexical and structural information. Through several case studies with different types of diagrams and tasks, we show that our generic approach obtains better results than dedicated state-of-the-art algorithms, such as AURA, PLTSDiff, or UMLDiff, on the exact same datasets used to introduce (and evaluate) these algorithms.},
  doi      = {10.1109/TSE.2013.9},
  keywords = {graph theory;optimisation;search problems;software engineering;MADMatch approach;many-to-many approximate diagram matching approach;error-tolerant graph matching;ETGM;software engineering;design evolution analysis;model comparison;design comparison;optimization problem;tabu search;lexical information;structural information;AURA algorithm;PLTSDiff algorithm;UMLDiff algorithm;Unified modeling language;Algorithm design and analysis;Software;Scalability;Software algorithms;Software engineering;Optimization;Diagram differencing;search-based software engineering;approximate graph matching;identifier splitting},
}

@InProceedings{6405277,
  author    = {A. {Corazza} and S. {Di Martino} and V. {Maggio}},
  title     = {LINSEN: An efficient approach to split identifiers and expand abbreviations},
  booktitle = {2012 28th IEEE International Conference on Software Maintenance (ICSM)},
  year      = {2012},
  pages     = {233-242},
  month     = {Sep.},
  abstract  = {Information Retrieval (IR) techniques are being exploited by an increasing number of tools supporting Software Maintenance activities. Indeed the lexical information embedded in the source code can be valuable for tasks such as concept location, clustering or recovery of traceability links. The application of such IR-based techniques relies on the consistency of the lexicon available in the different artifacts, and their effectiveness can worsen if programmers introduce abbreviations (e.g: rect) and/or do not strictly follow naming conventions such as Camel Case (e.g: UTFtoASCII). In this paper we propose an approach to automatically split identifiers in their composing words, and expand abbreviations. The solution is based on a graph model and performs in linear time with respect to the size of the dictionary, taking advantage of an approximate string matching technique. The proposed technique exploits a number of different dictionaries, referring to increasingly broader contexts, in order to achieve a disambiguation strategy based on the knowledge gathered from the most appropriate domain. The approach has been compared to other splitting and expansion techniques, using freely available oracles for the identifiers extracted from 24 C/C++ and Java open source systems. Results show an improvement in both splitting and expanding performance, in addition to a strong enhancement in the computational efficiency.},
  doi       = {10.1109/ICSM.2012.6405277},
  keywords  = {C++ language;information retrieval;Java;pattern clustering;public domain software;software maintenance;LINSEN;identifier splitting;abbreviation expansion;information retrieval techniques;software maintenance activities;lexical information;concept location;traceability links clustering;traceability links recovery;IR-based techniques;camel case;dictionary;approximate string matching technique;disambiguation strategy;C-C++;Java open source systems;oracles;Dictionaries;Software algorithms;Approximation algorithms;Context;Software maintenance;Conferences;Source Code Identifiers;Program Comprehension;Splitting;Expansion},
}

@InProceedings{6385106,
  author    = {L. {Guerrouj} and P. {Galinier} and Y. {Guéhéneuc} and G. {Antoniol} and M. {Di Penta}},
  title     = {TRIS: A Fast and Accurate Identifiers Splitting and Expansion Algorithm},
  booktitle = {2012 19th Working Conference on Reverse Engineering},
  year      = {2012},
  pages     = {103-112},
  month     = {Oct},
  abstract  = {Understanding source code identifiers, by identifying words composing them, is a necessary step for many program comprehension, reverse engineering, or redocumentation tasks. To this aim, researchers have proposed several identifier splitting and expansion approaches such as Samurai, TIDIER and more recently GenTest. The ultimate goal of such approaches is to help disambiguating conceptual information encoded in compound (or abbreviated) identifiers. This paper presents TRIS, TRee-based Identifier Splitter, a two-phases approach to split and expand program identifiers. First, TRIS pre-compiles transformed dictionary words into a tree representation, associating a cost to each transformation. In a second phase, it maps the identifier splitting/expansion problem into a minimization problem, i.e., the search of the shortest path (optimal split/expansion) in a weighted graph. We apply TRIS to a sample of 974 identifiers extracted from JHotDraw, 3,085 from Lynx, and to a sample of 489 identifiers extracted from 340 C programs. Also, we compare TRIS with GenTest on a set of 2,663 mixed Java, C and C++ identifiers. We report evidence that TRIS split (and expansion) is more accurate than state-of-the-art approaches and that it is also efficient in terms of computation time.},
  doi       = {10.1109/WCRE.2012.20},
  keywords  = {identification;program diagnostics;reverse engineering;source coding;trees (mathematics);identifier splitting algorithm;identifier expansion algorithm;source code identifiers;program comprehension;reverse engineering;redocumentation tasks;Samurai;TIDIER;GenTest;TRIS;tree-based identifier splitter;two-phase approach;transformed dictionary words;tree representation;minimization problem;shortest path;weighted graph;JHotDraw;Lynx;C programs;C++ identifiers;C identifiers;Java identifiers;state-of-the-art approaches;Dictionaries;Java;Complexity theory;Buildings;Software;Context;Identifier Splitting/Expansion;Program Comprehension;Linguistic Analysis;Optimal Path;Weighted Acyclic Graph},
}

@InProceedings{6405328,
  author    = {D. {Binkley} and D. {Lawrie} and C. {Uehlinger}},
  title     = {Vocabulary normalization improves IR-based concept location},
  booktitle = {2012 28th IEEE International Conference on Software Maintenance (ICSM)},
  year      = {2012},
  pages     = {588-591},
  month     = {Sep.},
  abstract  = {Tool support is crucial to modern software development, evolution, and maintenance. Early tools reused the static analysis performed by the compiler. These were followed by dynamic analysis tools and more recently tools that exploit natural language. This later class has the advantage that it can incorporate not only the code, but artifacts from all phases of software construction and its subsequent evolution. Unfortunately, the natural language found in source code often uses a vocabulary different from that used in other software artifacts and thus increases the vocabulary mismatch problem. This problem exists because many natural-language tools imported from Information Retrieval (IR) and Natural Language Processing (NLP) implicitly assume the use of a single natural language vocabulary. Vocabulary normalization, which goes well beyond simple identifier splitting, brings the vocabulary of the source into line with other artifacts. Consequently, it is expected to improve the performance of existing and future IR and NLP based tools. As a case study, an experiment with an LSI-based feature locator is replicated. Normalization universally improves performance. For the tersest queries, this improvement is over 180% (p <; 0.0001).},
  doi       = {10.1109/ICSM.2012.6405328},
  keywords  = {computational linguistics;information retrieval;natural language processing;program compilers;program diagnostics;software maintenance;software tools;system monitoring;vocabulary normalization;IR-based concept location;software development;software evolution;software maintenance;static analysis;compiler;dynamic analysis tools;natural language processing;source code;vocabulary mismatch;NLP;information retrieval;natural language vocabulary;LSI-based feature locator;tool support;Vocabulary;Software maintenance;Conferences;Natural language processing;Educational institutions;vocabulary normalization;information retrieval;concept location},
}

@InProceedings{5970159,
  author    = {B. {Dit} and L. {Guerrouj} and D. {Poshyvanyk} and G. {Antoniol}},
  title     = {Can Better Identifier Splitting Techniques Help Feature Location?},
  booktitle = {2011 IEEE 19th International Conference on Program Comprehension},
  year      = {2011},
  pages     = {11-20},
  month     = {June},
  abstract  = {The paper presents an exploratory study of two feature location techniques utilizing three strategies for splitting identifiers: Camel Case, Samurai and manual splitting of identifiers. The main research question that we ask in this study is if we had a perfect technique for splitting identifiers, would it still help improve accuracy of feature location techniques applied in different scenarios and settings? In order to answer this research question we investigate two feature location techniques, one based on Information Retrieval and the other one based on the combination of Information Retrieval and dynamic analysis, for locating bugs and features using various configurations of preprocessing strategies on two open-source systems, Rhino and jEdit. The results of an extensive empirical evaluation reveal that feature location techniques using Information Retrieval can benefit from better preprocessing algorithms in some cases, and that their improvement in effectiveness while using manual splitting over state-of-the-art approaches is statistically significant in those cases. However, the results for feature location technique using the combination of Information Retrieval and dynamic analysis do not show any improvement while using manual splitting, indicating that any preprocessing technique will suffice if execution data is available. Overall, our findings outline potential benefits of putting additional research efforts into defining more sophisticated source code preprocessing techniques as they can still be useful in situations where execution information cannot be easily collected.},
  doi       = {10.1109/ICPC.2011.47},
  keywords  = {information retrieval;identifier splitting technique;CamelCase;Samurai;information retrieval;preprocessing strategies;open source system;Rhino;jEdit;feature location technique;Software;Dictionaries;Gold;Manuals;Accuracy;Algorithm design and analysis;Large scale integration;feature location;information retrieval;dynamic analysis;identifier splitting algorithms},
}

@InProceedings{5645490,
  author    = {L. {Guerrouj}},
  title     = {Automatic Derivation of Concepts Based on the Analysis of Source Code Identifiers},
  booktitle = {2010 17th Working Conference on Reverse Engineering},
  year      = {2010},
  pages     = {301-304},
  month     = {Oct},
  abstract  = {The existing software engineering literature has empirically shown that a proper choice of identifiers influences software understandability and maintainability. Indeed, identifiers are developers' main up-to-date source of information and guide their cognitive processes during program understanding when the high-level documentation is scarce or outdated and when the source code is not sufficiently commented. Deriving domain terms from identifiers using high-level and domain concepts is not an easy task when naming conventions (e.g., Camel Case) are not used or strictly followed and-or when these words have been abbreviated or otherwise transformed. Our thesis is to develop an approach that overcomes the shortcomings of the existing approaches and maps identifiers to domain concepts even in the absence of naming conventions and-or the presence of abbreviations. Our approach uses a thesaurus of words and abbreviations to map terms or transformed words composing identifiers to dictionary words. It relies on an oracle that we manually build for the validation of our results. To evaluate our technique, we apply it to derive concepts from identifiers of different systems and open source projects. We also enrich it by the use of domain knowledge and context-aware dictionaries to analyze how sensitive are its performances to the use of contextual information and specialized knowledge.},
  doi       = {10.1109/WCRE.2010.45},
  keywords  = {software maintenance;system documentation;automatic derivation;source code identifiers;software engineering;software understandability;software maintainability;cognitive processes;program understanding;high-level documentation;maps identifiers;domain concepts;naming conventions;Dictionaries;Software;Conferences;Thesauri;Presses;Buildings;Speech recognition;Identifier Splitting;Program Comprehension;Linguistic Analysis;Software Quality},
}

@InProceedings{5069482,
  author    = {E. {Enslen} and E. {Hill} and L. {Pollock} and K. {Vijay-Shanker}},
  title     = {Mining source code to automatically split identifiers for software analysis},
  booktitle = {2009 6th IEEE International Working Conference on Mining Software Repositories},
  year      = {2009},
  pages     = {71-80},
  month     = {May},
  abstract  = {Automated software engineering tools (e.g., program search, concern location, code reuse, quality assessment, etc.) increasingly rely on natural language information from comments and identifiers in code. The first step in analyzing words from identifiers requires splitting identifiers into their constituent words. Unlike natural languages, where space and punctuation are used to delineate words, identifiers cannot contain spaces. One common way to split identifiers is to follow programming language naming conventions. For example, Java programmers often use camel case, where words are delineated by uppercase letters or non-alphabetic characters. However, programmers also create identifiers by concatenating sequences of words together with no discernible delineation, which poses challenges to automatic identifier splitting. In this paper, we present an algorithm to automatically split identifiers into sequences of words by mining word frequencies in source code. With these word frequencies, our identifier splitter uses a scoring technique to automatically select the most appropriate partitioning for an identifier. In an evaluation of over 8000 identifiers from open source Java programs, our Samurai approach outperforms the existing state of the art techniques.},
  doi       = {10.1109/MSR.2009.5069482},
  keywords  = {data mining;program diagnostics;software maintenance;source code mining;automatic split identifier;software analysis;word frequency mining;software maintenance;Software maintenance;Natural languages;Programming profession;Software tools;Java;Frequency;Open source software;Software quality;Information analysis;Quality assessment},
}

@Article{895039,
  author   = {B. {Van Houdt} and C. {Blondia}},
  title    = {Analysis of an identifier splitting algorithm combined with polling (ISAP) for contention resolution in a wireless access network},
  journal  = {IEEE Journal on Selected Areas in Communications},
  year     = {2000},
  volume   = {18},
  number   = {11},
  pages    = {2345-2355},
  month    = {Nov},
  abstract = {A contention resolution scheme for an uplink contention channel in a wireless access network is presented. The scheme consists of a tree algorithm, namely the identifier splitting algorithm (ISA), combined with a polling scheme. Initially, ISA is used, but at a certain level of the tree, the scheme switches to polling of the stations. This scheme is further enhanced by skipping a few levels in the tree when starting the algorithm (both in a static and a dynamic way) and by allowing multiple instants simultaneously. An analytical model of the system and its variants leads to the evaluation of its performance, by means of the delay density function and the throughput characteristics. This model is used to investigate the influence of the packet arrival rate, the instant at which the ISA scheme switches to polling, the starting level of the ISA scheme, and the use of multiple instances on the mean delay, the delay quantiles, and the throughput.},
  doi      = {10.1109/49.895039},
  keywords = {radio access networks;telecommunication congestion control;trees (mathematics);delays;packet radio networks;wireless LAN;cellular radio;time division multiple access;access protocols;identifier splitting algorithm;polling;contention resolution;wireless access network;uplink contention channel;tree algorithm;analytical model;performance evaluation;delay density function;throughput characteristics;packet arrival rate;ISA scheme starting level;mean delay;delay quantiles;throughput;wireless LAN;cellular radio;TDMA;ISAP protocol;Algorithm design and analysis;Instruction sets;Delay;Media Access Protocol;Throughput;Access protocols;Downlink;Wireless networks;Switches;Density functional theory},
}

@InProceedings{801991,
  author    = {B. {Van Houdt} and C. {Blondia} and O. {Casals} and J. {Garcia}},
  title     = {Packet level performance characteristics of a MAC protocol for wireless ATM LANs},
  booktitle = {Proceedings 24th Conference on Local Computer Networks. LCN'99},
  year      = {1999},
  pages     = {14-23},
  month     = {Oct},
  abstract  = {This paper determines packet level performance measures of a MAC protocol for a wireless ATM local area network. A key characteristic of the MAC protocol is the identifier splitting algorithm with polling, a contention resolution scheme used to inform the base station about the bandwidth needs of a mobile station when no piggybacking can be used. We consider higher layer packets that are generated at the mobile station and investigate the influence of the traffic characteristics of the packet arrival process on the efficiency of the protocol and on the delay that packets experience to access the shared medium.},
  doi       = {10.1109/LCN.1999.801991},
  keywords  = {asynchronous transfer mode;wireless LAN;access protocols;packet radio networks;telecommunication traffic;land mobile radio;time division multiple access;queueing theory;packet level performance characteristics;MAC protocol;wireless ATM LAN;performance measures;local area network;identifier splitting algorithm with polling;contention resolution;base station;bandwidth;mobile station;higher layer packets;traffic characteristics;packet arrival process;protocol efficiency;packet delay;shared medium access;time division multiplexing access;TDMA;frequency division duplex;queueing model;Media Access Protocol;Wireless application protocol;Wireless LAN;Local area networks;Asynchronous transfer mode;Bandwidth;Access protocols;Delay;Electronic mail;Switches},
}

@InCollection{PEREZLOPEZ2018247,
  author    = {David Pérez-López and Houssem Memmi and María del Carmen Gijón-López and Marta Maria Moreno and José Francisco Couceiro and Ana Centeno and María J. Martín-Palomo and Mireia Corell and Luis Noguera-Artiaga and Alejandro Galindo and Arturo Torrecillas and Alfonso Moriana},
  title     = {Chapter 11 - Irrigation of Pistachios: Strategies to Confront Water Scarcity},
  booktitle = {Water Scarcity and Sustainable Agriculture in Semiarid Environment},
  publisher = {Academic Press},
  year      = {2018},
  editor    = {Iván Francisco García Tejero and Víctor Hugo Durán Zuazo},
  pages     = {247 - 269},
  isbn      = {978-0-12-813164-0},
  abstract  = {Pistachio trees are capable to be profitable under rain-fed conditions. They also have a good response to low amounts of irrigation water, so are a great candidate to be considered for water-scarcity scenarios. The pistachio tree has a singular way of alternate bearing, losing a percentage of its floral buds in the year preceding the harvest year. This percentage of abscission of the fruit buds increases in situations of water stress, making irrigation strategy follow as a 1-year determinant for the next season fruit yield. Despite its high resistance to severe water stress conditions, the pistachio tree has high water requirements. To overcome this issue, regulated deficit irrigation (RDI) was tested as a technique to save water without affecting yield. Coupling the use of plant biosensors to this technique is a research approach that can improve its efficiency. The recourse to these techniques created a greater preference of the resultant product by the consumer.},
  doi       = {https://doi.org/10.1016/B978-0-12-813164-0.00011-9},
  keywords  = {Alternate bearing, Consumer preferences, Plant biosensors, Regulated deficit irrigation, Water deficit},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780128131640000119},
}

@Article{DAVIES201787,
  author   = {Catherine Davies and Helene Kreysa},
  title    = {Looking at a contrast object before speaking boosts referential informativeness, but is not essential},
  journal  = {Acta Psychologica},
  year     = {2017},
  volume   = {178},
  pages    = {87 - 99},
  issn     = {0001-6918},
  abstract = {Variation in referential form has traditionally been accounted for by theoretical frameworks focusing on linguistic and discourse features. Despite the explosion of interest in eye tracking methods in psycholinguistics, the role of visual scanning behaviour in informative reference production is yet to be comprehensively investigated. Here we examine the relationship between speakers' fixations to relevant referents and the form of the referring expressions they produce. Overall, speakers were fully informative across simple and (to a lesser extent) more complex displays, providing appropriately modified referring expressions to enable their addressee to locate the target object. Analysis of contrast fixations revealed that looking at a contrast object boosts but is not essential for full informativeness. Contrast fixations which take place immediately before speaking provide the greatest boost. Informative referring expressions were also associated with later speech onsets than underinformative ones. Based on the finding that fixations during speech planning facilitate but do not fully predict informative referring, direct visual scanning is ruled out as a prerequisite for informativeness. Instead, pragmatic expectations of informativeness may play a more important role. Results are consistent with a goal-based link between eye movements and language processing, here applied for the first time to production processes.},
  doi      = {https://doi.org/10.1016/j.actpsy.2017.06.001},
  keywords = {Reference, Speech production, Informativeness, Pragmatics, Eye movements},
  url      = {http://www.sciencedirect.com/science/article/pii/S0001691817300367},
}

@Article{GULCEHRE2017137,
  author   = {Caglar Gulcehre and Orhan Firat and Kelvin Xu and Kyunghyun Cho and Yoshua Bengio},
  title    = {On integrating a language model into neural machine translation},
  journal  = {Computer Speech \& Language},
  year     = {2017},
  volume   = {45},
  pages    = {137 - 148},
  issn     = {0885-2308},
  abstract = {Recent advances in end-to-end neural machine translation models have achieved promising results on high-resource language pairs such as En→ Fr and En→ De. One of the major factor behind these successes is the availability of high quality parallel corpora. We explore two strategies on leveraging abundant amount of monolingual data for neural machine translation. We observe improvements by both combining scores from neural language model trained only on target monolingual data with neural machine translation model and fusing hidden-states of these two models. We obtain up to 2 BLEU improvement over hierarchical and phrase-based baseline on low-resource language pair, Turkish→ English. Our method was initially motivated towards tasks with less parallel data, but we also show that it extends to high resource languages such as Cs→ En and De→ En translation tasks, where we obtain 0.39 and 0.47 BLEU improvements over the neural machine translation baselines, respectively.},
  doi      = {https://doi.org/10.1016/j.csl.2017.01.014},
  keywords = {Neural machine translation, Monolingual data, Language models, Low resource machine translation, Deep learning, Neural network},
  url      = {http://www.sciencedirect.com/science/article/pii/S0885230816301395},
}

@Article{HUGHES2014173,
  author   = {J.D. Hughes and D. Dutta and J. Vaze and S.S.H. Kim and G. Podger},
  title    = {An automated multi-step calibration procedure for a river system model},
  journal  = {Environmental Modelling \& Software},
  year     = {2014},
  volume   = {51},
  pages    = {173 - 183},
  issn     = {1364-8152},
  abstract = {Predicted climate change impact on future water availability in the Murray–Darling Basin (MDB) has highlighted the need for a whole of basin model that incorporates various physical and management characteristics for planning and operational purposes. Modelling platforms such as eWater Source Integrated Modelling System (Source) offer a useful framework in this regard, but at present lack automated calibration techniques to parameterise river system models. This paper presents an automated river system calibration procedure which is robust, repeatable, transparent and systematic. The procedure allows for river network calibration (as opposed to isolated reach by reach calibration), since this has more utility for basin planning and prediction. The calibration procedure routs upstream flow, estimates ungauged inputs via rainfall–runoff (RR) models, and estimates flow based split (distributary) functions and loss functions in complex river systems. This procedure was tested in the Northern Murray–Darling Basin (MDB) and results from the Border Rivers catchment are presented. The results from the Border Rivers case study demonstrate the applicability of the procedure with median calibration and evaluation NSE values of 0.88 and 0.79, respectively. The use of this procedure in the Border Rivers region has highlighted the likelihood of changing stream channel connections at higher flows in the lower reaches of the river network.},
  doi      = {https://doi.org/10.1016/j.envsoft.2013.09.024},
  keywords = {Large river basin, Irrigation, Unaccounted gains, Unaccounted loss, Loss function, Calibration, Routing, Floodplain, Irrigation},
  url      = {http://www.sciencedirect.com/science/article/pii/S1364815213002235},
}

@Article{WANG2014272,
  author   = {Weihua Wang and Ruifang Song and Mingchen Guo and Songshan Liu},
  title    = {Analysis on compound-split configuration of power-split hybrid electric vehicle},
  journal  = {Mechanism and Machine Theory},
  year     = {2014},
  volume   = {78},
  pages    = {272 - 288},
  issn     = {0094-114X},
  abstract = {In a power-split hybrid electric vehicle (HEV), the power-split device (PSD) can be classified into input-split, output-split and compound-split types. A systematic analysis methodology is proposed to find out all the suitable schemes for power-split HEV from a variety of compound-split configurations. Based upon the characteristics of mechanical point and PSD itself, three reasonable 4-node lever models are confirmed. In addition, the range of its lever lengths is achieved according to the motor torque and speed characteristics and the electric power ratio characteristic. Then the 4-node lever model is split backward into two single lever models to identify the inner connection of compound-split configurations. Using this analysis method proposed, it is proved that compound-split configuration is suitable for high-speed mode in power-split HEV and the compound-split configuration can be selected easily by setting two mechanical points at the desired transmission ratios. A dual-mode power-split hybrid powertrain system is obtained by adding clutches and the third planetary gear set to the compound-split configuration selected.},
  doi      = {https://doi.org/10.1016/j.mechmachtheory.2014.03.019},
  keywords = {Hybrid electric vehicle, Compound-split configuration, Power split, Planetary gear},
  url      = {http://www.sciencedirect.com/science/article/pii/S0094114X14001074},
}

@Article{CALCAGNO2010474,
  author   = {Cristiano Calcagno and Thomas Dinsdale-Young and Philippa Gardner},
  title    = {Adjunct elimination in Context Logic for trees},
  journal  = {Information and Computation},
  year     = {2010},
  volume   = {208},
  number   = {5},
  pages    = {474 - 499},
  issn     = {0890-5401},
  note     = {Special Issue: 14th Workshop on Logic, Language, Information and Computation (WoLLIC 2007)},
  abstract = {We study adjunct-elimination results for Context Logic applied to trees, following previous results by Lozes for Separation Logic and Ambient Logic. In fact, it is not possible to prove such elimination results for the original single-holed formulation of Context Logic. Instead, we prove our results for multi-holed Context Logic.},
  doi      = {https://doi.org/10.1016/j.ic.2009.02.013},
  keywords = {Context Logic, Adjunct elimination, Ehrenfeucht–Fraïssé games},
  url      = {http://www.sciencedirect.com/science/article/pii/S0890540109002363},
}

@Article{VIGNON2010443,
  author   = {Matthias Vignon and Pierre Sasal},
  title    = {Multiscale determinants of parasite abundance: A quantitative hierarchical approach for coral reef fishes},
  journal  = {International Journal for Parasitology},
  year     = {2010},
  volume   = {40},
  number   = {4},
  pages    = {443 - 451},
  issn     = {0020-7519},
  abstract = {During recent decades, there have been numerous attempts to identify the key determinants of parasite communities and several influential variables have been clarified at either infra-, component or compound community scales. However, in view of the possible complexity of interactions among determinants, the commonly-used exploratory and statistical modelling techniques have often failed to find meaningful ecological patterns from such data. Moreover, quantitative assessments of factors structuring species richness, abundance, community structure and species associations in parasite communities remain elusive. Recently, because they are ideally suited for the analysis of complex and highly interactive data, there has been increasing interest in the use of classification and regression tree analyses in several ecological fields. To date, such approaches have never been used by parasitologists for field data. This study aims to both introduce and illustrate the use of multivariate regression trees in order to investigate the determinants of parasite abundance in a multi-scale quantitative context. To do this, we used new field epidemiological data from 1489 coral reef fishes collected around two islands in French Polynesia. We evaluated the relative effect and interactions of several host traits and environmental factors on the abundance of metazoan parasite assemblage at several scales and assessed the impact of major factors on each parasite taxon. Our results suggest that the islands sampled, the host species and host size are equal predictors of parasite abundance at a global scale, whereas other factors proved to be significant predictors of a local pattern, depending on host family. We also discuss the potential use of regression trees for parasitologists as both an explorative and a promising predictive tool.},
  doi      = {https://doi.org/10.1016/j.ijpara.2009.09.010},
  keywords = {Parasite community, Multivariate regression tree, Statistical analysis, French Polynesia, Marine tropical fish, Serranidae, Lutjanidae},
  url      = {http://www.sciencedirect.com/science/article/pii/S002075190900397X},
}

@Article{GORZERINO2009802,
  author   = {Caroline Gorzerino and Alphonse Quemeneur and Anne Hillenweck and Maryse Baradat and Georges Delous and Martine Ollitrault and Didier Azam and Thierry Caquet and Laurent Lagadic},
  title    = {Effects of diquat and fomesafen applied alone and in combination with a nonylphenol polyethoxylate adjuvant on Lemna minor in aquatic indoor microcosms},
  journal  = {Ecotoxicology and Environmental Safety},
  year     = {2009},
  volume   = {72},
  number   = {3},
  pages    = {802 - 810},
  issn     = {0147-6513},
  abstract = {The influence of tank-mix adjuvants on pesticide toxicity remains largely unknown. Agral® 90, a nonylphenol polyethoxylated tank-mix adjuvant, has been used with diquat (bipyridylium herbicide) and fomesafen (diphenyl-ether herbicide) in aquatic indoor microcosms in order to compare the toxicity of the single compounds and of binary herbicide–adjuvant mixtures to Lemna minor. Twenty-four microcosms were used and treatments were performed with substances alone or with herbicide–adjuvant binary mixtures, at two concentrations levels (44.4 and 222.2μg/L for the herbicides, and 100 and 500μg/L for Agral 90). Toxicity was assessed weekly for 1 month through growth measurements, as inferred from the relative frond number (RFN) and relative frond area (RFA). Concentrations of diquat and fomesafen in water and sediments were measured weekly. The herbicides showed very different behaviour in microcosms, with a rapid disappearance of diquat from the aqueous phase whereas fomesafen levels remained almost constant over time. Diquat strongly inhibited the growth of L. minor whereas fomesafen had no effect on plant growth. Presence of the adjuvant only slightly reduced the effect of the lowest concentration of diquat, probably as a result of dispersion of the herbicide at the water surface. It is concluded that tank-mix adjuvant designed to improve herbicide efficiency in the terrestrial environment did not have any effect on aquatic plants when applied to the aquatic environment.},
  doi      = {https://doi.org/10.1016/j.ecoenv.2008.08.001},
  keywords = {Duckweed, Bipyridylium herbicide, Diphenyl-ether herbicide, Tank-mix adjuvant, Microcosm, Mixture toxicity},
  url      = {http://www.sciencedirect.com/science/article/pii/S0147651308002315},
}

@InCollection{HALPIN2008109,
  author    = {Terry Halpin and Tony Morgan},
  title     = {4 - Uniqueness Constraints},
  booktitle = {Information Modeling and Relational Databases (Second Edition)},
  publisher = {Morgan Kaufmann},
  year      = {2008},
  editor    = {Terry Halpin and Tony Morgan},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {109 - 157},
  address   = {San Francisco},
  edition   = {Second Edition},
  isbn      = {978-0-12-373568-3},
  doi       = {https://doi.org/10.1016/B978-012373568-3.50008-4},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780123735683500084},
}

@Article{KNUTSSON20071122,
  author   = {Ola Knutsson and Teresa Cerratto Pargman and Kerstin Severinson Eklundh and Stefan Westlund},
  title    = {Designing and developing a language environment for second language writers},
  journal  = {Computers \& Education},
  year     = {2007},
  volume   = {49},
  number   = {4},
  pages    = {1122 - 1146},
  issn     = {0360-1315},
  abstract = {This paper presents a field study carried out with learners who used a grammar checker in real writing tasks in an advanced course at a Swedish university. The objective of the study was to investigate how students made use of the grammar checker in their writing while learning Swedish as a second language. Sixteen students with different linguistic and cultural backgrounds participated in the study. A judgment procedure was conducted by the learners on the alarms from the grammar checker. The students’ texts were also collected in two versions; a version written before the session with the grammar checker, and a version after the session. This procedure made it possible to study to what extent the students followed the advice from the grammar checker, and how this was related to their judgments of its behavior. The results obtained demonstrated that although most of the alarms from the grammar checker were accurate, some alarms were very hard for the students to judge correctly. The results also showed that providing the student with feedback on different aspects of their target language use; not only on their errors, and facilitating the processes of language exploration and reflection are important processes to be supported in second-language learning environments. Based on these results, design principles were identified and integrated in the development of Grim, an interactive language-learning program for Swedish. We present the design of Grim, which is grounded in visualization of grammatical categories and examples of language use, providing tools for both focus on linguistic code features and language comprehension.},
  doi      = {https://doi.org/10.1016/j.compedu.2006.01.005},
  keywords = {Adult learning, Evaluation of CAL systems, Human–computer interface, Interactive learning environments},
  url      = {http://www.sciencedirect.com/science/article/pii/S0360131506000182},
}

@Article{AHLGREN200781,
  author   = {Per Ahlgren and Jaana Kekäläinen},
  title    = {Indexing strategies for Swedish full text retrieval under different user scenarios},
  journal  = {Information Processing \& Management},
  year     = {2007},
  volume   = {43},
  number   = {1},
  pages    = {81 - 102},
  issn     = {0306-4573},
  abstract = {This paper deals with Swedish full text retrieval and the problem of morphological variation of query terms in the document database. The effects of combination of indexing strategies with query terms on retrieval effectiveness were studied. Three of five tested combinations involved indexing strategies that used conflation, in the form of normalization. Further, two of these three combinations used indexing strategies that employed compound splitting. Normalization and compound splitting were performed by SWETWOL, a morphological analyzer for the Swedish language. A fourth combination attempted to group related terms by right hand truncation of query terms. The four combinations were compared to each other and to a baseline combination, where no attempt was made to counteract the problem of morphological variation of query terms in the document database. The five combinations were evaluated under six different user scenarios, where each scenario simulated a certain user type. The four alternative combinations outperformed the baseline, for each user scenario. The truncation combination had the best performance under each user scenario. The main conclusion of the paper is that normalization and right hand truncation (performed by a search expert) enhanced retrieval effectiveness in comparison to the baseline. The performance of the three combinations of indexing strategies with query terms based on normalization was not far below the performance of the truncation combination.},
  doi      = {https://doi.org/10.1016/j.ipm.2006.03.003},
  keywords = {Base word form index, Discounted cumulated gain, Indexing strategy, Inflected word form index, Truncation, User scenario},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306457306000240},
}

@Article{HA2007405,
  author   = {Huy Tài Hà and Adam Van Tuyl},
  title    = {Splittable ideals and the resolutions of monomial ideals},
  journal  = {Journal of Algebra},
  year     = {2007},
  volume   = {309},
  number   = {1},
  pages    = {405 - 425},
  issn     = {0021-8693},
  abstract = {We provide a new combinatorial approach to study the minimal free resolutions of edge ideals, that is, quadratic square-free monomial ideals. With this method we can recover most of the known results on resolutions of edge ideals with fuller generality, and at the same time, obtain new results. Past investigations on the resolutions of edge ideals usually reduced the problem to computing the dimensions of reduced homology or Koszul homology groups. Our approach circumvents the highly nontrivial problem of computing the dimensions of these groups and turns the problem into combinatorial questions about the associated simple graph. We also show that our technique extends successfully to the study of graded Betti numbers of arbitrary square-free monomial ideals viewed as facet ideals of simplicial complexes.},
  doi      = {https://doi.org/10.1016/j.jalgebra.2006.08.022},
  keywords = {Simple graphs, Simplicial complexes, Monomial ideals, Edge ideals, Facet ideals, Resolutions, Betti numbers},
  url      = {http://www.sciencedirect.com/science/article/pii/S0021869306005692},
}

@Article{LEHTOKANGAS2004973,
  author   = {Raija Lehtokangas and Eija Airio and Kalervo Järvelin},
  title    = {Transitive dictionary translation challenges direct dictionary translation in CLIR},
  journal  = {Information Processing \& Management},
  year     = {2004},
  volume   = {40},
  number   = {6},
  pages    = {973 - 988},
  issn     = {0306-4573},
  abstract = {The paper reports on experiments carried out in transitive translation, a branch of cross-language information retrieval (CLIR). By transitive translation we mean translation of search queries into the language of the document collection through an intermediate (or pivot) language. In our experiments, queries constructed from CLEF 2000 and 2001 Swedish, Finnish and German topics were translated into English through Finnish and Swedish by an automated translation process using morphological analyzers, stopword lists, electronic dictionaries, n-gramming of untranslatable words, and structured and unstructured queries. The results of the transitive runs were compared to the results of the bilingual runs, i.e. runs translating the same queries directly into English. The transitive runs using structured target queries performed well. The differences ranged from −6.6% to +2.9% units (or −25.5% to +7.8%) between the approaches. Thus transitive translation challenges direct translation and considerably simplifies global CLIR efforts.},
  doi      = {https://doi.org/10.1016/j.ipm.2003.10.005},
  keywords = {Cross-language information retrieval, Query translation, Structured queries, Transitive translation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306457303000864},
}

@Article{PIRKOLA2003391,
  author   = {Ari Pirkola and Deniz Puolamäki and Kalervo Järvelin},
  title    = {Applying query structuring in cross-language retrieval},
  journal  = {Information Processing \& Management},
  year     = {2003},
  volume   = {39},
  number   = {3},
  pages    = {391 - 402},
  issn     = {0306-4573},
  abstract = {We will explore various ways to apply query structuring in cross-language information retrieval. In the first test, English queries were translated into Finnish using an electronic dictionary, and were run in a Finnish newspaper database of 55,000 articles. Queries were structured by combining the Finnish translation equivalents of the same English query key using the syn-operator of the InQuery retrieval system. Structured queries performed markedly better than unstructured queries. Second, the effects of compound-based structuring using a proximity operator for the translation equivalents of query language compound components were tested. The method was not useful in syn-based queries but resulted in decrease in retrieval effectiveness. Proper names are often non-identical spelling variants in different languages. This allows n-gram based translation of names not included in a dictionary. In the third test, a query structuring method where the Boolean and-operator was used to assign more weight to keys translated through n-gram matching gave good results.},
  doi      = {https://doi.org/10.1016/S0306-4573(02)00091-2},
  keywords = {Compound word processing, Cross-language information retrieval, -Gram matching, Proper name searching, Structured queries},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306457302000912},
}

@Article{LIBBEN200350,
  author   = {Gary Libben and Martha Gibson and Yeo Bom Yoon and Dominiek Sandra},
  title    = {Compound fracture: The role of semantic transparency and morphological headedness},
  journal  = {Brain and Language},
  year     = {2003},
  volume   = {84},
  number   = {1},
  pages    = {50 - 64},
  issn     = {0093-934X},
  note     = {Brain and Language Special Issue},
  abstract = {This paper explores the role of semantic transparency in the representation and processing of English compounds. We focus on the question of whether semantic transparency is best viewed as a property of the entire multimorphemic string or as a property of constituent morphemes. Accordingly, we investigated the processing of English compound nouns that were categorized in terms of the semantic transparency of each of their constituents. Fully transparent such as bedroom are those in which the meanings of each of the constituents are transparently represented in the meaning of the compound as a whole. These compounds were contrasted with compounds such as strawberry, in which only the second constituent is semantically transparent, jailbird, in which only the first constituent is transparent, and hogwash, in which neither constituent is semantically transparent. We propose that significant insights can be achieved through such analysis of the transparency of individual morphemes. The two experiments that we report present evidence that both semantically transparent compounds and semantically opaque compounds show morphological constituency. The semantic transparency of the morphological head (the second constituent in a morphologically right-headed language such as English) was found to play a significant role in overall lexical decision latencies, in patterns of decomposition, and in the effects of stimulus repetition within the experiment.},
  doi      = {https://doi.org/10.1016/S0093-934X(02)00520-5},
  url      = {http://www.sciencedirect.com/science/article/pii/S0093934X02005205},
}

@Article{SAVOY200375,
  author   = {Jacques Savoy},
  title    = {Cross-language information retrieval: experiments based on CLEF 2000 corpora},
  journal  = {Information Processing \& Management},
  year     = {2003},
  volume   = {39},
  number   = {1},
  pages    = {75 - 115},
  issn     = {0306-4573},
  abstract = {Search engines play an essential role in the usability of Internet-based information systems and without them the Web would be much less accessible, and at the very least would develop at a much slower rate. Given that non-English users now tend to make up the majority in this environment, our main objective is to analyze and evaluate the retrieval effectiveness of various indexing and search strategies based on test-collections written in four different languages: English, French, German, and Italian. Our second objective is to describe and evaluate various approaches that might be implemented in order to effectively access document collections written in another language. As a third objective, we will explore the underlying problems involved in searching document collections written in the four different languages, and we will suggest and evaluate different database merging strategies capable of providing the user with a single unique result list.},
  doi      = {https://doi.org/10.1016/S0306-4573(02)00018-3},
  keywords = {Cross-language information retrieval, Bilingual information retrieval, French, German, Italian languages, Database merging strategies, Evaluation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306457302000183},
}

@Article{WILSON200319,
  author   = {Paul G Wilson and Jonathan P Turner and Graham K Westbrook},
  title    = {Structural architecture of the ocean–continent boundary at an oblique transform margin through deep-imaging seismic interpretation and gravity modelling: Equatorial Guinea, West Africa},
  journal  = {Tectonophysics},
  year     = {2003},
  volume   = {374},
  number   = {1},
  pages    = {19 - 40},
  issn     = {0040-1951},
  abstract = {Along the Rio Muni transform margin, the transition from continental to oceanic crust occurs across a region of approximately 75-km width. The crust in this transition region, termed proto-oceanic crust (POC), is neither purely oceanic nor continental in composition and structure. Improved seismic reflection images from the PROBE deep-imaging dataset, combined with gravity modelling, have shed new light on the structural architecture of the margin and the composition of the POC. On these newly migrated seismic reflection sections, four fracture zones associated with large steps in the Moho are identified, splitting the POC into three segments. Models in which these segments are composed of oceanic or stretched continental crust do not provide satisfactory predictions of the gravity anomaly. A model of serpentinized peridotite for two segments of POC, with the third segment composed of oceanic crust in between, does satisfy the observed gravity anomaly. Three alternative geological scenarios are proposed to explain the segmentation and composition of the POC: (a) serpentinized upper mantle becoming unroofed and emplaced at basement surface level along detachment surfaces confined to discrete segments by the fracture zones, (b) oblique-slip on transform faults that allow the circulation of water into the mantle and emplacement of serpentinized upper mantle material; or (c) intense faulting of anomalous oceanic crust as a result of magma depletion allowing hydrothermal circulation and the emplacement of serpentinized peridotites.},
  doi      = {https://doi.org/10.1016/S0040-1951(03)00326-3},
  keywords = {West Africa, Ocean–continent boundary, Seismic reflection, Gravity modelling, Serpentinization, Transform margin},
  url      = {http://www.sciencedirect.com/science/article/pii/S0040195103003263},
}

@Article{HANDWERKER200215,
  author   = {Jan Handwerker},
  title    = {Cell tracking with TRACE3D—a new algorithm},
  journal  = {Atmospheric Research},
  year     = {2002},
  volume   = {61},
  number   = {1},
  pages    = {15 - 34},
  issn     = {0169-8095},
  abstract = {An automated algorithm called TRACE3D is presented which identifies convective cells and tracks them in time and space by exclusively using radar reflectivity data as input. Identification of cells is performed by assembling contiguous regions that excel certain reflectivity thresholds. Tracking is done in that the position of a cell in a new radar image is predicted by an extrapolation procedure based on its former position; special care is taken in case of possible splitting and merging events. In comparing the results of the tracking algorithm with those from four test persons, TRACE3D shows a promising performance, and hence, it seems possible to apply this algorithm as a nowcasting tool.},
  doi      = {https://doi.org/10.1016/S0169-8095(01)00100-4},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169809501001004},
}

@InCollection{HALPIN2001109,
  author    = {Terry Halpin},
  title     = {4 - Uniqueness Constraints},
  booktitle = {Information Modeling and Relational Databases},
  publisher = {Academic Press},
  year      = {2001},
  editor    = {Terry Halpin},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {109 - 161},
  address   = {San Diego},
  isbn      = {978-1-55860-672-2},
  abstract  = {Publisher Summary
Uniqueness constraints play a pivotal role when the conceptual schema is later mapped onto a relational schema. Once uniqueness constraints have been added to a fact type, some further checks are made to see whether the fact type is of the right arity or length. In particular, there is a simple check based on uniqueness that shows that certain kinds of fact types are not elementary, and hence should be split. Constraints apply to the database and are either static or dynamic. Static constraints apply to each individual state of the database and may usually be specified on a conceptual schema diagram. Dynamic constraints restrict the possible transitions between states and are often expressed in other ways. An information system can only enforce constraints on its formal model of the application. The constraint schema for the “real world” need not be the same as for the “recorded world.” Uniqueness constraints apply to the recorded information and must be at least as strong as those in the real world.},
  doi       = {https://doi.org/10.1016/B978-155860672-2/50007-5},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558606722500075},
}

@Article{HEDLUND2001147,
  author   = {Turid Hedlund and Ari Pirkola and Kalervo Järvelin},
  title    = {Aspects of Swedish morphology and semantics from the perspective of mono- and cross-language information retrieval},
  journal  = {Information Processing \& Management},
  year     = {2001},
  volume   = {37},
  number   = {1},
  pages    = {147 - 161},
  issn     = {0306-4573},
  abstract = {This paper analyzes the features of the Swedish language from the viewpoint of mono- and cross-language information retrieval (CLIR). The study was motivated by the fact that Swedish is known poorly from the IR perspective. This paper shows that Swedish has unique features, in particular gender features, the use of fogemorphemes in the formation of compound words, and a high frequency of homographic words. Especially in dictionary-based CLIR, correct word normalization and compound splitting are essential. It was shown in this study, however, that publicly available morphological analysis tools used for normalization and compound splitting have pitfalls that might decrease the effectiveness of IR and CLIR. A comparative study was performed to test the degree of lexical ambiguity in Swedish, Finnish and English. The results suggest that part-of-speech tagging might be useful in Swedish IR due to the high frequency of homographic words.},
  doi      = {https://doi.org/10.1016/S0306-4573(00)00024-8},
  keywords = {Text retrieval, Cross-language information retrieval, Swedish language, Natural language processing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306457300000248},
}

@InCollection{CLEAVELAND2001391,
  author    = {Rance Cleaveland and Oleg Sokolsky},
  title     = {CHAPTER 6 - Equivalence and Preorder Checking for Finite-State Systems},
  booktitle = {Handbook of Process Algebra},
  publisher = {Elsevier Science},
  year      = {2001},
  editor    = {J.A. Bergstra and A. Ponse and S.A. Smolka},
  pages     = {391 - 424},
  address   = {Amsterdam},
  isbn      = {978-0-444-82830-9},
  abstract  = {This chapter surveys algorithms for computing semantic equivalences and refinement relations, or preorders, over states in finite-state labeled transitions systems. Methods for calculating a general equivalence, namely bisimulation equivalence, and a general preorder are described and shown to be useful as a basis for calculating other semantic relations as well. Two general classes of algorithms are considered: global ones, which require the a priori construction of the state space but are generally more efficient in the asymptotic case, and local, or on-the-fly ones, which avoid the construction of unnecessary states while incurring some additional computational overhead.},
  doi       = {https://doi.org/10.1016/B978-044482830-9/50024-2},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444828309500242},
}

@Article{FERNANDEZ1990219,
  author   = {Jean-Claude Fernandez},
  title    = {An implementation of an efficient algorithm for bisimulation equivalence},
  journal  = {Science of Computer Programming},
  year     = {1990},
  volume   = {13},
  number   = {2},
  pages    = {219 - 236},
  issn     = {0167-6423},
  abstract = {We present an efficient algorithm for bisimulation equivalence. Generally, bisimulation equivalence can be tested in O(mn) for a labeled transition system with m transitions and n states. In order to come up with a more efficient algorithm, we establish a relationship between bisimulation equivalence and the relational coarsest partition problem, solved by Paige and Tarjan in O(m log n) time. Given an initial partition and a binary relation, the problem is to find the coarsest partition compatible with them. Computing bisimulation equivalence can be viewed both as an instance and as a generalization of this problem: an instance, because only the universal partition is considered as an initial partition and a generalization since we want to find a partition compatible with a family of binary relations instead of one single binary relation. We describe how we have adapted the Paige-Tarjan algorithm of complexity O(m log n) to minimize labeled transition systems modulo bisimulation equivalence. This algorithm has been implemented in C and is used in Aldébaran, a tool for the verification of concurrent systems.},
  doi      = {https://doi.org/10.1016/0167-6423(90)90071-K},
  url      = {http://www.sciencedirect.com/science/article/pii/016764239090071K},
}

@Article{LOYOLA198753,
  author   = {E. Loyola and M. Herraiz and G. Reglero and P. Martin-Alvarez},
  title    = {Evaluation of a programmed temperature vaporizer for quantitative analysis by split injection},
  journal  = {Journal of Chromatography A},
  year     = {1987},
  volume   = {398},
  pages    = {53 - 61},
  issn     = {0021-9673},
  abstract = {The influence of several parameters affecting sampling with cold temperature-programmed injection were studied. The parameters were types and lengths of packings used in the glass insert, end temperatures in the sampling device, solvents and splitting ratio. The accuracy and precision of the sampling was measured for a test mixture of n-alkanes using a programmed temperature vaporizer in the split mode.},
  doi      = {https://doi.org/10.1016/S0021-9673(01)96495-7},
  url      = {http://www.sciencedirect.com/science/article/pii/S0021967301964957},
}

@Article{WATSON1959142,
  author  = {E.L. Grant Watson},
  title   = {Galls and their hosts: Some patterns of behaviour},
  journal = {British Homoeopathic journal},
  year    = {1959},
  volume  = {48},
  number  = {2},
  pages   = {142 - 146},
  issn    = {0007-0785},
  doi     = {https://doi.org/10.1016/S0007-0785(59)80055-7},
  url     = {http://www.sciencedirect.com/science/article/pii/S0007078559800557},
}

@Article{translationamt,
  author = {Translation, Statistical Machine},
  title  = {AMT},
}

@Article{shishkovaannotated,
  author = {Shishkova, Anna and Chernyak, Ekaterina},
  title  = {Annotated Suffix Tree Method for German Compound Splitting},
}

@Article{imcall,
  author = {im Walde, Sabine Schulte and Smolka, Eva},
  title  = {Call for Contributions for an edited volume in the “Phraseology and Multiword Expressions” book series: Who nicks the nickname?--An interdisciplinary, cross-lingual perspective on the role of constituents},
}

@Article{volkcompounds,
  author = {Volk, Martin and Mascarell, Laura and Fishel, Mark},
  title  = {Compounds, Coreferences and Multiword Translations},
}

@Article{shearingimproving,
  author = {Shearing, Steven and Kirov, Christo and Khayrallah, Huda and Yarowsky, David},
  title  = {Improving Low Resource Machine Translation using Morphological Glosses},
}

@Article{durranimachine,
  author = {Durrani, Nadir and Model, Operation Sequence and Peitz, Stephan and Training, Leave One Out},
  title  = {Machine Translation Panel},
}

@Article{ullmanparaphrasing,
  author = {Ullman, Edvin},
  title  = {Paraphrasing of Swedish Compound Nouns},
}

@Article{huetpreliminary,
  author  = {Huet, G{\'e}rard and Lankri, Idir},
  title   = {Preliminary Design of a Sanskrit Corpus Manager},
  journal = {Computational Sanskrit \& Digital Humanities},
  pages   = {259},
}

@Article{besbinartweet,
  author = {Besbinar, Beril and Sarigiannis, Dimitrios and Smeros, Panayiotis},
  title  = {Tweet Sentiment Classification},
}

@Article{alumae2019advanced,
  author  = {Alum{\"a}e, Tanel and Tilk, Ottokar and others},
  title   = {Advanced Rich Transcription System for Estonian Speech},
  journal = {arXiv preprint arXiv:1901.03601},
  year    = {2019},
}

@Article{ruiz2019assessing,
  author  = {Ruiz, Nicholas and Di Gangi, Mattia Antonino and Bertoldi, Nicola and Federico, Marcello},
  title   = {Assessing the Tolerance of Neural Machine Translation Systems Against Speech Recognition Errors},
  journal = {arXiv preprint arXiv:1904.10997},
  year    = {2019},
}

@Article{bollmann2019few,
  author  = {Bollmann, Marcel and Korchagina, Natalia and S{\o}gaard, Anders},
  title   = {Few-Shot and Zero-Shot Learning for Historical Text Normalization},
  journal = {arXiv preprint arXiv:1903.04870},
  year    = {2019},
}

@Article{lam2019interactive,
  author  = {Lam, Tsz Kin and Schamoni, Shigehiko and Riezler, Stefan},
  title   = {Interactive-Predictive Neural Machine Translation through Reinforcement and Imitation},
  journal = {arXiv preprint arXiv:1907.02326},
  year    = {2019},
}

@Article{fonollosa2019joint,
  author  = {Fonollosa, Jos{\'e} AR and Casas, Noe and Costa-juss{\`a}, Marta R},
  title   = {Joint Source-Target Self Attention with Locality Constraints},
  journal = {arXiv preprint arXiv:1905.06596},
  year    = {2019},
}

@InProceedings{fu2019keeping,
  author       = {Fu, Hao and Zheng, Zizhan and Zhu, Sencun and Mohapatra, Prasant},
  title        = {Keeping Context In Mind: Automating Mobile App Access Control with User Interface Inspection},
  booktitle    = {IEEE INFOCOM 2019-IEEE Conference on Computer Communications},
  year         = {2019},
  pages        = {2089--2097},
  organization = {IEEE},
}

@InProceedings{zuters2019morphology,
  author       = {ZUTERS, Jnis and Strazds, Gus and EONOVA, Viktorija},
  title        = {Morphology-Inspired Word Segmentation for Neural Machine Translation},
  booktitle    = {Databases and Information Systems X: Selected Papers from the Thirteenth International Baltic Conference, DB\&IS 2018},
  year         = {2019},
  volume       = {315},
  pages        = {225},
  organization = {IOS Press},
}

@InProceedings{waibel2019multimodal,
  author       = {Waibel, Alexander},
  title        = {Multimodal dialogue processing for machine translation},
  booktitle    = {The Handbook of Multimodal-Multisensor Interfaces},
  year         = {2019},
  pages        = {577--620},
  organization = {Association for Computing Machinery and Morgan \& Claypool},
}

@Article{schafer2019neural,
  author = {Sch{\"a}fer, Johannes},
  title  = {Neural networks at hate speech and offensive language detection with a focus on linguistic features},
  year   = {2019},
}

@Article{wu2019pay,
  author  = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann N and Auli, Michael},
  title   = {Pay less attention with lightweight and dynamic convolutions},
  journal = {arXiv preprint arXiv:1901.10430},
  year    = {2019},
}

@Article{so2019evolved,
  author  = {So, David R and Liang, Chen and Le, Quoc V},
  title   = {The evolved transformer},
  journal = {arXiv preprint arXiv:1901.11117},
  year    = {2019},
}

@InProceedings{stojanovski2019lmu,
  author    = {Stojanovski, Dario and Hangya, Viktor and Huck, Matthias and Fraser, Alexander},
  title     = {The LMU Munich Unsupervised Machine Translation System for WMT19},
  booktitle = {Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)},
  year      = {2019},
  pages     = {393--399},
}

@Article{post2018call,
  author  = {Post, Matt},
  title   = {A call for clarity in reporting bleu scores},
  journal = {arXiv preprint arXiv:1804.08771},
  year    = {2018},
}

@Article{steiner2018building,
  author = {Steiner, Petra and Ruppenhofer, Josef},
  title  = {Building a morphological treebank for German from a linguistic database},
  year   = {2018},
}

@Article{altinok2018demorphy,
  author  = {Altinok, Duygu},
  title   = {DEMorphy, German Language Morphological Analyzer},
  journal = {arXiv preprint arXiv:1803.00902},
  year    = {2018},
}

@Article{macken2018dutch,
  author    = {Macken, Lieve and Tezcan, Arda},
  title     = {Dutch compound splitting for bilingual terminology extraction},
  journal   = {Multiword Units in Machine Translation and Translation Technology},
  year      = {2018},
  volume    = {341},
  publisher = {John Benjamins Publishing Company},
}

@InProceedings{shearing2018improving,
  author    = {Shearing, Steven and Kirov, Christo and Khayrallah, Huda and Yarowsky, David},
  title     = {Improving Low Resource Machine Translation using Morphological Glosses (Non-archival Extended Abstract)},
  booktitle = {Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)},
  year      = {2018},
  pages     = {132--139},
}

@InProceedings{pinter2018investigating,
  author       = {Pint{\'e}r, G{\'a}bor and Schielke, Mira and Petrick, Rico},
  title        = {Investigating Word Segmentation Techniques for German Using Finite-State Transducers},
  booktitle    = {International Conference on Speech and Computer},
  year         = {2018},
  pages        = {511--521},
  organization = {Springer},
}

@InProceedings{hellwig2018multi,
  author    = {Hellwig, Oliver and Hettrich, Heinrich and Modi, Ashutosh and Pinkal, Manfred},
  title     = {Multi-layer Annotation of the Rigveda},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018)},
  year      = {2018},
}

@InProceedings{tebbifakhr2018multi,
  author    = {Tebbifakhr, Amirhossein and Agrawal, Ruchit and Negri, Matteo and Turchi, Marco},
  title     = {Multi-source transformer with combined losses for automatic post editing},
  booktitle = {Proceedings of the Third Conference on Machine Translation: Shared Task Papers},
  year      = {2018},
  pages     = {846--852},
}

@InProceedings{amirhossein2018multi,
  author       = {Amirhossein, Tebbifakhr and Agrawal, Ruchit Rajeshkumar and Chatterjee, Rajen and Negri, Matteo and Turchi, Marco},
  title        = {Multi-source transformer with combined losses for automatic post editing},
  booktitle    = {Third Conference on Machine Translation (WMT)},
  year         = {2018},
  pages        = {859--865},
  organization = {The Association for Computational Linguistics},
}

@Article{ruslan2018multi,
  author    = {Ruslan, Mitkov and Monti, Johanna and Pastor, Gloria Corpas and Violeta, Seretan},
  title     = {Multi-word unit processing in Machine Translation},
  year      = {2018},
  publisher = {John Benjamins},
}

@InProceedings{pathak2018nmt,
  author    = {Pathak, Aditya Kumar and Choudhary, Himanshu and Shah, Rajiv Ratn},
  title     = {NMT based Tamil Translation.},
  booktitle = {FIRE (Working Notes)},
  year      = {2018},
  pages     = {316--322},
}

@Article{hucka2018nostril,
  author  = {Hucka, Michael},
  title   = {Nostril: A nonsense string evaluator written in Python.},
  journal = {J. Open Source Software},
  year    = {2018},
  volume  = {3},
  number  = {25},
  pages   = {596},
}

@InProceedings{zuters2018semi,
  author       = {Zuters, J{\=a}nis and Strazds, Gus and Immers, K{\=a}rlis},
  title        = {Semi-automatic Quasi-morphological Word Segmentation for Neural Machine Translation},
  booktitle    = {International Baltic Conference on Databases and Information Systems},
  year         = {2018},
  pages        = {289--301},
  organization = {Springer},
}

@Article{kunz2018shallow,
  author    = {Kunz, Kerstin and Lapshinova-Koltunski, Ekaterina and Mart{\'\i}nez, Jos{\'e} Manuel Mart{\'\i}nez and Menzel, Katrin and Steiner, Erich},
  title     = {Shallow features as indicators of English--German contrasts in lexical cohesion},
  journal   = {Languages in Contrast},
  year      = {2018},
  volume    = {18},
  number    = {2},
  pages     = {175--206},
  publisher = {John Benjamins},
}

@Article{hucka2018spiral,
  author  = {Hucka, Michael},
  title   = {Spiral: splitters for identifiers in source code files.},
  journal = {J. Open Source Software},
  year    = {2018},
  volume  = {3},
  number  = {24},
  pages   = {653},
}

@Article{markovtsev2018splitting,
  author  = {Markovtsev, Vadim and Long, Waren and Bulychev, Egor and Keramitas, Romain and Slavnov, Konstantin and Markowski, Gabor},
  title   = {Splitting source code identifiers using Bidirectional LSTM Recurrent Neural Network},
  journal = {arXiv preprint arXiv:1805.11651},
  year    = {2018},
}

@InProceedings{raganato2018university,
  author       = {Raganato, Alessandro and Scherrer, Yves and Nieminen, Tommi and Hurskainen, Arvi and Tiedemann, J{\"o}rg and others},
  title        = {The University of Helsinki submissions to the WMT18 news task},
  booktitle    = {Proceedings of the Third Conference on Machine Translation Shared Task Papers},
  year         = {2018},
  organization = {Association for Computational Linguistics},
}

@Article{haagsma2017critical,
  author  = {Haagsma, Hessel and Nissim, Malvina},
  title   = {A Critical Assessment of a Method for Detecting Diachronic Meaning Shifts: Lessons Learnt from Experiments on Dutch},
  journal = {Computational Linguistics in the Netherlands Journal},
  year    = {2017},
  volume  = {7},
  pages   = {65--78},
}

@Article{koleva2017automatic,
  author    = {Koleva, Mariya and Farasyn, Melissa and Desmet, Bart and Breitbarth, Anne and Hoste, V{\'e}ronique},
  title     = {An automatic part-of-speech tagger for Middle Low German},
  journal   = {International Journal of Corpus Linguistics},
  year      = {2017},
  volume    = {22},
  number    = {1},
  pages     = {107--140},
  publisher = {John Benjamins},
}

@InProceedings{ek2017mainstreaming,
  author       = {Ek, Adam and Knuutinen, Sofia},
  title        = {Mainstreaming August Strindberg with Text Normalization},
  booktitle    = {Proceedings of the 21st Nordic Conference on Computational Linguistics, NoDaLiDa, 22-24 May 2017, Gothenburg, Sweden},
  year         = {2017},
  number       = {131},
  pages        = {266--270},
  organization = {Link{\"o}ping University Electronic Press},
}

@Article{tamchyna2017modeling,
  author  = {Tamchyna, Ale{\v{s}} and Marco, Marion Weller-Di and Fraser, Alexander},
  title   = {Modeling target-side inflection in neural machine translation},
  journal = {arXiv preprint arXiv:1707.06012},
  year    = {2017},
}

@Article{caglayan2017nmtpy,
  author    = {Caglayan, Ozan and Garc{\'\i}a-Mart{\'\i}nez, Mercedes and Bardet, Adrien and Aransa, Walid and Bougares, Fethi and Barrault, Lo{\"\i}c},
  title     = {Nmtpy: A flexible toolkit for advanced neural machine translation systems},
  journal   = {The Prague Bulletin of Mathematical Linguistics},
  year      = {2017},
  volume    = {109},
  number    = {1},
  pages     = {15--28},
  publisher = {De Gruyter Open},
}

@Article{gulcehre2017integrating,
  author    = {Gulcehre, Caglar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Bengio, Yoshua},
  title     = {On integrating a language model into neural machine translation},
  journal   = {Computer Speech \& Language},
  year      = {2017},
  volume    = {45},
  pages     = {137--148},
  publisher = {Elsevier},
}

@Article{bjarnadottir2017phrasal,
  author    = {Bjarnad{\'o}ttir, Krist{\'\i}n},
  title     = {Phrasal compounds in Modern Icelandic with reference to Icelandic word formation in general},
  journal   = {Further investigations into the nature of phrasal compounding},
  year      = {2017},
  volume    = {1},
  pages     = {13},
  publisher = {Language Science Press},
}

@InProceedings{markantonatou2017proceedings,
  author    = {Markantonatou, Stella and Ramisch, Carlos and Savary, Agata and Vincze, Veronika},
  title     = {Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)},
  booktitle = {Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)},
  year      = {2017},
}

@InProceedings{weller2017simple,
  author    = {Weller-Di Marco, Marion},
  title     = {Simple compound splitting for German},
  booktitle = {Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)},
  year      = {2017},
  pages     = {161--166},
}

@Article{koehn2017six,
  author  = {Koehn, Philipp and Knowles, Rebecca},
  title   = {Six challenges for neural machine translation},
  journal = {arXiv preprint arXiv:1706.03872},
  year    = {2017},
}

@InProceedings{fujinuma2017substring,
  author    = {Fujinuma, Yoshinari and Grissom II, Alvin},
  title     = {Substring Frequency Features for Segmentation of Japanese Katakana Words with Unlabeled Corpora},
  booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  year      = {2017},
  pages     = {74--79},
}

@InProceedings{bahar2017rwth,
  author    = {Bahar, Parnia and Rosendahl, Jan and Rossenbach, Nick and Ney, Hermann},
  title     = {The RWTH Aachen machine translation systems for IWSLT 2017},
  booktitle = {14th International Workshop on Spoken Language Translation},
  year      = {2017},
  pages     = {29--34},
}

@Article{belinkov2017neural,
  author  = {Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
  title   = {What do neural machine translation models learn about morphology?},
  journal = {arXiv preprint arXiv:1704.03471},
  year    = {2017},
}

@Article{fabre2016apports,
  author = {Fabre, C{\'e}cile and Hathout, Nabil},
  title  = {Apports de la s{\'e}mantique distributionnelle dans les recherches en linguistique},
  year   = {2016},
}

@Article{korchagina2016building,
  author = {Korchagina, Natalia},
  title  = {Building a Gold Standard for Temporal Entity Extraction from Medieval German Texts},
  year   = {2016},
}

@Article{suter2016cross,
  author = {Suter, Julia},
  title  = {Cross-lingual Information Retrieval},
  year   = {2016},
}

@MastersThesis{nordal2016cross,
  author = {Nordal, Martin},
  title  = {Cross-lingual information retrieval using compound word splitting},
  school = {NTNU},
  year   = {2016},
}

@InProceedings{loaiciga2016discontinuous,
  author    = {Lo{\'a}iciga, Sharid and Gulordava, Kristina},
  title     = {Discontinuous Verb Phrases in Parsing and Machine Translation of English and German},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
  year      = {2016},
  pages     = {2839--2845},
}

@Article{LoaicigaSanchez2016,
  author = {Loaiciga Sanchez, Sharid and Gulordava, Kristina},
  title  = {Discontinuous Verb Phrases in Parsing and Machine Translation of English and German},
  year   = {2016},
}

@InProceedings{rigouts2016dutch,
  author       = {Rigouts Terryn, Ayla and Macken, Lieve and Lefever, Els},
  title        = {Dutch hypernym detection: does decompounding help?},
  booktitle    = {Joint Second Workshop on Language and Ontology \& Terminology and Knowledge Structures (LangOnto2+ TermiKS)},
  year         = {2016},
  pages        = {74--78},
  organization = {European Language Resources Association (ELRA)},
}

@Article{freitag2016fast,
  author  = {Freitag, Markus and Al-Onaizan, Yaser},
  title   = {Fast domain adaptation for neural machine translation},
  journal = {arXiv preprint arXiv:1612.06897},
  year    = {2016},
}

@InProceedings{pettersson2016histsearch,
  author    = {Pettersson, Eva and Lindstr{\"o}m, Jonas and Jacobsson, Benny and Fiebranz, Rosemarie},
  title     = {HistSearch-Implementation and Evaluation of a Web-based Tool for Automatic Information Extraction from Historical Text.},
  booktitle = {HistoInformatics@ DH},
  year      = {2016},
  pages     = {25--36},
}

@InProceedings{ma2016letter,
  author    = {Ma, Jianqiang and Henrich, Verena and Hinrichs, Erhard},
  title     = {Letter sequence labeling for compound splitting},
  booktitle = {Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  year      = {2016},
  pages     = {76--81},
}

@Article{niehues2016pre,
  author  = {Niehues, Jan and Cho, Eunah and Ha, Thanh-Le and Waibel, Alex},
  title   = {Pre-translation for neural machine translation},
  journal = {arXiv preprint arXiv:1610.05243},
  year    = {2016},
}

@InProceedings{elsner2016proceedings,
  author    = {Elsner, Micha and Kuebler, Sandra},
  title     = {Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  booktitle = {Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  year      = {2016},
}

@Article{steiner2016refurbishing,
  author = {Steiner, Petra},
  title  = {Refurbishing a morphological database for german},
  year   = {2016},
}

@InProceedings{ehrlemark2016retrieving,
  author    = {Ehrlemark, Anna and Johansson, Richard and Lyngfelt, Benjamin},
  title     = {Retrieving Occurrences of Grammatical Constructions},
  booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
  year      = {2016},
  pages     = {815--824},
}

@Article{doval2016segmentacion,
  author    = {Doval, Yerai and G{\'o}mez-Rodr{\'\i}guez, Carlos and Vilares, Jes{\'u}s},
  title     = {Segmentaci{\'o}n de palabras en espa{\~n}ol mediante modelos del lenguaje basados en redes neuronales},
  journal   = {Procesamiento del Lenguaje Natural},
  year      = {2016},
  number    = {57},
  pages     = {75--82},
  publisher = {Sociedad Espa{\~n}ola para el Procesamiento del Lenguaje Natural},
}

@Article{reuter2016segmenting,
  author  = {Reuter, Jack and Pereira-Martins, Jhonata and Kalita, Jugal},
  title   = {Segmenting twitter hashtags},
  journal = {International Journal on Natural Language Computing},
  year    = {2016},
  volume  = {5},
  pages   = {23--36},
}

@InProceedings{shapiro2016splitting,
  author    = {Shapiro, Naomi Tachikawa},
  title     = {Splitting compounds with ngrams},
  booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
  year      = {2016},
  pages     = {630--640},
}

@InProceedings{ding2016jhu,
  author    = {Ding, Shuoyang and Duh, Kevin and Khayrallah, Huda and Koehn, Philipp and Post, Matt},
  title     = {The JHU machine translation systems for WMT 2016},
  booktitle = {Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers},
  year      = {2016},
  pages     = {272--280},
}

@Article{гращенков2016автоматический,
  author    = {Гращенков, Павел Валерьевич},
  title     = {Автоматический анализ русских сложных слов},
  journal   = {Вестник Московского университета. Серия 9. Филология},
  year      = {2016},
  number    = {6},
  publisher = {Федеральное государственное бюджетное образовательное учреждение высшего~…},
}

@InProceedings{weijie2015analysis,
  author       = {Weijie, Li},
  title        = {Analysis of Shop Cold Water Stability of Asphalt Concrete Composite},
  booktitle    = {AASRI International Conference on Industrial Electronics and Applications (IEA 2015)},
  year         = {2015},
  organization = {Atlantis Press},
}

@Article{escartin2015assessing,
  author  = {Escart{\'\i}n, Carla Parra and Alonso, H{\'e}ctor Mart{\'\i}nez},
  title   = {Assessing WordNet for bilingual compound dictionary extraction},
  journal = {MULTI-WORD UNITS IN MACHINE TRANSLATION AND TRANSLATION TECHNOLOGIES MUMTTT2015},
  year    = {2015},
  pages   = {19},
}

@Article{roth2015kompositum,
  author    = {Roth, Tobias},
  title     = {Kompositum oder Kollokation? Konkurrenz an der Syntax-Morphologie-Schnittstelle},
  journal   = {Sprachgebrauch und Sprachbewusstsein: Implikationen f{\"u}r die Sprachtheorie},
  year      = {2015},
  pages     = {155},
  publisher = {Walter de Gruyter GmbH \& Co KG},
}

@Article{dadhason2015kvistur,
  author  = {Da{\dh}ason, J{\'o}n Fri{\dh}rik and Bjarnad{\'o}ttir, Krist{\'\i}n},
  title   = {Kvistur: V{\'e}lr{\ae}n stofnhlutagreining samsettra or{\dh}a},
  journal = {Or{\dh} og tunga},
  year    = {2015},
  volume  = {17},
  pages   = {115--132},
}

@Article{de2015labeling,
  author    = {de Buy Wenniger, Gideon Maillette and Sima’an, Khalil},
  title     = {Labeling hierarchical phrase-based models without linguistic resources},
  journal   = {Machine Translation},
  year      = {2015},
  volume    = {29},
  number    = {3-4},
  pages     = {225--265},
  publisher = {Springer},
}

@InProceedings{jean2015montreal,
  author    = {Jean, S{\'e}bastien and Firat, Orhan and Cho, Kyunghyun and Memisevic, Roland and Bengio, Yoshua},
  title     = {Montreal neural machine translation systems for WMT’15},
  booktitle = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
  year      = {2015},
  pages     = {134--140},
}

@InProceedings{tiedemann2015morphological,
  author    = {Tiedemann, J{\"o}rg and Ginter, Filip and Kanerva, Jenna},
  title     = {Morphological segmentation and OPUS for Finnish-English machine translation},
  booktitle = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
  year      = {2015},
  pages     = {177--183},
}

@Article{daiber2015splitting,
  author  = {Daiber, Joachim and Quiroz, Lautaro and Wechsler, Roger and Frank, Stella},
  title   = {Splitting compounds by semantic analogy},
  journal = {arXiv preprint arXiv:1509.04473},
  year    = {2015},
}

@Article{marasek2015spoken,
  author  = {Marasek, Krzysztof and Brocki, {\L}ukasz and Korzinek, Danijel and Wo{\l}k, Krzysztof and Gubrynowicz, Ryszard},
  title   = {Spoken language translation for polish},
  journal = {arXiv preprint arXiv:1511.07788},
  year    = {2015},
}

@Article{waibel2015sprachbarrieren,
  author  = {Waibel, Alexander},
  title   = {Sprachbarrieren durchbrechen: Traum oder Wirklichkeit?},
  journal = {Nova Acta Leopoldina NF},
  year    = {2015},
  volume  = {122},
  number  = {410},
  pages   = {101--123},
}

@InProceedings{peter2015rwth,
  author    = {Peter, Jan-Thorsten and Toutounchi, Farzad and Wuebker, Joern and Ney, Hermann},
  title     = {The RWTH Aachen German-English Machine Translation System for WMT 2015},
  booktitle = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
  year      = {2015},
  pages     = {158--163},
}

@InProceedings{schwartz2015university,
  author    = {Schwartz, Lane and Bryce, Bill and Geigle, Chase and Massung, Sean and Liu, Yisi and Peng, Haoruo and Raja, Vignesh and Roy, Subhro and Upadhyay, Shyam},
  title     = {The University of Illinois submission to the WMT 2015 Shared Translation Task},
  booktitle = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
  year      = {2015},
  pages     = {192--198},
}

@InProceedings{liu2015towards,
  author    = {Liu, Shuangyan and d'Aquin, Mathieu and Motta, Enrico},
  title     = {Towards Linked Data Fact Validation through Measuring Consensus.},
  booktitle = {LDQ@ ESWC},
  year      = {2015},
}

@Article{hellwig2015using,
  author  = {Hellwig, Oliver},
  title   = {Using Recurrent Neural Networks for joint compound splitting and Sandhi resolution in Sanskrit},
  journal = {Proceedings of the 7th LTC},
  year    = {2015},
  pages   = {289--293},
}

@Article{de2015wiktionary,
  author  = {de Melo, Gerard},
  title   = {Wiktionary-based word embeddings},
  journal = {Proceedings of MT Summit XV},
  year    = {2015},
  pages   = {346--359},
}

@InProceedings{van2014taxonomy,
  author    = {Van Huyssteen, Gerhard and Verhoeven, Ben},
  title     = {A Taxonomy for Afrikaans and Dutch compounds},
  booktitle = {Proceedings of the First Workshop on Computational Approaches to Compound Analysis (ComAComA 2014)},
  year      = {2014},
  pages     = {31--40},
}

@Article{kirkedal2014automatic,
  author = {Kirkedal, Andreas S{\o}eborg},
  title  = {Automatic Phonetic Transcription for Danish Speech Recognition},
  year   = {2014},
}

@InProceedings{cap2014cims,
  author    = {Cap, Fabienne and Weller, Marion and Ramm, Anita and Fraser, Alexander},
  title     = {CimS--The CIS and IMS joint submission to WMT 2014 translating from English into German},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  year      = {2014},
  pages     = {71--78},
}

@InProceedings{niehues2014combining,
  author    = {Niehues, Jan and Allauzen, Alexander and Yvon, Fran{\c{c}}ois and Waibel, Alex},
  title     = {Combining Techniques from Different NN-based Language Models for Machine Translation},
  booktitle = {Proceedings of the 11th Conference of the Association for Machine Translation in the Americas, Vancouver, BC, Canada},
  year      = {2014},
}

@InProceedings{bick2014constraint,
  author       = {Bick, Eckhard},
  title        = {Constraint Grammar-Based Swedish-Danish Machine Translation},
  booktitle    = {International Conference on Natural Language Processing},
  year         = {2014},
  pages        = {216--227},
  organization = {Springer},
}

@InProceedings{brun2014decomposing,
  author    = {Brun, Caroline and Roux, Claude},
  title     = {Decomposing Hashtags to Improve Tweet Polarity Classification (D{\'e}composition des {\u{G}} hash tags {\u{g}} pour l’am{\'e}lioration de la classification en polarit{\'e} des {\u{G}} tweets {\u{g}})[in French]},
  booktitle = {Proceedings of TALN 2014 (Volume 2: Short Papers)},
  year      = {2014},
  pages     = {473--478},
}

@InProceedings{nizamuddin2014detection,
  author    = {Nizamuddin, Uddin and Dalianis, Hercules},
  title     = {Detection of spelling errors in Swedish clinical text},
  booktitle = {1st Nordic workshop on evaluation of spellchecking and proofing tools (NorWEST2014), SLTC 2014, Uppsala},
  year      = {2014},
}

@InProceedings{weller2014distinguishing,
  author    = {Weller, Marion and Cap, Fabienne and M{\"u}ller, Stefan and im Walde, Sabine Schulte and Fraser, Alexander},
  title     = {Distinguishing degrees of compositionality in compound splitting for statistical machine translation},
  booktitle = {Proceedings of the First Workshop on Computational Approaches to Compound Analysis (ComAComA 2014)},
  year      = {2014},
  pages     = {81--90},
}

@InProceedings{durrani2014edinburgh,
  author    = {Durrani, Nadir and Haddow, Barry and Koehn, Philipp and Heafield, Kenneth},
  title     = {Edinburgh’s phrase-based machine translation systems for WMT-14},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  year      = {2014},
  pages     = {97--104},
}

@InProceedings{escartin2014german,
  author    = {Escart{\'\i}n, Carla Parra and Peitz, Stephan and Ney, Hermann},
  title     = {German Compounds and Statistical Machine Translation. Can they get along?},
  booktitle = {Proceedings of the 10th Workshop on Multiword Expressions (MWE)},
  year      = {2014},
  pages     = {48--56},
}

@InProceedings{fishel2014handling,
  author    = {Fishel, Mark and Sennrich, Rico},
  title     = {Handling Technical OOVs in SMT},
  booktitle = {Proceedings of The Seventeenth Annual Conference of the European Association for Machine Translation (EAMT)},
  year      = {2014},
  pages     = {159--162},
}

@InProceedings{mediani2014improving,
  author    = {Mediani, Mohammed and Winebarger, Joshua and Waibel, Alexander},
  title     = {Improving in-domain data selection for small in-domain sets},
  booktitle = {Proceedings of IWSLT},
  year      = {2014},
}

@InProceedings{durrani2014investigating,
  author    = {Durrani, Nadir and Koehn, Philipp and Schmid, Helmut and Fraser, Alexander},
  title     = {Investigating the usefulness of generalized word representations in SMT},
  booktitle = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  year      = {2014},
  pages     = {421--432},
}

@Article{ablimit2014lexicon,
  author    = {Ablimit, Mijit and Kawahara, Tatsuya and Hamdulla, Askar},
  title     = {Lexicon optimization based on discriminative learning for automatic speech recognition of agglutinative language},
  journal   = {Speech communication},
  year      = {2014},
  volume    = {60},
  pages     = {78--87},
  publisher = {Elsevier},
}

@InProceedings{ceausu2014pre,
  author       = {Ceausu, Alexandru and Hunsicker, Sabine},
  title        = {Pre-ordering of phrase-based machine translation input in translation workflow.},
  booktitle    = {LREC},
  year         = {2014},
  volume       = {2014},
  pages        = {3589--3592},
  organization = {Citeseer},
}

@InProceedings{Verhoeven2014,
  author    = {Verhoeven, Ben and Daelemans, Walter and van Zaanen, Menno and van Huyssteen, Gerhard},
  title     = {Proceedings of the First Workshop on Computational Approaches to Compound Analysis (ComAComA 2014)},
  booktitle = {Proceedings of the First Workshop on Computational Approaches to Compound Analysis (ComAComA 2014)},
  year      = {2014},
}

@Article{verhoeven2014proceedings,
  author = {Verhoeven, Ben and Daelemans, Walter and van Zaanen, Menno and van Huyssteen, Gerhard},
  title  = {Proceedings of the First Workshop on Computational Approaches to Compound Analysis held at the 25th International Conference on Computational Linguistics (COLING 2014)},
  year   = {2014},
}

@InProceedings{orasan2014proceedings,
  author    = {Orasan, Constantin and Osenova, Petya and Vertan, Cristina},
  title     = {Proceedings of the Workshop on Automatic Text Simplification-Methods and Applications in the Multilingual Society (ATS-MA 2014)},
  booktitle = {Proceedings of the Workshop on Automatic Text Simplification-Methods and Applications in the Multilingual Society (ATS-MA 2014)},
  year      = {2014},
}

@InCollection{wolk2014real,
  author    = {Wo{\l}k, Krzysztof and Marasek, Krzysztof},
  title     = {Real-time statistical speech translation},
  booktitle = {New Perspectives in Information Systems and Technologies, Volume 1},
  publisher = {Springer},
  year      = {2014},
  pages     = {107--113},
}

@Article{prokopidis2014report,
  author = {Prokopidis, Prokopis and Papavassiliou, Vassilis and Toral, Antonio and Poch, Marc and Frontini, Francesca and Rubino, Francesco and Thurmair, Gregor},
  title  = {Report on the revised Corpus Acquisition \& Annotation subsystem and its components},
  year   = {2014},
}

@PhdThesis{leidig2014single,
  author = {Leidig, Sebastian and Schlippe, Dipl-Inform Tim and Schultz, Ing Tanja},
  title  = {Single and Combined Features for the Detection of Anglicisms in German and Afrikaans},
  school = {Bachelor’s Thesis, Karlsruhe Institute of Technology},
  year   = {2014},
}

@InProceedings{pimpale2014smt,
  author    = {Pimpale, Prakash B and Patel, Raj Nath and Sasikumar, M},
  title     = {SMT from Agglutinative Languages: Use of Suffix Separation and Word Splitting},
  booktitle = {Proceedings of the 11th International Conference on Natural Language Processing},
  year      = {2014},
  pages     = {2--10},
}

@InProceedings{junczys2014smt,
  author    = {Junczys-Dowmunt, Marcin and Pouliquen, Bruno},
  title     = {SMT of German Patents at WIPO: Decompounding and Verb Structure Pre-reordering},
  booktitle = {Proceedings of the 17th Annual Conference of the European Association for Machine Translation (EAMT2014)},
  year      = {2014},
  pages     = {217--220},
}

@InProceedings{li2014dcu,
  author    = {Li, Liangyou and Wu, Xiaofeng and Vaillo, Santiago Cortes and Xie, Jun and Way, Andy and Liu, Qun},
  title     = {The dcu-ictcas mt system at wmt 2014 on german-english translation task},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  year      = {2014},
  pages     = {136--141},
}

@InProceedings{peitz2014rwth,
  author    = {Peitz, Stephan and Wuebker, Joern and Freitag, Markus and Ney, Hermann},
  title     = {The rwth aachen german-english machine translation system for wmt 2014},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  year      = {2014},
  pages     = {157--162},
}

@PhdThesis{loginova2014traitement,
  author = {Loginova Clouet, Elizaveta},
  title  = {Traitement automatique des termes compos{\'e}s: segmentation, traduction et variation},
  school = {Nantes},
  year   = {2014},
}

@InProceedings{raju2014translation,
  author       = {Raju, BNV Narasimha and Raju, MSVS Bhadri and Satyanarayana, KVV},
  title        = {Translation approaches in Cross Language Information Retrieval},
  booktitle    = {International Conference on Computing and Communication Technologies},
  year         = {2014},
  pages        = {1--4},
  organization = {IEEE},
}

@InProceedings{santos2014using,
  author    = {Santos, Pedro Bispo},
  title     = {Using compound lists for german decompounding in a back-off scenario},
  booktitle = {Workshop on Computational, Cognitive, and Linguistic Approaches to the Analysis of Complex Words and Collocations (CCLCC 2014)},
  year      = {2014},
  pages     = {51--55},
}

@InProceedings{pettersson2014verb,
  author    = {Pettersson, Eva and Megyesi, Be{\'a}ta and Nivre, Joakim},
  title     = {Verb Phrase Extraction in a Historical Context},
  booktitle = {The First Swedish National SWE-CLARIN Workshop, Swedish Language Technology Conference, 13th Nov 2014, Uppsala, Sweden},
  year      = {2014},
}

@InProceedings{owczarzak2014wordsyoudontknow,
  author    = {Owczarzak, Karolina and de Haan, Ferdinand and Krupka, George and Hindle, Don},
  title     = {Wordsyoudontknow: Evaluation of lexicon-based decompounding with unknown handling},
  booktitle = {Proceedings of the First Workshop on Computational Approaches to Compound Analysis (ComAComA 2014)},
  year      = {2014},
  pages     = {63--71},
}

@InProceedings{sennrich2014zmorge,
  author    = {Sennrich, Rico and Kunz, Beat},
  title     = {Zmorge: A German Morphological Lexicon Extracted from Wiktionary.},
  booktitle = {LREC},
  year      = {2014},
  pages     = {1063--1067},
}

@Article{pegoalgumas,
  author = {P{\^e}go, T{\^a}nia Regina and do INESC, Grupo de Linguagen Natura{\l}},
  title  = {ALGUMAS OBSERVA{\c{C}}{\~O}ES},
}

@Article{decaracterizaccao,
  author = {de Sousa, Henrique Teixeira},
  title  = {Caracteriza{\c{c}}{\~a}o de um corpus jornal{\'\i}stico portugu{\^e}s},
}

@Article{conversaocid,
  author = {CONVERS{\~A}O, TRANSDUTOR DE ESTADOS FINITOS PARA and POTIGUAR, A PRON{\'U}NCIA DA VARIEDADE LINGU{\'I}STICA},
  title  = {CID IVAN DA COSTA CARVALHO},
}

@Article{ferreiraclassificaccao,
  author = {Ferreira, Marcelo Herton Pereira},
  title  = {Classifica{\c{c}}{\~a}o de pe{\c{c}}as processuais jur{\'\i}dicas: Intelig{\^e}ncia Artificial no Direito},
}

@Article{decurso,
  author = {DE APLICA{\c{C}}{\~O}ES, UMA API PARA CRIA{\c{C}}{\~A}O and DE RECUPERA{\c{C}}{\~A}O, DE},
  title  = {CURSO DE CI{\^E}NCIA DA COMPUTA{\c{C}}{\~A}O},
}

@Article{martinscurso,
  author = {MARTINS, FRANCIMARY MAC{\^E}DO and XX, XIXE},
  title  = {CURSO DE DOUTORADO INTERINSTITUCIONAL UFC/IFMA/UFMA/SEDUC},
}

@Article{dosdicionauro,
  author = {dos Santos, No{\'e}lia Maria F and Rodrigues, Manuel Ant{\'o}nio R},
  title  = {Dicionauro: Dicion{\'a}rio Infantil Multilingue e Multim{\'e}dia},
}

@Article{ferreiraera,
  author = {FERREIRA, LU{\'I}S GOMES},
  title  = {Er{\~A}!` rio mineral Lu{\~A}s Gomes Ferreira-SciELO Livros},
}

@Article{narukawaestudo,
  author = {NARUKAWA, CRISTINA MIYUKI},
  title  = {ESTUDO DE VOCABUL{\'A}RIO CONTROLADO NA INDEXA{\c{C}}{\~A}O AUTOM{\'A}TICA},
}

@PhdThesis{cachetaexploraccao,
  author = {Cacheta, Ana Katariny de Souza},
  title  = {Explora{\c{c}}{\~a}o da minera{\c{c}}{\~a}o de texto em documentos da sa{\'u}de em diferentes idiomas para acompanhamento m{\'e}dico de pacientes com doen{\c{c}}as cr{\^o}nicas},
  school = {Universidade de S{\~a}o Paulo},
}

@PhdThesis{miurageraccao,
  author = {Miura, Newton Kiyotaka},
  title  = {Gera{\c{c}}{\~a}o incremental de parsers dependentes de contexto para o portugu{\^e}s brasileiro.},
  school = {Universidade de S{\~a}o Paulo},
}

@Article{alvesile,
  author  = {Alves, Kelvin},
  title   = {IL{\^E} AS{\'E} INTERNET--CANDOMBLECISTAS NO CIBERESPA{\c{C}}O},
  journal = {ESTUDANDO CULTURA E COMUNICA{\c{C}}{\~A}O COM M{\'I}DIAS SOCIAIS},
  pages   = {112},
}

@PhdThesis{moreiralinguistica,
  author = {Moreira Filho, Jos{\'e} Lopes},
  title  = {Lingu{\'\i}stica e computa{\c{c}}{\~a}o em di{\'a}logo para an{\'a}lise de textos e cria{\c{c}}{\~a}o de atividades de leitura em l{\'\i}ngua inglesa},
  school = {Universidade de S{\~a}o Paulo},
}

@Article{oechslerprocessamento,
  author = {OECHSLER, THIAGO MINHAQUI},
  title  = {PROCESSAMENTO DE TEXTO ESCRITO EM LINGUAGEM NATURAL PARA UM SISTEMA CONVERSOR TEXTO-FALA},
}

@Article{dasystem,
  author = {da Silva, Edson Marchetti and Souza, Renato Rocha},
  title  = {System of information retrieval by searching compared, using as descriptors multiwords expressions obtained using a technique that evaluates the structure of the document},
}

@PhdThesis{chaerestudo,
  author = {Chaer, I{\'u}ri},
  title  = {Um estudo sobre a Teoria da Predi{\c{c}}{\~a}o aplicada {\`a} an{\'a}lise sem{\^a}ntica de Linguagens Naturais.},
  school = {Universidade de S{\~a}o Paulo},
}

@Article{costauniversidade,
  author = {COSTA, ALINE SILVA},
  title  = {UNIVERSIDADE ESTADUAL DO SUDOESTE DA BAHIA},
}

@Article{nogueira2019analise,
  author    = {Nogueira, Iara Cristina Ara{\'u}jo and others},
  title     = {An{\'a}lise do programa bag of tools na sua funcionalidade sobre conceitos matem{\'a}ticos, lingu{\'\i}sticos e computacionais},
  year      = {2019},
  publisher = {Universidade Federal Rural do Semi-{\'A}rido},
}

@Article{neto2018analise,
  author = {Neto, Bezerra and Arruda, Nic{\'a}cio},
  title  = {An{\'a}lise de sentimentos do Twitter como suporte aditivo para a previs{\~a}o da volatilidade do Bitcoin},
  year   = {2018},
}

@Article{figueiredo2018coletando,
  author    = {Figueiredo, Thomas Nogueira and others},
  title     = {Coletando e analisando informa{\c{c}}{\~o}es do Facebook com a linguagem R},
  year      = {2018},
  publisher = {Instituto Federal de Educa{\c{c}}{\~a}o, Ci{\^e}ncia e Tecnologia do Rio Grande do Norte},
}

@Article{gaspar2018construccao,
  author = {Gaspar, Wagner},
  title  = {Constru{\c{c}}{\~a}o, Compara{\c{c}}{\~a}o e Agrupamento Autom{\'a}ticos de Mapas Conceituais Como Apoio a Atividades Pedag{\'o}gicas},
  year   = {2018},
}

@MastersThesis{marques2018desenvolvimento,
  author = {Marques, F{\'a}bio Andrews Rocha},
  title  = {Desenvolvimento e avalia{\c{c}}{\~a}o do Nihongo Kotoba Shiken: um exame computadorizado de conhecimento da l{\'\i}ngua japonesa},
  school = {Brasil},
  year   = {2018},
}

@Article{pastro2018detecccao,
  author = {Pastro, Jonata Teixeira},
  title  = {Detec{\c{c}}{\~a}o de tweets sobre eventos de tr{\^a}nsito usando word embedding},
  year   = {2018},
}

@Article{finatto2018linguistica,
  author    = {Finatto, Maria Jos{\'e} Bocorny and Rebechi, Rozane Rodrigues and Sarmento, Simone and Bocorny, Ana Eliza Pereira},
  title     = {Lingu{\'\i}stica de corpus: perspectivas},
  year      = {2018},
  publisher = {UFRGS. Instituto de Letras},
}

@MastersThesis{santos2018plataforma,
  author = {Santos, Diego Soares dos},
  title  = {Uma plataforma distribu{\'\i}da de minera{\c{c}}{\~a}o de dados para big data: um estudo de caso aplicado {\`a} Secretaria de Tributa{\c{c}}{\~a}o do Rio Grande do Norte},
  school = {Brasil},
  year   = {2018},
}

@Article{silva2017aplicaccao,
  author    = {Silva, Ja{\'\i}naldo da Silva and BugineUGINE, Rayssa Reis},
  title     = {Aplica{\c{c}}{\~a}o de t{\'e}cnicas de minera{\c{c}}{\~a}o de texto para classifica{\c{c}}{\~a}o autom{\'a}tica de de requisi{\c{c}}{\~o}es de servi{\c{c}}os de TI},
  year      = {2017},
  publisher = {Ronaldo Amaral Santos},
}

@Article{antunes2017formaccao,
  author    = {Antunes, Roger Alfredo de Marci Rodrigues and others},
  title     = {Forma{\c{c}}{\~a}o de gent{\'\i}licos a partir de top{\^o}nimos: proposta de gera{\c{c}}{\~a}o autom{\'a}tica},
  year      = {2017},
  publisher = {Universidade Federal de S{\~a}o Carlos},
}

@Article{amaral2017recomendaccao,
  author = {Amaral, Wagner Gomes do},
  title  = {Recomenda{\c{c}}{\~a}o de vocabul{\'a}rios para mapeamento de dados conectados},
  year   = {2017},
}

@Article{belau2017proposta,
  author    = {Belau, Francini Scipioni},
  title     = {Uma proposta de representa{\c{c}}{\~a}o lingu{\'\i}stico-computacional da nega{\c{c}}{\~a}o com vistas {\`a} an{\'a}lise de sentimentos em contexto de ensino e aprendizagem on-line},
  year      = {2017},
  publisher = {Universidade do Vale do Rio dos Sinos},
}

@MastersThesis{fragoso2016algoritmos,
  author = {Fragoso, Rog{\'e}rio C{\'e}sar Peixoto},
  title  = {Algoritmos de sele{\c{c}}{\~a}o de caracter{\'\i}sticas personalizados por classe para categoriza{\c{c}}{\~a}o de texto},
  school = {Universidade Federal de Pernambuco},
  year   = {2016},
}

@Article{oliveira2016analise,
  author    = {Oliveira, Carolina Cardoso de and others},
  title     = {An{\'a}lise morfol{\'o}gica dos termos das pragas da cana-de-a{\c{c}}{\'u}car: subs{\'\i}dios para organiza{\c{c}}{\~a}o de uma base de dados morfol{\'o}gicos para o portugu{\^e}s},
  year      = {2016},
  publisher = {Universidade Federal de S{\~a}o Carlos},
}

@PhdThesis{de2016classes,
  author = {de Freitas, Maria Cl{\'a}udia},
  title  = {Classes de palavras—da Gr{\'e}cia Antiga ao Google: Um estudo motivado pela convers{\~a}o de tagsets},
  school = {PUC-Rio},
  year   = {2016},
}

@PhdThesis{rossi2016classificaccao,
  author = {Rossi, Rafael Geraldeli},
  title  = {Classifica{\c{c}}{\~a}o autom{\'a}tica de textos por meio de aprendizado de m{\'a}quina baseado em redes},
  school = {Universidade de S{\~a}o Paulo},
  year   = {2016},
}

@Article{wilkens2016distinccao,
  author = {Wilkens, Rodrigo Souza},
  title  = {Distin{\c{c}}{\~a}o de grupos lingu{\'\i}sticos atrav{\'e}s de desempenho da linguagem},
  year   = {2016},
}

@PhdThesis{torres2016escrita,
  author = {Torres, Lianet Sep{\'u}lveda},
  title  = {Escrita cient{\'\i}fica em portugu{\^e}s por hispano falantes: recursos lingu{\'\i}sticos-computacionais baseados em m{\'e}todos de alinhamento de textos paralelos},
  school = {Universidade de S{\~a}o Paulo},
  year   = {2016},
}

@PhdThesis{paim2016inferencia,
  author = {Paim, ALDO MARCELO},
  title  = {Infer{\^e}ncia de personalidade a partir de textos em portugu{\^e}s brasileiro utilizando l{\'e}xicos},
  school = {PhD thesis, Pontif{\i}cia Universidade Cat{\'o}lica do Paran{\'a}},
  year   = {2016},
}

@Article{de2016investigaccao,
  author = {de Souza Dias, M{\'a}rcio},
  title  = {Investiga{\c{c}}{\~a}o de modelos de coer{\^e}ncia local para sum{\'a}rios multidocumento},
  year   = {2016},
}

@PhdThesis{dias2016investigaccao,
  author = {Dias, M{\'a}rcio de Souza},
  title  = {Investiga{\c{c}}{\~a}o de modelos de coer{\^e}ncia local para sum{\'a}rios multidocumento},
  school = {Universidade de S{\~a}o Paulo},
  year   = {2016},
}

@Article{lima2016mineraccao,
  author    = {Lima, Ana Carolina Esp{\'\i}rito Santo and others},
  title     = {Minera{\c{c}}{\~a}o de m{\'\i}dias sociais como ferramenta para a an{\'a}lise de tr{\'\i}ade da persona virtual},
  year      = {2016},
  publisher = {Universidade Presbiteriana Mackenzie},
}

@Article{ferreira2016ontospares,
  author    = {Ferreira, Albertina Maria Gomes},
  title     = {OntoSPARES: da linguagem natural {\`a}s ontologias. Contributos para a classifica{\c{c}}{\~a}o autom{\'a}tica de dados hist{\'o}ricos (s{\'e}c. XVI-XVIII)},
  year      = {2016},
  publisher = {Universidade de {\'E}vora},
}

@PhdThesis{ferman2016programaccao,
  author = {Ferman, Fabio},
  title  = {PROGRAMA{\c{C}}{\~A}O GEN{\'E}TICA DE {\'A}RVORES DE REGRAS PARA NORMALIZA{\c{C}}{\~A}O DE TEXTOS},
  school = {Universidade Federal do Rio de Janeiro},
  year   = {2016},
}

@Article{alves2016riso,
  author    = {Alves, George Marcelo Rodrigues and others},
  title     = {RISO-GCT--Determina{\c{c}}{\~a}o do contexto temporal de conceitos em textos.},
  year      = {2016},
  publisher = {Universidade Federal de Campina Grande},
}

@Article{fernandes2016serendipity,
  author    = {Fernandes, Woquiton Lima and others},
  title     = {Serendipity prospec{\c{c}}{\~a}o sem{\^a}ntica de dados qualitativos em Educa{\c{c}}{\~a}o Especial},
  year      = {2016},
  publisher = {Universidade Federal de S{\~a}o Carlos},
}

@Article{forte2015analise,
  author = {Forte, Ana Catarina Barbosa},
  title  = {An{\'a}lise de coment{\'a}rios de clientes com o aux{\'\i}lio a t{\'e}cnicas de Text Mining para determinar o n{\'\i}vel de (in) satisfa{\c{c}}{\~a}o},
  year   = {2015},
}

@Article{fioravanti2015unidades,
  author    = {Fioravanti, Sueli Cabrera},
  title     = {As unidades heterogen{\'e}ricas em dicion{\'a}rios bil{\'\i}ngues de espanhol para aprendizes brasileiros: an{\'a}lise do tratamento lexicogr{\'a}fico},
  year      = {2015},
  publisher = {Universidade Estadual Paulista (UNESP)},
}

@Article{thiele2015desambiguaccao,
  author    = {Thiele, Pablo Frederico Oliveira and others},
  title     = {Desambigua{\c{c}}{\~a}o de anota{\c{c}}{\~o}es morfossint{\'a}ticas feitas por MTMDD},
  year      = {2015},
  publisher = {Pontif{\'\i}cia Universidade Cat{\'o}lica do Rio Grande do Sul},
}

@MastersThesis{picoli2015descriccao,
  author = {Picoli, Larissa},
  title  = {Descri{\c{c}}{\~a}o de verbos de base adjetiva derivados com os sufixos ecer e izar, para processamento autom{\'a}tico de linguagem natural.},
  school = {Universidade Federal do Esp{\'\i}rito Santo},
  year   = {2015},
}

@Article{abreu2015encadear,
  author  = {Abreu, Carla and Teixeira, Jorge and Oliveira, Eug{\'e}nio},
  title   = {ENCADEAr: ENCADEAmento autom{\'a}tico de not{\'\i}cias},
  journal = {Oslo Studies in Language},
  year    = {2015},
  volume  = {7},
  number  = {1},
}

@Article{prestes2015extraccao,
  author = {Prestes, Kassius Vargas},
  title  = {Extra{\c{c}}{\~a}o multil{\'\i}ngue de termos multipalavra em corpora compar{\'a}veis},
  year   = {2015},
}

@PhdThesis{correia2015syntax,
  author = {Correia, Jos{\'e} Miguel Pinheiro and Baptista, Jorge and Mamede, Nuno},
  title  = {Syntax Deep Explorer},
  school = {Ph. D. thesis. Instituto Superior T{\'e}cnico},
  year   = {2015},
}

@MastersThesis{fiorio2015software,
  author = {Fiorio, Rosaine},
  title  = {Um software de apoio {\`a} aprendizagem de gram{\'a}tica e estilo liter{\'a}rio da l{\'\i}ngua portuguesa brasileira},
  school = {Universidade Tecnol{\'o}gica Federal do Paran{\'a}},
  year   = {2015},
  type   = {{B.S.} thesis},
}

@PhdThesis{cabrita2014identificar,
  author = {Cabrita, Viviana and Mamede, Nuno and Baptista, Jorge},
  title  = {Identificar, ordenar e relacionar eventos},
  school = {Master thesis, Instituto Superior T{\'e}cnico, Universidade T{\'e}cnica de Lisboa~…},
  year   = {2014},
}

@PhdThesis{carregosa2014sistema,
  author = {Carregosa, Felipe Borda},
  title  = {SISTEMA WEB INTERATIVO E ADAPTATIVO DE RESPOSTA AUTOM{\'A}TICA},
  school = {Universidade Federal do Rio de Janeiro},
  year   = {2014},
}

@PhdThesis{de2013categorizaccao,
  author = {de Oliveira Gomes, Neide},
  title  = {Categoriza{\c{c}}{\~a}o de Textos-Estudo de Caso: Documentos de Pedidos de Patente no Idioma Portugu{\^e}s},
  school = {PUC--Rio},
  year   = {2013},
}

@Article{gamonal2013copa,
  author    = {Gamonal, Maucha Andrade and others},
  title     = {Copa 2014 FrameNet Brasil: diretrizes para a constitui{\c{c}}{\~a}o de um dicion{\'a}rio eletr{\^o}nico tril{\'\i}ngue a partir da an{\'a}lise de frames da experi{\^e}ncia tur{\'\i}stica},
  year      = {2013},
  publisher = {Universidade Federal de Juiz de Fora},
}

@Article{hilgert2013extraccao,
  author    = {Hilgert, Lucas Welter and others},
  title     = {Extra{\c{c}}{\~a}o de vocabul{\'a}rio multil{\'\i}ngue a partir de documenta{\c{c}}{\~a}o de software},
  year      = {2013},
  publisher = {Pontif{\'\i}cia Universidade Cat{\'o}lica do Rio Grande do Sul},
}

@Article{vicente2013lexman,
  author  = {Vicente, Alexandre},
  title   = {LexMan: um Segmentador e Analisador Morfol{\'o}gico com transdutores},
  journal = {Unpublished master’s thesis, Instituto Superior T{\'e}cnico-Universidade T{\'e}cnica de Lisboa, Portugal},
  year    = {2013},
}

@Article{araujo2013abordagem,
  author    = {ARA{\'U}JO J{\'U}NIOR, Jos{\'e} Gildo de and others},
  title     = {Uma abordagem para a indexa{\c{c}}{\~a}o sem{\^a}ntica de documentos textuais baseada em fontes heterog{\^e}neas de informa{\c{c}}{\~a}o.},
  year      = {2013},
  publisher = {Universidade Federal de Campina Grande},
}

@Article{foschiera2012semantica,
  author    = {Foschiera, Silvia Matturro Panzardi},
  title     = {A sem{\^a}ntica da emo{\c{c}}{\~a}o: um estudo contrastivo a partir da FrameNet e da roda das emo{\c{c}}{\~o}es},
  year      = {2012},
  publisher = {Universidade do Vale do Rio dos Sinos},
}

@Article{lima2012analise,
  author    = {Lima, Ana Carolina Esp{\'\i}rito Santo and others},
  title     = {An{\'a}lise de sentimento e desambigua{\c{c}}{\~a}o no contexto da tv social},
  year      = {2012},
  publisher = {Universidade Presbiteriana Mackenzie},
}

@Article{coleti2012base,
  author    = {Coleti, Joel Sossai and others},
  title     = {Base de dados morfol{\'o}gicos de terminologias do portugu{\^e}s do Brasil: descri{\c{c}}{\~a}o e an{\'a}lise morfol{\'o}gica com vistas {\`a} disponibiliza{\c{c}}{\~a}o on-line},
  year      = {2012},
  publisher = {Universidade Federal de S{\~a}o Carlos},
}

@MastersThesis{rangel2012descriccao,
  author = {Rangel, Carlos Augusto Lessa},
  title  = {Descri{\c{c}}{\~a}o de Estruturas do Tipo Dar N1 em N2 para o Processamento Autom{\'a}tico de Linguagem Natural},
  school = {Universidade Federal do Esp{\'\i}rito Santo},
  year   = {2012},
}

@Article{dos2012inteligavsncia,
  author  = {DOS SANTOS, JONIMARIS},
  title   = {INTELIG{\~A}{\v{S}}NCIA ARTIFICIAL NO SISTEMA DE SUPORTE A PRODUTOS E SERVI{\~A}‡ OS EM CENTRAIS DE ATENDIMENTO AO CLIENTE},
  journal = {REPOSIT{\'O}RIO DE RELAT{\'O}RIOS-Sistemas de Informa{\c{c}}{\~a}o},
  year    = {2012},
  volume  = {1},
  number  = {2},
}

@Article{bastos2012presencca,
  author = {Bastos, Helvia Pereira Pinto},
  title  = {Presen{\c{c}}a plus: modelo de identifica{\c{c}}{\~a}o de presen{\c{c}}a social em ambientes virtuais de ensino e aprendizagem},
  year   = {2012},
}

@MastersThesis{baldan2012ambiente,
  author = {Baldan, Maikson A},
  title  = {Um Ambiente para Constru{\c{c}}{\~a}o de Perfis a Partir de Textos Pessoais},
  school = {Universidade Federal do Esp{\'\i}rito Santo},
  year   = {2012},
}

@Article{lara2012estudo,
  author = {Lara, Leandro Zanetti},
  title  = {Um estudo acerca da representa{\c{c}}{\~a}o sem{\^a}ntico-lexical no modelo da gram{\'a}tica discursivo-funcional},
  year   = {2012},
}

@Article{noll2012modelo,
  author    = {Noll, Rodrigo Perozzo and others},
  title     = {Um modelo para a an{\'a}lise de impacto em c{\'o}digo fonte usando ontologias e recupera{\c{c}}{\~a}o de informa{\c{c}}{\~a}o},
  year      = {2012},
  publisher = {Pontif{\'\i}cia Universidade Cat{\'o}lica do Rio Grande do Sul},
}

@Article{ceci2012modelo,
  author = {Ceci, Fl{\'a}vio and others},
  title  = {Um Modelo Semi-autom{\'a}tico Para a Constru{\c{c}}{\~a}o e Manuten{\c{c}}{\~a}o de Ontologias a partir de bases de documentos n{\~a}o estruturados},
  year   = {2012},
}

@PhdThesis{caputo2011construccao,
  author = {Caputo, Graziella Martins},
  title  = {CONSTRU{\c{C}}{\~A}O DE MAPAS CONCEITUAIS PARA DOM{\'I}NIOS ESPEC{\'I}FICOS DE DADOS OBTIDOS DA WEB},
  school = {Universidade Federal do Rio de Janeiro},
  year   = {2011},
}

@Article{severo2011mediation,
  author = {Severo, Carlos Emilio Padilla},
  title  = {E-mediation: mapeamento de ind{\'\i}cios de media{\c{c}}{\~a}o por meio de um sistema de minera{\c{c}}{\~a}o de textos},
  year   = {2011},
}

@Article{narukawa2011estudo,
  author    = {Narukawa, Cristina Miyuki},
  title     = {Estudo de vocabul{\'a}rio controlado na indexa{\c{c}}{\~a}o autom{\'a}tica: aplica{\c{c}}{\~a}o no processo de indexa{\c{c}}{\~a}o do Sistema de Indizaci{\'o}n Semiautomatica (SISA)},
  year      = {2011},
  publisher = {Universidade Estadual Paulista (UNESP)},
}

@Article{azevedo2011mineraforum,
  author = {Azevedo, Breno Fabr{\'\i}cio Terra},
  title  = {MineraF{\'o}rum: um recurso de apoio para an{\'a}lise qualitativa em f{\'o}runs de discuss{\~a}o},
  year   = {2011},
}

@InProceedings{figueira2011novo,
  author       = {Figueira, Helena and Mendes, Afonso and Mendes, Pedro and Pinto, Cl{\'a}udia},
  title        = {O novo acordo ortogr{\'a}fico e os correctores autom{\'a}ticos},
  booktitle    = {Lusofonia tempo de reciprocidades: actas/IX Congresso da Associa{\c{c}}{\~a}o Internacional de Lusitanistas, Madeira, 4 a 9 de agosto de 2008},
  year         = {2011},
  pages        = {65--78},
  organization = {Edi{\c{c}}{\^o}es Afrontamento},
}

@PhdThesis{zerbinatti2010extraccao,
  author = {Zerbinatti, Leandro},
  title  = {Extra{\c{c}}{\~a}o de conhecimento de laudos de radiologia tor{\'a}cica utilizando t{\'e}cnicas de processamento estat{\'\i}stico de linguagem natural.},
  school = {Universidade de S{\~a}o Paulo},
  year   = {2010},
}

@Article{santos2010findyourhelp,
  author    = {Santos, Marcos Lapa dos and others},
  title     = {FINDYOURHELP: UM M{\'O}DULO PARA APOIAR A BUSCA POR ESPECIALISTAS NO AMBIENTE VIRTUAL DE APRENDIZAGEM MOODLE},
  year      = {2010},
  publisher = {Universidade Salvador},
}

@PhdThesis{duque2010instanciaccao,
  author = {DUQUE, JULIANA LILIAN},
  title  = {INSTANCIA{\c{C}}{\~A}O, VALIDA{\c{C}}{\~A}O E EXTENS{\~A}O DE UMA METODOLOGIA DE EXTRA{\c{C}}{\~A}O DE INFORMA{\c{C}}{\~A}O PARA OS ASSUNTOS “EFEITOS” E “TRATAMENTOS” NA DOEN{\c{C}}A ANEMIA FALCIFORME},
  school = {UNIVERSIDADE FEDERAL DE S{\~A}O CARLOS},
  year   = {2010},
}

@Article{matos2010metodologia,
  author    = {Matos, Pablo Freire and others},
  title     = {Metodologia de pr{\'e}-processamento textual para extra{\c{c}}{\~a}o de informa{\c{c}}{\~a}o sobre efeitos de doen{\c{c}}as em artigos cient{\'\i}ficos do dom{\'\i}nio biom{\'e}dico},
  year      = {2010},
  publisher = {Universidade Federal de S{\~a}o Carlos},
}

@MastersThesis{Kowata2010,
  author = {Kowata, Juliana Hiroko},
  title  = {Uma abordagem computacional para constru{\c{c}}{\~a}o de mapas conceituais a partir de textos em l{\'\i}ngua portuguesa do Brasil},
  school = {Universidade Federal do Esp{\'\i}rito Santo},
  year   = {2010},
}

@MastersThesis{kowata2010abordagem,
  author = {KOWATA, JULIANA HIROKO},
  title  = {Uma Abordagem para Constru{\c{c}}{\~a}o Autom{\'a}tica de Mapas Conceituais a partir de textos em L{\'\i}ngua Portuguesa (Brasil)},
  school = {Universidade Federal do Esp{\'\i}rito Santo},
  year   = {2010},
}

@Article{falciforme2009ciencia,
  author = {Falciforme, Anemia and Matos, Pablo Freire},
  title  = {CI{\^E}NCIA DA COMPUTA{\c{C}}{\~A}O},
  year   = {2009},
}

@Article{moura2009contribuiccoes,
  author    = {Moura, Maria Fernanda},
  title     = {Contribui{\c{c}}{\~o}es para a constru{\c{c}}{\~a}o de taxonomias de t{\'o}picos em dom{\'\i}nios restritos utilizando aprendizado estat{\'\i}stico},
  journal   = {Embrapa Inform{\'a}tica Agropecu{\'a}ria-Tese/disserta{\c{c}}{\~a}o (ALICE)},
  year      = {2009},
  publisher = {2009.},
}

@MastersThesis{rodrigues2009descriccao,
  author = {RODRIGUES, CAS},
  title  = {Descri{\c{c}}{\~a}o de Formaliza{\c{c}}{\~a}o de Verbos de A{\c{c}}{\~a}o-Processo para Elabora{\c{c}}{\~a}o de Parser},
  school = {Universidade Federal do Esp{\'\i}rito Santo},
  year   = {2009},
}

@PhdThesis{palma2009expressoes,
  author = {Palma, Cristina Maria Balbino},
  title  = {Express{\~o}es fixas adverbiais: descri{\c{c}}{\~a}o l{\'e}xico-sint{\'a}ctica e subs{\'\i}dios para um estudo contrastivo portugu{\^e}s-espanhol},
  year   = {2009},
}

@PhdThesis{vilela2009geraccao,
  author = {Vilela, Rui Miguel Rodrigues dos Santos},
  title  = {Gera{\c{c}}{\~a}o de dicion{\'a}rios para correc{\c{c}}{\~a}o ortogr{\'a}fica do portugu{\^e}s},
  year   = {2009},
}

@Article{santos2009produccao,
  author    = {Santos, Guilherme Spolavori dos and others},
  title     = {Produ{\c{c}}{\~a}o de textos paralelos em l{\'\i}ngua portuguesa e uma interl{\'\i}ngua de Libras},
  year      = {2009},
  publisher = {Pontif{\'\i}cia Universidade Cat{\'o}lica do Rio Grande do Sul},
}

@Article{bazzon2009terminologia,
  author    = {Bazzon, Solange Cristina Maida and others},
  title     = {Terminologia da ind{\'u}stria de artefatos de borracha: proposta de um vocabul{\'a}rio},
  year      = {2009},
  publisher = {Universidade Federal de S{\~a}o Carlos},
}

@Article{braga2008algoritmos,
  author = {Braga, Daniela Filipa Macedo Moreira da and others},
  title  = {Algoritmos de processamento da linguagem natural para sistemas de conversao texto-fala em portugu{\^e}s},
  year   = {2008},
}

@Article{junior2008criaccao,
  author = {Junior, Arnaldo Candido},
  title  = {Cria{\c{c}}{\~a}o de um ambiente para o processamento de c{\'o}rpus de Portugu{\^e}s Hist{\'o}rico},
  year   = {2008},
}

@PhdThesis{candido2008criaccao,
  author = {Candido Junior, Arnaldo},
  title  = {Cria{\c{c}}{\~a}o de um ambiente para o processamento de c{\'o}rpus de Portugu{\^e}s Hist{\'o}rico},
  school = {Universidade de S{\~a}o Paulo},
  year   = {2008},
}

@PhdThesis{barbas2008extracccao,
  author = {Barbas, J{\'u}lio Fl{\'a}vio Tavares},
  title  = {Extrac{\c{c}}{\~a}o autom{\'a}tica de informa{\c{c}}{\~a}o e conhecimento em textos no {\^a}mbito B2B},
  school = {FCT-UNL},
  year   = {2008},
}

@Article{figueiras2008implicaccoes,
  author  = {Figueiras, Helena and Mendes, Afonso and Mendes, Pedro and Pinto, Cl{\'a}udia},
  title   = {Implica{\c{c}}{\~o}es do acordo ortogr{\'a}fico da l{\'\i}ngua portuguesa de 1990 em programas inform{\'a}ticos de corre{\c{c}}{\~a}o ortogr{\'a}fica},
  journal = {Revista Lus{\'o}fona de Humanidades e Tecnologias},
  year    = {2008},
  number  = {12},
}

@Article{ferreira2008anglicismo,
  author = {Ferreira, Karoll Ribeiro and others},
  title  = {O anglicismo na linguagem da minera{\c{c}}{\~a}o: um estudo explorat{\'o}rio},
  year   = {2008},
}

@Article{pasqualotti2008reconhecimento,
  author    = {Pasqualotti, Paulo Roberto},
  title     = {Reconhecimento de express{\~o}es de emo{\c{c}}{\~o}es na intera{\c{c}}{\~a}o mediada por computador},
  year      = {2008},
  publisher = {Universidade do Vale do Rio do Sinos},
}

@Article{domingues2007desenvolvimento,
  author    = {Domingues, Paulo Eduardo and others},
  title     = {Desenvolvimento de m{\'e}todo para consulta em linguagem natural de componentes de software},
  year      = {2007},
  publisher = {Pontif{\'\i}cia Universidade Cat{\'o}lica de Campinas},
}

@Article{da2007introduccao,
  author  = {da Silva, Bento Carlos Dias and Montilha, Gisele and Rino, Lucia Helena Machado and Specia, Lucia and Nunes, Maria das Gra{\c{c}}as Volpe and de Oliveira Jr, Osvaldo Novais and Martins, Ronaldo Teixeira and Pardo, Thiago Alexandre Salgueiro},
  title   = {Introdu{\c{c}}{\~a}o ao Processamento das L{\'\i}nguas Naturais e Algumas Aplica{\c{c}}{\~o}es},
  journal = {S{\'e}rie de Relat{\'o}rios do N{\'u}cleo Interinstitucional de Ling{\"u}{\'\i}stica Computacional},
  year    = {2007},
  volume  = {3},
}

@PhdThesis{carvalheira2007metodo,
  author = {Carvalheira, Luiz Carlos da Cruz},
  title  = {M{\'e}todo semi-autom{\'a}tico de constru{\c{c}}{\~a}o de ontologias parciais de dom{\'\i}nio com base em textos.},
  school = {Universidade de S{\~a}o Paulo},
  year   = {2007},
}

@Article{coelho2007stemming,
  author = {Coelho, Alexandre Ramos},
  title  = {Stemming para a l{\'\i}ngua portuguesa: estudo, an{\'a}lise e melhoria do algoritmo RSLP},
  year   = {2007},
}

@Article{aluisio2006se,
  author  = {Alu{\'\i}sio, Sandra Maria and de Barcellos Almeida, Gladis Maria},
  title   = {O que {\'e} e como se constr{\'o}i um corpus? Li{\c{c}}{\~o}es aprendidas na compila{\c{c}}{\~a}o de v{\'a}rios corpora para pesquisa lingu{\'\i}stica},
  journal = {Calidosc{\'o}pio},
  year    = {2006},
  volume  = {4},
  number  = {3},
  pages   = {156--178},
}

@Article{santos2006resumo,
  author = {Santos, Diana},
  title  = {Resumo da actividade da Linguateca de 15 de Maio de 2003 a 15 de Dezembro de 2006},
  year   = {2006},
}

@Article{bortolon2006modelo,
  author    = {Bortolon, Andre and others},
  title     = {Um modelo para a extra{\c{c}}{\~a}o de conceitos e estabelecimento de contextos em sistemas baseados em conhecimento},
  year      = {2006},
  publisher = {Florian{\'o}polis, SC},
}

@Article{garcia2005contxt,
  author = {Garcia, Luis Fernando Fortes},
  title  = {ConTXT: contexto como fator de adapta{\c{c}}{\~a}o em dispositivos de computa{\c{c}}{\~a}o m{\'o}vel},
  year   = {2005},
}

@Article{aires2005uso,
  author = {Aires, Rachel Virg{\'\i}nia Xavier},
  title  = {Uso de marcadores estil{\'\i}sticos para a busca na Web em portugu{\^e}s},
  year   = {2005},
}

@PhdThesis{macedo2004especificaccao,
  author = {Macedo, Alessandra Alaniz},
  title  = {Especifica{\c{c}}{\~a}o, instancia{\c{c}}{\~a}o e experimenta{\c{c}}{\~a}o de um arcabou{\c{c}}o para cria{\c{c}}{\~a}o autom{\'a}tica de liga{\c{c}}{\~o}es hipertexto entre informa{\c{c}}{\~o}es homog{\^e}neas},
  school = {Universidade de S{\~a}o Paulo},
  year   = {2004},
}

@Article{santos2003avalon,
  author    = {Santos, Diana and Rocha, Paulo},
  title     = {AvalON: uma iniciativa de avalia{\c{c}}{\~a}o conjunta para o portugu{\^e}s},
  journal   = {quot; In Am{\'a}lia Mendes; Tiago Freitas (ed) Actas do XVIII Encontro Nacional da Associa{\c{c}}{\~a}o Portuguesa de Lingu{\'\i}stica (APL 2002)(Porto Portugal 2-4 de Outubro de 2002) Lisboa: APL},
  year      = {2003},
  publisher = {APL},
}

@Article{razera2003etiquetador,
  author  = {Razera, Fernando Milan and Araujo, Marcus Paulo and Fraga, Vladimir Figueiredo},
  title   = {Etiquetador Morfol{\'o}gico para o Portugu{\^e}s},
  journal = {Trabalho de Conclus{\~a}o},
  year    = {2003},
}

@Article{aires2003linguarudo,
  author = {Aires, Rachel Virg{\'\i}nia Xavier},
  title  = {Linguarudo-Uma Arquitetura Ling{\"u}isticamente motivada para Recupera{\c{c}}{\~a}o de Informa{\c{c}}{\~a}o de textos em portugu{\^e}s},
  year   = {2003},
}

@Misc{specia2002desenvolvimento,
  author    = {Specia, Lucia and Rino, Lucia Helena Machado},
  title     = {O desenvolvimento de um l{\'e}xico para a gera{\c{c}}{\~a}o de estruturas conceituais UNL},
  year      = {2002},
  publisher = {S{\'e}rie de Relat{\'o}rios T{\'e}cnicos do NILC, NILC-TR-02-14. S{\~a}o Carlos, Setembro, 25p},
}

@Article{santos2002modelo,
  author    = {Santos, Francisco das Chagas Mendes dos and others},
  title     = {Um modelo de parser para aplica{\c{c}}{\~a}o em ambientes de projeto de sistemas mec{\^a}nicos},
  year      = {2002},
  publisher = {Florian{\'o}polis, SC},
}

@Misc{greghi2001processo,
  author    = {Greghi, Juliana Galvani and Martins, Ronaldo Teixeira and Nunes, Maria das Gra{\c{c}}as Volpe},
  title     = {O Processo de Desenvolvimento da BDL-NILC},
  year      = {2001},
  publisher = {S{\'e}rie de Relat{\'o}rios do NILC, NILC-TR-01-7. S{\~a}o Carlos, Outubro, 57p},
}

@Article{ranchhod2001uso,
  author  = {Ranchhod, Elisabete Marques},
  title   = {O uso de dicion{\'a}rios e de aut{\'o}matos finitos na representa{\c{c}}{\~a}o lexical das l{\'\i}nguas naturais},
  journal = {Tratamento das L{\'\i}nguas por Computador. Uma Introdu{\c{c}}{\~a}o {\`a} Lingu{\'\i}stica Computacional e suas Aplica{\c{c}}{\~o}es},
  year    = {2001},
  pages   = {13--47},
}

@Article{santos1999comparaccao,
  author  = {Santos, Diana},
  title   = {Compara{\c{c}}{\~a}o de corpora em portugu{\^e}s: algumas experi{\^e}ncias},
  journal = {L{\'\i}ngua Portuguesa no Computador, S{\~a}o Paulo},
  year    = {1999},
}

@Article{caccamo1998ambiente,
  author    = {C{\'a}ccamo, Mario Jos{\'e} and others},
  title     = {Um ambiente para a analise superficial de linguas baseado em automatos finitos},
  year      = {1998},
  publisher = {[sn]},
}

@PhdThesis{wives1997tecnicas,
  author = {Wives, Leandro Krug},
  title  = {T{\'e}cnicas de Recupera{\c{c}}{\~a}o de Informa{\c{c}}{\~o}es Com {\^E}nfase em Informa{\c{c}}{\~o}es Textuais},
  school = {UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL},
  year   = {1997},
}

@PhdThesis{baptista1995estabelecimento,
  author = {Baptista, Jorge},
  title  = {Estabelecimento e formaliza{\c{c}}{\~a}o de classes de nomes compostos},
  school = {Tese de mestrado, Faculdade de Letras da Universidade de Lisboa},
  year   = {1995},
}

@Article{wittmann1995portugues,
  author  = {Wittmann, Luzia Helena and P{\^e}go, T{\^a}nia and Santos, Diana and do INESC, Grupo de Linguagem Natural},
  title   = {Portugu{\^e}s Brasileiro e Portugu{\^e}s de Portugal: algumas observa{\c{c}}{\~o}es},
  journal = {Actas do XI Encontro Nacional da Associa{\c{c}}ao Portuguesa de Lingu{\i}stica},
  year    = {1995},
  pages   = {465--487},
}

@Article{Patel2019437,
  author          = {Patel, R.N. and Pimpale, P.B. and Sasikumar, M.},
  title           = {Machine translation in Indian languages: Challenges and resolution},
  journal         = {Journal of Intelligent Systems},
  year            = {2019},
  volume          = {28},
  number          = {3},
  pages           = {437-445},
  note            = {cited By 1},
  abstract        = {English to Indian language machine translation poses the challenge of structural and morphological divergence. This paper describes English to Indian language statistical machine translation using preordering and suffix separation. The preordering uses rules to transfer the structure of the source sentences prior to training and translation. This syntactic restructuring helps statistical machine translation to tackle the structural divergence and hence provides better translation quality. The suffix separation is used to tackle the morphological divergence between English and highly agglutinative Indian languages. We demonstrate that the use of preordering and suffix separation helps in improving the quality of English to Indian language machine translation. © 2019 Walter de Gruyter GmbH, Berlin/Boston.},
  affiliation     = {KBCS Division, Centre for Development of Advanced Computing, Gulmohar Cross Road No. 9, Opp Juhu Shopping Center, Juhu, Mumbai, 400049, India},
  author_keywords = {reordering; Statistical machine translation; suffix and compound splitting; transliteration},
  document_type   = {Article},
  doi             = {10.1515/jisys-2018-0014},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053124049&doi=10.1515%2fjisys-2018-0014&partnerID=40&md5=5b1e11decdb5614448ceda0e259a43a4},
}

@Conference{Sugisaki2018141,
  author        = {Sugisaki, K. and Tuggener, D.},
  title         = {German compound splitting using the compound productivity of morphemes},
  year          = {2018},
  pages         = {141-147},
  note          = {cited By 0},
  abstract      = {In this work, we present a novel compound splitting method for German by capturing the compound productivity of morphemes. We use a giga web corpus to create a lexicon and decompose noun compounds by computing the probabilities of compound elements as bound and free morphemes. Furthermore, we provide a uniformed evaluation of several unsupervised approaches and morphological analysers for the task. Our method achieved a high F1 score of 0.92, which was a comparable result to state-of-the-art methods. © KONVENS 2018 - Conference on Natural Language Processing / Die Konferenz zur Verarbeitung Naturlicher Sprache.All right reserved.},
  affiliation   = {German Department, University of Zurich, Schönberggasse 9, Zurich, 8001, Switzerland; School of Engineering, Zurich University of Applied Sciences, Steinberggasse 13, Winterthur, 8400, Switzerland},
  document_type = {Conference Paper},
  journal       = {KONVENS 2018 - Conference on Natural Language Processing / Die Konferenz zur Verarbeitung Naturlicher Sprache},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064207728&partnerID=40&md5=9ad88e6eff661a660d155b41ada1fbb9},
}

@Conference{Li2018175,
  author        = {Li, J. and Du, Q. and Shi, K. and He, Y. and Wang, X. and Xu, J.},
  title         = {Helpful or not? an investigation on the feasibility of identifier splitting via CNN-BiLSTM-CRF},
  year          = {2018},
  volume        = {2018-July},
  pages         = {175-181},
  note          = {cited By 0},
  abstract      = {We recently introduced a new technique to handle source code identifier splitting. The proposed technique, denoted as CNN-BiLSTM-CRF[a neural network composed of a convolutional neural network(CNN), bidirectional long short-Term memory networks(BiLSTM) and conditional random fields(CRFs)] enables us to obtain a model that splits identifiers correctly and effectively. This technique combines the use of a CNN layer with the mature BiLSTM-CRF model. The experimental results indicate that CNN-BiLSTM-CRF delivers outstanding performance on all four of the evaluation oracles. More importantly, we endeavored to provide insight into the practical feasibility of this technique by considering the aspects of generality, data size in demand and construction cost, etc. Finally, we reasoned out that CNN-BiLSTM-CRF should be helpful and improvable for identifier splitting in practical works in terms of the accuracy and feasibility. This was validated by multifaceted experiments. Index Terms-identifier splitting, source code mining, program comprehension, CNN, BiLSTM-CRF, feasibility investigation. © 2018 Universitat zu Koln. All rights reserved.},
  affiliation   = {School of Software Engineering, Tongji University, China; Software Engineering RandD Centre, Jishi Building, Tongji University, China; Smart City Labotary, Jishi Building, Tongji University, China; Shanghai Research and Development Center, Baidu Inc, China},
  document_type = {Conference Paper},
  doi           = {10.18293/SEKE2018-167},
  journal       = {Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056829823&doi=10.18293%2fSEKE2018-167&partnerID=40&md5=da7e9e1bd5ebdb234108fb0e7e22f2f9},
}

@Conference{NoAuthor2017,
  title         = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
  year          = {2017},
  volume        = {1},
  note          = {cited By 0},
  abstract      = {The proceedings contain 297 papers. The topics discussed include: adversarial multi-task learning for text classification; neural end-to-end learning for computational argumentation mining; neural symbolic machines: learning semantic parsers on freebase with weak supervision; morph-fitting: fine-tuning word vector spaces with simple language-specific rules; lexical features in coreference resolution: to be used with caution; alternative objective functions for training MT evaluation metrics; a principled framework for evaluating summarizers: comparing models of summary quality against human judgments; vector space models for evaluating semantic fluency in autism; evaluating compound splitters extrinsically with textual entailment; and an analysis of action recognition datasets for language and vision tasks.},
  document_type = {Conference Review},
  journal       = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
  page_count    = {2189},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040944171&partnerID=40&md5=32afcce8baaf4dd269f848c5e1f2df4d},
}

@Conference{2017,
  title         = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
  year          = {2017},
  volume        = {2},
  note          = {cited By 0},
  abstract      = {The proceedings contain 297 papers. The topics discussed include: adversarial multi-task learning for text classification; neural end-to-end learning for computational argumentation mining; neural symbolic machines: learning semantic parsers on freebase with weak supervision; morph-fitting: fine-tuning word vector spaces with simple language-specific rules; lexical features in coreference resolution: to be used with caution; alternative objective functions for training MT evaluation metrics; a principled framework for evaluating summarizers: comparing models of summary quality against human judgments; vector space models for evaluating semantic fluency in autism; evaluating compound splitters extrinsically with textual entailment; and an analysis of action recognition datasets for language and vision tasks.},
  document_type = {Conference Review},
  journal       = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
  page_count    = {2873},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040563381&partnerID=40&md5=f45eb642350433696b687c9df0520cb6},
}

@Conference{Jagfeld201758,
  author        = {Jagfeld, G. and Ziering, P. and Van Der Plas, L.},
  title         = {Evaluating compound splitters extrinsically with textual entailment},
  year          = {2017},
  volume        = {2},
  pages         = {58-63},
  note          = {cited By 0},
  abstract      = {Traditionally, compound splitters are evaluated intrinsically on gold-standard data or extrinsically on the task of statistical machine translation. We explore a novel way for the extrinsic evaluation of compound splitters, namely recognizing textual entailment. Compound splitting has great potential for this novel task that is both transparent and well-defined. Moreover, we show that it addresses certain aspects that are either ignored in intrinsic evaluations or compensated for by task-internal mechanisms in statistical machine translation. We show significant improvements using different compound splitting methods on a German textual entailment dataset. © 2017 Association for Computational Linguistics.},
  affiliation   = {Institute for Natural Language Processing, University of Stuttgart, Germany; Institute of Linguistics, University of Malta, Malta},
  document_type = {Conference Paper},
  doi           = {10.18653/v1/P17-2010},
  journal       = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040556164&doi=10.18653%2fv1%2fP17-2010&partnerID=40&md5=abad0047ea69c94b560a54599233b45f},
}

@Conference{Shishkova201642,
  author          = {Shishkova, A. and Chernyak, E.},
  title           = {Annotated suffix tree method for German compound splitting},
  year            = {2016},
  volume          = {1886},
  pages           = {42-47},
  note            = {cited By 0},
  abstract        = {The paper presents an unsupervised and knowledge-free approach to compound splitting. Although the research is focused on German compounds, the method is expected to be extensible to other compounding languages. The approach is based on the annotated suffix tree (AST) method proposed and modified by Mirkin et al. To the best of our knowledge, annotated suffix trees have not yet been used for compound splitting. The main idea of the approach is to match all the substrings of a word (suffixes and prefixes separately) against an AST, determining the longest and sufficiently frequent substring to perform a candidate split. A simplification considers only the suffixes (or prefixes) and splits a word at the beginning of the selected suffix (the longest and sufficiently frequent one). The results are evaluated by precision and recall.},
  affiliation     = {National Research University, Higher School of Economics, Moscow, Russian Federation},
  author_keywords = {Annotated suffix tree; Compound splitting; German language},
  document_type   = {Conference Paper},
  journal         = {CEUR Workshop Proceedings},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029175537&partnerID=40&md5=c9ad928c2c810005449090bf909e5a0a},
}

@Conference{NoAuthor2016,
  title         = {CEUR Workshop Proceedings},
  year          = {2016},
  volume        = {1886},
  note          = {cited By 0},
  abstract      = {The proceedings contain 16 papers. The topics discussed include: using a hybrid algorithm for lemmatization of a diachronic corpus; classification of e-commerce websites by product categories; automatic generation of lexical exercises; evaluation of distributional compositional operations on collocations through semantic similarity; identification of singleton mentions in Russian; annotated suffix tree method for German compound splitting; building NLP pipeline for Russian with a handful of linguistic knowledge; Morphchecker for nonstandard data: a tool for morphological error correction in learner corpora; formal concept lattices as semantic maps; syntactic annotation for a Hittite corpus: problems and principles; lexis meets meter: attraction of lexical units in Russian verse; dynamics of core of language vocabulary; big-data-augmented approach to emerging technologies identification: case of agriculture and food sector; quantum logic and natural language processing; and transforming RuThes thesaurus to generate Russian WordNet.},
  document_type = {Conference Review},
  journal       = {CEUR Workshop Proceedings},
  page_count    = {141},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029174464&partnerID=40&md5=a158b39ec293bf3538d7d84aff05054b},
}

@Conference{Khatiwada2016,
  author        = {Khatiwada, S. and Kelly, M. and Mahmoud, A.},
  title         = {STAC: A tool for Static Textual Analysis of Code},
  year          = {2016},
  volume        = {2016-July},
  note          = {cited By 4},
  abstract      = {Static textual analysis techniques have been recently applied to process and synthesize source code. The underlying tenet is that important information is embedded in code identifiers and internal code comments. Such information can be analyzed to provide automatic aid for several software engineering activities. To facilitate this line of work, we present STAC, a tool for supporting Static Textual Analysis of Code. STAC is designed as a light-weight stand-alone tool that provides a practical one-stop solution for code indexing. Code indexing is the process of extracting important textual information from source code. Accurate indexing has been found to significantly influence the performance of code retrieval and analysis methods. STAC provides features for extracting and processing textual patterns found in Java, C++, and C# code artifacts. These features include identifier splitting, stemming, lemmatization, and spell-checking. STAC is also provided as an API to help researchers to integrate basic code indexing features into their code. © 2016 IEEE.},
  affiliation   = {Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, LA 70808, United States},
  art_number    = {7503746},
  document_type = {Conference Paper},
  doi           = {10.1109/ICPC.2016.7503746},
  journal       = {IEEE International Conference on Program Comprehension},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979735899&doi=10.1109%2fICPC.2016.7503746&partnerID=40&md5=e2527d1a6b7dde72270740ca4502929d},
}

@Conference{Ziering2016644,
  author        = {Ziering, P. and Van Der Plas, L.},
  title         = {Towards unsupervised and language-independent compound splitting using inflectional morphological transformations},
  year          = {2016},
  pages         = {644-653},
  note          = {cited By 4},
  abstract      = {In this paper, we address the task of language-independent, knowledge-lean and unsupervised compound splitting, which is an essential component for many natural language processing tasks such as machine translation. Previous methods on statistical compound splitting either include language-specific knowledge (e.g., linking elements) or rely on parallel data, which results in limited applicability. We aim to overcome these limitations by learning compounding morphology from inflectional information derived from lemmatized monolingual corpora. In experiments for Germanic languages, we show that our approach significantly outperforms language-dependent stateof-the-art methods in finding the correct split point and that word inflection is a good approximation for compounding morphology. ©2016 Association for Computational Linguistics.},
  affiliation   = {Institute for Natural Language Processing, University of Stuttgart, Germany; Institute of Linguistics, University of Malta, Malta},
  document_type = {Conference Paper},
  journal       = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994128765&partnerID=40&md5=5b0b9b0941575eb4aac9f0edfea61654},
}

@Conference{Riedl2016617,
  author        = {Riedl, M. and Biemann, C.},
  title         = {Unsupervised compound splitting with distributional semantics rivals supervised methods},
  year          = {2016},
  pages         = {617-622},
  note          = {cited By 8},
  abstract      = {In this paper we present a word decompounding method that is based on distributional semantics. Our method does not require any linguistic knowledge and is initialized using a large monolingual corpus. The core idea of our approach is that parts of compounds (like "candle" and "stick") are semantically similar to the entire compound, which helps to exclude spurious splits (like "candles" and "tick"). We report results for German and Dutch: For German, our unsupervised method comes on par with the performance of a rule-based and a supervised method and significantly outperforms two unsupervised baselines. For Dutch, our method performs only slightly below a rule-based optimized compound splitter. ©2016 Association for Computational Linguistics.},
  affiliation   = {Language Technology, Computer Science Department, Technische Universität Darmstadt, Hochschulstrasse 10, Darmstadt, D-64289, Germany},
  document_type = {Conference Paper},
  journal       = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994120942&partnerID=40&md5=907e2881fa0d72e3de877cd94f519d2d},
}

@Conference{Hirschmann20163199,
  author        = {Hirschmann, F. and Nam, J. and Fürnkranz, J.},
  title         = {What makes word-level neural machine translation hard: A case study on English-German translation},
  year          = {2016},
  pages         = {3199-3208},
  note          = {cited By 3},
  abstract      = {Traditional machine translation systems often require heavy feature engineering and the combination of multiple techniques for solving different subproblems. In recent years, several end-to-end learning architectures based on recurrent neural networks have been proposed. Unlike traditional systems, Neural Machine Translation (NMT) systems learn the parameters of the model and require only minimal preprocessing. Memory and time constraints allow to take only a fixed number of words into account, which leads to the out-of-vocabulary (OOV) problem. In this work, we analyze why the OOV problem arises and why it is considered a serious problem in German. We study the effectiveness of compound word splitters for alleviating the OOV problem, resulting in a 2.5+ BLEU points improvement over a baseline on the WMT' 14 German-to-English translation task. For English-to-German translation, we use target-side compound splitting through a special syntax during training that allows the model to merge compound words and gain 0.2 BLEU points. © 1963-2018 ACL.},
  affiliation   = {Knowledge Engineering Group, Technische Universität Darmstadt, Darmstadt, Germany},
  document_type = {Conference Paper},
  journal       = {COLING 2016 - 26th International Conference on Computational Linguistics, Proceedings of COLING 2016: Technical Papers},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039147994&partnerID=40&md5=92b1d72c2067f0a3005d477c56d828af},
}

@Article{Carvalho2015117,
  author          = {Carvalho, N.R. and Almeida, J.J. and Henriques, P.R. and Varanda, M.J.},
  title           = {From source code identifiers to natural language terms},
  journal         = {Journal of Systems and Software},
  year            = {2015},
  volume          = {100},
  pages           = {117-128},
  note            = {cited By 10},
  abstract        = {Program comprehension techniques often explore program identifiers, to infer knowledge about programs. The relevance of source code identifiers as one relevant source of information about programs is already established in the literature, as well as their direct impact on future comprehension tasks. Most programming languages enforce some constrains on identifiers strings (e.g., white spaces or commas are not allowed). Also, programmers often use word combinations and abbreviations, to devise strings that represent single, or multiple, domain concepts in order to increase programming linguistic efficiency (convey more semantics writing less). These strings do not always use explicit marks to distinguish the terms used (e.g., CamelCase or underscores), so techniques often referred as hard splitting are not enough. This paper introduces Lingua::IdSplitter a dictionary based algorithm for splitting and expanding strings that compose multi-term identifiers. It explores the use of general programming and abbreviations dictionaries, but also a custom dictionary automatically generated from software natural language content, prone to include application domain terms and specific abbreviations. This approach was applied to two software packages, written in C, achieving a f-measure of around 90% for correctly splitting and expanding identifiers. A comparison with current state-of-the-art approaches is also presented. © 2014 Elsevier Inc.},
  affiliation     = {Department of Informatics, University of Minho, Campus de Gualtar, Braga, 4710-057, Portugal; Polytechnic Institute of Bragança, Campus de Santa Apolónia, Bragança, 5300-253, Portugal},
  author_keywords = {Identifier splitting; Natural language processing; Program comprehension},
  document_type   = {Article},
  doi             = {10.1016/j.jss.2014.10.013},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919460270&doi=10.1016%2fj.jss.2014.10.013&partnerID=40&md5=b2f690f4be9ae86e85985c8c428a0953},
}

@Article{Bretschneider2015207,
  author          = {Bretschneider, C. and Zillner, S.},
  title           = {Semantic splitting of German medical compounds},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2015},
  volume          = {9302},
  pages           = {207-215},
  note            = {cited By 0},
  abstract        = {Compounding is widespread in highly inflectional languages with a quarter of all nouns created by composition. In our field of study, the German medical language, the amount of compounds significantly outnumbers this figure with 64%. Thus, their correct splitting is a high-impact preprocessing step for any NLP-based application. In this work we address two challenges of medical decomposition: First, we introduce the consideration of unknown constituents in order to split compounds that were not recognized as such so far. Second, our approach builds on the corpus-based approach of Koehn and Knight and adds semantic knowledge from domain ontologies to increase the accuracy during disambiguation of the various split options. Using this first-of-a-kind semantic approach in a study on decomposition of German medical compounds, we outperform the existing approaches by far. © Springer International Publishing Switzerland 2015.},
  affiliation     = {Siemens AG, Corporate Technology, Munich, Germany; Center for Information and Language Processing, University Munich, Munich, Germany; School of International Business and Entrepreneurship, Steinbeis University, Berlin, Germany},
  author_keywords = {Compound splitting; Medical NLP; Ontology; Semantics},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-319-24033-6_24},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951778429&doi=10.1007%2f978-3-319-24033-6_24&partnerID=40&md5=b80f85ba2c958dce5bb50b1ba2869b01},
}

@Article{Pecina2014165,
  author          = {Pecina, P. and Dušek, O. and Goeuriot, L. and Hajič, J. and Hlaváčová, J. and Jones, G.J.F. and Kelly, L. and Leveling, J. and Mareček, D. and Novák, M. and Popel, M. and Rosa, R. and Tamchyna, A. and Urešová, Z.},
  title           = {Adaptation of machine translation for multilingual information retrieval in the medical domain},
  journal         = {Artificial Intelligence in Medicine},
  year            = {2014},
  volume          = {61},
  number          = {3},
  pages           = {165-185},
  note            = {cited By 23},
  abstract        = {Objective: We investigate machine translation (MT) of user search queries in the context of cross-lingual information retrieval (IR) in the medical domain. The main focus is on techniques to adapt MT to increase translation quality; however, we also explore MT adaptation to improve effectiveness of cross-lingual IR. Methods and data: Our MT system is Moses, a state-of-the-art phrase-based statistical machine translation system. The IR system is based on the BM25 retrieval model implemented in the Lucene search engine. The MT techniques employed in this work include in-domain training and tuning, intelligent training data selection, optimization of phrase table configuration, compound splitting, and exploiting synonyms as translation variants. The IR methods include morphological normalization and using multiple translation variants for query expansion. The experiments are performed and thoroughly evaluated on three language pairs: Czech-English, German-English, and French-English. MT quality is evaluated on data sets created within the Khresmoi project and IR effectiveness is tested on the CLEF eHealth 2013 data sets. Results: The search query translation results achieved in our experiments are outstanding - our systems outperform not only our strong baselines, but also Google Translate and Microsoft Bing Translator in direct comparison carried out on all the language pairs. The baseline BLEU scores increased from 26.59 to 41.45 for Czech-English, from 23.03 to 40.82 for German-English, and from 32.67 to 40.82 for French-English. This is a 55% improvement on average. In terms of the IR performance on this particular test collection, a significant improvement over the baseline is achieved only for French-English. For Czech-English and German-English, the increased MT quality does not lead to better IR results. Conclusions: Most of the MT techniques employed in our experiments improve MT of medical search queries. Especially the intelligent training data selection proves to be very successful for domain adaptation of MT. Certain improvements are also obtained from German compound splitting on the source language side. Translation quality, however, does not appear to correlate with the IR performance - better translation does not necessarily yield better retrieval. We discuss in detail the contribution of the individual techniques and state-of-the-art features and provide future research directions. © 2014 Elsevier B.V.},
  affiliation     = {Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University in Prague, Malostranské nám. 25, 118 00 Prague 1, Czech Republic; CNGL Centre for Global Intelligent Content, School of Computing, Dublin City University, Glasnevin, Dublin 9, Ireland},
  author_keywords = {Compound splitting; Cross-language information retrieval; Domain adaptation of statistical machine translation; Intelligent training data selection for machine translation; Medical query translation; Statistical machine translation},
  document_type   = {Article},
  doi             = {10.1016/j.artmed.2014.01.004},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904321749&doi=10.1016%2fj.artmed.2014.01.004&partnerID=40&md5=0f3963e3d9b30a067ad27552815ea4a4},
}

@Article{Hill20141754,
  author          = {Hill, E. and Binkley, D. and Lawrie, D. and Pollock, L. and Vijay-Shanker, K.},
  title           = {An empirical study of identifier splitting techniques},
  journal         = {Empirical Software Engineering},
  year            = {2014},
  volume          = {19},
  number          = {6},
  pages           = {1754-1780},
  note            = {cited By 15},
  abstract        = {Researchers have shown that program analyses that drive software development and maintenance tools supporting search, traceability and other tasks can benefit from leveraging the natural language information found in identifiers and comments. Accurate natural language information depends on correctly splitting the identifiers into their component words and abbreviations. While conventions such as camel-casing can ease this task, conventions are not well-defined in certain situations and may be modified to improve readability, thus making automatic splitting more challenging. This paper describes an empirical study of state-of-the-art identifier splitting techniques and the construction of a publicly available oracle to evaluate identifier splitting algorithms. In addition to comparing current approaches, the results help to guide future development and evaluation of improved identifier splitting approaches. © 2013, Springer Science+Business Media New York.},
  affiliation     = {Department of Computer Science, Montclair State University, Montclair, NJ 07043, United States; Department of Computer Science, Loyola University Maryland, Baltimore, MD 21210, United States; Department of Computer and Information Sciences, University of Delaware, Newark, DE 19716, United States},
  author_keywords = {Identifier names; Program comprehension; Software engineering tools; Source code text analysis},
  document_type   = {Article},
  doi             = {10.1007/s10664-013-9261-0},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909993591&doi=10.1007%2fs10664-013-9261-0&partnerID=40&md5=4caa900c8b8977abd2599ad8fc9ee126},
}

@Article{Guerrouj20141706,
  author          = {Guerrouj, L. and Di Penta, M. and Guéhéneuc, Y.-G. and Antoniol, G.},
  title           = {An experimental investigation on the effects of context on source code identifiers splitting and expansion},
  journal         = {Empirical Software Engineering},
  year            = {2014},
  volume          = {19},
  number          = {6},
  pages           = {1706-1753},
  note            = {cited By 6},
  abstract        = {Recent and past studies indicate that source code lexicon plays an important role in program comprehension. Developers often compose source code identifiers with abbreviated words and acronyms, and do not always use consistent mechanisms and explicit separators when creating identifiers. Such choices and inconsistencies impede the work of developers that must understand identifiers by decomposing them into their component terms, and mapping them onto dictionary, application or domain words. When software documentation is scarce, outdated or simply not available, developers must therefore use the available contextual information to understand the source code. This paper aims at investigating how developers split and expand source code identifiers, and, specifically, the extent to which different kinds of contextual information could support such a task. In particular, we consider (i) an internal context consisting of the content of functions and source code files in which the identifiers are located, and (ii) an external context involving external documentation. We conducted a family of two experiments with 63 participants, including bachelor, master, Ph.D. students, and post-docs. We randomly sampled a set of 50 identifiers from a corpus of open source C programs and we asked participants to split and expand them with the availability (or not) of internal and external contexts. We report evidence on the usefulness of contextual information for identifier splitting and acronym/abbreviation expansion. We observe that the source code files are more helpful than just looking at function source code, and that the application-level contextual information does not help any further. The availability of external sources of information only helps in some circumstances. Also, in some cases, we observe that participants better expanded acronyms than abbreviations, although in most cases both exhibit the same level of accuracy. Finally, results indicated that the knowledge of English plays a significant effect in identifier splitting/expansion. The obtained results confirm the conjecture that contextual information is useful in program comprehension, including when developers split and expand identifiers to understand them. We hypothesize that the integration of identifier splitting and expansion tools with IDE could help to improve developers’ productivity. © 2013, Springer Science+Business Media New York.},
  affiliation     = {SOCCER Lab., DGIGL, École Polytechnique de Montréal, QC, Canada; University of Sannio, Sannio, Italy},
  author_keywords = {Identifier splitting and expansion; Program understanding; Task context},
  document_type   = {Article},
  doi             = {10.1007/s10664-013-9260-1},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910005434&doi=10.1007%2fs10664-013-9260-1&partnerID=40&md5=35d6ab1cf7d4a2406a37b234777286ef},
}

@Conference{Fishel2014159,
  author        = {Fishel, M. and Sennrich, R.},
  title         = {Handling technical OOVs in SMT},
  year          = {2014},
  pages         = {159-162},
  note          = {cited By 0},
  abstract      = {We present a project on machine translation of software help desk tickets, a highly technical text domain. The main source of translation errors were out-of-vocabulary tokens (OOVs), most of which were either in-domain German compounds or technical token sequences that must be preserved verbatim in the output. We describe our efforts on compound splitting and treatment of non-translatable tokens, which lead to a significant translation quality gain. © 2014 The authors.},
  affiliation   = {Institute of Computational Linguistics, University of Zurich, Binzmühlestr. 14, Zürich, CH-8050, Switzerland},
  document_type = {Conference Paper},
  journal       = {Proceedings of the 17th Annual Conference of the European Association for Machine Translation, EAMT 2014},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000716797&partnerID=40&md5=3634713662e062110d558fbc30f94db4},
}

@Article{Wołk2014107,
  author          = {Wołk, K. and Marasek, K.},
  title           = {Real-time statistical speech translation},
  journal         = {Advances in Intelligent Systems and Computing},
  year            = {2014},
  volume          = {275 AISC},
  number          = {VOLUME 1},
  pages           = {107-113},
  note            = {cited By 8},
  abstract        = {This research investigates the Statistical Machine Translation approaches to translate speech in real time automatically. Such systems can be used in a pipeline with speech recognition and synthesis software in order to produce a real-time voice communication system between foreigners. We obtained three main data sets from spoken proceedings that represent three different types of human speech. TED, Europarl, and OPUS parallel text corpora were used as the basis for training of language models, for developmental tuning and testing of the translation system. We also conducted experiments involving part of speech tagging, compound splitting, linear language model interpolation, TrueCasing and morphosyntactic analysis. We evaluated the effects of variety of data preparations on the translation results using the BLEU, NIST, METEOR and TER metrics and tried to give answer which metric is most suitable for PL-EN language pair. © Springer International Publishing Switzerland 2014.},
  affiliation     = {Department of Multimedia, Polish Japanese Institute of Information Technology, Koszykowa 86, 02-008 Warsaw, Poland},
  author_keywords = {Knowledge-free learning; Machine learning; Machine translation; NLP; Speech translation},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-319-05951-8_11},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904602811&doi=10.1007%2f978-3-319-05951-8_11&partnerID=40&md5=d9d3d16efc7102d26081575e8d464b8b},
}

@Conference{Binkley2013401,
  author        = {Binkley, D. and Lawrie, D. and Pollock, L. and Hill, E. and Vijay-Shanker, K.},
  title         = {A dataset for evaluating identifier splitters},
  year          = {2013},
  pages         = {401-404},
  note          = {cited By 6},
  abstract      = {Software engineering and evolution techniques have recently started to exploit the natural language information in source code. A key step in doing so is splitting identifiers into their constituent words. While simple in concept, identifier splitting raises several challenging issues, leading to a range of splitting techniques. Consequently, the research community would benefit from a dataset (i.e., a gold set) that facilitates comparative studies of identifier splitting techniques. A gold set of 2,663 split identifiers was constructed from 8,522 individual human splitting judgements and can be obtained from www.cs.loyola.edu/∼binkley/ludiso. This set's construction and observations aimed at its effective use are described. © 2013 IEEE.},
  affiliation   = {Loyola University Maryland, Baltimore, MD 21210, United States; University of Delaware, Newark, DE 19716, United States; Montclair State University, Montclair, NJ, 07043, United States},
  art_number    = {6624055},
  document_type = {Conference Paper},
  doi           = {10.1109/MSR.2013.6624055},
  journal       = {IEEE International Working Conference on Mining Software Repositories},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889056852&doi=10.1109%2fMSR.2013.6624055&partnerID=40&md5=20c554dc45e942680416227bea5a5bdf},
}

@Article{Kpodjedo20131090,
  author          = {Kpodjedo, S. and Ricca, F. and Galinier, P. and Antoniol, G. and Gueheneuc, Y.-G.},
  title           = {Madmatch: Many-to-many approximate diagram matching for design comparison},
  journal         = {IEEE Transactions on Software Engineering},
  year            = {2013},
  volume          = {39},
  number          = {8},
  pages           = {1090-1111},
  note            = {cited By 13},
  abstract        = {Matching algorithms play a fundamental role in many important but difficult software engineering activities, especially design evolution analysis and model comparison. We present MADMatch, a fast and scalable many-to-many approximate diagram matching approach based on an error-tolerant graph matching (ETGM) formulation. Diagrams are represented as graphs, costs are assigned to possible differences between two given graphs, and the goal is to retrieve the cheapest matching. We address the resulting optimization problem with a tabu search enhanced by the novel use of lexical and structural information. Through several case studies with different types of diagrams and tasks, we show that our generic approach obtains better results than dedicated state-of-the-art algorithms, such as AURA, PLTSDiff, or UMLDiff, on the exact same datasets used to introduce (and evaluate) these algorithms. © 1976-2012 IEEE.},
  affiliation     = {Ecole Polytechnique of Montreal, DGIGL, 2900 - Edouard Montpetit, Montreal, QC H3C 3A7, Canada; Universita di Genova, Genova 16126, Italy},
  art_number      = {6568862},
  author_keywords = {approximate graph matching; Diagram differencing; identifier splitting; search-based software engineering},
  document_type   = {Article},
  doi             = {10.1109/TSE.2013.9},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881038069&doi=10.1109%2fTSE.2013.9&partnerID=40&md5=9a548cfb32b924a02c660b079f2e1e6b},
}

@Article{Guerrouj2013575,
  author          = {Guerrouj, L. and Di Penta, M. and Antoniol, G. and Guéh́eneuc, Y.-G.},
  title           = {TIDIER: An identifier splitting approach using speech recognition techniques},
  journal         = {Journal of software: Evolution and Process},
  year            = {2013},
  volume          = {25},
  number          = {6},
  pages           = {575-599},
  note            = {cited By 21},
  abstract        = {The software engineering literature reports empirical evidence on the relation between various characteristics of a software system and its quality. Among other factors, recent studies have shown that a proper choice of identifiers influences understandability and maintainability. Indeed, identifiers are developers' main source of information and guide their cognitive processes during program comprehension when high-level documentation is scarce or outdated and when source code is not sufficiently commented. This paper proposes a novel approach to recognize words composing source code identifiers. The approach is based on an adaptation of Dynamic Time Warping used to recognize words in continuous speech. The approach overcomes the limitations of existing identifier-splitting approaches when naming conventions (e.g., Camel Case) are not used or when identifiers contain abbreviations. We apply the approach on a sample of more than 1000 identifiers extracted from 340 C programs and compare its results with a simple Camel Case splitter and with an implementation of an alternative identifier splitting approach, Samurai. Results indicate the capability of the novel approach: (i) to outperform the alternative ones, when using a dictionary augmented with domain knowledge or a contextual dictionary and (ii) to expand 48% of a set of selected abbreviations into dictionary words. Copyright © 2011 John Wiley & Sons, Ltd.},
  affiliation     = {DGIGL/P TIDEJ Team/SOCCER Lab., École Polytechnique de Montréal, Québec, Canada; RCOST - Department of Engineering, University of Sannio, Italy},
  author_keywords = {Identifier splitting; Linguistic analysis; Program comprehension},
  document_type   = {Article},
  doi             = {10.1002/smr.539},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883654687&doi=10.1002%2fsmr.539&partnerID=40&md5=79227717373f850527ad7601246a17bb},
}

@Conference{NoAuthor2012,
  title         = {Proceedings - 19th Working Conference on Reverse Engineering, WCRE 2012},
  year          = {2012},
  note          = {cited By 0},
  abstract      = {The proceedings contain 55 papers. The topics discussed include: assuring software quality by code smell detection; structured binary editing with a CFG transformation algebra; Astra: bottom-up construction of structured artifact repositories; detection and recovery of functions and their arguments in a retargetable decompiler; towards static analysis of virtualization-obfuscated binaries; understanding android fragmentation with topic analysis of vendor-specific bugs; using network analysis for recommendation of central software classes; TRIS: a fast and accurate identifiers splitting and expansion algorithm; using bug report similarity to enhance bug localisation; SCAN: an approach to label and relate execution trace segments; feature location in a collection of product variants; reverse engineering iOS mobile applications; and precise detection of uninitialized variables using dynamic analysis - extending to aggregate and vector types.},
  document_type = {Conference Review},
  journal       = {Proceedings - Working Conference on Reverse Engineering, WCRE},
  page_count    = {526},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872332293&partnerID=40&md5=94138d1316529b8539e10feb7abee73c},
}

@Conference{2012,
  title         = {Proceedings of the 1st International Workshop on Software Mining, SoftwareMining-2012 - Held in Conjunction with the 18th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD-2012},
  year          = {2012},
  note          = {cited By 0},
  abstract      = {The proceedings contain 4 papers. The topics discussed include: source code identifier splitting using yahoo image and web search engine; software systems through complex networks science: review, analysis and applications; labeled topic detection of open source software from mining mass textual project profiles; and rank-directed layout of UML class diagrams.},
  document_type = {Conference Review},
  journal       = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  page_count    = {35},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869851232&partnerID=40&md5=32e139e50c5b76a74c5d787072b9caf5},
}

@Conference{Srinivasan20121113,
  author          = {Srinivasan, S. and Bhattacharya, S. and Chakraborty, R.},
  title           = {Segmenting web-domains and hashtags using length specific models},
  year            = {2012},
  pages           = {1113-1122},
  note            = {cited By 13},
  abstract        = {Segmentation of a string of English language characters into a sequence of words has many applications. Here, we study two applications in the internet domain. First application is the web domain segmentation which is crucial for monetization of broken URLs. Secondly, we propose and study a novel application of twitter hashtag segmentation for increasing recall on twitter searches. Existing methods for word segmentation use unsupervised language models. We find that when using multiple corpora, the joint probability model from multiple corpora performs significantly better than the individual corpora. Motivated by this, we propose weighted joint probability model, with weights specific to each corpus. We propose to train the weights in a supervised manner using max-margin methods. The supervised probability models improve segmentation accuracy over joint probability models. Finally, we observe that length of segments is an important parameter for word segmentation, and incorporate length-specific weights into our model. The length specific models further improve segmentation accuracy over supervised probability models. For all models proposed here, inference problem can be solved using the dynamic programming algorithm. We test our methods on five different datasets, two from web domains data, and three from news headlines data from an LDC dataset. The supervised length specific models show significant improvements over unsupervised single corpus and joint probability models. Cross-testing between the datasets confirm that supervised probability models trained on all datasets, and length specific models trained on news headlines data, generalize well. Segmentation of hashtags result in significant improvement in recall on searches for twitter trends. © 2012 ACM.},
  affiliation     = {Yahoo SDC, Bangalore, India; Yahoo Labs., Bangalore, India; Indian Statistical Institute, Kolkata, India},
  author_keywords = {compound splitting; hashtag segmentation; structured learning; web domain segmentation; word segmentation},
  document_type   = {Conference Paper},
  doi             = {10.1145/2396761.2398410},
  journal         = {ACM International Conference Proceeding Series},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871061019&doi=10.1145%2f2396761.2398410&partnerID=40&md5=4a4ef08843f61ac76f3dbc0b0b0a71a5},
}

@Conference{Sureka20121,
  author          = {Sureka, A.},
  title           = {Source code identifier splitting using yahoo image and web search engine},
  year            = {2012},
  pages           = {1-8},
  note            = {cited By 1},
  abstract        = {Source-code or program identifiers are sequence of characters consisting of one or more tokens representing domain concepts. Splitting or tokenizing identifiers that does not contain explicit markers or clues such as came-casing or using underscore as a token separatoris a technically challenging problem. In this paper, we present a technique for automatic tokenization and splitting of source-code identifiers using Yahoo web search and image search similarity distance. We present an algorithm that decides the split position based on various factors such as conceptual correlations and semantic relatedness between the left and right splits strings of a given identifier, popularity of the token and its length. The number of hits or search results returned by the web and image search engine serves as a proxy to measures such as term popularity and correlation. We perform a series of experiments to validate the proposed approach and present performance results.},
  affiliation     = {Indraprastha Institute of Information Technology, Delhi IIIT-D, New Delhi, India},
  author_keywords = {Identifier Splitting; Identifier Tokenization; Mining Software Repositories; Program Comprehension; Yahoo Similarity Distance},
  document_type   = {Conference Paper},
  doi             = {10.1145/2384416.2384417},
  journal         = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869760971&doi=10.1145%2f2384416.2384417&partnerID=40&md5=042160e97cea76d60ed1709ec697fb7f},
}

@Conference{Biggers2012164,
  author          = {Biggers, L.R.},
  title           = {The effects of identifier retention and stop word removal on a latent Dirichlet allocation based feature location technique},
  year            = {2012},
  pages           = {164-169},
  note            = {cited By 1},
  abstract        = {Feature location, an important task in program comprehension, occurs when the developer identifies the source code entity or entities responsible for implementing a functionality. Researchers have applied static analysis techniques to multiple software maintenance tasks, including feature localization. Static analysis techniques operate on a document corpus. Configuration and preprocessing decisions are required to build a suitable source code corpus for a static analysis technique. Currently, there is little guidance in the software engineering literature for making such configuration decisions. This paper focuses on two preprocessing methods for source code corpora, identifier splitting and stop word lists. We experiment on three open source Java test suites, i.e. Mylyn 1.0.1, Rhino 1.5R5, and Rhino 1.6R5. Our results indicate that identifier splitting and stop word list decisions do not significantly affect the performance of the LDA based feature location technique. © 2012 ACM.},
  affiliation     = {Department of Computer Science, University of Alabama, Tuscaloosa, AL 35487-0290, United States},
  author_keywords = {feature location; information retrieval; program comprehension; software evolution and maintenance; static analysis},
  document_type   = {Conference Paper},
  doi             = {10.1145/2184512.2184551},
  journal         = {Proceedings of the Annual Southeast Conference},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862667183&doi=10.1145%2f2184512.2184551&partnerID=40&md5=596eddec20f80762f4b8b0c7f84166e1},
}

@Conference{Guerrouj2012103,
  author          = {Guerrouj, L. and Galinier, P. and Guéhéneuc, Y.-G. and Antoniol, G. and Di Penta, M.},
  title           = {TRIS: A fast and accurate identifiers splitting and expansion algorithm},
  year            = {2012},
  pages           = {103-112},
  note            = {cited By 7},
  abstract        = {Understanding source code identifiers, by identifying words composing them, is a necessary step for many program comprehension, reverse engineering, or redocumentation tasks. To this aim, researchers have proposed several identifier splitting and expansion approaches such as Samurai, TIDIER and more recently GenTest. The ultimate goal of such approaches is to help disambiguating conceptual information encoded in compound (or abbreviated) identifiers. This paper presents TRIS, TRee-based Identifier Splitter, a two-phases approach to split and expand program identifiers. First, TRIS pre-compiles transformed dictionary words into a tree representation, associating a cost to each transformation. In a second phase, it maps the identifier splitting/expansion problem into a minimization problem, i.e., the search of the shortest path (optimal split/expansion) in a weighted graph. We apply TRIS to a sample of 974 identifiers extracted from JHotDraw, 3,085 from Lynx, and to a sample of 489 identifiers extracted from 340 C programs. Also, we compare TRIS with GenTest on a set of 2,663 mixed Java, C and C++ identifiers. We report evidence that TRIS split (and expansion) is more accurate than state-of-the-art approaches and that it is also efficient in terms of computation time. © 2012 IEEE.},
  affiliation     = {DGIGL, École Polytechnique de Montréal, Canada; Dept. of Engineering, University of Sannio, Italy},
  art_number      = {6385106},
  author_keywords = {Identifier Splitting/Expansion; Linguistic Analysis; Optimal Path; Program Comprehension; Weighted Acyclic Graph},
  document_type   = {Conference Paper},
  doi             = {10.1109/WCRE.2012.20},
  journal         = {Proceedings - Working Conference on Reverse Engineering, WCRE},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872297690&doi=10.1109%2fWCRE.2012.20&partnerID=40&md5=cde8887fb5e9ca9bd4f353e3c9bd9ad8},
}

@Conference{Binkley2012588,
  author          = {Binkley, D. and Lawrie, D. and Uehlinger, C.},
  title           = {Vocabulary normalization improves IR-based concept location},
  year            = {2012},
  pages           = {588-591},
  note            = {cited By 7},
  abstract        = {Tool support is crucial to modern software development, evolution, and maintenance. Early tools reused the static analysis performed by the compiler. These were followed by dynamic analysis tools and more recently tools that exploit natural language. This later class has the advantage that it can incorporate not only the code, but artifacts from all phases of software construction and its subsequent evolution. Unfortunately, the natural language found in source code often uses a vocabulary different from that used in other software artifacts and thus increases the vocabulary mismatch problem. This problem exists because many natural-language tools imported from Information Retrieval (IR) and Natural Language Processing (NLP) implicitly assume the use of a single natural language vocabulary. Vocabulary normalization, which goes well beyond simple identifier splitting, brings the vocabulary of the source into line with other artifacts. Consequently, it is expected to improve the performance of existing and future IR and NLP based tools. As a case study, an experiment with an LSI-based feature locator is replicated. Normalization universally improves performance. For the tersest queries, this improvement is over 180% (p < 0.0001). © 2012 IEEE.},
  affiliation     = {Computer Science Department, Loyola University Maryland, Baltimore, United States},
  art_number      = {6405328},
  author_keywords = {concept location; information retrieval; vocabulary normalization},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICSM.2012.6405328},
  journal         = {IEEE International Conference on Software Maintenance, ICSM},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873184808&doi=10.1109%2fICSM.2012.6405328&partnerID=40&md5=e62cf648b2b0e48402cf9e76791ac74b},
}

@Conference{Dit201111,
  author          = {Dit, B. and Guerrouj, L. and Poshyvanyk, D. and Antoniol, G.},
  title           = {Can better identifier splitting techniques help feature location?},
  year            = {2011},
  pages           = {11-20},
  note            = {cited By 48},
  abstract        = {The paper presents an exploratory study of two feature location techniques utilizing three strategies for splitting identifiers: Camel Case, Samurai and manual splitting of identifiers. The main research question that we ask in this study is if we had a perfect technique for splitting identifiers, would it still help improve accuracy of feature location techniques applied in different scenarios and settings? In order to answer this research question we investigate two feature location techniques, one based on Information Retrieval and the other one based on the combination of Information Retrieval and dynamic analysis, for locating bugs and features using various configurations of preprocessing strategies on two open-source systems, Rhino and jEdit. The results of an extensive empirical evaluation reveal that feature location techniques using Information Retrieval can benefit from better preprocessing algorithms in some cases, and that their improvement in effectiveness while using manual splitting over state-of-the-art approaches is statistically significant in those cases. However, the results for feature location technique using the combination of Information Retrieval and dynamic analysis do not show any improvement while using manual splitting, indicating that any preprocessing technique will suffice if execution data is available. Overall, our findings outline potential benefits of putting additional research efforts into defining more sophisticated source code preprocessing techniques as they can still be useful in situations where execution information cannot be easily collected. © 2011 IEEE.},
  affiliation     = {Department of Computer Science, College of William and Mary, Williamsburg, VA, United States; Department of Computer Science Engineering, École Polytechnique de Montréal, Québec, QC, Canada},
  art_number      = {5970159},
  author_keywords = {dynamic analysis; feature location; identifier splitting algorithms; information retrieval},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICPC.2011.47},
  journal         = {IEEE International Conference on Program Comprehension},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052408614&doi=10.1109%2fICPC.2011.47&partnerID=40&md5=c445c67d85da6e8e3c2ad216b5250e62},
}

@Conference{Henrich2011420,
  author        = {Henrich, V. and Hinrichs, E.},
  title         = {Determining immediate constituents of compounds in GermaNet},
  year          = {2011},
  pages         = {420-426},
  note          = {cited By 11},
  abstract      = {In order to be able to systematically link compounds in GermaNet to their constituent parts, compound splitting needs to be applied recursively and has to identify the immediate constituents at each level of analysis. Existing tools for compound splitting for German only offer an analysis of all component parts of a compound at once without any grouping of subconstituents. Thus, existing tools for splitting compounds were adapted to overcome this issue. Algorithms combining three heterogeneous kinds of compound splitters are developed to achieve better results. The best overall result with an accuracy of 92.42% is achieved by a hybrid combined compound splitter that takes into account all knowledge provided by the individual compound splitters, and in addition some domain knowledge about German derivation morphology and compounding.},
  affiliation   = {University of Tübingen, Germany},
  document_type = {Conference Paper},
  journal       = {International Conference Recent Advances in Natural Language Processing, RANLP},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866877956&partnerID=40&md5=4e50f1d779c887840937a10cea5dcd74},
}

@Conference{Wang2011357,
  author          = {Wang, K. and Thrasher, C. and Hsu, B.-J.},
  title           = {Web scale NLP: A case study on URL word breaking},
  year            = {2011},
  pages           = {357-366},
  note            = {cited By 32},
  abstract        = {This paper uses the URL word breaking task as an example to elaborate what we identify as crucialin designingstatistical natural language processing (NLP) algorithmsfor Web scale applications: (1) rudimentary multilingual capabilities to cope with the global nature of the Web, (2) multi-style modeling to handle diverse language styles seen in the Web contents, (3) fast adaptation to keep pace with the dynamic changes of the Web, (4) minimal heuristic assumptions for generalizability and robustness, and (5) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost. We first show that the state-of-the-art word breaking techniquescan be unified and generalized under the Bayesian minimum risk (BMR) framework that, using a Web scale N-gram, can meet the first three requirements. We discuss how the existing techniques can be viewed asintroducing additional assumptions to the basic BMR framework, and describe a generic yet efficient implementation called word synchronous beam search.Testing the framework and its implementation on a series of large scale experiments reveals the following. First, the language style used to build the modelplays a critical role in the word breaking task, and the most suitable for the URL word breaking task appears to be that ofthe document title where the best performance is obtained. Models created from other language styles, such as from document body, anchor text, and even queries, exhibit varying degrees of mismatch. Although all styles benefit from increasing modeling power which, in our experiments, corresponds tothe use of a higher order N-gram, the gain is most recognizable for the title model. The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models, but are less effective and, in many cases,lead to poorer performance than the matched model with minimal assumptions. For the matched model based on document titles, an accuracy rate of 97.18% can already be achieved using simple trigram without any heuristics. Copyright © 2011 by the Association for Computing Machinery, Inc. (ACM).},
  affiliation     = {Microsoft Research, ISRC, One Microsoft Way, Redmond, WA 98052, United States},
  author_keywords = {Compound splitting; Multi-style language model; URL segmentation; Web scale word breaking; Word segmentation},
  document_type   = {Conference Paper},
  doi             = {10.1145/1963405.1963457},
  journal         = {Proceedings of the 20th International Conference on World Wide Web, WWW 2011},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052123854&doi=10.1145%2f1963405.1963457&partnerID=40&md5=f0884c46e20c2b2cf89b5c9a1bb92dd2},
}

@Conference{Guerrouj2010301,
  author          = {Guerrouj, L.},
  title           = {Automatic derivation of concepts based on the analysis of source code identifiers},
  year            = {2010},
  pages           = {301-304},
  note            = {cited By 3},
  abstract        = {The existing software engineering literature has empirically shown that a proper choice of identifiers influences software understandability and maintainability. Indeed, identifiers are developers' main up-to-date source of information and guide their cognitive processes during program understanding when the high-level documentation is scarce or outdated and when the source code is not sufficiently commented. Deriving domain terms from identifiers using high-level and domain concepts is not an easy task when naming conventions (e.g., Camel Case) are not used or strictly followed and-or when these words have been abbreviated or otherwise transformed. Our thesis is to develop an approach that overcomes the shortcomings of the existing approaches and maps identifiers to domain concepts even in the absence of naming conventions and-or the presence of abbreviations. Our approach uses a thesaurus of words and abbreviations to map terms or transformed words composing identifiers to dictionary words. It relies on an oracle that we manually build for the validation of our results. To evaluate our technique, we apply it to derive concepts from identifiers of different systems and open source projects. We also enrich it by the use of domain knowledge and context-aware dictionaries to analyze how sensitive are its performances to the use of contextual information and specialized knowledge. © 2010 IEEE.},
  affiliation     = {DGIGL - SOCCER Lab., Ptidej Team, Ecole Polytechnique de Montréal, QC, Canada},
  art_number      = {5645490},
  author_keywords = {Identifier splitting; Linguistic analysis; Program comprehension; Software quality},
  document_type   = {Conference Paper},
  doi             = {10.1109/WCRE.2010.45},
  journal         = {Proceedings - Working Conference on Reverse Engineering, WCRE},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650655941&doi=10.1109%2fWCRE.2010.45&partnerID=40&md5=085e4056d0a775ee451b7bed2ae95111},
}

@Article{Kumar201057,
  author          = {Kumar, A. and Mittal, V. and Kulkarni, A.},
  title           = {Sanskrit compound processor},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2010},
  volume          = {6465 LNAI},
  pages           = {57-69},
  note            = {cited By 3},
  abstract        = {Sanskrit is very rich in compound formation. Typically a compound does not code the relation between its components explicitly. To understand the meaning of a compound, it is necessary to identify its components, discover the relations between them and finally generate a paraphrase of the compound. In this paper, we discuss the automatic segmentation and type identification of a compound using simple statistics that results from the manually annotated data. © 2010 Springer-Verlag Berlin Heidelberg.},
  affiliation     = {Department of Sanskrit Studies, University of Hyderabad, India; Language Technologies Research Centre, IIIT, Hyderabad, India},
  author_keywords = {Optimality Theory; Sanskrit Compound Splitter; Sanskrit Compound Type Identifier; Sanskrit Compounds},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-17528-2_5},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651063736&doi=10.1007%2f978-3-642-17528-2_5&partnerID=40&md5=ba8a099b10cf2e63862a66f42bd24e2f},
}

@Article{Zeman2010216,
  author          = {Zeman, D.},
  title           = {Using TectoMT as a preprocessing tool for phrase-based statistical machine translation},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2010},
  volume          = {6231 LNAI},
  pages           = {216-223},
  note            = {cited By 0},
  abstract        = {We present a systematic comparison of preprocessing techniques for two language pairs: English-Czech and English-Hindi. The two target languages, although both belonging to the Indo-European language family, show significant differences in morphology, syntax and word order. We describe how TectoMT, a successful framework for analysis and generation of language, can be used as preprocessor for a phrase-based MT system.We compare the two language pairs and the optimal sets of source-language transformations applied to them. The following transformations are examples of possible preprocessing steps: lemmatization; retokenization, compound splitting; removing/adding words lacking counterparts in the other language; phrase reordering to resemble the target word order; marking syntactic functions. TectoMT, as well as all other tools and data sets we use, are freely available on the Web. © 2010 Springer-Verlag Berlin Heidelberg.},
  affiliation     = {Univerzita Karlova v Praze, ÚFAL, Malostranské náměstí 25, 11800 Prague, Czech Republic},
  author_keywords = {phrase-based translation; preprocessing; reordering},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-15760-8_28},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049263442&doi=10.1007%2f978-3-642-15760-8_28&partnerID=40&md5=d8df80e3e3bbfd901f36029213a636fe},
}

@Conference{Enslen200971,
  author        = {Enslen, E. and Hill, E. and Pollock, L. and Vijay-Shanker, K.},
  title         = {Mining source code to automatically split identifiers for software analysis},
  year          = {2009},
  pages         = {71-80},
  note          = {cited By 102},
  abstract      = {Automated software engineering tools (e.g., program search, concern location, code reuse, quality assessment, etc.) increasingly rely on natural language information from comments and identifiers in code. The first step in analyzing words from identifiers requires splitting identifiers into their constituent words. Unlike natural languages, where space and punctuation are used to delineate words, identifiers cannot contain spaces. One common way to split identifiers is to follow programming language naming conventions. For example, Java programmers often use camel case, where words are delineated by uppercase letters or non-alphabetic characters. However, programmers also create identifiers by concatenating sequences of words together with no discernible delineation, which poses challenges to automatic identifier splitting. In this paper, we present an algorithm to automatically split identifiers into sequences of words by mining word frequencies in source code. With these word frequencies, our identifier splitter uses a scoring technique to automatically select the most appropriate partitioning for an identifier. In an evaluation of over 8000 identifiers from open source Java programs, our Samurai approach outperforms the existing state of the art techniques. © 2009 IEEE.},
  affiliation   = {Department of Computer and Information Sciences, University of Delaware, Newark, DE 19716, United States},
  art_number    = {5069482},
  document_type = {Conference Paper},
  doi           = {10.1109/MSR.2009.5069482},
  journal       = {Proceedings of the 2009 6th IEEE International Working Conference on Mining Software Repositories, MSR 2009},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349897886&doi=10.1109%2fMSR.2009.5069482&partnerID=40&md5=d807de27d697935bc4b1aa42bcb2c29a},
}

@Article{Stymne2008464,
  author        = {Stymne, S.},
  title         = {German compounds in factored statistical machine translation},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2008},
  volume        = {5221 LNAI},
  pages         = {464-475},
  note          = {cited By 9},
  abstract      = {An empirical method for splitting German compounds is explored by varying it in a number of ways to investigate the consequences for factored statistical machine translation between English and German in both directions. Compound splitting is incorporated into translation in a preprocessing step, performed on training data and on German translation input. For translation into German, compounds are merged based on part-of-speech in a postprocessing step. Compound parts are marked, to separate them from ordinary words. Translation quality is improved in both translation directions and the number of untranslated words in the English output is reduced. Different versions of the splitting algorithm performs best in the two different translation directions. © 2008 Springer-Verlag Berlin Heidelberg.},
  affiliation   = {Department of Computer and Information Science, Linköping University, Sweden},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-540-85287-2_44},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-52149110825&doi=10.1007%2f978-3-540-85287-2_44&partnerID=40&md5=a1f4fbab019cd70b45de69094f44a5e7},
}

@Conference{Stymne2008182,
  author        = {Stymne, S. and Holmqvist, M.},
  title         = {Processing of Swedish compounds for phrase-based statistical machine translation},
  year          = {2008},
  pages         = {182-191},
  note          = {cited By 6},
  abstract      = {We investigated the effects of processing Swedish compounds for phrase-based SMT between Swedish and English. Compounds were split in a pre-processing step using an unsupervised empirical method. After translation into Swedish, compounds were merged, using a novel merging algorithm. We investigated two ways of handling compound parts, by marking them as compound parts or by normalizing them to a canonical form. We found that compound splitting did improve translation into Swedish, according to automatic metrics. For translation into English the results were not consistent across automatic metrics. However, error analysis of compound translation showed a small improvement in the systems that used splitting. The number of untranslated words in the English output was reduced by 50%.},
  affiliation   = {Department of Computer and Information Science, Linköping University, Sweden},
  document_type = {Conference Paper},
  journal       = {Proceedings of the 12th European Association for Machine Translation Conference, EAMT 2008},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857535918&partnerID=40&md5=11ef39e004bb3623703d713985652303},
}

@Article{Bordag2008881,
  author          = {Bordag, S.},
  title           = {Unsupervised and Knowledge-free morpheme segmentation and analysis},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2008},
  volume          = {5152 LNCS},
  pages           = {881-891},
  note            = {cited By 13},
  abstract        = {This paper presents a revised version of an unsupervised and knowledge-free morpheme boundary detection algorithm based on letter successor variety (LSV) and a trie classifier [1]. Additional knowledge about relatedness of the found morphs is obtained from a morphemic analysis based on contextual similarity. For the boundary detection the challenge of increasing recall of found morphs while retaining a high precision is tackled by adding a compound splitter, iterating the LSV analysis and dividing the trie classifier into two distinctly applied clasifiers. The result is a significantly improved overall performance and a decreased reliance on corpus size. Further possible improvements and analyses are discussed. © 2008 Springer-Verlag Berlin Heidelberg.},
  affiliation     = {Natural Language Processing Department, University of Leipzig, Germany},
  author_keywords = {Distributed similarity; Letter successor variety; Morpheme analysis; Morpheme boundary detection},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-540-85760-0-113},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349825872&doi=10.1007%2f978-3-540-85760-0-113&partnerID=40&md5=6281e6daae5eb412cf1b5e3754629d23},
}

@Article{Ahlgren200781,
  author          = {Ahlgren, P. and Kekäläinen, J.},
  title           = {Indexing strategies for Swedish full text retrieval under different user scenarios},
  journal         = {Information Processing and Management},
  year            = {2007},
  volume          = {43},
  number          = {1},
  pages           = {81-102},
  note            = {cited By 6},
  abstract        = {This paper deals with Swedish full text retrieval and the problem of morphological variation of query terms in the document database. The effects of combination of indexing strategies with query terms on retrieval effectiveness were studied. Three of five tested combinations involved indexing strategies that used conflation, in the form of normalization. Further, two of these three combinations used indexing strategies that employed compound splitting. Normalization and compound splitting were performed by SWETWOL, a morphological analyzer for the Swedish language. A fourth combination attempted to group related terms by right hand truncation of query terms. The four combinations were compared to each other and to a baseline combination, where no attempt was made to counteract the problem of morphological variation of query terms in the document database. The five combinations were evaluated under six different user scenarios, where each scenario simulated a certain user type. The four alternative combinations outperformed the baseline, for each user scenario. The truncation combination had the best performance under each user scenario. The main conclusion of the paper is that normalization and right hand truncation (performed by a search expert) enhanced retrieval effectiveness in comparison to the baseline. The performance of the three combinations of indexing strategies with query terms based on normalization was not far below the performance of the truncation combination. © 2006 Elsevier Ltd. All rights reserved.},
  affiliation     = {University College of Borås, Swedish School, Library and Information Science, 501 90 Borås, Sweden; University of Tampere, Department of Information Studies, Kanslerinrinne 1, 33014 Tampere, Finland},
  author_keywords = {Base word form index; Discounted cumulated gain; Indexing strategy; Inflected word form index; Truncation; User scenario},
  document_type   = {Article},
  doi             = {10.1016/j.ipm.2006.03.003},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748311879&doi=10.1016%2fj.ipm.2006.03.003&partnerID=40&md5=db68e318ded198f53c49414be315f238},
}

@Conference{Bordag2007,
  author          = {Bordag, S.},
  title           = {Unsupervised and knowledge-free morpheme segmentation and analysis},
  year            = {2007},
  volume          = {1173},
  note            = {cited By 0},
  abstract        = {This paper presents a revised version of an unsupervised and knowledge-free morpheme boundary detection algorithm based on letter successor variety (LSV) and a trie classifier [5]. Additionally a morphemic analysis based on contextual similarity provides knowledge about relatedness of the found morphs. For the boundary detection the challenge of increasing recall of found morphs while retaining a high precision is tackled by adding a compound splitter, iterating the LSV analysis and dividing the trie classifier into two distinctly applied clasifiers. The result is a significantly improved overall performance and a decreased reliance on corpus size. Further possible improvements and analyses are discussed.},
  affiliation     = {University of Leipzig, Germany},
  author_keywords = {Distributed similarity; Letter successor variety; Morpheme analysis; Morpheme boundary detection},
  document_type   = {Conference Paper},
  journal         = {CEUR Workshop Proceedings},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921960169&partnerID=40&md5=af8fc25f3c4af981dbe962e6e760ad24},
}

@Article{Pedersen200775,
  author        = {Pedersen, B.S.},
  title         = {Using shallow linguistic analysis to improve search on Danish compounds},
  journal       = {Natural Language Engineering},
  year          = {2007},
  volume        = {13},
  number        = {1},
  pages         = {75-90},
  note          = {cited By 0},
  abstract      = {In this paper we focus on a specific search-related query expansion topic, namely search on Danish compounds and expansion to some of their synonymous phrases. Compounds constitute a specific issue in search, in particular in languages where they are written in one word, as is the case for Danish and the other Scandinavian languages. For such languages, expansion of the query compound into separate lemmas is a way of finding the often frequent alternative synonymous phrases in which the content of a compound can also be expressed. However, it is crucial to note that the number of irrelevant hits is generally very high when using this expansion strategy. The aim of this paper is therefore to examine how we can obtain better search results on split compounds, partly by looking at the internal structure of the original compound, partly by analyzing the context in which the split compound occurs. In this context, we pursue two hypotheses: (1) that some categories of compounds are more likely to have synonymous 'split' counterparts than others; and (2) that search results where both the search words (obtained by splitting the compound) occur in the same noun phrase, are more likely to contain a synonymous phrase to the original compound query. The search results from 410 enhanced compound queries are used as a test bed for our experiments. On these search results, we perform a shallow linguistic analysis and introduce a new, linguistically based threshold for retrieved hits. The results obtained by using this strategy demonstrate that compound splitting combined with a shallow linguistic analysis focusing on the argument structure of the compound head as well as on the recognition of NPs, can improve search by substantially bringing down the number of irrelevant hits. © 2006 Cambridge University Press.},
  affiliation   = {Center for Sprogteknologi, University of Copenhagen, Njalsgade 80, DK-2300 Copenhagen S, Denmark},
  document_type = {Article},
  doi           = {10.1017/S1351324906004256},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847289690&doi=10.1017%2fS1351324906004256&partnerID=40&md5=b20e47119434e911c56c063423f9c5b1},
}

@Article{Popović2006616,
  author        = {Popović, M. and Stein, D. and Ney, H.},
  title         = {Statistical machine translation of german compound words},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2006},
  volume        = {4139 LNAI},
  pages         = {616-624},
  note          = {cited By 20},
  abstract      = {German compound words pose special problems to statistical machine translation systems: the occurence of each of the components in the training data is not sufficient for successful translation. Even if the compound itself has been seen during training, the system may not be capable of translating it properly into two or more words. If German is the target language, the system might generate only separated components or may not be capable of choosing the correct compound. In this work, we investigate and compare different strategies for the treatment of German compound words in statistical machine translation systems. For translation from German, we compare linguistic-based and corpusbased compound splitting. For translation into German, we investigate splitting and rejoining German compounds, as well as joining English potential components. Additionaly, we investigate word alignments enhanced with knowledge about the splitting points of German compounds. The translation quality is consistently improved by all methods for both translation directions. © Springer-Verlag Berlin Heidelberg 2006.},
  affiliation   = {Informatik VI, Computer Science Department, RWTH Aachen University, Ahornstrasse 55, 52056 Aachen, Germany},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749674758&partnerID=40&md5=5a2ba01a1897f9b2a062138b3e9f05d3},
}

@Article{Ahlgren2006681,
  author          = {Ahlgren, P. and Kekäläinen, J.},
  title           = {Swedish full text retrieval: Effectiveness of different combinations of indexing strategies with query terms},
  journal         = {Information Retrieval},
  year            = {2006},
  volume          = {9},
  number          = {6},
  pages           = {681-697},
  note            = {cited By 5},
  abstract        = {In this paper, which treats Swedish full text retrieval, the problem of morphological variation of query terms in the document database is studied. The Swedish CLEF 2003 test collection was used, and the effects of combination of indexing strategies with query terms on retrieval effectiveness were studied. Four of the seven tested combinations involved indexing strategies that used normalization, a form of conflation. All of these four combinations employed compound splitting, both during indexing and at query phase. SWETWOL, a morphological analyzer for the Swedish language, was used for normalization and compound splitting. A fifth combination used stemming, while a sixth attempted to group related terms by right hand truncation of query terms. The truncation was performed by a search expert. These six combinations were compared to each other and to a baseline combination, where no attempt was made to counteract the problem of morphological variation of query terms in the document database. Both the truncation combination, the four combinations based on normalization and the stemming combination outperformed the baseline. Truncation had the best performance. The main conclusion of the paper is that truncation, normalization and stemming enhanced retrieval effectiveness in comparison to the baseline. Further, normalization and stemming were not far below truncation. © Springer Science+Business Media, LLC 2006.},
  affiliation     = {Swedish School of Library and Information Science, University College of Borås, 501 90 Borås, Sweden; Department of Information Studies, 33014 Tampere, Finland},
  author_keywords = {Indexing strategies; Morphological analysis; Stemming; Swedish; Truncation},
  document_type   = {Review},
  doi             = {10.1007/s10791-006-9009-1},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749254069&doi=10.1007%2fs10791-006-9009-1&partnerID=40&md5=d74c4096df819a32e61f01451d6f1a9b},
}

@Article{Kettunen2005476,
  author          = {Kettunen, K. and Kunttu, T. and Järvelin, K.},
  title           = {To stem or lemmatize a highly inflectional language in a probabilistic IR environment?},
  journal         = {Journal of Documentation},
  year            = {2005},
  volume          = {61},
  number          = {4},
  pages           = {476-496},
  note            = {cited By 20},
  abstract        = {Purpose - To show that stem generation compares well with lemmatization as a morphological tool for a highly inflectional language for IR purposes in a best-match retrieval system. Design/methodology/approach - Effects of three different morphological methods - lemmatization, stemming and stem production - for Finnish are compared in a probabilistic IR environment (INQUERY). Evaluation is done using a four-point relevance scale which is partitioned differently in different test settings. Findings - Results show that stem production, a lighter method than morphological lemmatization, compares well with lemmatization in a best-match IR environment. Differences in performance between stem production and lemmatization are small and they are not statistically significant in most of the tested settings. It is also shown that hitherto a rather neglected method of morphological processing for Finnish, stemming, performs reasonably well although the stemmer used - a Porter stemmer implementation - is far from optimal for a morphologically complex language like Finnish. In another series of tests, the effects of compound splitting and derivational expansion of queries are tested. Practical implications - Usefulness of morphological lemmatization and stem generation for IR purposes can be estimated with many factors. On the average P-R level they seem to behave very close to each other in a probabilistic IR system. Thus, the choice of the used method with highly inflectional languages needs to be estimated along other dimensions too. Originality/value - Results are achieved using Finnish as an example of a highly inflectional language. The results are of interest for anyone who is interested in processing of morphological variation of a highly inflected language for IR purposes. © Emerald Group Publishing Limited.},
  affiliation     = {Department of Information Studies, University of Tampere, Tampere, Finland},
  author_keywords = {Information research; Languages},
  document_type   = {Article},
  doi             = {10.1108/00220410510607480},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-23744456817&doi=10.1108%2f00220410510607480&partnerID=40&md5=1a5faad125a6089080e9339a2715ac21},
}

@Article{NoAuthor20041,
  title         = {4th Workshop of the Cross-Language Evaluation Forum, CLEF 2003},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2004},
  volume        = {3237},
  pages         = {1-701},
  note          = {cited By 0},
  abstract      = {The proceedings contain 65 papers. The special focus in this conference is on Ad-hoc Text Retrieval Tracks, Monolingual Experiments, Domain-Specific Document Retrieval, Interactive Cross-Language Retrieval and Cross-Language Question Answering. The topics include: Analysis of the reliability of the multilingual topic set for the cross language evaluation forum; the impact of word normalization methods and merging strategies on multilingual IR; combining query translation and document translation in cross-language retrieval; multilingual information retrieval using open, transparent resources in clef 2003; monolingual, bilingual and multilingual information retrieval; language-dependent and language-independent approaches to cross-lingual text retrieval; multilingual retrieval experiments with MIMOR at the university of Hildesheim; concept-based searching and merging for multilingual information retrieval; automatically generated phrases and relevance feedback for improving CLIR; merging results by predicted retrieval effectiveness; miracle approaches to multilingual information retrieval; experiments to evaluate probabilistic models for automatic stemmer generation and query word translation; regular sound changes for cross-language information retrieval; experiments with machine translation for monolingual, bilingual and multilingual retrieval; comparing weighting models for monolingual information retrieval; pruning texts with nlp and expanding queries with an ontology; report on clef-2003 monolingual tracks; selective compound splitting of Swedish queries for Boolean combinations of truncated terms; experiments with self organizing maps in clef 2003; a data-compression approach to the monolingual girt task; natural language access to the girt4 data and translation selection and document selection.},
  document_type = {Conference Review},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943327424&partnerID=40&md5=0c20883da0670dabd600422a87a09de5},
}

@Article{Hollink200433,
  author          = {Hollink, V. and Kamps, J. and Monz, C. and De Rijke, M.},
  title           = {Monolingual document retrieval for European languages},
  journal         = {Information Retrieval},
  year            = {2004},
  volume          = {7},
  number          = {1-2},
  pages           = {33-52},
  note            = {cited By 54},
  abstract        = {Recent years have witnessed considerable advances in information retrieval for European languages other than English. We give an overview of commonly used techniques and we analyze them with respect to their impact on retrieval effectiveness. The techniques considered range from linguistically motivated techniques, such as morphological normalization and compound splitting, to knowledge-free approaches, such as n-gram indexing. Evaluations are carried out against data from the CLEF campaign, covering eight European languages. Our results show that for many of these languages a modicum of linguistic techniques may lead to improvements in retrieval effectiveness, as can the use of language independent techniques.},
  affiliation     = {Lang. and Inference Technology Group, ILLC, University of Amsterdam, Nieuwe Achtergracht 166, 1018 WV, Amsterdam, Netherlands; Social Science Informatics (SWI), Department of Psychology, University of Amsterdam, Amsterdam, Netherlands},
  author_keywords = {Cross-lingual information retrieval; European languages; Monolingual document retrieval; Morphological normalization; Tokenization},
  document_type   = {Review},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-3843105784&partnerID=40&md5=f9c951e94a8a59c020fd5a0c4b3bbf46},
}

@Article{Kojima2004367,
  author        = {Kojima, Y. and Itoh, H. and Mano, H. and Ogawa, Y.},
  title         = {Ricoh at CLEF 2003},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2004},
  volume        = {3237},
  pages         = {367-372},
  note          = {cited By 1},
  abstract      = {This paper describes RICOH's participation in the Monolingual Information Retrieval tasks of the Cross-Language Evaluation Forum (CLEF) 2003. We applied our system using the same kind of stemmer, the same options and different parameters to five European languages and compared the results for each langauge. Although the overall performance of the system was reasonable, there were two problems. The first was the lack of a compound splitter for German and the second was the failure of query expansion when there were few relevant documents. © Springer-Verlag 2004.},
  affiliation   = {Software RandD Group, RICOH CO., Ltd., 1-1-17 Koishikawa, Bunkyo-ku, Tokyo 112-0002, Japan},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-24944574936&partnerID=40&md5=0dd2d85c0b086f439347bed6ff3b692e},
}

@Article{Cöster2004337,
  author        = {Cöster, R. and Sahlgren, M. and Karlgren, J.},
  title         = {Selective compound splitting of swedish queries for boolean combinations of truncated terms},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2004},
  volume        = {3237},
  pages         = {337-344},
  note          = {cited By 3},
  abstract      = {In languages that use compound words such as Swedish, it is often neccessary to split compound words when indexing documents or queries. One of the problems is that it is difficult to find constituents that express a concept similar to that expressed by the compound. The approach taken here is to expand a query with the leading constituents of the compound words. Every query term is truncated so as to increase recall by hopefully finding other compounds with the leading constituent as prefix. This approach increases recall in a rather uncontrolled way, so we use a Boolean quorum-level search method to rank documents both according to a tf-idf factor but also to the number of matching Boolean combinations. The Boolean combinations performed relatively well, taking into consideration that the queries were very short (maximum of five search terms). Also included in this paper are the results of two other methods we are currently working on in our lab; one for re-ranking search results on the basis of stylistic analysis of documents, and one for dimensionality reduction using Random Indexing. © Springer-Verlag 2004.},
  affiliation   = {Swedish Institute of Computer Science, SICS, Box 1263, SE-164 29 Kista, Sweden},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048871540&partnerID=40&md5=f5b2b1834f7cad7abe59755b8a103d03},
}

@Conference{Ordelman2003225,
  author        = {Ordelman, R. and Van Hessen, A. and De Jong, F.},
  title         = {Compound decomposition in Dutch large vocabulary speech recognition},
  year          = {2003},
  pages         = {225-228},
  note          = {cited By 23},
  abstract      = {This paper addresses compound splitting for Dutch in the context of broadcast news transcription. Language models were created using original text versions and text versions that were decomposed using a data-driven compound splitting algorithm. Language model performances were compared in terms of outof- vocabulary rates and word error rates in a real-world broadcast news transcription task. It was concluded that compound splitting does improve ASR performance. Best results were obtained when frequent compounds were not decomposed.},
  affiliation   = {Department of Computer Science, University of Twente, Netherlands},
  document_type = {Conference Paper},
  journal       = {EUROSPEECH 2003 - 8th European Conference on Speech Communication and Technology},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009170957&partnerID=40&md5=9373eec435a757ed1ac5b17f6d94fcd8},
}

@Conference{Cöster2003,
  author        = {Cöster, R. and Sahlgren, M. and Karlgren, J.},
  title         = {Selective compound splitting of Swedish queries for Boolean combinations of truncated terms},
  year          = {2003},
  volume        = {1169},
  note          = {cited By 0},
  abstract      = {Swedish is a compounding language, and therefore it is important to split compound words so that useful word constituents can be found. One of the problems is that it is difficult to find constituents that express a concept similar to that expressed by the compound. The approach taken in this paper is to look at how the leading constituent of the compound word can be used to expand a search query. The constituent was added to the original query, while still keeping the compound. Every word was then truncated so as to increase recall by hopefully finding other compounds with the leading constituent as prefix. Since this approach increase recall in a rather uncontrolled way, we also used a Boolean quorum-level type of query combination so that documents were ranked according to both the tf-idf factor but also to the number of matching Boolean combinations. The Boolean combinations performed relatively well, taken into consideration that the queries were very short (maximum five search terms). Also included in this paper are the results of two other methods we are currently working on in our lab; one for re-ranking search results on the basis of stylistic analysis of documents, and one for dimensionality reduction using Random Indexing.},
  affiliation   = {Swedish Institute of Computer Science, SICS, Box 1263, Kista, SE-16429, Sweden},
  document_type = {Conference Paper},
  journal       = {CEUR Workshop Proceedings},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978080613&partnerID=40&md5=9f2649a19fbda7e2c22704b89df5b266},
}

@Article{deVries2001149,
  author        = {de Vries, A.P.},
  title         = {A poor man’s approach to CLEF},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2001},
  volume        = {2069},
  pages         = {149-155},
  note          = {cited By 2},
  abstract      = {The primary goal of our participation in CLEF is to acquire experience with supporting cross-lingual retrieval. We submitted runs for all four target languages, but our main interest has been in the bilingual Dutch to English runs. We investigated whether we can obtain a reasonable performance without expensive (but high quality) resources; we have used only ‘off-the-shelf’, freely available tools for stopping, stemming, compound-splitting (only for Dutch) and translation. Although our results are encouraging, we must conclude that a poor man’s approach should not expect to result in rich men’s retrieval results. © Springer-Verlag Berlin Heidelberg 2001.},
  affiliation   = {CWI, Amsterdam, Netherlands; University of Twente, Enschede, Netherlands},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947322423&partnerID=40&md5=b31734990ae64c19871251c93bb3c20f},
}

@Article{Hedlund2001147,
  author        = {Hedlund, T. and Pirkola, A. and Järvelin, K.},
  title         = {Aspects of Swedish morphology and semantics from the perspective of mono- and cross-language information retrieval},
  journal       = {Information Processing and Management},
  year          = {2001},
  volume        = {37},
  number        = {1},
  pages         = {147-161},
  note          = {cited By 30},
  abstract      = {This paper analyzes the features of the Swedish language from the viewpoint of mono- and cross-language information retrieval (CLIR). The study was motivated by the fact that Swedish is known poorly from the IR perspective. This paper shows that Swedish has unique features, in particular gender features, the use of fogemorphemes in the formation of compound words, and a high frequency of homographic words. Especially in dictionary-based CLIR, correct word normalization and compound splitting are essential. It was shown in this study, however, that publicly available morphological analysis tools used for normalization and compound splitting have pitfalls that might decrease the effectiveness of IR and CLIR. A comparative study was performed to test the degree of lexical ambiguity in Swedish, Finnish and English. The results suggest that part-of-speech tagging might be useful in Swedish IR due to the high frequency of homographic words.},
  affiliation   = {Department of Information Studies, Univ. Tampere, PO Box 607, FIN-33101, Tampere, Finland},
  document_type = {Article},
  doi           = {10.1016/S0306-4573(00)00024-8},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035152106&doi=10.1016%2fS0306-4573%2800%2900024-8&partnerID=40&md5=7a27a4aaed911888c61fa3e179606b73},
}

@Conference{Monz2001,
  author        = {Monz, C. and De Rijke, M.},
  title         = {The university of Amsterdam at CLEF 2001},
  year          = {2001},
  volume        = {1167},
  note          = {cited By 1},
  abstract      = {This paper describes the official runs of our team for CLEF-2001. We took part in the monolingual task, for Dutch, German, and Italian. The focus of our experiments was on the effects of morphological analyses such as stemming and compound splitting on retrieval effectiveness. Confirming earlier reports on retrieval in compound splitting languages such as Dutch and German, we found improvements to be around 25% for German and as much as 55% for Dutch. For Italian, lexicon-based stemming resulted in gains of up to 25%. Copyright © 2001 for the individual papers by the papers' authors.},
  affiliation   = {Institute for Logic, Language and Computation (ILLC), University of Amsterdam, Amsterdam, 1018 TV, Netherlands},
  document_type = {Conference Paper},
  journal       = {CEUR Workshop Proceedings},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921953688&partnerID=40&md5=85d483b1ce4dbd8f4a9881095044adb8},
}

@Article{VanHoudt20002345,
  author        = {Van Houdt, B. and Blondia, C.},
  title         = {Analysis of an identifier splitting algorithm combined with polling (ISAP) for contention resolution in a wireless access network},
  journal       = {IEEE Journal on Selected Areas in Communications},
  year          = {2000},
  volume        = {18},
  number        = {11},
  pages         = {2345-2355},
  note          = {cited By 12},
  abstract      = {In this paper, a contention resolution scheme for an uplink contention channel in a wireless access network is presented. The scheme consists of a tree algorithm, namely the identifier splitting algorithm (ISA), combined with a polling scheme. Initially, ISA is used, but at a certain level of the tree, the scheme switches to polling of the stations. This scheme is further enhanced by skipping a few levels in the tree when starting the algorithm (both in a static and a dynamic way) and by allowing multiple instants simultaneously. An analytical model of the system and its variants leads to the evaluation of its performance, by means of the delay density function and the throughput characteristics. This model is used to investigate the influence of the packet arrival rate, the instant at which the ISA scheme switches to polling, the starting level of the ISA scheme, and the use of multiple instances on the mean delay, the delay quantiles, and the throughput.},
  affiliation   = {University of Antwerp, Dept. of Math. and Computer Science, Perf. Anal. Telecommunication S., Universiteitsplein, 1, B-2610 Antwerp, Belgium},
  document_type = {Article},
  doi           = {10.1109/49.895039},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034313661&doi=10.1109%2f49.895039&partnerID=40&md5=f161824d2f7fbf5fbea52208e852968e},
}

@Article{VanHoudt200091,
  author        = {Van Houdt, B. and Blondia, C.},
  title         = {Performance evaluation of the identifier splitting algorithm with polling in wireless ATM networks},
  journal       = {International Journal of Wireless Information Networks},
  year          = {2000},
  volume        = {7},
  number        = {2},
  pages         = {91-103},
  note          = {cited By 4},
  abstract      = {This paper presents a performance analysis of the Identifier Splitting Algorithm combined with polling, a contention resolution scheme used to inform the Base Station about the bandwidth needs of the Mobile Station in a wireless ATM network. An analytical model leads to the evaluation of performance parameters which determine the throughput and the access delay of the algorithm for different system parameters. This analysis is used to investigate the influence of the system parameters on the performance, from which guidelines for parameter tuning can be derived.},
  affiliation   = {University of Antwerp, Dept. of Math. and Computer Science, Perf. Anal. Telecommunication S., Universiteitsplein, 1, B-2610 Antwerp, Belgium},
  document_type = {Article},
  doi           = {10.1023/A:1009531503222},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034171489&doi=10.1023%2fA%3a1009531503222&partnerID=40&md5=689222302516911242640a02682795f9},
}

@Conference{VanHoudt199914,
  author        = {Van Houdt, B. and Blondia, C. and Casals, O. and Garcia, J.},
  title         = {Packet level performance characteristics of a MAC protocol for wireless ATM LANs},
  year          = {1999},
  pages         = {14-23},
  note          = {cited By 0},
  abstract      = {This paper determines packet level performance measures of a MAC protocol for a wireless ATM local area network. A key characteristic of the MAC protocol is the Identifier Splitting Algorithm with Polling, a contention resolution scheme used to inform the Base Station about the bandwidth needs of a Mobile Station when no piggybacking can be used. We consider higher layer packets that are generated at the Mobile Station and investigate the influence of the traffic characteristics of the packet arrival process on the efficiency of the protocol and on the delay that packets experience to access the shared medium.},
  affiliation   = {Univ of Antwerp, Belgium},
  document_type = {Conference Paper},
  journal       = {Conference on Local Computer Networks},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033336183&partnerID=40&md5=81ecadb6949074e1086de2efc46cb9ca},
}

@Conference{VanHoudt199910,
  author        = {Van Houdt, B. and Blondia, C. and Casals, O. and Garcia, J. and Vazquez, D.},
  title         = {Performance evaluation of a MAC protocol for wireless ATM networks supporting the ATM service categories},
  year          = {1999},
  pages         = {10-17},
  note          = {cited By 0},
  abstract      = {This paper presents a Medium Access Control (MAC) protocol for broadband wireless LANs based on the ATM transfer mode, together with the evaluation of its performance in terms of throughput and access delay. Important characteristics of the MAC protocol are the way information between the Mobile Stations (MS) and Base Station (BS) is exchanged and the algorithm used to allocate the bandwidth in order to support the service categories. The performance is heavily influenced by the way the BS is informed about the bandwidth needs of the MSs. In order to obtain an efficient system, a contention resolution scheme based on an Identifier Splitting Algorithm combined with polling is proposed for that purpose, in case no piggybacking can be used. A detailed analytical evaluation, both on cell level and higher layer packet level, is performed, leading to an assessment of the efficiency and the access delay of the system.},
  affiliation   = {University of Antwerp, Dept. Mathematics and Computer Science, Antwerp, B 2610, Belgium; Polytechnic University of Catalunya, Computer Architecture Department, Barcelona, E 08034, Spain},
  document_type = {Conference Paper},
  doi           = {10.1145/313256.313268},
  journal       = {Proceedings of the 2nd ACM International Workshop on Wireless Mobile Multimedia, WOWMOM 1999},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029391820&doi=10.1145%2f313256.313268&partnerID=40&md5=c6ef3e8a4fa86a5aa32ab722acfedfe0},
}

@Article{Kraaij1998605,
  author        = {Kraaij, W. and Pohlmann, R.},
  title         = {Comparing the effect of syntactic vs. statistical phrase indexing strategies for Dutch},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {1998},
  volume        = {1513},
  pages         = {605-616},
  note          = {cited By 11},
  abstract      = {In this paper we describe the results of experiments contrasting syntactic phrase indexing with statistical phrase indexing for Dutch texts. Our results showed that we at least need a compound split- ting algorithm for good quality retrieval for Dutch texts. If we then add either syntactic or statistical phrases, performance generally improves, but this effect is never statistically significant. If we compare syntactic vs. statistical phrase indexing, syntactic phrases are slightly superior to statistical phrases, particularly at high precision. At higher recall levels syntactic and statistical phrases are equally effective. However, since a compound splitting algorithm requires a dictionary and knowledge about constraints on compound formation, a purely non-linguistic indexing strategy, with or without phrases, does not seem to be very effective for Dutch. © Springer-Verlag Berlin Heidelberg 1998.},
  affiliation   = {Institute of Applied Physics, Netherlands Organisation for Applied Scientific Research (TNO), Delft, Netherlands; Utrecht Institute of Linguistics OTS, Utrecht University, Utrecht, Netherlands},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945190539&partnerID=40&md5=63384c07b04912a56b313e6b019a4cb3},
}

@Article{Haque2019,
  author  = {Haque, Rejwanul and Hasanuzzaman, Mohammed and Way, Andy},
  title   = {Ruslan Mitkov, Johanna Monti, Gloria Corpas Pastor, and Violeta Seretan (eds): Multiword units in machine translation and translation technology},
  journal = {Machine Translation},
  year    = {2019},
  month   = {Aug},
  issn    = {1573-0573},
  day     = {28},
  doi     = {10.1007/s10590-019-09239-4},
  url     = {https://doi.org/10.1007/s10590-019-09239-4},
}

@Article{Simpson2018,
  author   = {Simpson, Andrew and Ngo, Binh},
  title    = {Classifier syntax in Vietnamese},
  journal  = {Journal of East Asian Linguistics},
  year     = {2018},
  volume   = {27},
  number   = {3},
  pages    = {211--246},
  month    = {Aug},
  issn     = {1572-8560},
  abstract = {Vietnamese is a language with a three-way split in the appearance of numeral classifiers when nouns are counted: some nouns always require classifiers (obligatory-classifier nouns), others occur only optionally with classifiers (optional-classifier nouns), and a third group never combines with a classifier (non-classified nouns). This distribution provides potentially important information on the much debated question of whether classifiers functionally combine with numerals (Bale and Coon in Linguist Inq 45:695--707, 2014) or with nouns (Li in Linguist Inq 29(4):693--702, 1998; Cheng and Sybesma in Linguist Inq 30:509--542, 1999). It also appears to challenge Chierchia's (Nat Lang Semant 6(4):339--405, 1998) characterization of the basic semantic type of nouns found in different languages, which assumes a uniform pattern of classifier occurrence in numeral classifier languages. Having described the broad distribution of classifiers in Vietnamese and the questions this raises, the article probes the syntactic properties of classifiers with the three types of noun in the language, considering double classifier patterns, fragment answers, passive constructions, and the use of classifiers with certain compound nouns. Evidence from such phenomena is shown to support the hypothesis that a uniform syntactic structure is actually projected with nouns of all types in Vietnamese, but sometimes masked by the use of nouns to overtly lexicalize both the N and CL positions in nominal projections through N-to-Cl movement.},
  day      = {01},
  doi      = {10.1007/s10831-018-9181-5},
  url      = {https://doi.org/10.1007/s10831-018-9181-5},
}

@InProceedings{10.1007/978-3-319-77113-7_30,
  author    = {Kohail, Sarah and Biemann, Chris},
  title     = {Matching, Re-Ranking and Scoring: Learning Textual Similarity by Incorporating Dependency Graph Alignment and Coverage Features},
  booktitle = {Computational Linguistics and Intelligent Text Processing},
  year      = {2018},
  editor    = {Gelbukh, Alexander},
  pages     = {377--390},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {In this work, we introduce a supervised model for learning textual similarity, which can identify and score similarity between a set of candidate texts and a given query text. By combining dependency graph similarity and coverage features with lexical similarity measures using neural networks, we show that most relevant documents to a given text can be more accurately ranked and scored than if the lexical similarity measures were used in isolation. Additionally, we introduce an approximate dependency subgraph alignment approach allowing node gaps and mismatch, where a certain word in one dependency graph cannot be mapped to any word in the other graph. We apply our model to two different applications, namely re-ranking for improving document retrieval precision on a new dataset, and automatic short answer scoring on a standard dataset. Experimental results indicate that our approach is easily adaptable to different tasks and languages, and works well for long texts as well as short texts.},
  isbn      = {978-3-319-77113-7},
}

@InProceedings{10.1007/978-3-319-97571-9_23,
  author    = {Zuters, J{\={a}}nis and Strazds, Gus and Immers, K{\={a}}rlis},
  title     = {Semi-automatic Quasi-morphological Word Segmentation for Neural Machine Translation},
  booktitle = {Databases and Information Systems},
  year      = {2018},
  editor    = {Lupeikiene, Audrone and Vasilecas, Olegas and Dzemyda, Gintautas},
  pages     = {289--301},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {This paper proposes the Prefix-Root-Postfix-Encoding (PRPE) algorithm, which performs close-to-morphological segmentation of words as part of text pre-processing in machine translation. PRPE is a cross-language algorithm requiring only minor tweaking to adapt it for any particular language, a property which makes it potentially useful for morphologically rich languages with no morphological analysers available. As a key part of the proposed algorithm we introduce the `Root alignment' principle to extract potential sub-words from a corpus, as well as a special technique for constructing words from potential sub-words. We conducted experiments with two different neural machine translation systems, training them on parallel corpora for English-Latvian and Latvian-English translation. Evaluation of translation quality showed improvements in BLEU scores when the data were pre-processed using the proposed algorithm, compared to a couple of baseline word segmentation algorithms. Although we were able to demonstrate improvements in both translation directions and for both NMT systems, they were relatively minor, and our experiments show that machine translation with inflected languages remains challenging, especially with translation direction towards a highly inflected language.},
  isbn      = {978-3-319-97571-9},
}

@Article{Cha2017,
  author   = {Cha, Myung-Hoon and Kim, Dong-Oh and Kim, Hong-Yeon and Kim, Young-Kyun},
  title    = {Adaptive metadata rebalance in exascale file system},
  journal  = {The Journal of Supercomputing},
  year     = {2017},
  volume   = {73},
  number   = {4},
  pages    = {1337--1359},
  month    = {Apr},
  issn     = {1573-0484},
  abstract = {This paper presents an effective method of metadata rebalance in exascale distributed file systems. Exponential data growth has led to the need for an adaptive and robust distributed file system whose typical architecture is composed of a large cluster of metadata servers and data servers. Though each metadata server can have an equally divided subset from the entire metadata set at first, there will eventually be a global imbalance in the placement of metadata among metadata servers, and this imbalance worsens over time. To ensure that disproportionate metadata placement will not have a negative effect on the intrinsic performance of a metadata server cluster, it is necessary to recover the balanced performance of the cluster periodically. However, this cannot be easily done because rebalancing seriously hampers the normal operation of a file system. This situation continues to get worse with both an ever-present heavy workload on the file system and frequent failures of server components at exascale. As one of the primary reasons for such a degraded performance, file system clients frequently fail to look up metadata from the metadata server cluster during the period of metadata rebalance; thus, metadata operations cannot proceed at their normal speed. We propose a metadata rebalance model that minimizes failures of metadata operations during the metadata rebalance period and validate the proposed model through a cost analysis. The analysis results demonstrate that our model supports the feasibility of online metadata rebalance without the normal operation obstruction and increases the chances of maintaining balance in a huge cluster of metadata servers.},
  day      = {01},
  doi      = {10.1007/s11227-016-1812-x},
  url      = {https://doi.org/10.1007/s11227-016-1812-x},
}

@InProceedings{10.1007/978-3-319-10888-9_23,
  author    = {Bick, Eckhard},
  title     = {Constraint Grammar-Based Swedish-Danish Machine Translation},
  booktitle = {Advances in Natural Language Processing},
  year      = {2014},
  editor    = {Przepi{\'o}rkowski, Adam and Ogrodniczuk, Maciej},
  pages     = {216--227},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {This paper describes and evaluates a grammar-based machine translation system for the Swedish-Danish language pair. Source-language structural analysis, polysemy resolution, syntactic movement rules and target-language agreement are based on Constraint Grammar morphosyntactic tags and dependency trees. Lexical transfer rules exploit dependency links to access contextual information, such as syntactic argument function, semantic type and quantifiers, or to integrate verbal features, e.g. diathesis and auxiliaries. Out-of-vocabulary words are handled by derivational and compound analysis with a combined coverage of 99.3{\%}, as well as systematic morpho-phonemic transliterations for the remaining cases. The system achieved BLEU scores of 0.65-0.8 depending on references and outperformed both STMT and RBMT competitors by a large margin.},
  isbn      = {978-3-319-10888-9},
}

@InProceedings{10.1007/978-3-642-54903-8_23,
  author    = {Durrani, Nadir and Al-Onaizan, Yaser and Ittycheriah, Abraham},
  title     = {Improving Egyptian-to-English SMT by Mapping Egyptian into MSA},
  booktitle = {Computational Linguistics and Intelligent Text Processing},
  year      = {2014},
  editor    = {Gelbukh, Alexander},
  pages     = {271--282},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {One of the aims of DARPA BOLT project is to translate the Egyptian blog data into English. While the parallel data for MSA-English is abundantly available, sparsely exists for Egyptian-English and Egyptian-MSA. A notable drop in the translation quality is observed when translating Egyptian to English in comparison with translating from MSA to English. One of the reasons for this drop is the high OOV rate, where as another is the dialectal differences between training and test data. This work is focused on improving Egyptian-to-English translation by bridging the gap between Egyptian and MSA. First we try to reduce the OOV rate by proposing MSA candidates for the unknown Egyptian words through different methods such as spelling correction, suggesting synonyms based on context etc. Secondly we apply convolution model using English as a pivot to map Egyptian words into MSA. We then evaluate our edits by running decoder built on MSA-to-English data. Our spelling-based correction shows an improvement of  1.7 BLEU points over the baseline system, that translates unedited Egyptian into English.},
  isbn      = {978-3-642-54903-8},
}

@InProceedings{10.1007/978-3-642-40802-1_14,
  author    = {Ganguly, Debasis and Leveling, Johannes and Jones, Gareth J. F.},
  title     = {A Case Study in Decompounding for Bengali Information Retrieval},
  booktitle = {Information Access Evaluation. Multilinguality, Multimodality, and Visualization},
  year      = {2013},
  editor    = {Forner, Pamela and M{\"u}ller, Henning and Paredes, Roberto and Rosso, Paolo and Stein, Benno},
  pages     = {108--119},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Decompounding has been found to improve information retrieval (IR) effectiveness for compounding languages such as Dutch, German, or Finnish. No previous studies, however, exist on the effect of decomposition of compounds in IR for Indian languages. In this case study, we investigate the effect of decompounding for Bengali, a highly agglutinative Indian language. The standard approach of decompounding for IR, i.e. indexing compound parts (constituents) in addition to compound words, has proven beneficial for European languages. Our experiments reported in this paper show that such a standard approach does not work particularly well for Bengali IR. Some unique characteristics of Bengali compounds are: i) only one compound constituent may be a valid word in contrast to the stricter requirement of both being so; and ii) the first character of the right constituent can be modified by the rules of Sandhi in contrast to simple concatenation. As a solution, we firstly propose a more relaxed decompounding where a compound word is decomposed into only one constituent if the other constituent is not a valid word, and secondly we perform selective decompounding by ensuring that constituents often co-occur with the compound word, which indicates how related the constituents and the compound are. We perform experiments on Bengali ad-hoc IR collections from FIRE 2008 to 2012. Our experiments show that both the relaxed decomposition and the co-occurrence-based constituent selection proves more effective than the standard frequency-based decomposition method, improving mean average precision (MAP) up to 2.72{\%} and recall up to 1.8{\%}, compared to not decompounding words.},
  isbn      = {978-3-642-40802-1},
}

@InProceedings{10.1007/978-3-642-40087-2_25,
  author    = {Loponen, Aki and Paik, Jiaul H. and J{\"a}rvelin, Kalervo},
  title     = {UTA Stemming and Lemmatization Experiments in the FIRE Bengali Ad Hoc Task},
  booktitle = {Multilingual Information Access in South Asian Languages},
  year      = {2013},
  editor    = {Majumder, Prasenjit and Mitra, Mandar and Bhattacharyya, Pushpak and Subramaniam, L. Venkata and Contractor, Danish and Rosso, Paolo},
  pages     = {258--268},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {UTA participated in the monolingual Bengali ad hoc Track at FIRE 2010. As Bengali is highly inflectional, we experimented with three language normalizers: one stemmer, YASS, and two lemmatizers, GRALE and StaLe. YASS is a corpus-based unsupervised statistical stemmer capable of handling several languages through suffix removal. GRALE is a novel graph-based lemmatizer for Bengali, but extendable for other agglutinative languages. StaLe is a statistical rule-based lemmatizer that has been implemented for several languages. We analyze 9 runs, using the three systems for the title (T) and title-and-description (TD) and title-description-and-narrative (TDN). The T runs were the least effective with MAP about 0.34 (P@10 about 0.30). All the TD runs delivered a MAP close to 0.45 (P@10 about 0.37), while the TDN runs gave a MAP of 0.50 to 0.52 (P@10 about 0.41). The performances of the three normalizers are close to each other, but they have different strengths in other aspects. The performances compare well with the ones other groups obtained in the monolingual Bengali ad hoc Track at FIRE 2010.},
  isbn      = {978-3-642-40087-2},
}

@InProceedings{10.1007/978-3-642-34456-5_17,
  author    = {Zhang, Jiajun and Zhai, Feifei and Zong, Chengqing},
  title     = {Handling Unknown Words in Statistical Machine Translation from a New Perspective},
  booktitle = {Natural Language Processing and Chinese Computing},
  year      = {2012},
  editor    = {Zhou, Ming and Zhou, Guodong and Zhao, Dongyan and Liu, Qun and Zou, Lei},
  pages     = {176--187},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Unknown words are one of the key factors which drastically impact the translation quality. Traditionally, nearly all the related research work focus on obtaining the translation of the unknown words in different ways. In this paper, we propose a new perspective to handle unknown words in statistical machine translation. Instead of trying great effort to find the translation of unknown words, this paper focuses on determining the semantic function the unknown words serve as in the test sentence and keeping the semantic function unchanged in the translation process. In this way, unknown words will help the phrase reordering and lexical selection of their surrounding words even though they still remain untranslated. In order to determine the semantic function of each unknown word, this paper employs the distributional semantic model and the bidirectional language model. Extensive experiments on Chinese-to-English translation show that our methods can substantially improve the translation quality.},
  isbn      = {978-3-642-34456-5},
}

@InProceedings{10.1007/978-3-642-20095-3_31,
  author    = {Blancafort, Helena and de Loupy, Claude},
  title     = {Clues to Compare Languages for Morphosyntactic Analysis: A Study Run on Parallel Corpora and Morphosyntactic Lexicons},
  booktitle = {Human Language Technology. Challenges for Computer Science and Linguistics},
  year      = {2011},
  editor    = {Vetulani, Zygmunt},
  pages     = {339--350},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The aim of the present work is to find clues on how to compare the difficulties of five languages for morphosyntactic analysis and the development of lexicographic resources running a corpora and lexical comparative study on multilingual parallel corpora and morphosyntactic lexicons. First, we ran some corpus-based experiments without any other type of knowledge, following classical measures used in lexical statistics. Then we carried out further experiments on the corpora using morphosyntactic lexicons. Finally, we plotted given diagrams using different clues to offer an overview of the difficulty of a language for the development of morphosyntactic resources.},
  isbn      = {978-3-642-20095-3},
}

@InProceedings{10.1007/978-3-642-20161-5_77,
  author    = {Herbert, Benjamin and Szarvas, Gy{\"o}rgy and Gurevych, Iryna},
  title     = {Combining Query Translation Techniques to Improve Cross-Language Information Retrieval},
  booktitle = {Advances in Information Retrieval},
  year      = {2011},
  editor    = {Clough, Paul and Foley, Colum and Gurrin, Cathal and Jones, Gareth J. F. and Kraaij, Wessel and Lee, Hyowon and Mudoch, Vanessa},
  pages     = {712--715},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {In this paper we address the combination of query translation approaches for cross-language information retrieval (CLIR). We translate queries with Google Translate and extend them with new translations obtained by mapping noun phrases in the query to concepts in the target language using Wikipedia. For two CLIR collections, we show that the proposed model provides meaningful translations that improve the strong baseline CLIR model based on a top performing SMT system.},
  isbn      = {978-3-642-20161-5},
}

@InProceedings{10.1007/978-3-642-15760-8_28,
  author    = {Zeman, Daniel},
  title     = {Using TectoMT as a Preprocessing Tool for Phrase-Based Statistical Machine Translation},
  booktitle = {Text, Speech and Dialogue},
  year      = {2010},
  editor    = {Sojka, Petr and Hor{\'a}k, Ale{\v{s}} and Kope{\v{c}}ek, Ivan and Pala, Karel},
  pages     = {216--223},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We present a systematic comparison of preprocessing techniques for two language pairs: English-Czech and English-Hindi. The two target languages, although both belonging to the Indo-European language family, show significant differences in morphology, syntax and word order. We describe how TectoMT, a successful framework for analysis and generation of language, can be used as preprocessor for a phrase-based MT system.We compare the two language pairs and the optimal sets of source-language transformations applied to them. The following transformations are examples of possible preprocessing steps: lemmatization; retokenization, compound splitting; removing/adding words lacking counterparts in the other language; phrase reordering to resemble the target word order; marking syntactic functions. TectoMT, as well as all other tools and data sets we use, are freely available on the Web.},
  isbn      = {978-3-642-15760-8},
}

@InProceedings{10.1007/978-3-540-78135-6_12,
  author    = {Alfonseca, Enrique and Bilac, Slaven and Pharies, Stefan},
  title     = {German Decompounding in a Difficult Corpus},
  booktitle = {Computational Linguistics and Intelligent Text Processing},
  year      = {2008},
  editor    = {Gelbukh, Alexander},
  pages     = {128--139},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Splitting compound words has proved to be useful in areas such as Machine Translation, Speech Recognition or Information Retrieval (IR). In the case of IR systems, they usually have to cope with noisy data, as user queries are usually written quickly and submitted without review. This work attempts at improving the current approaches for German decompounding when applied to query keywords. The results show an increase of more than 10{\%} in accuracy compared to other state-of-the-art methods.},
  isbn      = {978-3-540-78135-6},
}

@InProceedings{10.1007/978-3-540-78135-6_11,
  author    = {Holz, Florian and Biemann, Chris},
  title     = {Unsupervised and Knowledge-Free Learning of Compound Splits and Periphrases},
  booktitle = {Computational Linguistics and Intelligent Text Processing},
  year      = {2008},
  editor    = {Gelbukh, Alexander},
  pages     = {117--127},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We present an approach for knowledge-free and unsupervised recognition of compound nouns for languages that use one-word-compounds such as Germanic and Scandinavian languages. Our approach works by creating a candidate list of compound splits based on the word list of a large corpus. Then, we filter this list using the following criteria: (a) frequencies of compounds and parts,(b) length of parts.},
  isbn      = {978-3-540-78135-6},
}

@Article{Kettunen2007,
  author   = {Kettunen, Kimmo and Airio, Eija and J{\"a}rvelin, Kalervo},
  title    = {Restricted inflectional form generation in management of morphological keyword variation},
  journal  = {Information Retrieval},
  year     = {2007},
  volume   = {10},
  number   = {4},
  pages    = {415--444},
  month    = {Oct},
  issn     = {1573-7659},
  abstract = {Word form normalization through lemmatization or stemming is a standard procedure in information retrieval because morphological variation needs to be accounted for and several languages are morphologically non-trivial. Lemmatization is effective but often requires expensive resources. Stemming is also effective in most contexts, generally almost as good as lemmatization and typically much less expensive; besides it also has a query expansion effect. However, in both approaches the idea is to turn many inflectional word forms to a single lemma or stem both in the database index and in queries. This means extra effort in creating database indexes. In this paper we take an opposite approach: we leave the database index un-normalized and enrich the queries to cover for surface form variation of keywords. A potential penalty of the approach would be long queries and slow processing. However, we show that it only matters to cover a negligible number of possible surface forms even in morphologically complex languages to arrive at a performance that is almost as good as that delivered by stemming or lemmatization. Moreover, we show that, at least for typical test collections, it only matters to cover nouns and adjectives in queries. Furthermore, we show that our findings are particularly good for short queries that resemble normal searches of web users. Our approach is called FCG (for Frequent Case (form) Generation). It can be relatively easily implemented for Latin/Greek/Cyrillic alphabet languages by examining their (typically very skewed) nominal form statistics in a small text sample and by creating surface form generators for the 3--9 most frequent forms. We demonstrate the potential of our FCG approach for several languages of varying morphological complexity: Swedish, German, Russian, and Finnish in well-known test collections. Applications include in particular Web IR in languages poor in morphological resources.},
  day      = {01},
  doi      = {10.1007/s10791-007-9030-z},
  url      = {https://doi.org/10.1007/s10791-007-9030-z},
}

@InBook{Nivre2006,
  pages     = {45--86},
  title     = {Dependency Parsing},
  publisher = {Springer Netherlands},
  year      = {2006},
  address   = {Dordrecht},
  isbn      = {978-1-4020-4889-0},
  booktitle = {Inductive Dependency Parsing},
  doi       = {10.1007/1-4020-4889-0_3},
  url       = {https://doi.org/10.1007/1-4020-4889-0_3},
}

@InProceedings{10.1007/11878773_20,
  author    = {Tordai, Anna and de Rijke, Maarten},
  title     = {Four Stemmers and a Funeral: Stemming in Hungarian at CLEF 2005},
  booktitle = {Accessing Multilingual Information Repositories},
  year      = {2006},
  editor    = {Peters, Carol and Gey, Fredric C. and Gonzalo, Julio and M{\"u}ller, Henning and Jones, Gareth J. F. and Kluck, Michael and Magnini, Bernardo and de Rijke, Maarten},
  pages     = {179--186},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We developed algorithmic stemmers for Hungarian and used them for the ad-hoc monolingual task for CLEF 2005. Our goal was to determine what degree of stemming is the most effective. Although on average the stemmers did not perform as well as the the best n-gram, we found that stemming over a broad range of suffixes especially on nouns is highly useful.},
  isbn      = {978-3-540-45700-8},
}

@InProceedings{10.1007/11816508_44,
  author    = {Mishne, Gilad and de Rijke, Maarten},
  title     = {Language Model Mixtures for Contextual Ad Placement in Personal Blogs},
  booktitle = {Advances in Natural Language Processing},
  year      = {2006},
  editor    = {Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo and Pahikkala, Tapio},
  pages     = {435--446},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We introduce a method for content-based advertisement selection for personal blog pages, based on combining multiple representations of the blog. The core idea behind the method is that personal blogs represent individuals, whose interests can be modeled by the language used in the blog itself combined with the language used in related sources of information, such as comments posted to a blog post or the blogger's community. An evaluation of our ad placement method shows improvement over state-of-the-art ad placement methods which were not designed for blog pages.},
  isbn      = {978-3-540-37336-0},
}

@InProceedings{10.1007/11816508_61,
  author    = {Popovi{\'{c}}, Maja and Stein, Daniel and Ney, Hermann},
  title     = {Statistical Machine Translation of German Compound Words},
  booktitle = {Advances in Natural Language Processing},
  year      = {2006},
  editor    = {Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo and Pahikkala, Tapio},
  pages     = {616--624},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {German compound words pose special problems to statistical machine translation systems: the occurence of each of the components in the training data is not sufficient for successful translation. Even if the compound itself has been seen during training, the system may not be capable of translating it properly into two or more words. If German is the target language, the system might generate only separated components or may not be capable of choosing the correct compound. In this work, we investigate and compare different strategies for the treatment of German compound words in statistical machine translation systems. For translation from German, we compare linguistic-based and corpus-based compound splitting. For translation into German, we investigate splitting and rejoining German compounds, as well as joining English potential components. Additionaly, we investigate word alignments enhanced with knowledge about the splitting points of German compounds. The translation quality is consistently improved by all methods for both translation directions.},
  isbn      = {978-3-540-37336-0},
}

@Article{Airio2006,
  author   = {Airio, Eija},
  title    = {Word normalization and decompounding in mono- and bilingual IR},
  journal  = {Information Retrieval},
  year     = {2006},
  volume   = {9},
  number   = {3},
  pages    = {249--271},
  month    = {Jun},
  issn     = {1573-7659},
  abstract = {The present research studies the impact of decompounding and two different word normalization methods, stemming and lemmatization, on monolingual and bilingual retrieval. The languages in the monolingual runs are English, Finnish, German and Swedish. The source language of the bilingual runs is English, and the target languages are Finnish, German and Swedish. In the monolingual runs, retrieval in a lemmatized compound index gives almost as good results as retrieval in a decompounded index, but in the bilingual runs differences are found: retrieval in a lemmatized decompounded index performs better than retrieval in a lemmatized compound index. The reason for the poorer performance of indexes without decompounding in bilingual retrieval is the difference between the source language and target languages: phrases are used in English, while compounds are used instead of phrases in Finnish, German and Swedish. No remarkable performance differences could be found between stemming and lemmatization.},
  day      = {01},
  doi      = {10.1007/s10791-006-0884-2},
  url      = {https://doi.org/10.1007/s10791-006-0884-2},
}

@InProceedings{10.1007/11506157_40,
  author    = {Aura, Tuomas and Nagarajan, Aarthi and Gurtov, Andrei},
  title     = {Analysis of the HIP Base Exchange Protocol},
  booktitle = {Information Security and Privacy},
  year      = {2005},
  editor    = {Boyd, Colin and Gonz{\'a}lez Nieto, Juan Manuel},
  pages     = {481--493},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The Host Identity Protocol (HIP) is an Internet security and multi-addressing mechanism specified by the IETF. HIP introduces a new layer between the transport and network layers of the TCP/IP stack that maps host identifiers to network locations, thus separating the two conflicting roles that IP addresses have in the current Internet. This paper analyzes the security and functionality of the HIP base exchange, which is a classic key exchange protocol with some novel features for authentication and DoS protection. The base exchange is the most stable part of the HIP specification with multiple existing implementations. We point out several security issues in the current protocol and propose changes that are compatible with the goals of HIP.},
  isbn      = {978-3-540-31684-8},
}

@InProceedings{10.1007/11519645_12,
  author    = {Kamps, Jaap and Adafre, Sisay Fissaha and de Rijke, Maarten},
  title     = {Effective Translation, Tokenization and Combination for Cross-Lingual Retrieval},
  booktitle = {Multilingual Information Access for Text, Speech and Images},
  year      = {2005},
  editor    = {Peters, Carol and Clough, Paul and Gonzalo, Julio and Jones, Gareth J. F. and Kluck, Michael and Magnini, Bernardo},
  pages     = {123--134},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Our approach to cross-lingual document retrieval starts from the assumption that effective monolingual retrieval is at the core of any cross-language retrieval system. We devote particular attention to three crucial ingredients of our approach to cross-lingual retrieval. First, effective tokenization techniques are essential to cope with morphological variations common in many European languages. Second, effective combination methods allow us to combine the best of different strategies. Finally, effective translation methods for translating queries or documents turn a monolingual retrieval system into a cross-lingual retrieval system proper. The viability of our approach is shown by a series of experiments in monolingual, bilingual, and multilingual retrieval.},
  isbn      = {978-3-540-32051-7},
}

@InProceedings{10.1007/11519645_23,
  author    = {Tomlinson, Stephen},
  title     = {Finnish, Portuguese and Russian Retrieval with Hummingbird SearchServerTM at CLEF 2004},
  booktitle = {Multilingual Information Access for Text, Speech and Images},
  year      = {2005},
  editor    = {Peters, Carol and Clough, Paul and Gonzalo, Julio and Jones, Gareth J. F. and Kluck, Michael and Magnini, Bernardo},
  pages     = {221--232},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Hummingbird participated in the Finnish, Portuguese, Russian and French monolingual information retrieval tasks of the Cross-Language Evaluation Forum (CLEF) 2004. SearchServer's experimental lexical stemmers significantly increased mean average precision for each of the 4 languages. For Finnish, mean average precision was significantly higher with SearchServer's experimental decompounding option enabled. Using the stemming interpretations which led to the highest score in each document instead of using the same interpretations for all documents was of significant benefit for Russian.},
  isbn      = {978-3-540-32051-7},
}

@InProceedings{10.1007/11519645_11,
  author    = {Moulinier, Isabelle and Williams, Ken},
  title     = {Report on Thomson Legal and Regulatory Experiments at CLEF-2004},
  booktitle = {Multilingual Information Access for Text, Speech and Images},
  year      = {2005},
  editor    = {Peters, Carol and Clough, Paul and Gonzalo, Julio and Jones, Gareth J. F. and Kluck, Michael and Magnini, Bernardo},
  pages     = {110--122},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Thomson Legal and Regulatory participated in the CLEF-2004 monolingual and bilingual tracks. Monolingual experiments included Portuguese, Russian and Finnish. We investigated a new query structure to handle Finnish compounds. Our main focus was bilingual search from German to French. Our approach used query translation and post-translation pseudo-relevance feedback. We compared two translation models for query translation, and captured compound translations through fertility probabilities. While the fertility-based approach picks good terms, it does not help improve bilingual retrieval. Pseudo-relevance feedback, on the other hand, resulted in improved average precision.},
  isbn      = {978-3-540-32051-7},
}

@InProceedings{10.1007/11519645_10,
  author    = {Nadeau, David and Jarmasz, Mario and Barri{\`e}re, Caroline and Foster, George and St-Jacques, Claude},
  title     = {Using COTS Search Engines and Custom Query Strategies at CLEF},
  booktitle = {Multilingual Information Access for Text, Speech and Images},
  year      = {2005},
  editor    = {Peters, Carol and Clough, Paul and Gonzalo, Julio and Jones, Gareth J. F. and Kluck, Michael and Magnini, Bernardo},
  pages     = {100--109},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {This paper presents a system for bilingual information retrieval using commercial off-the-shelf search engines (COTS). Several custom query construction, expansion and translation strategies are compared. We present the experiments and the corresponding results for the CLEF 2004 event.},
  isbn      = {978-3-540-32051-7},
}

@Article{Hedlund2004,
  author   = {Hedlund, Turid and Airio, Eija and Keskustalo, Heikki and Lehtokangas, Raija and Pirkola, Ari and J{\"a}rvelin, Kalervo},
  title    = {Dictionary-Based Cross-Language Information Retrieval: Learning Experiences from CLEF 2000--2002},
  journal  = {Information Retrieval},
  year     = {2004},
  volume   = {7},
  number   = {1},
  pages    = {99--119},
  month    = {Jan},
  issn     = {1573-7659},
  abstract = {In this study the basic framework and performance analysis results are presented for the three year long development process of the dictionary-based UTACLIR system. The tests expand from bilingual CLIR for three language pairs Swedish, Finnish and German to English, to six language pairs, from English to French, German, Spanish, Italian, Dutch and Finnish, and from bilingual to multilingual. In addition, transitive translation tests are reported. The development process of the UTACLIR query translation system will be regarded from the point of view of a learning process. The contribution of the individual components, the effectiveness of compound handling, proper name matching and structuring of queries are analyzed. The results and the fault analysis have been valuable in the development process. Overall the results indicate that the process is robust and can be extended to other languages. The individual effects of the different components are in general positive. However, performance also depends on the topic set and the number of compounds and proper names in the topic, and to some extent on the source and target language. The dictionaries used affect the performance significantly.},
  day      = {01},
  doi      = {10.1023/B:INRT.0000009442.34054.55},
  url      = {https://doi.org/10.1023/B:INRT.0000009442.34054.55},
}

@Article{Sebastiani2004,
  author  = {Sebastiani, Fabrizio},
  title   = {Introduction: Special Issue on the 25th European Conference on Information Retrieval Research},
  journal = {Information Retrieval},
  year    = {2004},
  volume  = {7},
  number  = {3},
  pages   = {235--237},
  month   = {Sep},
  issn    = {1573-7659},
  day     = {01},
  doi     = {10.1023/B:INRT.0000011242.39361.cb},
  url     = {https://doi.org/10.1023/B:INRT.0000011242.39361.cb},
}

@InProceedings{10.1007/3-540-36271-1_16,
  author    = {Dalianis, Hercules},
  title     = {Evaluating a Spelling Support in a Search Engine},
  booktitle = {Natural Language Processing and Information Systems},
  year      = {2002},
  editor    = {Andersson, Birger and Bergholtz, Maria and Johannesson, Paul},
  pages     = {183--190},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The information in a database is usually accessed using SQL or some other query language, but if one uses a free text retrieval system the retrieval of text based information becomes much easier and user friendly, since one can use natural languages techniques such as automatic spell checking and stemming. The free text retrieval system needs first to index the database but then it is just to search the database. Normally a search engine does not give any answers to queries when the search words does not exist in the index, therefore we connected a spell checker module into a search engine and evaluated it. The domain used was the web site of the Swedish National Tax Board (Riksskatteverket, RSV), where the search engine was used between April and Sept 2001. One million queries were made by the public. Of these queries 10 percent were ``misspelled'' or erroneous and our spell checker corrected around 90 percent of these.},
  isbn      = {978-3-540-36271-5},
}

@InProceedings{10.1007/3-540-46154-X_49,
  author    = {Hecht, Robert and Riedler, J{\"u}rgen and Backfried, Gerhard},
  title     = {Fitting German into N-Gram Language Models},
  booktitle = {Text, Speech and Dialogue},
  year      = {2002},
  editor    = {Sojka, Petr and Kope{\v{c}}ek, Ivan and Pala, Karel},
  pages     = {341--346},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We report on a series of experiments addressing the fact that German is less suited than English for word-based n-gram language models. Several systems were trained at different vocabulary sizes using various sets of lexical units. They were evaluated against a newly created corpus of German and Austrian broadcast news.},
  isbn      = {978-3-540-46154-8},
}

@InProceedings{10.1007/3-540-44645-1_14,
  author    = {de Vries, Arjen P.},
  title     = {A Poor Man's Approach to CLEF},
  booktitle = {Cross-Language Information Retrieval and Evaluation},
  year      = {2001},
  editor    = {Peters, Carol},
  pages     = {149--155},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The primary goal of our participation in CLEF is to acquire experience with supporting cross-lingual retrieval. We submitted runs for all four target languages, but our main interest has been in the bilingual Dutch to English runs. We investigated whether we can obtain a reasonable performance without expensive (but high quality) resources; we have used only `off-the-shelf', freely available tools for stopping, stemming, compound-splitting (only for Dutch) and translation. Although our results are encouraging, we must conclude that a poor man's approach should not expect to result in rich men's retrieval results.},
  isbn      = {978-3-540-44645-3},
}

@InProceedings{10.1007/3-540-44645-1_27,
  author    = {Goldsmith, John A. and Higgins, Derrick and Soglasnova, Svetlana},
  title     = {Automatic Language-Specific Stemming in Information Retrieval},
  booktitle = {Cross-Language Information Retrieval and Evaluation},
  year      = {2001},
  editor    = {Peters, Carol},
  pages     = {273--283},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We employ Automorphology, an MDL-based algorithm that determines the suffixes present in a language-sample with no prior knowledge of the language in question, and describe our experiments on the usefulness of this approach for Information Retrieval, employing this stemmer in a SMARTbased IR engine.},
  isbn      = {978-3-540-44645-3},
}

@InProceedings{10.1007/3-540-44645-1_20,
  author    = {Hedlund, Turid and Keskustalo, Heikki and Pirkola, Ari and Sepponen, Mikko and J{\"a}rvelin, Kalervo},
  title     = {Bilingual Tests with Swedish, Finnish, and German Queries: Dealing with Morphology, Compound Words, and Query Structure},
  booktitle = {Cross-Language Information Retrieval and Evaluation},
  year      = {2001},
  editor    = {Peters, Carol},
  pages     = {210--223},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We designed, implemented and evaluated an automated method for query construction for CLIR from Finnish, Swedish and German to English. This method seeks to automatically extract topical information from request sentences written in one of the source languages and to create a target language query, based on translations given by a translation dictionary. We paid particular attention to morphology, compound words and query structure. we tested this approach in the bilingual track of CLEF. All the source languages are compound languages, i.e., languages rich in compound words. A compound word refers to a multi-word expression where the component words are written together. Because source language request words may appear in various inflected forms not included in a translation dictionary, morphological normalization was used to aid dictionary translation. The query resulting from this process may be structured according to the translation alternatives of each source language word or remain as an unstructured word list.},
  isbn      = {978-3-540-44645-3},
}

@InProceedings{10.1007/3-540-44645-1_9,
  author    = {Braschler, Martin},
  title     = {CLEF 2000 --- Overview of Results},
  booktitle = {Cross-Language Information Retrieval and Evaluation},
  year      = {2001},
  editor    = {Peters, Carol},
  pages     = {89--101},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The first CLEF campaign was a big success in attracting increased participation when compared to its predecessor, the TREC8 cross-language track. Both the number of participants and of experiments has grown considerably. This paper presents details of the various subtasks, and attempts to summarize the main results and research directions that were observed. Additionally, the CLEF collection is examined with respect to the completeness of its relevance assessments. The analysis indicates that the CLEF relevance assessments are of comparable quality to those of the well-known and trusted TREC ad-hoc collections.},
  isbn      = {978-3-540-44645-3},
}

@InProceedings{10.1007/3-540-44645-1_17,
  author    = {Oard, Douglas W. and Levow, Gina-Anne and Cabezas, Clara I.},
  title     = {CLEF Experiments at Maryland: Statistical Stemming and Backoff Translation},
  booktitle = {Cross-Language Information Retrieval and Evaluation},
  year      = {2001},
  editor    = {Peters, Carol},
  pages     = {176--187},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The University of Maryland participated in the CLEF 2000 multilingual task, submitting three official runs that explored the impact of applying language-independent stemming techniques to dictionarybased cross-language information retrieval. The paper begins by describing a cross-language information retrieval architecture based on balanced document translation. A four-stage backoff strategy for improving the coverage of dictionary-based translation techniques is then introduced, and an implementation based on automatically trained statistical stemming is presented. Results indicate that competitive performance can be achieved using four-stage backoff translation in conjunction with freely available bilingual dictionaries, but that the the usefulness of the statistical stemming algorithms that were tried varies considerably across the three languages to which they were applied.},
  isbn      = {978-3-540-44645-3},
}

@Article{Alkula2001,
  author   = {Alkula, Riitta},
  title    = {From Plain Character Strings to Meaningful Words: Producing Better Full Text Databases for Inflectional and Compounding Languages with Morphological Analysis Software},
  journal  = {Information Retrieval},
  year     = {2001},
  volume   = {4},
  number   = {3},
  pages    = {195--208},
  month    = {Sep},
  issn     = {1573-7659},
  abstract = {The paper deals with linguistic processing and retrieval techniques in fulltext databases. Special attention is focused on the characteristics of highly inflectional languages, and how morphological structure of a language should be taken into account, when designing and developing information retrieval systems. Finnish is used as an example of a language, which has a more complicated inflectional structure than the English language. In the FULLTEXT project, natural language analysis modules for Finnish were incorporated into the commercial BASIS information retrieval system, which is based on inverted files and Boolean searching. Several test databases were produced, each using one or two Finnish morphological analysis programs.},
  day      = {01},
  doi      = {10.1023/A:1011942104443},
  url      = {https://doi.org/10.1023/A:1011942104443},
}

@InProceedings{10.1007/3-540-44645-1_24,
  author    = {Gollins, Tim and Sanderson, Mark},
  title     = {Sheffield University CLEF 2000 Submission --- Bilingual Track: German to English},
  booktitle = {Cross-Language Information Retrieval and Evaluation},
  year      = {2001},
  editor    = {Peters, Carol},
  pages     = {245--252},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We investigated dictionary based cross language information retrieval using lexical triangulation. Lexical triangulation combines the results of different transitive translations. Transitive translation uses a pivot language to translate between two languages when no direct translation resource is available. We took German queries and translated them via Spanish, or Dutch into English. We compared the results of retrieval experiments using these queries, with other versions created by combining the transitive translations or created by direct translation. Direct dictionary translation of a query introduces considerable ambiguity that damages retrieval, an average precision 79{\%} below monolingual in this research. Transitive translation introduces more ambiguity, giving results worse than 88{\%} below direct translation. We have shown that lexical triangulation between two transitive translations can eliminate much of the additional ambiguity introduced by transitive translation.},
  isbn      = {978-3-540-44645-3},
}

@InProceedings{10.1007/3-540-49478-2_42,
  author    = {Oard, Douglas W.},
  title     = {A Comparative Study of Query and Document Translation for Cross-Language Information Retrieval},
  booktitle = {Machine Translation and the Information Soup},
  year      = {1998},
  editor    = {Farwell, David and Gerber, Laurie and Hovy, Eduard},
  pages     = {472--483},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Cross-language retrieval systems use queries in one natural language to guide retrieval of documents that might be written in another. Acquisition and representation of translation knowledge plays a central role in this process. This paper explores the utility of two sources of translation knowledge for cross-language retrieval. We have implemented six query translation techniques that use bilingual term lists and one based on direct use of the translation output from an existing machine translation system; these are compared with a document translation technique that uses output from the same machine translation system. Average precision measures on a TREC collection suggest that arbitrarily selecting a single dictionary translation is typically no less effective than using every translation in the dictionary, that query translation using a machine translation system can achieve somewhat better effectiveness than simpler techniques, and that document translation may result in further improvements in retrieval effectiveness under some conditions.},
  isbn      = {978-3-540-49478-2},
}

@InBook{Downing1998,
  pages     = {83--120},
  title     = {Prosodic misalignment and reduplication},
  publisher = {Springer Netherlands},
  year      = {1998},
  author    = {Downing, Laura J.},
  editor    = {Booij, Geert and Van Marle, Jaap},
  address   = {Dordrecht},
  isbn      = {978-94-011-4998-3},
  abstract  = {In KiHehe and IsiXhosa, the verb stem is the base for reduplication, and the reduplicant is prefixed to the stem ((la; 2a). As in other languages (McCarthy {\&} Prince 1986, 1993a,b, 1995), the reduplicant (RED) is misaligned with vowel-initial stems: infixed after the initial vowel, in IsiXhosa (lb) and ``exfixed'' before a prefix which is syllabified with the initial vowel in KiHehe ((2b); notice the infinitive prefix ku- /kw-is also reduplicated):},
  booktitle = {Yearbook of Morphology 1997},
  doi       = {10.1007/978-94-011-4998-3_4},
  url       = {https://doi.org/10.1007/978-94-011-4998-3_4},
}

@Comment{jabref-meta: databaseType:bibtex;}
