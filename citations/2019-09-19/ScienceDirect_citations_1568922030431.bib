@article{MA2019120972,
title = "A time-series based aggregation scheme for topic detection in Weibo short texts",
journal = "Physica A: Statistical Mechanics and its Applications",
pages = "120972",
year = "2019",
issn = "0378-4371",
doi = "https://doi.org/10.1016/j.physa.2019.04.208",
url = "http://www.sciencedirect.com/science/article/pii/S037843711930576X",
author = "Tinghuai Ma and Jing Li and Xinnian Liang and Yuan Tian and Abdullah Al-Dhelaan and Mohammed Al-Dhelaan",
keywords = "Topic discovery, Short texts, Aggregation, Time-series",
abstract = "Discovering hot topics within social network like Twitter and Weibo, has received much attention in recent years. While topic models such as Latent Dirichlet Allocation (LDA) have been successfully applied in topic discovery, they are often less coherent when applied to microblog content which is known as “posts”. In this paper, we propose a time-series based aggregation scheme for topic modeling in Weibo. As Weibo topics are coherent within a time slice, we divide Weibo dataset into groups by time slice. With this scheme, posts in every group are aggregated into several longer pseudo-documents using paragraph-vector based similarity algorithms. While applying this scheme to LDA model, we dramatically decrease the topic model perplexity and increase the clustering quality, which also allows for better discovery of underlying topics in Weibo. Furthermore, we can let other topic models extended on LDA be directly used on such short texts."
}
@article{OCHODEK2011885,
title = "Improving the reliability of transaction identification in use cases",
journal = "Information and Software Technology",
volume = "53",
number = "8",
pages = "885 - 897",
year = "2011",
note = "Advances in functional size measurement and effort estimation - Extended best papers",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2011.02.004",
url = "http://www.sciencedirect.com/science/article/pii/S0950584911000401",
author = "M. Ochodek and B. Alchimowicz and J. Jurkiewicz and J. Nawrocki",
keywords = "Use-case transactions, Use Case Points, Functional size measurement, Natural language processing",
abstract = "Context
The concept of transactions is used in Use Case Points (UCP), and in many other functional size measurement methods, to capture the smallest unit of functionality that should be considered while measuring the size of a system. Unfortunately, in the case of the UCP method at least four methods for use-case transaction identification have been proposed so far. The different approaches to transaction identification and difficulties related to the analysis of requirements expressed in natural language can lead to problems in the reliability of functional size measurement.
Objective
The goal of this study was to evaluate reliability of transaction identification in use cases (with the methods mentioned in the literature), analyze their weaknesses, and propose some means for their improvement.
Method
A controlled experiment on a group of 120 students was performed to investigate if the methods for transaction identification, known from the literature, provide similar results. In addition, a qualitative analysis of the experiment data was performed to investigate the potential problems related to transaction identification in use cases. During the experiment a use-case benchmark specification was used. The automatic methods for transaction identification, proposed in the paper have been validated using the same benchmark by comparing the outcomes provided by these methods to on-average number of transactions identified by the participants of the experiment.
Results
A significant difference in the median number of transactions was observed between groups using different methods of transaction identification. The Kruskal–Wallis test was performed with the significance level α set to 0.05 and followed by the post-hoc analysis performed according to the procedure proposed by Conover. Also a large intra-method variability was observed. The ratios between the maximum and minimum number of transactions identified by the participants using the same method were equal to 1.96, 3.83, 2.03, and 2.21. The proposed automatic methods for transaction identification provided results consistent with those provided by the participants of the experiment and functional measurement experts. The relative error between the number of transaction identified by the tool and on-average number of transactions identified by the participants of the experiment ranged from 3% to 7%.
Conclusions
Human-performed transaction identification is error prone and quite subjective. Its reliability can be improved by automating the process with the use of natural language processing techniques."
}
@incollection{BLACHE20171,
title = "1 - Delayed Interpretation, Shallow Processing and Constructions: the Basis of the “Interpret Whenever Possible” Principle",
editor = "Bernadette Sharp and Florence Sèdes and Wiesław Lubaszewski",
booktitle = "Cognitive Approach to Natural Language Processing",
publisher = "Elsevier",
pages = "1 - 19",
year = "2017",
isbn = "978-1-78548-253-3",
doi = "https://doi.org/10.1016/B978-1-78548-253-3.50001-9",
url = "http://www.sciencedirect.com/science/article/pii/B9781785482533500019",
author = "Philippe Blache",
keywords = "Aggregating by cohesion, Chunks, Delayed processing, “” Principle, Segment-and-store, Segmentation operations, Working memory",
abstract = "Abstract:
From different perspectives, natural language processing, linguistics and psycholinguistics shed light on the way humans process language. However, this knowledge remains scattered: classical studies usually focus on language processing subtasks (e.g. lexical access) or modules (e.g. morphology, syntax), without being aggregated into a unified framework. It then remains very difficult to find a general model unifying the different sources of information into a unique architecture."
}
@article{COTELO20154743,
title = "A modular approach for lexical normalization applied to Spanish tweets",
journal = "Expert Systems with Applications",
volume = "42",
number = "10",
pages = "4743 - 4754",
year = "2015",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2015.02.003",
url = "http://www.sciencedirect.com/science/article/pii/S0957417415000962",
author = "J.M. Cotelo and F.L. Cruz and J.A. Troyano and F.J. Ortega",
keywords = "Twitter, Text normalization, Domain adaptation",
abstract = "Twitter is a social media platform with widespread success where millions of people continuously express ideas and opinions about a myriad of topics. It is a huge and interesting source of data but most of these texts are usually written hastily and very abbreviated, rendering them unsuitable for traditional Natural Language Processing (NLP). The two main contributions of this work are: the characterization of the textual error phenomena in Twitter and the proposal of a modular normalization system that improves the textual quality of tweets. Instead of focusing on a single technique, we propose an extensible normalization system that relies on the combination of several independent “expert modules”, each one addressing an very specific error phenomenon in its own way, thus increasing module accuracy and lowering the module building costs. Broadly speaking, the system resembles to an “expert board”: modules independently propose correction candidates for each Out of Vocabulary (OOV) word, rank the candidates and the best one is selected. In order to evaluate our proposal, we perform several experiments using texts from Twitter written in Spanish about a specific topic. The flexibility of defining resources at different language levels (core language, domain, genre) combined with the modular architecture lead to lower costs and a good performance: requiring a minimal effort for building the resources and achieving more than 82% of accuracy compared to the 31% yielded by the baseline."
}
@article{CHEN201740,
title = "Emotion classification of YouTube videos",
journal = "Decision Support Systems",
volume = "101",
pages = "40 - 50",
year = "2017",
issn = "0167-9236",
doi = "https://doi.org/10.1016/j.dss.2017.05.014",
url = "http://www.sciencedirect.com/science/article/pii/S0167923617300994",
author = "Yen-Liang Chen and Chia-Ling Chang and Chin-Sheng Yeh",
keywords = "Data mining, Sentiments analysis, Machine learning, YouTube",
abstract = "Watching online videos is a major leisure activity among Internet users. The largest video website, YouTube, stores billions of videos on its servers. Thus, previous studies have applied automatic video categorization methods to enable users to find videos corresponding to their needs; however, emotion has not been a factor considered in these classification methods. Therefore, this study classified YouTube videos into six emotion categories (i.e., happiness, anger, disgust, fear, sadness, and surprise). Through unsupervised and supervised learning methods, this study first categorized videos according to emotion. An ensemble model was subsequently applied to integrate the classification results of both methods. The experimental results confirm that the proposed method effectively facilitates the classification of YouTube videos into suitable emotion categories."
}
@article{FERNANDES2009349,
title = "The metamorphosis of the statistical segmentation output: Lexicalization during artificial language learning",
journal = "Cognition",
volume = "112",
number = "3",
pages = "349 - 366",
year = "2009",
issn = "0010-0277",
doi = "https://doi.org/10.1016/j.cognition.2009.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S0010027709001085",
author = "Tânia Fernandes and Régine Kolinsky and Paulo Ventura",
keywords = "Speech segmentation, Artificial language learning, Lexicalization",
abstract = "This study combined artificial language learning (ALL) with conventional experimental techniques to test whether statistical speech segmentation outputs are integrated into adult listeners’ mental lexicon. Lexicalization was assessed through inhibitory effects of novel neighbors (created by the parsing process) on auditory lexical decisions to real words. Both immediately after familiarization and post-one week, ALL outputs were lexicalized only when the cues available during familiarization (transitional probabilities and wordlikeness) suggested the same parsing (Experiments 1 and 3). No lexicalization effect occurred with incongruent cues (Experiments 2 and 4). Yet, ALL differed from chance, suggesting a dissociation between item knowledge and lexicalization. Similarly contrasted results were found when frequency of occurrence of the stimuli was equated during familiarization (Experiments 3 and 4). Our findings thus indicate that ALL outputs may be lexicalized as far as the segmentation cues are congruent, and that this process cannot be accounted for by raw frequency."
}
@article{FENG201982,
title = "Anomaly detection in ad-hoc networks based on deep learning model: A plug and play device",
journal = "Ad Hoc Networks",
volume = "84",
pages = "82 - 89",
year = "2019",
issn = "1570-8705",
doi = "https://doi.org/10.1016/j.adhoc.2018.09.014",
url = "http://www.sciencedirect.com/science/article/pii/S1570870518306887",
author = "Fang Feng and Xin Liu and Binbin Yong and Rui Zhou and Qingguo Zhou",
keywords = "Ad hoc network, Security, Deep learning, Anomaly detection",
abstract = "Ad-hoc network is a temporary self-organizing network that needs no fixed infrastructure. So it has been applied extensively in many areas requesting temporary communication such as military field, emergency disaster relief and road traffic. While, due to the feature of self-organization and wireless communication channels, ad-hoc network is more vulnerable to various attacks compared to the traditional network. In this paper, we proposed a plug and play device to detect Denial of Service (DoS) and privacy attacks. This device mainly includes capture tool and deep learning detection model. Capture tool is used to grab packets in ad-hoc networks, deep learning detection model is used for detecting attacks. An alarm will be triggered if the detected result is attack. In this way, we can avoid the detected attack to spreading out in larger scale. The proposed method can be used as the second line of dense to issue the early-warning signal. In the experiment, first, we use Deep neural network (DNN) detection model to detect DoS attacks; next, we use DNN, Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) detection model to detect XSS and SQL attacks. The results show that these detection models can achieve very high Accuracy, Precision, Recall and F1−score. In addition, the time efficiency among the CNN, the LSTM and the DNN is in acceptable range. It proofs that the proposed method can be effectively applied for attack detection. It is important to note that the proposed method can be extended to all other attacks with little modification in ad-hoc networks."
}
@article{GUNTHER2019168,
title = "‘Understanding’ differs between English and German: Capturing systematic language differences of complex words",
journal = "Cortex",
volume = "116",
pages = "168 - 175",
year = "2019",
note = "Structure in words: the present and future of morphological processing in a multidisciplinary perspective",
issn = "0010-9452",
doi = "https://doi.org/10.1016/j.cortex.2018.09.007",
url = "http://www.sciencedirect.com/science/article/pii/S0010945218302946",
author = "Fritz Günther and Eva Smolka and Marco Marelli",
abstract = "In morphological processing, research has repeatedly found different priming effects by English and German native speakers in the overt priming paradigm. In English, priming effects were found for word pairs with a morphological and semantic relation (SUCCESSFUL-success), but not for pairs without a semantic relation (SUCCESSOR-success). By contrast, morphological priming effects in German occurred for pairs both with a semantic relation (AUFSTEHEN-stehen, ‘stand up’-‘stand’) and without (VERSTEHEN-stehen, ‘understand’-‘stand’). These behavioural differences have been taken to indicate differential language processing and memory representations in these languages. We examine whether these behavioural differences can be explained with differences in the language structure between English and German. To this end, we employed new developments in distributional semantics as a computational method to obtain both observed and compositional representations for transparent and opaque complex word meanings, that can in turn be used to quantify the degree of semantic predictability of the morphological system of a language. We compared the similarities between transparent and opaque words and their stems, and observed a difference between German and English, with German showing a higher morphological systematicity. The present results indicate that the investigated cross-linguistic effect can be attributed to quantitatively-characterized differences in the speakers' language experience, as approximated by linguistic corpora."
}
@incollection{SRIHARI2006203,
title = "Handwriting Recognition, Automatic",
editor = "Keith Brown",
booktitle = "Encyclopedia of Language & Linguistics (Second Edition)",
publisher = "Elsevier",
edition = "Second Edition",
address = "Oxford",
pages = "203 - 211",
year = "2006",
isbn = "978-0-08-044854-1",
doi = "https://doi.org/10.1016/B0-08-044854-2/00982-2",
url = "http://www.sciencedirect.com/science/article/pii/B0080448542009822",
author = "S. Srihari",
keywords = "character recognition, dynamic handwriting, forensic document examination, handwriting recognition, linguistic postprocessing, off-line handwriting, on-line handwriting, questioned document analysis, signature verification, static handwriting, word recognition, writer identification",
abstract = "Handwriting is a common means of recording personal information and communication, even with the introduction of new technologies. Machine recognition of handwriting has practical significance, as in reading handwritten notes in a PDA, postal addresses on envelopes, amounts in bank checks, handwritten fields in forms, etc. This overview describes the nature of handwritten language, how it is transduced into electronic data and the basic concepts behind written language recognition algorithms. Both the dynamic or on-line case (which pertains to the availability of trajectory data during writing) and the static or off-line case (which pertains to scanned images) are considered. Algorithms for preprocessing, character and word recognition, and performance with practical systems are indicated. Variations of handwriting recognition such as signature verification and writer identification are also described."
}
@article{EV201540,
title = "A novel approach to generate MCQs from domain ontology: Considering DL semantics and open-world assumption",
journal = "Journal of Web Semantics",
volume = "34",
pages = "40 - 54",
year = "2015",
issn = "1570-8268",
doi = "https://doi.org/10.1016/j.websem.2015.05.005",
url = "http://www.sciencedirect.com/science/article/pii/S1570826815000475",
author = "Vinu E.V. and Sreenivasa Kumar P.",
keywords = "OWL ontologies, Semantic web, Multiple choice questions, Automatic question generation",
abstract = "Ontologies are structures, used for knowledge representation, which model domain knowledge in the form of concepts, roles, instances and their relationships. This knowledge can be exploited by an assessment system in the form of multiple choice questions (MCQs). The existing approaches, which use ontologies expressed in the Web Ontology Language (OWL) for MCQ generation, are limited to simple concept related questions — “What is C?” or “Which of the following is an example of C?” (where C is a concept symbol) — or analogy type questions involving roles. There are no efforts in the literature which make use of the terminological axioms in the ontology such as existential, universal and cardinality restrictions on concepts and roles for MCQ generation. Also, there are no systematic methods for generating incorrect answers (distractors) from ontologies. Distractor generation process has to be given much importance, since the generated distractors determine the quality and hardness of an MCQ. We propose two new MCQ generation approaches, which generate MCQs that are very useful and realistic in conducting assessment tests, and the corresponding distractor generating techniques. Our distractor generation techniques, unlike other methods, consider the open-world assumption, so that the generated MCQs will always be valid (falsity of distractors is ensured). Furthermore, we present a measure to determine the difficulty level (a value between 0 and 1) of the generated MCQs. The proposed system is implemented, and experiments on specific ontologies have shown the effectiveness of the approaches. We also did an empirical study by generating question items from a real-world ontology and validated our results with the help of domain experts."
}
@article{RAGHURAM2014423,
title = "Unsupervised, low latency anomaly detection of algorithmically generated domain names by generative probabilistic modeling",
journal = "Journal of Advanced Research",
volume = "5",
number = "4",
pages = "423 - 433",
year = "2014",
note = "Cyber Security",
issn = "2090-1232",
doi = "https://doi.org/10.1016/j.jare.2014.01.001",
url = "http://www.sciencedirect.com/science/article/pii/S2090123214000022",
author = "Jayaram Raghuram and David J. Miller and George Kesidis",
keywords = "Anomaly detection, Algorithmically generated domain names, Malicious domain names, Domain name modeling, Fast flux",
abstract = "We propose a method for detecting anomalous domain names, with focus on algorithmically generated domain names which are frequently associated with malicious activities such as fast flux service networks, particularly for bot networks (or botnets), malware, and phishing. Our method is based on learning a (null hypothesis) probability model based on a large set of domain names that have been white listed by some reliable authority. Since these names are mostly assigned by humans, they are pronounceable, and tend to have a distribution of characters, words, word lengths, and number of words that are typical of some language (mostly English), and often consist of words drawn from a known lexicon. On the other hand, in the present day scenario, algorithmically generated domain names typically have distributions that are quite different from that of human-created domain names. We propose a fully generative model for the probability distribution of benign (white listed) domain names which can be used in an anomaly detection setting for identifying putative algorithmically generated domain names. Unlike other methods, our approach can make detections without considering any additional (latency producing) information sources, often used to detect fast flux activity. Experiments on a publicly available, large data set of domain names associated with fast flux service networks show encouraging results, relative to several baseline methods, with higher detection rates and low false positive rates."
}
@article{XIE2019178,
title = "An integrated service recommendation approach for service-based system development",
journal = "Expert Systems with Applications",
volume = "123",
pages = "178 - 194",
year = "2019",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2019.01.025",
url = "http://www.sciencedirect.com/science/article/pii/S0957417419300260",
author = "Fang Xie and Jian Wang and Ruibin Xiong and Neng Zhang and Yutao Ma and Keqing He",
keywords = "Service recommendation, Service-based system, Heterogeneous information network, Word embedding, Collaborative filtering",
abstract = "With the wide adoption of service-oriented computing and cloud computing, service-based systems (SBSs), a kind of software systems that can offer certain functionalities by leveraging one or more Web services, become increasingly popular. A challenging issue in SBS development is to find suitable services from a variety of available (semantics different) services. Towards this issue, we propose a new service recommendation approach that can integrate diverse information of SBSs and their component services. In this research, SBSs, services, their respective attributes (e.g. content and categories) and SBS-service composition relations are modeled as a heterogeneous information network (HIN); and several semantic similarities between SBSs are measured on a set of meta-paths in the HIN. Particularly, a word embedding technique is used to learn word vectors from the content of SBSs and services, which contribute to better functional similarities between SBSs. Afterwards, the combinational weights of different similarities are optimized using a Bayesian personalized ranking algorithm. Services are finally recommended based on collaborative filtering. We identify two recommendation scenarios with different SBS requirements. By conducting a series of experiments on a real-world dataset crawled from the ProgrammableWeb, we validate the effectiveness of our approach and find out the optimal combinations of SBS similarities for those two scenarios."
}
@article{ZHAO20161247,
title = "Entity disambiguation to Wikipedia using collective ranking",
journal = "Information Processing & Management",
volume = "52",
number = "6",
pages = "1247 - 1257",
year = "2016",
issn = "0306-4573",
doi = "https://doi.org/10.1016/j.ipm.2016.06.002",
url = "http://www.sciencedirect.com/science/article/pii/S0306457316301893",
author = "Gang Zhao and Ji Wu and Dingding Wang and Tao Li",
keywords = "Named entity disambiguation, Feedback-query-expansion, Re-ranking",
abstract = "Entity disambiguation is a fundamental task of semantic Web annotation. Entity Linking (EL) is an essential procedure in entity disambiguation, which aims to link a mention appearing in a plain text to a structured or semi-structured knowledge base, such as Wikipedia. Existing research on EL usually annotates the mentions in a text one by one and treats entities independent to each other. However this might not be true in many application scenarios. For example, if two mentions appear in one text, they are likely to have certain intrinsic relationships. In this paper, we first propose a novel query expansion method for candidate generation utilizing the information of co-occurrences of mentions. We further propose a re-ranking model which can be iteratively adjusted based on the prediction in the previous round. Experiments on real-world data demonstrate the effectiveness of our proposed methods for entity disambiguation."
}
@article{BENLAHBIB201980,
title = "An Unsupervised Approach for Reputation Generation",
journal = "Procedia Computer Science",
volume = "148",
pages = "80 - 86",
year = "2019",
note = "THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2019.01.011",
url = "http://www.sciencedirect.com/science/article/pii/S1877050919300110",
author = "Abdessamad Benlahbib and El Habib Nfaoui",
keywords = "Semantic analysis, Opinion mining, Reputation generation, Machine learning",
abstract = "Nowadays, watching a movie, buying a product, making hotel reservations and other e-commerce trades are strung to consulting other peoples reviews and recommendations on the target entity. Indeed, Amazon, IMDB (Internet Movie Database) as well as several websites provide a convenient platform where users share freely their opinions and their subjective attitudes towards the target entity with no restrictions. However, those opinions are too much to be examined one by one, this is why a general reputation value makes the task of choosing the right product much easier. In this paper, we propose a reputation generation approach based on opinion clustering and semantic analysis. In our approach, opinions are grouped into a number of clusters that contain opinions with the same attitude or preference. By aggregating the ratings attached to the clusters, we generate the reputation of an entity. Experimental results demonstrate the effectiveness of the proposed approach in generating reputation value."
}
@article{VYDRIN201375,
title = "Bamana Reference Corpus (BRC)",
journal = "Procedia - Social and Behavioral Sciences",
volume = "95",
pages = "75 - 80",
year = "2013",
note = "Corpus Resources for Descriptive and Applied Studies. Current Challenges and Future Directions: Selected Papers from the 5th International Conference on Corpus Linguistics (CILC2013)",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2013.10.624",
url = "http://www.sciencedirect.com/science/article/pii/S187704281304144X",
author = "Valentin Vydrin",
keywords = "bambara, manding, mande, Corpus Bambara de Référence",
abstract = "The Bambara Reference Corpus (Corpus Bambara de Référence) is one of the first corpora for the languages of Africa south of Sahara of more than a million words, and probably the only one freely accessible on the Internet. The entire corpus is tone- marked, POS-tagged and glossed in French. In the paper, tools and resources developed for the Bambara Reference Corpus are surveyed and the process of corpus building is described."
}
@article{MASSACEREDA20181102,
title = "Syntactic analysis of natural language sentences based on rewriting systems and adaptivity",
journal = "Procedia Computer Science",
volume = "130",
pages = "1102 - 1107",
year = "2018",
note = "The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT-2018) / Affiliated Workshops",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2018.04.164",
url = "http://www.sciencedirect.com/science/article/pii/S1877050918305301",
author = "Paulo Roberto Massa Cereda and Newton Kiyotaka Miura and João José Neto",
keywords = "rewriting systems, adaptive technology, natural language processing",
abstract = "The intricate, dependent structures found in natural language pose as a challenge for computational processing. Existing approaches resort to either probabilistic models or case-oriented syntactic mappings, leading to unsatisfactory or excessively convoluted grammatical rules. As a means to reduce complexity and offer an incremental, hierarchical approach to the phenomenon of context sensitivity, this paper presents a rule-based rewriting system using adaptive technology for syntactic analysis of sentences in natural language. We provide a detailed description of a sentence with dependent constructs being decomposed into a syntactic tree through successive reductions as a proof of concept."
}
@article{STOYKOVA2012400,
title = "The inflectional morphology of Bulgarian possessive and reflexive-possessive pronouns in Universal Networking Language",
journal = "Procedia Technology",
volume = "1",
pages = "400 - 406",
year = "2012",
note = "First World Conference on Innovation and Computer Sciences (INSODE 2011)",
issn = "2212-0173",
doi = "https://doi.org/10.1016/j.protcy.2012.02.091",
url = "http://www.sciencedirect.com/science/article/pii/S2212017312000928",
author = "Velislava Stoykova",
keywords = "Artificial Intelligence, Information technology & Languages, Knowledge Engineering, Semantic Networks, UNL",
abstract = "The paper analyses the application of grammar knowledge representation of Bulgarian inflectional morphology. We present the implementation of inflectional morphology of Bulgarian possessive and reflexive-possessive pronouns using the semantic networks interpretation. It is focused on modelling the inflectional morphology of Bulgarian possessive and reflexive-possessive pronouns using the Universal Networking Language (UNL). The formal analysis and related encodings take into account the specific morphological and syntactic properties of possessive and reflexive-possessive pronouns, and represent their inflectional morphology interpretation with respect to the feature of definiteness. The definitions of presented inflectional rules are given after the detailed analysis of both semantic and grammar features of the related pronouns. The evaluation of generated inflected forms is given by some example words. The analysis of basic problems of rule-based inflectional morphology grammar representation of Bulgarian possessive and reflexive-possessive pronouns is offered, and the semantic network using rule-based knowledge representation and non-monotonic reasoning in UNL framework with related encoding is presented. The particular model is explained in terms of its linguistic motivation."
}
@article{KAMPER2017154,
title = "A segmental framework for fully-unsupervised large-vocabulary speech recognition",
journal = "Computer Speech & Language",
volume = "46",
pages = "154 - 174",
year = "2017",
issn = "0885-2308",
doi = "https://doi.org/10.1016/j.csl.2017.04.008",
url = "http://www.sciencedirect.com/science/article/pii/S0885230816301905",
author = "Herman Kamper and Aren Jansen and Sharon Goldwater",
keywords = "Unsupervised speech processing, Representation learning, Segmentation, Clustering, Language acquisition",
abstract = "Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early term discovery systems focused on identifying isolated recurring patterns in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units—effectively performing unsupervised speech recognition. This article presents the first attempt we are aware of to apply such a system to large-vocabulary multi-speaker data. Our system uses a Bayesian modelling framework with segmental word representations: each word segment is represented as a fixed-dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector. We compare our system on English and Xitsonga datasets to state-of-the-art baselines, using a variety of measures including word error rate (obtained by mapping the unsupervised output to ground truth transcriptions). Very high word error rates are reported—in the order of 70–80% for speaker-dependent and 80–95% for speaker-independent systems—highlighting the difficulty of this task. Nevertheless, in terms of cluster quality and word segmentation metrics, we show that by imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, both single-speaker and multi-speaker versions of our system outperform a purely bottom-up single-speaker syllable-based approach. We also show that the discovered clusters can be made less speaker- and gender-specific by using an unsupervised autoencoder-like feature extractor to learn better frame-level features (prior to embedding). Our system’s discovered clusters are still less pure than those of unsupervised term discovery systems, but provide far greater coverage."
}
@article{NGUYEN2019104842,
title = "Learning short-text semantic similarity with word embeddings and external knowledge sources",
journal = "Knowledge-Based Systems",
volume = "182",
pages = "104842",
year = "2019",
issn = "0950-7051",
doi = "https://doi.org/10.1016/j.knosys.2019.07.013",
url = "http://www.sciencedirect.com/science/article/pii/S095070511930317X",
author = "Hien T. Nguyen and Phuc H. Duong and Erik Cambria",
keywords = "Paraphrase identification, Sentence similarity, Short text similarity, Semantic textual similarity",
abstract = "We present a novel method based on interdependent representations of short texts for determining their degree of semantic similarity. The method represents each short text as two dense vectors: the former is built using the word-to-word similarity based on pre-trained word vectors, the latter is built using the word-to-word similarity based on external sources of knowledge. We also developed a preprocessing algorithm that chains coreferential named entities together and performs word segmentation to preserve the meaning of phrasal verbs and idioms. We evaluated the proposed method on three popular datasets, namely Microsoft Research Paraphrase Corpus, STS2015 and P4PIN, and obtained state-of-the-art results on all three without using prior knowledge of natural language, e.g., part-of-speech tags or parse tree, which indicates the interdependent representations of short text pairs are effective and efficient for semantic textual similarity tasks."
}
@article{KEPUSKA2009e2772,
title = "A novel Wake-Up-Word speech recognition system, Wake-Up-Word recognition task, technology and evaluation",
journal = "Nonlinear Analysis: Theory, Methods & Applications",
volume = "71",
number = "12",
pages = "e2772 - e2789",
year = "2009",
issn = "0362-546X",
doi = "https://doi.org/10.1016/j.na.2009.06.089",
url = "http://www.sciencedirect.com/science/article/pii/S0362546X09008220",
author = "V.Z. Këpuska and T.B. Klein",
keywords = "Wake-Up-Word, Speech recognition, Hidden Markov Models, Support Vector Machines, Mel-scale cepstral coefficients, Linear prediction spectrum, Enhanced spectrum, HTK, Microsoft SAPI",
abstract = "Wake-Up-Word (WUW) is a new paradigm in speech recognition (SR) that is not yet widely recognized. This paper defines and investigates WUW speech recognition, describes details of this novel solution and the technology that implements it. WUW SR is defined as detection of a single word or phrase when spoken in the alerting context of requesting attention, while rejecting all other words, phrases, sounds, noises and other acoustic events and the same word or phrase spoken in non-alerting context with virtually 100% accuracy. In order to achieve this accuracy, the following innovations were accomplished: (1) Hidden Markov Model triple scoring with Support Vector Machine classification, (2) Combining multiple speech feature streams: Mel-scale Filtered Cepstral Coefficients (MFCCs), Linear Prediction Coefficients (LPC)-smoothed MFCCs, and Enhanced MFCC, and (3) Improved Voice Activity Detector with Support Vector Machines. WUW detection and recognition performance is 2514%, or 26 times better than HTK for the same training & testing data, and 2271%, or 24 times better than Microsoft SAPI 5.1 recognizer. The out-of-vocabulary rejection performance is over 65,233%, or 653 times better than HTK, and 5900% to 42,900%, or 60 to 430 times better than the Microsoft SAPI 5.1 recognizer. This solution that utilizes a new recognition paradigm applies not only to WUW task but also to any general Speech Recognition tasks."
}
@incollection{SCHULTZ20061,
title = "Chapter 1 - Introduction",
editor = " Schultz and  Kirchhoff",
booktitle = "Multilingual Speech Processing",
publisher = "Academic Press",
address = "Burlington",
pages = "1 - 4",
year = "2006",
isbn = "978-0-12-088501-5",
doi = "https://doi.org/10.1016/B978-012088501-5/50004-4",
url = "http://www.sciencedirect.com/science/article/pii/B9780120885015500044",
author = " Schultz and  Kirchhoff",
abstract = "Publisher Summary
This introductory chapter provides the organization of the book. The book presents the state of the art of multilingual speech processing—that is, the techniques that are required to support spoken input and output in a large variety of languages. Speech processing has a number of different subfields, including speech coding and enhancement, signal processing, speech recognition, speech synthesis, keyword spotting, language identification, and speaker identification. The book provides an overview of multilingual issues in acoustic modeling, language modeling, and dictionary construction for speech recognition, speech synthesis, and automatic language identification. Speech-to-speech translation and automated dialog systems are discussed in detail as example applications. Throughout the book, two main topics are addressed: the challenges for current speech processing models posed by different languages, and the feasibility of sharing data and system components across languages and dialects of related types. In addition to describing modeling approaches, an overview of significant ongoing research programs as well as trends, prognoses, and open research issues are also provided."
}
@article{FERRERO2014470,
title = "Computer-assisted Revision in Spanish Academic Texts: Peer-assessment",
journal = "Procedia - Social and Behavioral Sciences",
volume = "141",
pages = "470 - 483",
year = "2014",
note = "4th World Conference on Learning Teaching and Educational Leadership (WCLTA-2013)",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2014.05.083",
url = "http://www.sciencedirect.com/science/article/pii/S1877042814035071",
author = "Carmen López Ferrero and Irene Renau and Rogelio Nazar and Sergi Torner",
keywords = "Academic discourse, computer-assisted revision, n-gram language models, peer-assessment, written competence evaluation;",
abstract = "This paper presents a series of experiments in automatic correction of spelling and grammar errors with a statistic and corpus- driven methodology. The language of the experiments is Spanish, but the method can be easily extrapolated to other languages since we do not use language-specific resources. Our main motivation is to develop a tool that could assist university students to write academic texts, because this kind of system is practically nonexistent in the present, especially in Spanish. Our work is based on previous descriptions, which identify the most problematic phenomena in academic writing at university level. We aim to develop a tool for automatic detection and correction of some of those problematic issues at different linguistic levels such as spelling, grammar and vocabulary."
}
@article{GENG2020183,
title = "Semantic relation extraction using sequential and tree-structured LSTM with attention",
journal = "Information Sciences",
volume = "509",
pages = "183 - 192",
year = "2020",
issn = "0020-0255",
doi = "https://doi.org/10.1016/j.ins.2019.09.006",
url = "http://www.sciencedirect.com/science/article/pii/S0020025519308515",
author = "ZhiQiang Geng and GuoFei Chen and YongMing Han and Gang Lu and Fang Li",
keywords = "Semantic relation extraction, Dependency syntax, Tree-structured LSTM, Attention",
abstract = "Semantic relation extraction is crucial to automatically constructing a knowledge graph (KG), and it supports a variety of downstream natural language processing (NLP) tasks such as query answering (QA), semantic search and textual entailment. In addition, the semantic relation extraction task is mainly responsible for identifying entity pairs from raw texts and extracting the semantic relations between the extracted entity pairs. Existing methods consider only lexical-level features and often ignore syntactic features, resulting in poor relation extraction performance. By analyzing the necessity of the syntactic dependency and the contributions of words in a sentence to relation extraction, this paper proposes an end-to-end method that uses bidirectional tree-structured long short-term memory (LSTM) to extract structural features based on the dependency tree of a sentence. To enhance the performance of the relation extraction, the bidirectional sequential LSTM with attention is used to identify word-based features including the positional information of entity pairs and the contribution of words. Then, structural features and word-based features are concatenated to optimize the relation extraction performance. Finally, the proposed method is used on the SemEval 2010 task 8 and the CoNLL04 datasets to validate its performance. The experimental results show that the proposed method achieves state-of-the-art results on the SemEval 2010 task 8 and the CoNLL04 datasets."
}
@article{PERRUCHET20141,
title = "New evidence for chunk-based models in word segmentation",
journal = "Acta Psychologica",
volume = "149",
pages = "1 - 8",
year = "2014",
note = "Including Special section articles of Temporal Processing Within and Across Senses - Part-2",
issn = "0001-6918",
doi = "https://doi.org/10.1016/j.actpsy.2014.01.015",
url = "http://www.sciencedirect.com/science/article/pii/S0001691814000262",
author = "Pierre Perruchet and Bénédicte Poulin-Charronnat and Barbara Tillmann and Ronald Peereman",
keywords = "Word segmentation, Chunking, Modeling, Artificial language",
abstract = "There is large evidence that infants are able to exploit statistical cues to discover the words of their language. However, how they proceed to do so is the object of enduring debates. The prevalent position is that words are extracted from the prior computation of statistics, in particular the transitional probabilities between syllables. As an alternative, chunk-based models posit that the sensitivity to statistics results from other processes, whereby many potential chunks are considered as candidate words, then selected as a function of their relevance. These two classes of models have proven to be difficult to dissociate. We propose here a procedure, which leads to contrasted predictions regarding the influence of a first language, L1, on the segmentation of a second language, L2. Simulations run with PARSER (Perruchet & Vinter, 1998), a chunk-based model, predict that when the words of L1 become word-external transitions of L2, learning of L2 should be depleted until reaching below chance level, at least before extensive exposure to L2 reverses the effect. In the same condition, a transitional-probability based model predicts above-chance performance whatever the duration of exposure to L2. PARSER's predictions were confirmed by experimental data: Performance on a two-alternative forced choice test between words and part-words from L2 was significantly below chance even though part-words were less cohesive in terms of transitional probabilities than words."
}
@incollection{GUDIVADA2018xvii,
title = "Preface",
editor = "Venkat N. Gudivada and C.R. Rao",
series = "Handbook of Statistics",
publisher = "Elsevier",
volume = "38",
pages = "xvii - xxi",
year = "2018",
booktitle = "Computational Analysis and Understanding of Natural Languages: Principles, Methods and Applications",
issn = "0169-7161",
doi = "https://doi.org/10.1016/S0169-7161(18)30052-X",
url = "http://www.sciencedirect.com/science/article/pii/S016971611830052X",
author = "Venkat N. Gudivada and C.R. Rao"
}
@article{GILLOUX1993267,
title = "Research into the new generation of character and mailing address recognition systems at the French post office research center",
journal = "Pattern Recognition Letters",
volume = "14",
number = "4",
pages = "267 - 276",
year = "1993",
note = "Postal Processing and Character Recognition",
issn = "0167-8655",
doi = "https://doi.org/10.1016/0167-8655(93)90092-R",
url = "http://www.sciencedirect.com/science/article/pii/016786559390092R",
author = "Michel Gilloux",
keywords = "Address recognition, digit recognition, handwriting recognition",
abstract = "We review applications of character recognition in a postal context and describe its present state-of-the-art. We then give a survey of the research projects conducted at the French post office research center (SRTP) on the recognition of printed and handwritten mailing addresses for small envelopes and flat mail and on the recognition of postal check values."
}
@article{BINNENPOORTE2005433,
title = "Multiword expressions in spoken language: An exploratory study on pronunciation variation",
journal = "Computer Speech & Language",
volume = "19",
number = "4",
pages = "433 - 449",
year = "2005",
note = "Special issue on Multiword Expression",
issn = "0885-2308",
doi = "https://doi.org/10.1016/j.csl.2004.11.003",
url = "http://www.sciencedirect.com/science/article/pii/S0885230805000124",
author = "Diana Binnenpoorte and Catia Cucchiarini and Lou Boves and Helmer Strik",
abstract = "The study presented in this paper was aimed at exploring the possibilities of modelling specific pronunciation characteristics of multiword expressions (MWEs) for both automatic speech recognition (ASR) and automatic phonetic transcription (APT). For this purpose, we first drew up an inventory of frequently found N-grams extracted from orthographic transcriptions of spontaneous speech contained in a large corpus of spoken Dutch. These N-grams were filtered and subsequently assigned to linguistic categories. For a small selection of these N-grams we examined the phonetic transcriptions contained in the corpus. We found that the pronunciation of these N-grams differed to a large extent from the canonical form. In order to determine whether this is a general characteristic of spontaneous speech or rather the effect of the specific status of these N-grams, we analysed the pronunciations of the individual words composing the N-grams in two context conditions: (1) in the N-gram context and (2) in any other context. We found that words in N-grams do indeed have peculiar pronunciation patterns. This seems to suggest that the N-grams investigated may be considered as MWEs that should be treated as lexical entries in the pronunciation lexicons used in ASR and APT, with their own specific pronunciation variants."
}