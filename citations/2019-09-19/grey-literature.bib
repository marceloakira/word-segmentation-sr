% Encoding: UTF-8

@misc{jenks_python_nodate,
	title = {Python {Word} {Segmentation}},
	url = {https://github.com/grantjenks/python-wordsegment},
	abstract = {Based on code from the chapter "Natural Language Corpus Data" by Peter Norvig from the book "Beautiful Data" (Segaran and Hammerbacher, 2009). 
Data files are derived from the Google Web Trillion Word Corpus, as described by Thorsten Brants and Alex Franz, and distributed by the Linguistic Data Consortium. This module contains only a subset of that data. The unigram data includes only the most common 333,000 words. Similarly, bigram data includes only the most common 250,000 phrases. Every word and phrase is lowercased with punctuation removed.},
	urldate = {2019-09-19},
	author = {Jenks, Grant}
}

@misc{baziotis_ekphrasis_nodate,
	title = {Ekphrasis},
	url = {https://github.com/cbaziotis/ekphrasis},
	abstract = {Ekphrasis is a text processing tool, geared towards text from social networks, such as Twitter or Facebook. Ekphrasis performs tokenization, word normalization, word segmentation (for splitting hashtags) and spell correction, using word statistics from 2 big corpora (english Wikipedia, twitter - 330mil english tweets).},
	urldate = {2019-09-19},
	author = {Baziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos}
}

@misc{frcchang_zpar_nodate,
	title = {{ZPar}},
	url = {https://github.com/frcchang/zpar},
	abstract = {ZPar statistical parser. Universal language support (depending on the availability of training data), with language-specific features for Chinese and English. Currently support word segmentation, POS tagging, dependency and phrase-structure parsing.},
	urldate = {2019-09-19},
	author = {frcchang}
}

@misc{lanxiaowei_ik-analyzer_nodate,
	title = {{IK}-{Analyzer}},
	url = {https://github.com/yida-lxw/IK},
	abstract = {The source code of IK-Analyzer,Supported Arabic numerals and Chinese characters, Chinese figures and Chinese characters and Arabic Numbers and English letters of the word segmentation},
	urldate = {2019-09-19},
	author = {Lanxiaowei}
}

@misc{fibla_bilingual_nodate,
	title = {Bilingual {Word} {Segmentation}},
	url = {https://github.com/laiafr/bilingual_wordseg},
	abstract = {This project aims to model balanced bilinguals and monolinguals of 3 different languages, with language switching happening every other sentence, or every 100 sentences.

We test the performance of different algorithms on word segmentation that represent different coginitve strategies that infants could be brining into the word segmentation task.

This repository contains the collected corpora from English, Catalan and Spanish which are also freely available in CHILDES https://childes.talkbank.org As well as all the steps to process the data wich are descrived in the recipes of each language.},
	urldate = {2019-09-19},
	author = {Fibla, Laia}
}

@misc{carvalho_shenmeci_nodate,
	title = {shenmeci},
	url = {https://github.com/rhcarvalho/shenmeci},
	abstract = {Chinese word segmentation and Chinese-English online dictionary},
	urldate = {2019-09-19},
	author = {Carvalho, Rodolfo}
}

@misc{li_uninlp-phd_nodate,
	title = {uninlp-phd},
	url = {https://github.com/xxli/uninlp-phd},
	abstract = {Java codes for basic natural language processing tasks, including Pinyin-to-Character Conversion, Chinese word segmentation, Part-of-Speech tagging, English chunking, dependency parsing},
	urldate = {2019-09-19},
	author = {Li, Xinxin}
}

@misc{vittal_text-reconstruction_nodate,
	title = {Text-{Reconstruction}},
	url = {https://github.com/sujay-vittal/Text-Reconstruction},
	abstract = {This project involves two tasks - word segmentation and vowel insertion. Word segmentation often comes up when processing many non-English languages, in which words might not be flanked by spaces on either end, such as written Chinese or long compound German words. Vowel insertion is relevant for languages like Arabic or Hebrew, where modern script eschews notations for vowel sounds and the human reader infers them from context. The goal of Vowel Insertion is to insert vowels back into segmented words in a way that maximizes sentence fluency (i.e., minimizes sentence cost). A bigram cost function is used.},
	urldate = {2019-09-19},
	author = {Vittal, Sujay}
}

@misc{garbe_fast_nodate,
	title = {Fast {Word} {Segmentation} of {Noisy} {Text}},
	url = {https://towardsdatascience.com/fast-word-segmentation-for-noisy-text-2c2c41f9e8da},
	abstract = {A string can be divided in several ways. Each distinct segmentation variant is called a composition. This article evaluates different types of word segmentation and propose several algorithms for each one of the basics concepts},
	language = {Inglês},
	urldate = {2019-09-19},
	author = {Garbe, Wolf}
}

@techreport{norvig_statistical_nodate,
	type = {Relaçtório {Técnico}},
	title = {Statistical {Natural} {Language} {Processing} in {Python}. or {How} {To} {Do} {Things} {With} {Words}. {And} {Counters}. or {Everything} {I} {Needed} to {Know} {About} {NLP} {I} learned {From} {Sesame} {Street}. {Except} {Kneser}-{Ney} {Smoothing}. {The} {Count} {Didn}'t {Cover} {That}.},
	url = {https://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb},
	abstract = {In this notebook Peter Norvig show us how to work with statistical natural language processing. With a theorical/pratical approach, Norvig shows the fundamentals about word segmentation using python.},
	language = {Inglês},
	urldate = {2019-09-19},
	author = {Norvig, Peter}
}
@Comment{jabref-meta: databaseType:bibtex;}
