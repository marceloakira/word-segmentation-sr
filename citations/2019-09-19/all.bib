% Encoding: UTF-8

@InProceedings{Yao:2018:SWF:3297156.3297232,
  author    = {Yao, Rui and Cao, Yang and Ding, Zhiming and Guo, Limin},
  title     = {A Sensitive Words Filtering Model Based on Web Text Features},
  booktitle = {Proceedings of the 2018 2Nd International Conference on Computer Science and Artificial Intelligence},
  year      = {2018},
  series    = {CSAI '18},
  pages     = {516--520},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3297232},
  doi       = {10.1145/3297156.3297232},
  isbn      = {978-1-4503-6606-9},
  keywords  = {False advertisements, feature extraction, machine learning, sensitive word discrimination, text classification},
  location  = {Shenzhen, China},
  numpages  = {5},
  url       = {http://doi.acm.org/10.1145/3297156.3297232},
}

@InProceedings{Adegbola:2016:PUI:2872518.2890563,
  author    = {Adegbola, Tunde},
  title     = {Pattern-based Unsupervised Induction Of Yor\`{u}B\'{a} Morphology},
  booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
  year      = {2016},
  series    = {WWW '16 Companion},
  pages     = {599--604},
  address   = {Republic and Canton of Geneva, Switzerland},
  publisher = {International World Wide Web Conferences Steering Committee},
  acmid     = {2890563},
  doi       = {10.1145/2872518.2890563},
  isbn      = {978-1-4503-4144-8},
  keywords  = {frequent patterns, frequent segments, machine learning of morphology, morphological segmentation, probabilistic models, word labels},
  location  = {Montr\&\#233;al, Qu\&\#233;bec, Canada},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2872518.2890563},
}

@Article{Sun:2014:FAO:2687969.2687973,
  author     = {Sun, Xu and Li, Wenjie and Wang, Houfeng and Lu, Qin},
  title      = {Feature-frequency: Adaptive On-line Training for Fast and Accurate Natural Language Processing},
  journal    = {Comput. Linguist.},
  year       = {2014},
  volume     = {40},
  number     = {3},
  pages      = {563--586},
  month      = sep,
  issn       = {0891-2017},
  acmid      = {2687973},
  address    = {Cambridge, MA, USA},
  doi        = {10.1162/COLI_a_00193},
  issue_date = {September 2014},
  numpages   = {24},
  publisher  = {MIT Press},
  url        = {http://dx.doi.org/10.1162/COLI_a_00193},
}

@Article{Goldberg:2013:WSU:2464100.2464107,
  author     = {Goldberg, Yoav and Elhadad, Michael},
  title      = {Word Segmentation, Unknown-word Resolution, and Morphological Agreement in a Hebrew Parsing System},
  journal    = {Comput. Linguist.},
  year       = {2013},
  volume     = {39},
  number     = {1},
  pages      = {121--160},
  month      = mar,
  issn       = {0891-2017},
  acmid      = {2464107},
  address    = {Cambridge, MA, USA},
  doi        = {10.1162/COLI_a_00137},
  issue_date = {March 2013},
  numpages   = {40},
  publisher  = {MIT Press},
  url        = {http://dx.doi.org/10.1162/COLI_a_00137},
}

@InProceedings{Srinivasan:2012:SWH:2396761.2398410,
  author    = {Srinivasan, Sriram and Bhattacharya, Sourangshu and Chakraborty, Rudrasis},
  title     = {Segmenting Web-domains and Hashtags Using Length Specific Models},
  booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
  year      = {2012},
  series    = {CIKM '12},
  pages     = {1113--1122},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2398410},
  doi       = {10.1145/2396761.2398410},
  isbn      = {978-1-4503-1156-4},
  keywords  = {compound splitting, hashtag segmentation, structured learning, web domain segmentation, word segmentation},
  location  = {Maui, Hawaii, USA},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/2396761.2398410},
}

@InProceedings{Hewlett:2011:FUW:2002736.2002843,
  author    = {Hewlett, Daniel and Cohen, Paul},
  title     = {Fully Unsupervised Word Segmentation with BVE and MDL},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2},
  year      = {2011},
  series    = {HLT '11},
  pages     = {540--545},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {2002843},
  isbn      = {978-1-932432-88-6},
  location  = {Portland, Oregon},
  numpages  = {6},
  url       = {http://dl.acm.org/citation.cfm?id=2002736.2002843},
}

@Article{Zhang:2011:SPU:1970420.1970425,
  author     = {Zhang, Yue and Clark, Stephen},
  title      = {Syntactic Processing Using the Generalized Perceptron and Beam Search},
  journal    = {Comput. Linguist.},
  year       = {2011},
  volume     = {37},
  number     = {1},
  pages      = {105--151},
  month      = mar,
  issn       = {0891-2017},
  acmid      = {1970425},
  address    = {Cambridge, MA, USA},
  doi        = {10.1162/coli_a_00037},
  issue_date = {March 2011},
  numpages   = {47},
  publisher  = {MIT Press},
  url        = {http://dx.doi.org/10.1162/coli_a_00037},
}

@InProceedings{Goldwater:2011:UNH:2140458.2140459,
  author    = {Goldwater, Sharon},
  title     = {Unsupervised NLP and Human Language Acquisition: Making Connections to Make Progress},
  booktitle = {Proceedings of the First Workshop on Unsupervised Learning in NLP},
  year      = {2011},
  series    = {EMNLP '11},
  pages     = {1--1},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {2140459},
  isbn      = {978-1-937284-13-8},
  location  = {Edinburgh, Scotland},
  numpages  = {1},
  url       = {http://dl.acm.org/citation.cfm?id=2140458.2140459},
}

@InProceedings{Wang:2011:WSN:1963405.1963457,
  author    = {Wang, Kuansan and Thrasher, Christopher and Hsu, Bo-June Paul},
  title     = {Web Scale NLP: A Case Study on Url Word Breaking},
  booktitle = {Proceedings of the 20th International Conference on World Wide Web},
  year      = {2011},
  series    = {WWW '11},
  pages     = {357--366},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1963457},
  doi       = {10.1145/1963405.1963457},
  isbn      = {978-1-4503-0632-4},
  keywords  = {compound splitting, multi-style language model, url segmentation, web scale word breaking, word segmentation},
  location  = {Hyderabad, India},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/1963405.1963457},
}

@InProceedings{Hewlett:2011:WSG:2018936.2018941,
  author    = {Hewlett, Daniel and Cohen, Paul},
  title     = {Word Segmentation As General Chunking},
  booktitle = {Proceedings of the Fifteenth Conference on Computational Natural Language Learning},
  year      = {2011},
  series    = {CoNLL '11},
  pages     = {39--47},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {2018941},
  isbn      = {978-1-932432-92-3},
  location  = {Portland, Oregon},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=2018936.2018941},
}

@InProceedings{Paul:2010:IMB:1868850.1868910,
  author    = {Paul, Michael and Finch, Andrew and Sumita, Eiichiro},
  title     = {Integration of Multiple Bilingually-learned Segmentation Schemes into Statistical Machine Translation},
  booktitle = {Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR},
  year      = {2010},
  series    = {WMT '10},
  pages     = {400--408},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1868910},
  isbn      = {978-1-932432-71-8},
  location  = {Uppsala, Sweden},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=1868850.1868910},
}

@InProceedings{Onishi:2010:PLS:1858842.1858843,
  author    = {Onishi, Takashi and Utiyama, Masao and Sumita, Eiichiro},
  title     = {Paraphrase Lattice for Statistical Machine Translation},
  booktitle = {Proceedings of the ACL 2010 Conference Short Papers},
  year      = {2010},
  series    = {ACLShort '10},
  pages     = {1--5},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1858843},
  location  = {Uppsala, Sweden},
  numpages  = {5},
  url       = {http://dl.acm.org/citation.cfm?id=1858842.1858843},
}

@InProceedings{Cho:2009:NWS:1667583.1667594,
  author    = {Cho, Han-Cheol and Lee, Do-Gil and Lee, Jung-Tae and Stenetorp, Pontus and Tsujii, Jun'ichi and Rim, Hae-Chang},
  title     = {A Novel Word Segmentation Approach for Written Languages with Word Boundary Markers},
  booktitle = {Proceedings of the ACL-IJCNLP 2009 Conference Short Papers},
  year      = {2009},
  series    = {ACLShort '09},
  pages     = {29--32},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1667594},
  location  = {Suntec, Singapore},
  numpages  = {4},
  url       = {http://dl.acm.org/citation.cfm?id=1667583.1667594},
}

@InProceedings{Ma:2009:BMD:1609067.1609128,
  author    = {Ma, Yanjun and Way, Andy},
  title     = {Bilingually Motivated Domain-adapted Word Segmentation for Statistical Machine Translation},
  booktitle = {Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics},
  year      = {2009},
  series    = {EACL '09},
  pages     = {549--557},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1609128},
  location  = {Athens, Greece},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=1609067.1609128},
}

@InProceedings{Johnson:2009:INB:1620754.1620800,
  author    = {Johnson, Mark and Goldwater, Sharon},
  title     = {Improving Nonparameteric Bayesian Inference: Experiments on Unsupervised Word Segmentation with Adaptor Grammars},
  booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  year      = {2009},
  series    = {NAACL '09},
  pages     = {317--325},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1620800},
  isbn      = {978-1-932432-41-1},
  location  = {Boulder, Colorado},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=1620754.1620800},
}

@InProceedings{Paul:2009:LIW:1667780.1667788,
  author    = {Paul, Michael and Finch, Andrew and Sumita, Eiichiro},
  title     = {Language Independent Word Segmentation for Statistical Machine Translation},
  booktitle = {Proceedings of the 3rd International Universal Communication Symposium},
  year      = {2009},
  series    = {IUCS '09},
  pages     = {36--40},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1667788},
  doi       = {10.1145/1667780.1667788},
  isbn      = {978-1-60558-641-0},
  location  = {Tokyo, Japan},
  numpages  = {5},
  url       = {http://doi.acm.org/10.1145/1667780.1667788},
}

@InProceedings{Liang:2009:OEU:1620754.1620843,
  author    = {Liang, Percy and Klein, Dan},
  title     = {Online EM for Unsupervised Models},
  booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  year      = {2009},
  series    = {NAACL '09},
  pages     = {611--619},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1620843},
  isbn      = {978-1-932432-41-1},
  location  = {Boulder, Colorado},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=1620754.1620843},
}

@InProceedings{Ng:2009:SDW:1687878.3252572,
  title     = {Session Details: Word Segmentation and POS Tagging},
  booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1},
  year      = {2009},
  series    = {ACL '09},
  pages     = {--},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  note      = {Session Chair-Ng, Hwee Tou},
  acmid     = {3252572},
  isbn      = {978-1-932432-45-9},
  location  = {Suntec, Singapore},
  url       = {http://dl.acm.org/citation.cfm?id=1687878.3252572},
}

@InProceedings{Parlak:2009:SIR:1571941.1572126,
  author    = {Parlak, Siddika and Saraclar, Murat},
  title     = {Spoken Information Retrieval for Turkish Broadcast News},
  booktitle = {Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year      = {2009},
  series    = {SIGIR '09},
  pages     = {782--783},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1572126},
  doi       = {10.1145/1571941.1572126},
  isbn      = {978-1-60558-483-6},
  keywords  = {speech retrieval, spoken document retrieval, spoken term detection, subword indexing},
  location  = {Boston, MA, USA},
  numpages  = {2},
  url       = {http://doi.acm.org/10.1145/1571941.1572126},
}

@InProceedings{Dyer:2009:UMS:1626431.1626461,
  author    = {Dyer, Chris and Setiawan, Hendra and Marton, Yuval and Resnik, Philip},
  title     = {The University of Maryland Statistical Machine Translation System for the Fourth Workshop on Machine Translation},
  booktitle = {Proceedings of the Fourth Workshop on Statistical Machine Translation},
  year      = {2009},
  series    = {StatMT '09},
  pages     = {145--149},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1626461},
  location  = {Athens, Greece},
  numpages  = {5},
  url       = {http://dl.acm.org/citation.cfm?id=1626431.1626461},
}

@InProceedings{Pandey:2008:UHS:1390749.1390765,
  author    = {Pandey, Amaresh Kumar and Siddiqui, Tanveer J},
  title     = {An Unsupervised Hindi Stemmer with Heuristic Improvements},
  booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
  year      = {2008},
  series    = {AND '08},
  pages     = {99--105},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1390765},
  doi       = {10.1145/1390749.1390765},
  isbn      = {978-1-60558-196-5},
  keywords  = {Hindi, heuristics and optimal word segment, unsupervised morphological analyzer},
  location  = {Singapore},
  numpages  = {7},
  url       = {http://doi.acm.org/10.1145/1390749.1390765},
}

@InProceedings{Johnson:2008:UWS:1626324.1626328,
  author    = {Johnson, Mark},
  title     = {Unsupervised Word Segmentation for Sesotho Using Adaptor Grammars},
  booktitle = {Proceedings of the Tenth Meeting of ACL Special Interest Group on Computational Morphology and Phonology},
  year      = {2008},
  series    = {SigMorPhon '08},
  pages     = {20--27},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1626328},
  isbn      = {978-1-932432-12-1},
  location  = {Columbus, Ohio},
  numpages  = {8},
  url       = {http://dl.acm.org/citation.cfm?id=1626324.1626328},
}

@Article{Creutz:2007:UMM:1187415.1187418,
  author     = {Creutz, Mathias and Lagus, Krista},
  title      = {Unsupervised Models for Morpheme Segmentation and Morphology Learning},
  journal    = {ACM Trans. Speech Lang. Process.},
  year       = {2007},
  volume     = {4},
  number     = {1},
  pages      = {3:1--3:34},
  month      = feb,
  issn       = {1550-4875},
  acmid      = {1187418},
  address    = {New York, NY, USA},
  articleno  = {3},
  doi        = {10.1145/1187415.1187418},
  issue_date = {January 2007},
  keywords   = {Efficient storage, highly inflecting and compounding languages, language independent methods, maximum a posteriori (MAP) estimation, morpheme lexicon and segmentation, unsupervised learning},
  numpages   = {34},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1187415.1187418},
}

@InProceedings{Kan:2004:WPC:1013367.1013426,
  author    = {Kan, Min-Yen},
  title     = {Web Page Classification Without the Web Page},
  booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers \&Amp; Posters},
  year      = {2004},
  series    = {WWW Alt. '04},
  pages     = {262--263},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1013426},
  doi       = {10.1145/1013367.1013426},
  isbn      = {1-58113-912-8},
  keywords  = {abbreviation expansion, text categorization, uniform resource locator, word segmentation},
  location  = {New York, NY, USA},
  numpages  = {2},
  url       = {http://doi.acm.org/10.1145/1013367.1013426},
}

@InProceedings{Smith:2002:WCR:1118693.1118706,
  author    = {Smith, Noah A.},
  title     = {From Words to Corpora: Recognizing Translation},
  booktitle = {Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10},
  year      = {2002},
  series    = {EMNLP '02},
  pages     = {95--102},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1118706},
  doi       = {10.3115/1118693.1118706},
  numpages  = {8},
  url       = {https://doi.org/10.3115/1118693.1118706},
}

@InProceedings{Tashiro:1994:RTC:991886.991986,
  author    = {Tashiro, Toshihisa and Uratani, Noriyoshi and Morimoto, Tsuyoshi},
  title     = {Restructuring Tagged Corpora with Morpheme Adjustment Rules},
  booktitle = {Proceedings of the 15th Conference on Computational Linguistics - Volume 1},
  year      = {1994},
  series    = {COLING '94},
  pages     = {569--573},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {991986},
  doi       = {10.3115/991886.991986},
  location  = {Kyoto, Japan},
  numpages  = {5},
  url       = {https://doi.org/10.3115/991886.991986},
}

@Article{GENG2020183,
  author   = {ZhiQiang Geng and GuoFei Chen and YongMing Han and Gang Lu and Fang Li},
  title    = {Semantic relation extraction using sequential and tree-structured LSTM with attention},
  journal  = {Information Sciences},
  year     = {2020},
  volume   = {509},
  pages    = {183 - 192},
  issn     = {0020-0255},
  abstract = {Semantic relation extraction is crucial to automatically constructing a knowledge graph (KG), and it supports a variety of downstream natural language processing (NLP) tasks such as query answering (QA), semantic search and textual entailment. In addition, the semantic relation extraction task is mainly responsible for identifying entity pairs from raw texts and extracting the semantic relations between the extracted entity pairs. Existing methods consider only lexical-level features and often ignore syntactic features, resulting in poor relation extraction performance. By analyzing the necessity of the syntactic dependency and the contributions of words in a sentence to relation extraction, this paper proposes an end-to-end method that uses bidirectional tree-structured long short-term memory (LSTM) to extract structural features based on the dependency tree of a sentence. To enhance the performance of the relation extraction, the bidirectional sequential LSTM with attention is used to identify word-based features including the positional information of entity pairs and the contribution of words. Then, structural features and word-based features are concatenated to optimize the relation extraction performance. Finally, the proposed method is used on the SemEval 2010 task 8 and the CoNLL04 datasets to validate its performance. The experimental results show that the proposed method achieves state-of-the-art results on the SemEval 2010 task 8 and the CoNLL04 datasets.},
  doi      = {https://doi.org/10.1016/j.ins.2019.09.006},
  keywords = {Semantic relation extraction, Dependency syntax, Tree-structured LSTM, Attention},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025519308515},
}

@Article{MA2019120972,
  author   = {Tinghuai Ma and Jing Li and Xinnian Liang and Yuan Tian and Abdullah Al-Dhelaan and Mohammed Al-Dhelaan},
  title    = {A time-series based aggregation scheme for topic detection in Weibo short texts},
  journal  = {Physica A: Statistical Mechanics and its Applications},
  year     = {2019},
  pages    = {120972},
  issn     = {0378-4371},
  abstract = {Discovering hot topics within social network like Twitter and Weibo, has received much attention in recent years. While topic models such as Latent Dirichlet Allocation (LDA) have been successfully applied in topic discovery, they are often less coherent when applied to microblog content which is known as “posts”. In this paper, we propose a time-series based aggregation scheme for topic modeling in Weibo. As Weibo topics are coherent within a time slice, we divide Weibo dataset into groups by time slice. With this scheme, posts in every group are aggregated into several longer pseudo-documents using paragraph-vector based similarity algorithms. While applying this scheme to LDA model, we dramatically decrease the topic model perplexity and increase the clustering quality, which also allows for better discovery of underlying topics in Weibo. Furthermore, we can let other topic models extended on LDA be directly used on such short texts.},
  doi      = {https://doi.org/10.1016/j.physa.2019.04.208},
  keywords = {Topic discovery, Short texts, Aggregation, Time-series},
  url      = {http://www.sciencedirect.com/science/article/pii/S037843711930576X},
}

@Article{XIE2019178,
  author   = {Fang Xie and Jian Wang and Ruibin Xiong and Neng Zhang and Yutao Ma and Keqing He},
  title    = {An integrated service recommendation approach for service-based system development},
  journal  = {Expert Systems with Applications},
  year     = {2019},
  volume   = {123},
  pages    = {178 - 194},
  issn     = {0957-4174},
  abstract = {With the wide adoption of service-oriented computing and cloud computing, service-based systems (SBSs), a kind of software systems that can offer certain functionalities by leveraging one or more Web services, become increasingly popular. A challenging issue in SBS development is to find suitable services from a variety of available (semantics different) services. Towards this issue, we propose a new service recommendation approach that can integrate diverse information of SBSs and their component services. In this research, SBSs, services, their respective attributes (e.g. content and categories) and SBS-service composition relations are modeled as a heterogeneous information network (HIN); and several semantic similarities between SBSs are measured on a set of meta-paths in the HIN. Particularly, a word embedding technique is used to learn word vectors from the content of SBSs and services, which contribute to better functional similarities between SBSs. Afterwards, the combinational weights of different similarities are optimized using a Bayesian personalized ranking algorithm. Services are finally recommended based on collaborative filtering. We identify two recommendation scenarios with different SBS requirements. By conducting a series of experiments on a real-world dataset crawled from the ProgrammableWeb, we validate the effectiveness of our approach and find out the optimal combinations of SBS similarities for those two scenarios.},
  doi      = {https://doi.org/10.1016/j.eswa.2019.01.025},
  keywords = {Service recommendation, Service-based system, Heterogeneous information network, Word embedding, Collaborative filtering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417419300260},
}

@Article{BENLAHBIB201980,
  author   = {Abdessamad Benlahbib and El Habib Nfaoui},
  title    = {An Unsupervised Approach for Reputation Generation},
  journal  = {Procedia Computer Science},
  year     = {2019},
  volume   = {148},
  pages    = {80 - 86},
  issn     = {1877-0509},
  note     = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
  abstract = {Nowadays, watching a movie, buying a product, making hotel reservations and other e-commerce trades are strung to consulting other peoples reviews and recommendations on the target entity. Indeed, Amazon, IMDB (Internet Movie Database) as well as several websites provide a convenient platform where users share freely their opinions and their subjective attitudes towards the target entity with no restrictions. However, those opinions are too much to be examined one by one, this is why a general reputation value makes the task of choosing the right product much easier. In this paper, we propose a reputation generation approach based on opinion clustering and semantic analysis. In our approach, opinions are grouped into a number of clusters that contain opinions with the same attitude or preference. By aggregating the ratings attached to the clusters, we generate the reputation of an entity. Experimental results demonstrate the effectiveness of the proposed approach in generating reputation value.},
  doi      = {https://doi.org/10.1016/j.procs.2019.01.011},
  keywords = {Semantic analysis, Opinion mining, Reputation generation, Machine learning},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050919300110},
}

@Article{FENG201982,
  author   = {Fang Feng and Xin Liu and Binbin Yong and Rui Zhou and Qingguo Zhou},
  title    = {Anomaly detection in ad-hoc networks based on deep learning model: A plug and play device},
  journal  = {Ad Hoc Networks},
  year     = {2019},
  volume   = {84},
  pages    = {82 - 89},
  issn     = {1570-8705},
  abstract = {Ad-hoc network is a temporary self-organizing network that needs no fixed infrastructure. So it has been applied extensively in many areas requesting temporary communication such as military field, emergency disaster relief and road traffic. While, due to the feature of self-organization and wireless communication channels, ad-hoc network is more vulnerable to various attacks compared to the traditional network. In this paper, we proposed a plug and play device to detect Denial of Service (DoS) and privacy attacks. This device mainly includes capture tool and deep learning detection model. Capture tool is used to grab packets in ad-hoc networks, deep learning detection model is used for detecting attacks. An alarm will be triggered if the detected result is attack. In this way, we can avoid the detected attack to spreading out in larger scale. The proposed method can be used as the second line of dense to issue the early-warning signal. In the experiment, first, we use Deep neural network (DNN) detection model to detect DoS attacks; next, we use DNN, Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) detection model to detect XSS and SQL attacks. The results show that these detection models can achieve very high Accuracy, Precision, Recall and F1−score. In addition, the time efficiency among the CNN, the LSTM and the DNN is in acceptable range. It proofs that the proposed method can be effectively applied for attack detection. It is important to note that the proposed method can be extended to all other attacks with little modification in ad-hoc networks.},
  doi      = {https://doi.org/10.1016/j.adhoc.2018.09.014},
  keywords = {Ad hoc network, Security, Deep learning, Anomaly detection},
  url      = {http://www.sciencedirect.com/science/article/pii/S1570870518306887},
}

@Article{NGUYEN2019104842,
  author   = {Hien T. Nguyen and Phuc H. Duong and Erik Cambria},
  title    = {Learning short-text semantic similarity with word embeddings and external knowledge sources},
  journal  = {Knowledge-Based Systems},
  year     = {2019},
  volume   = {182},
  pages    = {104842},
  issn     = {0950-7051},
  abstract = {We present a novel method based on interdependent representations of short texts for determining their degree of semantic similarity. The method represents each short text as two dense vectors: the former is built using the word-to-word similarity based on pre-trained word vectors, the latter is built using the word-to-word similarity based on external sources of knowledge. We also developed a preprocessing algorithm that chains coreferential named entities together and performs word segmentation to preserve the meaning of phrasal verbs and idioms. We evaluated the proposed method on three popular datasets, namely Microsoft Research Paraphrase Corpus, STS2015 and P4PIN, and obtained state-of-the-art results on all three without using prior knowledge of natural language, e.g., part-of-speech tags or parse tree, which indicates the interdependent representations of short text pairs are effective and efficient for semantic textual similarity tasks.},
  doi      = {https://doi.org/10.1016/j.knosys.2019.07.013},
  keywords = {Paraphrase identification, Sentence similarity, Short text similarity, Semantic textual similarity},
  url      = {http://www.sciencedirect.com/science/article/pii/S095070511930317X},
}

@Article{GUNTHER2019168,
  author   = {Fritz Günther and Eva Smolka and Marco Marelli},
  title    = {‘Understanding’ differs between English and German: Capturing systematic language differences of complex words},
  journal  = {Cortex},
  year     = {2019},
  volume   = {116},
  pages    = {168 - 175},
  issn     = {0010-9452},
  note     = {Structure in words: the present and future of morphological processing in a multidisciplinary perspective},
  abstract = {In morphological processing, research has repeatedly found different priming effects by English and German native speakers in the overt priming paradigm. In English, priming effects were found for word pairs with a morphological and semantic relation (SUCCESSFUL-success), but not for pairs without a semantic relation (SUCCESSOR-success). By contrast, morphological priming effects in German occurred for pairs both with a semantic relation (AUFSTEHEN-stehen, ‘stand up’-‘stand’) and without (VERSTEHEN-stehen, ‘understand’-‘stand’). These behavioural differences have been taken to indicate differential language processing and memory representations in these languages. We examine whether these behavioural differences can be explained with differences in the language structure between English and German. To this end, we employed new developments in distributional semantics as a computational method to obtain both observed and compositional representations for transparent and opaque complex word meanings, that can in turn be used to quantify the degree of semantic predictability of the morphological system of a language. We compared the similarities between transparent and opaque words and their stems, and observed a difference between German and English, with German showing a higher morphological systematicity. The present results indicate that the investigated cross-linguistic effect can be attributed to quantitatively-characterized differences in the speakers' language experience, as approximated by linguistic corpora.},
  doi      = {https://doi.org/10.1016/j.cortex.2018.09.007},
  url      = {http://www.sciencedirect.com/science/article/pii/S0010945218302946},
}

@InCollection{GUDIVADA2018xvii,
  author    = {Venkat N. Gudivada and C.R. Rao},
  title     = {Preface},
  booktitle = {Computational Analysis and Understanding of Natural Languages: Principles, Methods and Applications},
  publisher = {Elsevier},
  year      = {2018},
  editor    = {Venkat N. Gudivada and C.R. Rao},
  volume    = {38},
  series    = {Handbook of Statistics},
  pages     = {xvii - xxi},
  doi       = {https://doi.org/10.1016/S0169-7161(18)30052-X},
  issn      = {0169-7161},
  url       = {http://www.sciencedirect.com/science/article/pii/S016971611830052X},
}

@Article{MASSACEREDA20181102,
  author   = {Paulo Roberto Massa Cereda and Newton Kiyotaka Miura and João José Neto},
  title    = {Syntactic analysis of natural language sentences based on rewriting systems and adaptivity},
  journal  = {Procedia Computer Science},
  year     = {2018},
  volume   = {130},
  pages    = {1102 - 1107},
  issn     = {1877-0509},
  note     = {The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT-2018) / Affiliated Workshops},
  abstract = {The intricate, dependent structures found in natural language pose as a challenge for computational processing. Existing approaches resort to either probabilistic models or case-oriented syntactic mappings, leading to unsatisfactory or excessively convoluted grammatical rules. As a means to reduce complexity and offer an incremental, hierarchical approach to the phenomenon of context sensitivity, this paper presents a rule-based rewriting system using adaptive technology for syntactic analysis of sentences in natural language. We provide a detailed description of a sentence with dependent constructs being decomposed into a syntactic tree through successive reductions as a proof of concept.},
  doi      = {https://doi.org/10.1016/j.procs.2018.04.164},
  keywords = {rewriting systems, adaptive technology, natural language processing},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050918305301},
}

@InCollection{BLACHE20171,
  author    = {Philippe Blache},
  title     = {1 - Delayed Interpretation, Shallow Processing and Constructions: the Basis of the “Interpret Whenever Possible” Principle},
  booktitle = {Cognitive Approach to Natural Language Processing},
  publisher = {Elsevier},
  year      = {2017},
  editor    = {Bernadette Sharp and Florence Sèdes and Wiesław Lubaszewski},
  pages     = {1 - 19},
  isbn      = {978-1-78548-253-3},
  abstract  = {Abstract:
From different perspectives, natural language processing, linguistics and psycholinguistics shed light on the way humans process language. However, this knowledge remains scattered: classical studies usually focus on language processing subtasks (e.g. lexical access) or modules (e.g. morphology, syntax), without being aggregated into a unified framework. It then remains very difficult to find a general model unifying the different sources of information into a unique architecture.},
  doi       = {https://doi.org/10.1016/B978-1-78548-253-3.50001-9},
  keywords  = {Aggregating by cohesion, Chunks, Delayed processing, “” Principle, Segment-and-store, Segmentation operations, Working memory},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781785482533500019},
}

@Article{KAMPER2017154,
  author   = {Herman Kamper and Aren Jansen and Sharon Goldwater},
  title    = {A segmental framework for fully-unsupervised large-vocabulary speech recognition},
  journal  = {Computer Speech \& Language},
  year     = {2017},
  volume   = {46},
  pages    = {154 - 174},
  issn     = {0885-2308},
  abstract = {Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early term discovery systems focused on identifying isolated recurring patterns in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units—effectively performing unsupervised speech recognition. This article presents the first attempt we are aware of to apply such a system to large-vocabulary multi-speaker data. Our system uses a Bayesian modelling framework with segmental word representations: each word segment is represented as a fixed-dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector. We compare our system on English and Xitsonga datasets to state-of-the-art baselines, using a variety of measures including word error rate (obtained by mapping the unsupervised output to ground truth transcriptions). Very high word error rates are reported—in the order of 70–80% for speaker-dependent and 80–95% for speaker-independent systems—highlighting the difficulty of this task. Nevertheless, in terms of cluster quality and word segmentation metrics, we show that by imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, both single-speaker and multi-speaker versions of our system outperform a purely bottom-up single-speaker syllable-based approach. We also show that the discovered clusters can be made less speaker- and gender-specific by using an unsupervised autoencoder-like feature extractor to learn better frame-level features (prior to embedding). Our system’s discovered clusters are still less pure than those of unsupervised term discovery systems, but provide far greater coverage.},
  doi      = {https://doi.org/10.1016/j.csl.2017.04.008},
  keywords = {Unsupervised speech processing, Representation learning, Segmentation, Clustering, Language acquisition},
  url      = {http://www.sciencedirect.com/science/article/pii/S0885230816301905},
}

@Article{CHEN201740,
  author   = {Yen-Liang Chen and Chia-Ling Chang and Chin-Sheng Yeh},
  title    = {Emotion classification of YouTube videos},
  journal  = {Decision Support Systems},
  year     = {2017},
  volume   = {101},
  pages    = {40 - 50},
  issn     = {0167-9236},
  abstract = {Watching online videos is a major leisure activity among Internet users. The largest video website, YouTube, stores billions of videos on its servers. Thus, previous studies have applied automatic video categorization methods to enable users to find videos corresponding to their needs; however, emotion has not been a factor considered in these classification methods. Therefore, this study classified YouTube videos into six emotion categories (i.e., happiness, anger, disgust, fear, sadness, and surprise). Through unsupervised and supervised learning methods, this study first categorized videos according to emotion. An ensemble model was subsequently applied to integrate the classification results of both methods. The experimental results confirm that the proposed method effectively facilitates the classification of YouTube videos into suitable emotion categories.},
  doi      = {https://doi.org/10.1016/j.dss.2017.05.014},
  keywords = {Data mining, Sentiments analysis, Machine learning, YouTube},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167923617300994},
}

@Article{ZHAO20161247,
  author   = {Gang Zhao and Ji Wu and Dingding Wang and Tao Li},
  title    = {Entity disambiguation to Wikipedia using collective ranking},
  journal  = {Information Processing \& Management},
  year     = {2016},
  volume   = {52},
  number   = {6},
  pages    = {1247 - 1257},
  issn     = {0306-4573},
  abstract = {Entity disambiguation is a fundamental task of semantic Web annotation. Entity Linking (EL) is an essential procedure in entity disambiguation, which aims to link a mention appearing in a plain text to a structured or semi-structured knowledge base, such as Wikipedia. Existing research on EL usually annotates the mentions in a text one by one and treats entities independent to each other. However this might not be true in many application scenarios. For example, if two mentions appear in one text, they are likely to have certain intrinsic relationships. In this paper, we first propose a novel query expansion method for candidate generation utilizing the information of co-occurrences of mentions. We further propose a re-ranking model which can be iteratively adjusted based on the prediction in the previous round. Experiments on real-world data demonstrate the effectiveness of our proposed methods for entity disambiguation.},
  doi      = {https://doi.org/10.1016/j.ipm.2016.06.002},
  keywords = {Named entity disambiguation, Feedback-query-expansion, Re-ranking},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306457316301893},
}

@Article{COTELO20154743,
  author   = {J.M. Cotelo and F.L. Cruz and J.A. Troyano and F.J. Ortega},
  title    = {A modular approach for lexical normalization applied to Spanish tweets},
  journal  = {Expert Systems with Applications},
  year     = {2015},
  volume   = {42},
  number   = {10},
  pages    = {4743 - 4754},
  issn     = {0957-4174},
  abstract = {Twitter is a social media platform with widespread success where millions of people continuously express ideas and opinions about a myriad of topics. It is a huge and interesting source of data but most of these texts are usually written hastily and very abbreviated, rendering them unsuitable for traditional Natural Language Processing (NLP). The two main contributions of this work are: the characterization of the textual error phenomena in Twitter and the proposal of a modular normalization system that improves the textual quality of tweets. Instead of focusing on a single technique, we propose an extensible normalization system that relies on the combination of several independent “expert modules”, each one addressing an very specific error phenomenon in its own way, thus increasing module accuracy and lowering the module building costs. Broadly speaking, the system resembles to an “expert board”: modules independently propose correction candidates for each Out of Vocabulary (OOV) word, rank the candidates and the best one is selected. In order to evaluate our proposal, we perform several experiments using texts from Twitter written in Spanish about a specific topic. The flexibility of defining resources at different language levels (core language, domain, genre) combined with the modular architecture lead to lower costs and a good performance: requiring a minimal effort for building the resources and achieving more than 82% of accuracy compared to the 31% yielded by the baseline.},
  doi      = {https://doi.org/10.1016/j.eswa.2015.02.003},
  keywords = {Twitter, Text normalization, Domain adaptation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417415000962},
}

@Article{EV201540,
  author   = {Vinu E.V. and Sreenivasa Kumar P.},
  title    = {A novel approach to generate MCQs from domain ontology: Considering DL semantics and open-world assumption},
  journal  = {Journal of Web Semantics},
  year     = {2015},
  volume   = {34},
  pages    = {40 - 54},
  issn     = {1570-8268},
  abstract = {Ontologies are structures, used for knowledge representation, which model domain knowledge in the form of concepts, roles, instances and their relationships. This knowledge can be exploited by an assessment system in the form of multiple choice questions (MCQs). The existing approaches, which use ontologies expressed in the Web Ontology Language (OWL) for MCQ generation, are limited to simple concept related questions — “What is C?” or “Which of the following is an example of C?” (where C is a concept symbol) — or analogy type questions involving roles. There are no efforts in the literature which make use of the terminological axioms in the ontology such as existential, universal and cardinality restrictions on concepts and roles for MCQ generation. Also, there are no systematic methods for generating incorrect answers (distractors) from ontologies. Distractor generation process has to be given much importance, since the generated distractors determine the quality and hardness of an MCQ. We propose two new MCQ generation approaches, which generate MCQs that are very useful and realistic in conducting assessment tests, and the corresponding distractor generating techniques. Our distractor generation techniques, unlike other methods, consider the open-world assumption, so that the generated MCQs will always be valid (falsity of distractors is ensured). Furthermore, we present a measure to determine the difficulty level (a value between 0 and 1) of the generated MCQs. The proposed system is implemented, and experiments on specific ontologies have shown the effectiveness of the approaches. We also did an empirical study by generating question items from a real-world ontology and validated our results with the help of domain experts.},
  doi      = {https://doi.org/10.1016/j.websem.2015.05.005},
  keywords = {OWL ontologies, Semantic web, Multiple choice questions, Automatic question generation},
  url      = {http://www.sciencedirect.com/science/article/pii/S1570826815000475},
}

@Article{FERRERO2014470,
  author   = {Carmen López Ferrero and Irene Renau and Rogelio Nazar and Sergi Torner},
  title    = {Computer-assisted Revision in Spanish Academic Texts: Peer-assessment},
  journal  = {Procedia - Social and Behavioral Sciences},
  year     = {2014},
  volume   = {141},
  pages    = {470 - 483},
  issn     = {1877-0428},
  note     = {4th World Conference on Learning Teaching and Educational Leadership (WCLTA-2013)},
  abstract = {This paper presents a series of experiments in automatic correction of spelling and grammar errors with a statistic and corpus- driven methodology. The language of the experiments is Spanish, but the method can be easily extrapolated to other languages since we do not use language-specific resources. Our main motivation is to develop a tool that could assist university students to write academic texts, because this kind of system is practically nonexistent in the present, especially in Spanish. Our work is based on previous descriptions, which identify the most problematic phenomena in academic writing at university level. We aim to develop a tool for automatic detection and correction of some of those problematic issues at different linguistic levels such as spelling, grammar and vocabulary.},
  doi      = {https://doi.org/10.1016/j.sbspro.2014.05.083},
  keywords = {Academic discourse, computer-assisted revision, n-gram language models, peer-assessment, written competence evaluation;},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877042814035071},
}

@Article{PERRUCHET20141,
  author   = {Pierre Perruchet and Bénédicte Poulin-Charronnat and Barbara Tillmann and Ronald Peereman},
  title    = {New evidence for chunk-based models in word segmentation},
  journal  = {Acta Psychologica},
  year     = {2014},
  volume   = {149},
  pages    = {1 - 8},
  issn     = {0001-6918},
  note     = {Including Special section articles of Temporal Processing Within and Across Senses - Part-2},
  abstract = {There is large evidence that infants are able to exploit statistical cues to discover the words of their language. However, how they proceed to do so is the object of enduring debates. The prevalent position is that words are extracted from the prior computation of statistics, in particular the transitional probabilities between syllables. As an alternative, chunk-based models posit that the sensitivity to statistics results from other processes, whereby many potential chunks are considered as candidate words, then selected as a function of their relevance. These two classes of models have proven to be difficult to dissociate. We propose here a procedure, which leads to contrasted predictions regarding the influence of a first language, L1, on the segmentation of a second language, L2. Simulations run with PARSER (Perruchet & Vinter, 1998), a chunk-based model, predict that when the words of L1 become word-external transitions of L2, learning of L2 should be depleted until reaching below chance level, at least before extensive exposure to L2 reverses the effect. In the same condition, a transitional-probability based model predicts above-chance performance whatever the duration of exposure to L2. PARSER's predictions were confirmed by experimental data: Performance on a two-alternative forced choice test between words and part-words from L2 was significantly below chance even though part-words were less cohesive in terms of transitional probabilities than words.},
  doi      = {https://doi.org/10.1016/j.actpsy.2014.01.015},
  keywords = {Word segmentation, Chunking, Modeling, Artificial language},
  url      = {http://www.sciencedirect.com/science/article/pii/S0001691814000262},
}

@Article{RAGHURAM2014423,
  author   = {Jayaram Raghuram and David J. Miller and George Kesidis},
  title    = {Unsupervised, low latency anomaly detection of algorithmically generated domain names by generative probabilistic modeling},
  journal  = {Journal of Advanced Research},
  year     = {2014},
  volume   = {5},
  number   = {4},
  pages    = {423 - 433},
  issn     = {2090-1232},
  note     = {Cyber Security},
  abstract = {We propose a method for detecting anomalous domain names, with focus on algorithmically generated domain names which are frequently associated with malicious activities such as fast flux service networks, particularly for bot networks (or botnets), malware, and phishing. Our method is based on learning a (null hypothesis) probability model based on a large set of domain names that have been white listed by some reliable authority. Since these names are mostly assigned by humans, they are pronounceable, and tend to have a distribution of characters, words, word lengths, and number of words that are typical of some language (mostly English), and often consist of words drawn from a known lexicon. On the other hand, in the present day scenario, algorithmically generated domain names typically have distributions that are quite different from that of human-created domain names. We propose a fully generative model for the probability distribution of benign (white listed) domain names which can be used in an anomaly detection setting for identifying putative algorithmically generated domain names. Unlike other methods, our approach can make detections without considering any additional (latency producing) information sources, often used to detect fast flux activity. Experiments on a publicly available, large data set of domain names associated with fast flux service networks show encouraging results, relative to several baseline methods, with higher detection rates and low false positive rates.},
  doi      = {https://doi.org/10.1016/j.jare.2014.01.001},
  keywords = {Anomaly detection, Algorithmically generated domain names, Malicious domain names, Domain name modeling, Fast flux},
  url      = {http://www.sciencedirect.com/science/article/pii/S2090123214000022},
}

@Article{VYDRIN201375,
  author   = {Valentin Vydrin},
  title    = {Bamana Reference Corpus (BRC)},
  journal  = {Procedia - Social and Behavioral Sciences},
  year     = {2013},
  volume   = {95},
  pages    = {75 - 80},
  issn     = {1877-0428},
  note     = {Corpus Resources for Descriptive and Applied Studies. Current Challenges and Future Directions: Selected Papers from the 5th International Conference on Corpus Linguistics (CILC2013)},
  abstract = {The Bambara Reference Corpus (Corpus Bambara de Référence) is one of the first corpora for the languages of Africa south of Sahara of more than a million words, and probably the only one freely accessible on the Internet. The entire corpus is tone- marked, POS-tagged and glossed in French. In the paper, tools and resources developed for the Bambara Reference Corpus are surveyed and the process of corpus building is described.},
  doi      = {https://doi.org/10.1016/j.sbspro.2013.10.624},
  keywords = {bambara, manding, mande, Corpus Bambara de Référence},
  url      = {http://www.sciencedirect.com/science/article/pii/S187704281304144X},
}

@Article{STOYKOVA2012400,
  author   = {Velislava Stoykova},
  title    = {The inflectional morphology of Bulgarian possessive and reflexive-possessive pronouns in Universal Networking Language},
  journal  = {Procedia Technology},
  year     = {2012},
  volume   = {1},
  pages    = {400 - 406},
  issn     = {2212-0173},
  note     = {First World Conference on Innovation and Computer Sciences (INSODE 2011)},
  abstract = {The paper analyses the application of grammar knowledge representation of Bulgarian inflectional morphology. We present the implementation of inflectional morphology of Bulgarian possessive and reflexive-possessive pronouns using the semantic networks interpretation. It is focused on modelling the inflectional morphology of Bulgarian possessive and reflexive-possessive pronouns using the Universal Networking Language (UNL). The formal analysis and related encodings take into account the specific morphological and syntactic properties of possessive and reflexive-possessive pronouns, and represent their inflectional morphology interpretation with respect to the feature of definiteness. The definitions of presented inflectional rules are given after the detailed analysis of both semantic and grammar features of the related pronouns. The evaluation of generated inflected forms is given by some example words. The analysis of basic problems of rule-based inflectional morphology grammar representation of Bulgarian possessive and reflexive-possessive pronouns is offered, and the semantic network using rule-based knowledge representation and non-monotonic reasoning in UNL framework with related encoding is presented. The particular model is explained in terms of its linguistic motivation.},
  doi      = {https://doi.org/10.1016/j.protcy.2012.02.091},
  keywords = {Artificial Intelligence, Information technology & Languages, Knowledge Engineering, Semantic Networks, UNL},
  url      = {http://www.sciencedirect.com/science/article/pii/S2212017312000928},
}

@Article{OCHODEK2011885,
  author   = {M. Ochodek and B. Alchimowicz and J. Jurkiewicz and J. Nawrocki},
  title    = {Improving the reliability of transaction identification in use cases},
  journal  = {Information and Software Technology},
  year     = {2011},
  volume   = {53},
  number   = {8},
  pages    = {885 - 897},
  issn     = {0950-5849},
  note     = {Advances in functional size measurement and effort estimation - Extended best papers},
  abstract = {Context
The concept of transactions is used in Use Case Points (UCP), and in many other functional size measurement methods, to capture the smallest unit of functionality that should be considered while measuring the size of a system. Unfortunately, in the case of the UCP method at least four methods for use-case transaction identification have been proposed so far. The different approaches to transaction identification and difficulties related to the analysis of requirements expressed in natural language can lead to problems in the reliability of functional size measurement.
Objective
The goal of this study was to evaluate reliability of transaction identification in use cases (with the methods mentioned in the literature), analyze their weaknesses, and propose some means for their improvement.
Method
A controlled experiment on a group of 120 students was performed to investigate if the methods for transaction identification, known from the literature, provide similar results. In addition, a qualitative analysis of the experiment data was performed to investigate the potential problems related to transaction identification in use cases. During the experiment a use-case benchmark specification was used. The automatic methods for transaction identification, proposed in the paper have been validated using the same benchmark by comparing the outcomes provided by these methods to on-average number of transactions identified by the participants of the experiment.
Results
A significant difference in the median number of transactions was observed between groups using different methods of transaction identification. The Kruskal–Wallis test was performed with the significance level α set to 0.05 and followed by the post-hoc analysis performed according to the procedure proposed by Conover. Also a large intra-method variability was observed. The ratios between the maximum and minimum number of transactions identified by the participants using the same method were equal to 1.96, 3.83, 2.03, and 2.21. The proposed automatic methods for transaction identification provided results consistent with those provided by the participants of the experiment and functional measurement experts. The relative error between the number of transaction identified by the tool and on-average number of transactions identified by the participants of the experiment ranged from 3% to 7%.
Conclusions
Human-performed transaction identification is error prone and quite subjective. Its reliability can be improved by automating the process with the use of natural language processing techniques.},
  doi      = {https://doi.org/10.1016/j.infsof.2011.02.004},
  keywords = {Use-case transactions, Use Case Points, Functional size measurement, Natural language processing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584911000401},
}

@Article{KEPUSKA2009e2772,
  author   = {V.Z. Këpuska and T.B. Klein},
  title    = {A novel Wake-Up-Word speech recognition system, Wake-Up-Word recognition task, technology and evaluation},
  journal  = {Nonlinear Analysis: Theory, Methods \& Applications},
  year     = {2009},
  volume   = {71},
  number   = {12},
  pages    = {e2772 - e2789},
  issn     = {0362-546X},
  abstract = {Wake-Up-Word (WUW) is a new paradigm in speech recognition (SR) that is not yet widely recognized. This paper defines and investigates WUW speech recognition, describes details of this novel solution and the technology that implements it. WUW SR is defined as detection of a single word or phrase when spoken in the alerting context of requesting attention, while rejecting all other words, phrases, sounds, noises and other acoustic events and the same word or phrase spoken in non-alerting context with virtually 100% accuracy. In order to achieve this accuracy, the following innovations were accomplished: (1) Hidden Markov Model triple scoring with Support Vector Machine classification, (2) Combining multiple speech feature streams: Mel-scale Filtered Cepstral Coefficients (MFCCs), Linear Prediction Coefficients (LPC)-smoothed MFCCs, and Enhanced MFCC, and (3) Improved Voice Activity Detector with Support Vector Machines. WUW detection and recognition performance is 2514%, or 26 times better than HTK for the same training & testing data, and 2271%, or 24 times better than Microsoft SAPI 5.1 recognizer. The out-of-vocabulary rejection performance is over 65,233%, or 653 times better than HTK, and 5900% to 42,900%, or 60 to 430 times better than the Microsoft SAPI 5.1 recognizer. This solution that utilizes a new recognition paradigm applies not only to WUW task but also to any general Speech Recognition tasks.},
  doi      = {https://doi.org/10.1016/j.na.2009.06.089},
  keywords = {Wake-Up-Word, Speech recognition, Hidden Markov Models, Support Vector Machines, Mel-scale cepstral coefficients, Linear prediction spectrum, Enhanced spectrum, HTK, Microsoft SAPI},
  url      = {http://www.sciencedirect.com/science/article/pii/S0362546X09008220},
}

@Article{FERNANDES2009349,
  author   = {Tânia Fernandes and Régine Kolinsky and Paulo Ventura},
  title    = {The metamorphosis of the statistical segmentation output: Lexicalization during artificial language learning},
  journal  = {Cognition},
  year     = {2009},
  volume   = {112},
  number   = {3},
  pages    = {349 - 366},
  issn     = {0010-0277},
  abstract = {This study combined artificial language learning (ALL) with conventional experimental techniques to test whether statistical speech segmentation outputs are integrated into adult listeners’ mental lexicon. Lexicalization was assessed through inhibitory effects of novel neighbors (created by the parsing process) on auditory lexical decisions to real words. Both immediately after familiarization and post-one week, ALL outputs were lexicalized only when the cues available during familiarization (transitional probabilities and wordlikeness) suggested the same parsing (Experiments 1 and 3). No lexicalization effect occurred with incongruent cues (Experiments 2 and 4). Yet, ALL differed from chance, suggesting a dissociation between item knowledge and lexicalization. Similarly contrasted results were found when frequency of occurrence of the stimuli was equated during familiarization (Experiments 3 and 4). Our findings thus indicate that ALL outputs may be lexicalized as far as the segmentation cues are congruent, and that this process cannot be accounted for by raw frequency.},
  doi      = {https://doi.org/10.1016/j.cognition.2009.05.002},
  keywords = {Speech segmentation, Artificial language learning, Lexicalization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0010027709001085},
}

@InCollection{SCHULTZ20061,
  author    = {Schultz and Kirchhoff},
  title     = {Chapter 1 - Introduction},
  booktitle = {Multilingual Speech Processing},
  publisher = {Academic Press},
  year      = {2006},
  editor    = {Schultz and Kirchhoff},
  pages     = {1 - 4},
  address   = {Burlington},
  isbn      = {978-0-12-088501-5},
  abstract  = {Publisher Summary
This introductory chapter provides the organization of the book. The book presents the state of the art of multilingual speech processing—that is, the techniques that are required to support spoken input and output in a large variety of languages. Speech processing has a number of different subfields, including speech coding and enhancement, signal processing, speech recognition, speech synthesis, keyword spotting, language identification, and speaker identification. The book provides an overview of multilingual issues in acoustic modeling, language modeling, and dictionary construction for speech recognition, speech synthesis, and automatic language identification. Speech-to-speech translation and automated dialog systems are discussed in detail as example applications. Throughout the book, two main topics are addressed: the challenges for current speech processing models posed by different languages, and the feasibility of sharing data and system components across languages and dialects of related types. In addition to describing modeling approaches, an overview of significant ongoing research programs as well as trends, prognoses, and open research issues are also provided.},
  doi       = {https://doi.org/10.1016/B978-012088501-5/50004-4},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780120885015500044},
}

@InCollection{SRIHARI2006203,
  author    = {S. Srihari},
  title     = {Handwriting Recognition, Automatic},
  booktitle = {Encyclopedia of Language \& Linguistics (Second Edition)},
  publisher = {Elsevier},
  year      = {2006},
  editor    = {Keith Brown},
  pages     = {203 - 211},
  address   = {Oxford},
  edition   = {Second Edition},
  isbn      = {978-0-08-044854-1},
  abstract  = {Handwriting is a common means of recording personal information and communication, even with the introduction of new technologies. Machine recognition of handwriting has practical significance, as in reading handwritten notes in a PDA, postal addresses on envelopes, amounts in bank checks, handwritten fields in forms, etc. This overview describes the nature of handwritten language, how it is transduced into electronic data and the basic concepts behind written language recognition algorithms. Both the dynamic or on-line case (which pertains to the availability of trajectory data during writing) and the static or off-line case (which pertains to scanned images) are considered. Algorithms for preprocessing, character and word recognition, and performance with practical systems are indicated. Variations of handwriting recognition such as signature verification and writer identification are also described.},
  doi       = {https://doi.org/10.1016/B0-08-044854-2/00982-2},
  keywords  = {character recognition, dynamic handwriting, forensic document examination, handwriting recognition, linguistic postprocessing, off-line handwriting, on-line handwriting, questioned document analysis, signature verification, static handwriting, word recognition, writer identification},
  url       = {http://www.sciencedirect.com/science/article/pii/B0080448542009822},
}

@Article{BINNENPOORTE2005433,
  author   = {Diana Binnenpoorte and Catia Cucchiarini and Lou Boves and Helmer Strik},
  title    = {Multiword expressions in spoken language: An exploratory study on pronunciation variation},
  journal  = {Computer Speech \& Language},
  year     = {2005},
  volume   = {19},
  number   = {4},
  pages    = {433 - 449},
  issn     = {0885-2308},
  note     = {Special issue on Multiword Expression},
  abstract = {The study presented in this paper was aimed at exploring the possibilities of modelling specific pronunciation characteristics of multiword expressions (MWEs) for both automatic speech recognition (ASR) and automatic phonetic transcription (APT). For this purpose, we first drew up an inventory of frequently found N-grams extracted from orthographic transcriptions of spontaneous speech contained in a large corpus of spoken Dutch. These N-grams were filtered and subsequently assigned to linguistic categories. For a small selection of these N-grams we examined the phonetic transcriptions contained in the corpus. We found that the pronunciation of these N-grams differed to a large extent from the canonical form. In order to determine whether this is a general characteristic of spontaneous speech or rather the effect of the specific status of these N-grams, we analysed the pronunciations of the individual words composing the N-grams in two context conditions: (1) in the N-gram context and (2) in any other context. We found that words in N-grams do indeed have peculiar pronunciation patterns. This seems to suggest that the N-grams investigated may be considered as MWEs that should be treated as lexical entries in the pronunciation lexicons used in ASR and APT, with their own specific pronunciation variants.},
  doi      = {https://doi.org/10.1016/j.csl.2004.11.003},
  url      = {http://www.sciencedirect.com/science/article/pii/S0885230805000124},
}

@Article{GILLOUX1993267,
  author   = {Michel Gilloux},
  title    = {Research into the new generation of character and mailing address recognition systems at the French post office research center},
  journal  = {Pattern Recognition Letters},
  year     = {1993},
  volume   = {14},
  number   = {4},
  pages    = {267 - 276},
  issn     = {0167-8655},
  note     = {Postal Processing and Character Recognition},
  abstract = {We review applications of character recognition in a postal context and describe its present state-of-the-art. We then give a survey of the research projects conducted at the French post office research center (SRTP) on the recognition of printed and handwritten mailing addresses for small envelopes and flat mail and on the recognition of postal check values.},
  doi      = {https://doi.org/10.1016/0167-8655(93)90092-R},
  keywords = {Address recognition, digit recognition, handwriting recognition},
  url      = {http://www.sciencedirect.com/science/article/pii/016786559390092R},
}

@Article{2019Yiarxiv:1904.07994,
  author  = {Yi Zhu and Ivan Vulić and Anna Korhonen},
  title   = {A Systematic Study of Leveraging Subword Information for Learning Word Representations},
  journal = {arxiv:1904.07994},
  year    = {2019},
  url     = {http://arxiv.org/abs/1904.07994v2},
}

@Article{2019Zhimingarxiv:1908.06748,
  author  = {Zhiming Li and Qing Wu and Kun Qian},
  title   = {Adabot: Fault-Tolerant Java Decompiler},
  journal = {arxiv:1908.06748},
  year    = {2019},
  url     = {http://arxiv.org/abs/1908.06748v1},
}

@Article{2019Guan-Linarxiv:1907.03040,
  author  = {Guan-Lin Chao and Ian Lane},
  title   = {BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer},
  journal = {arxiv:1907.03040},
  year    = {2019},
  url     = {http://arxiv.org/abs/1907.03040v1},
}

@Article{2019Marcelyarxiv:1907.00184,
  author  = {Marcely Zanon Boito and Aline Villavicencio and Laurent Besacier},
  title   = {Empirical Evaluation of Sequence-to-Sequence Models for Word Discovery in Low-resource Settings},
  journal = {arxiv:1907.00184},
  year    = {2019},
  url     = {http://arxiv.org/abs/1907.00184v2},
}

@Article{2019Peiarxiv:1906.05551,
  author  = {Pei Zhang and Boxing Chen and Niyu Ge and Kai Fan},
  title   = {Lattice Transformer for Speech Translation},
  journal = {arxiv:1906.05551},
  year    = {2019},
  url     = {http://arxiv.org/abs/1906.05551v1},
}

@Article{2019Congyingarxiv:1906.08449,
  author  = {Congying Xia and Chenwei Zhang and Tao Yang and Yaliang Li and Nan Du and Xian Wu and Wei Fan and Fenglong Ma and Philip Yu},
  title   = {Multi-Grained Named Entity Recognition},
  journal = {arxiv:1906.08449},
  year    = {2019},
  url     = {http://arxiv.org/abs/1906.08449v1},
}

@Article{2019Rachelarxiv:1907.05854,
  author  = {Rachel Bawden and Nikolay Bogoychev and Ulrich Germann and Roman Grundkiewicz and Faheem Kirefu and Antonio Valerio Miceli Barone and Alexandra Birch},
  title   = {The University of Edinburgh's Submissions to the WMT19 News Translation Task},
  journal = {arxiv:1907.05854},
  year    = {2019},
  url     = {http://arxiv.org/abs/1907.05854v1},
}

@Article{2018Aaronarxiv:1809.02237,
  author  = {Aaron Smith and Bernd Bohnet and Miryam de Lhoneux and Joakim Nivre and Yan Shao and Sara Stymne},
  title   = {82 Treebanks, 34 Models: Universal Dependency Parsing with Multi-Treebank Models},
  journal = {arxiv:1809.02237},
  year    = {2018},
  url     = {http://arxiv.org/abs/1809.02237v1},
}

@Article{2018Lucasarxiv:1802.06053,
  author  = {Lucas Ondel and Pierre Godard and Laurent Besacier and Elin Larsen and Mark Hasegawa-Johnson and Odette Scharenborg and Emmanuel Dupoux and Lukas Burget and François Yvon and Sanjeev Khudanpur},
  title   = {Bayesian Models for Unit Discovery on a Very Low Resource Language},
  journal = {arxiv:1802.06053},
  year    = {2018},
  url     = {http://arxiv.org/abs/1802.06053v2},
}

@Article{Doval_2018,
  author    = {Yerai Doval and Carlos G{\'{o}}mez-Rodr{\'{\i}}guez},
  title     = {Comparing neural- and N-gram-based language models for word segmentation},
  journal   = {Journal of the Association for Information Science and Technology},
  year      = {2018},
  volume    = {70},
  number    = {2},
  pages     = {187--197},
  month     = {dec},
  doi       = {10.1002/asi.24082},
  publisher = {Wiley},
  url       = {https://doi.org/10.1002%2Fasi.24082},
}

@Article{2018Duyguarxiv:1805.02036,
  author  = {Duygu Ataman and Marcello Federico},
  title   = {Compositional Representation of Morphologically-Rich Input for Neural Machine Translation},
  journal = {arxiv:1805.02036},
  year    = {2018},
  url     = {http://arxiv.org/abs/1805.02036v1},
}

@InProceedings{KITADA_2018,
  author    = {Shunsuke KITADA and Ryunosuke KOTANI and Hitoshi IYATOMI},
  title     = {End-to-End Text Classification via Image-based Embedding using Character-level Networks},
  booktitle = {2018 {IEEE} Applied Imagery Pattern Recognition Workshop ({AIPR})},
  year      = {2018},
  month     = {oct},
  publisher = {{IEEE}},
  doi       = {10.1109/aipr.2018.8707407},
  url       = {https://doi.org/10.1109%2Faipr.2018.8707407},
}

@Article{2018Frédericarxiv:1808.09551,
  author  = {Fréderic Godin and Kris Demuynck and Joni Dambre and Wesley De Neve and Thomas Demeester},
  title   = {Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?},
  journal = {arxiv:1808.09551},
  year    = {2018},
  url     = {http://arxiv.org/abs/1808.09551v1},
}

@Article{2018Hainanarxiv:1811.04284,
  author  = {Hainan Xu and Shuoyang Ding and Shinji Watanabe},
  title   = {Improving End-to-end Speech Recognition with Pronunciation-assisted Sub-word Modeling},
  journal = {arxiv:1811.04284},
  year    = {2018},
  url     = {http://arxiv.org/abs/1811.04284v2},
}

@Article{2018Kazuyaarxiv:1811.09353,
  author  = {Kazuya Kawakami and Chris Dyer and Phil Blunsom},
  title   = {Learning to Discover, Ground and Use Words with Segmental Neural Language Models},
  journal = {arxiv:1811.09353},
  year    = {2018},
  url     = {http://arxiv.org/abs/1811.09353v2},
}

@Article{2018Dominikarxiv:1806.05482,
  author  = {Dominik Macháček and Jonáš Vidra and Ondřej Bojar},
  title   = {Morphological and Language-Agnostic Word Segmentation for NMT},
  journal = {arxiv:1806.05482},
  year    = {2018},
  url     = {http://arxiv.org/abs/1806.05482v1},
}

@Article{2018Ennoarxiv:1811.04791,
  author  = {Enno Hermann and Herman Kamper and Sharon Goldwater},
  title   = {Multilingual and Unsupervised Subword Modeling for Zero-Resource Languages},
  journal = {arxiv:1811.04791},
  year    = {2018},
  url     = {http://arxiv.org/abs/1811.04791v1},
}

@InProceedings{Hermann_2018,
  author    = {Enno Hermann and Sharon Goldwater},
  title     = {Multilingual Bottleneck Features for Subword Modeling in Zero-resource Languages},
  booktitle = {Interspeech 2018},
  year      = {2018},
  month     = {sep},
  publisher = {{ISCA}},
  doi       = {10.21437/interspeech.2018-2334},
  url       = {https://doi.org/10.21437%2Finterspeech.2018-2334},
}

@Article{2018Wonarxiv:1810.13113,
  author  = {Won Ik Cho and Sung Jun Cheon and Woo Hyun Kang and Ji Won Kim and Nam Soo Kim},
  title   = {Real-time Automatic Word Segmentation for User-generated Text},
  journal = {arxiv:1810.13113},
  year    = {2018},
  url     = {http://arxiv.org/abs/1810.13113v2},
}

@Article{2018Geewookarxiv:1809.00918,
  author  = {Geewook Kim and Kazuki Fukui and Hidetoshi Shimodaira},
  title   = {Segmentation-free Compositional $n$-gram Embedding},
  journal = {arxiv:1809.00918},
  year    = {2018},
  url     = {http://arxiv.org/abs/1809.00918v2},
}

@InProceedings{Brito_2018,
  author    = {Raphael C. Brito and Hansenclever F. Bassani},
  title     = {Self-Organizing Maps with Variable Input Length for Motif Discovery and Word Segmentation},
  booktitle = {2018 International Joint Conference on Neural Networks ({IJCNN})},
  year      = {2018},
  month     = {jul},
  publisher = {{IEEE}},
  doi       = {10.1109/ijcnn.2018.8489090},
  url       = {https://doi.org/10.1109%2Fijcnn.2018.8489090},
}

@Article{2018Biaoarxiv:1810.12546,
  author  = {Biao Zhang and Deyi Xiong and Jinsong Su and Qian Lin and Huiji Zhang},
  title   = {Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks},
  journal = {arxiv:1810.12546},
  year    = {2018},
  url     = {http://arxiv.org/abs/1810.12546v1},
}

@Article{2018Yanarxiv:1807.02974,
  author  = {Yan Shao and Christian Hardmeier and Joakim Nivre},
  title   = {Universal Word Segmentation: Implementation and Interpretation},
  journal = {arxiv:1807.02974},
  year    = {2018},
  url     = {http://arxiv.org/abs/1807.02974v1},
}

@Article{2018Pierrearxiv:1806.06734,
  author  = {Pierre Godard and Marcely Zanon-Boito and Lucas Ondel and Alexandre Berard and François Yvon and Aline Villavicencio and Laurent Besacier},
  title   = {Unsupervised Word Segmentation from Speech with Attention},
  journal = {arxiv:1806.06734},
  year    = {2018},
  url     = {http://arxiv.org/abs/1806.06734v1},
}

@Article{2018Harisarxiv:1806.05432,
  author  = {Haris Bin Zia and Agha Ali Raza and Awais Athar},
  title   = {Urdu Word Segmentation using Conditional Random Fields (CRFs)},
  journal = {arxiv:1806.05432},
  year    = {2018},
  url     = {http://arxiv.org/abs/1806.05432v1},
}

@Article{2018Yuanhaoarxiv:1804.01778,
  author  = {Yuanhao Liu and Sheng Yu},
  title   = {Word Segmentation as Graph Partition},
  journal = {arxiv:1804.01778},
  year    = {2018},
  url     = {http://arxiv.org/abs/1804.01778v1},
}

@Article{2018Yansenarxiv:1811.09362,
  author  = {Yansen Wang and Ying Shen and Zhun Liu and Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency},
  title   = {Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors},
  journal = {arxiv:1811.09362},
  year    = {2018},
  url     = {http://arxiv.org/abs/1811.09362v2},
}

@Article{Kamper_2017,
  author    = {Herman Kamper and Aren Jansen and Sharon Goldwater},
  title     = {A segmental framework for fully-unsupervised large-vocabulary speech recognition},
  journal   = {Computer Speech {\&} Language},
  year      = {2017},
  volume    = {46},
  pages     = {154--174},
  month     = {nov},
  doi       = {10.1016/j.csl.2017.04.008},
  publisher = {Elsevier {BV}},
  url       = {https://doi.org/10.1016%2Fj.csl.2017.04.008},
}

@Article{2017Hermanarxiv:1703.08135,
  author  = {Herman Kamper and Karen Livescu and Sharon Goldwater},
  title   = {An embedded segmental K-means model for unsupervised segmentation and clustering of speech},
  journal = {arxiv:1703.08135},
  year    = {2017},
  url     = {http://arxiv.org/abs/1703.08135v2},
}

@Article{2017Jiearxiv:1708.07279,
  author  = {Jie Yang and Zhiyang Teng and Meishan Zhang and Yue Zhang},
  title   = {Combining Discrete and Neural Features for Sequence Labeling},
  journal = {arxiv:1708.07279},
  year    = {2017},
  url     = {http://arxiv.org/abs/1708.07279v1},
}

@Article{2017Yanarxiv:1709.03756,
  author  = {Yan Shao},
  title   = {Cross-lingual Word Segmentation and Morpheme Segmentation as Sequence Labelling},
  journal = {arxiv:1709.03756},
  year    = {2017},
  url     = {http://arxiv.org/abs/1709.03756v1},
}

@Article{2017Xinchiarxiv:1707.00248,
  author  = {Xinchi Chen and Zhan Shi and Xipeng Qiu and Xuanjing Huang},
  title   = {DAG-based Long Short-Term Memory for Neural Word Segmentation},
  journal = {arxiv:1707.00248},
  year    = {2017},
  url     = {http://arxiv.org/abs/1707.00248v1},
}

@Article{2017Hanarxiv:1712.08841,
  author  = {Han He and Lei Wu and Xiaokun Yang and Hua Yan and Zhimin Gao and Yi Feng and George Townsend},
  title   = {Dual Long Short-Term Memory Networks for Sub-Character Representation Learning},
  journal = {arxiv:1712.08841},
  year    = {2017},
  url     = {http://arxiv.org/abs/1712.08841v2},
}

@Article{2017Hanarxiv:1712.02856,
  author  = {Han He and Lei Wu and Hua Yan and Zhimin Gao and Yi Feng and George Townsend},
  title   = {Effective Neural Solution for Multi-Criteria Word Segmentation},
  journal = {arxiv:1712.02856},
  year    = {2017},
  url     = {http://arxiv.org/abs/1712.02856v2},
}

@Article{2017Claraarxiv:1704.08352,
  author  = {Clara Vania and Adam Lopez},
  title   = {From Characters to Words to in Between: Do We Capture Morphology?},
  journal = {arxiv:1704.08352},
  year    = {2017},
  url     = {http://arxiv.org/abs/1704.08352v1},
}

@Article{2017Jakearxiv:1710.07729,
  author  = {Jake Ryland Williams and Giovanni C. Santia},
  title   = {Is space a word, too?},
  journal = {arxiv:1710.07729},
  year    = {2017},
  url     = {http://arxiv.org/abs/1710.07729v1},
}

@Article{2017Duyguarxiv:1707.09879,
  author  = {Duygu Ataman and Matteo Negri and Marco Turchi and Marcello Federico},
  title   = {Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English},
  journal = {arxiv:1707.09879},
  year    = {2017},
  url     = {http://arxiv.org/abs/1707.09879v1},
}

@Article{2017Matthiasarxiv:1704.00559,
  author  = {Matthias Sperber and Graham Neubig and Jan Niehues and Alex Waibel},
  title   = {Neural Lattice-to-Sequence Models for Uncertain Inputs},
  journal = {arxiv:1704.00559},
  year    = {2017},
  url     = {http://arxiv.org/abs/1704.00559v2},
}

@Article{2017Jiearxiv:1704.08960,
  author  = {Jie Yang and Yue Zhang and Fei Dong},
  title   = {Neural Word Segmentation with Rich Pretraining},
  journal = {arxiv:1704.08960},
  year    = {2017},
  url     = {http://arxiv.org/abs/1704.08960v1},
}

@Article{2016Shumingarxiv:1611.04233,
  author  = {Shuming Ma and Xu Sun},
  title   = {A New Recurrent Neural CRF for Learning Non-linear Edge Features},
  journal = {arxiv:1611.04233},
  year    = {2016},
  url     = {http://arxiv.org/abs/1611.04233v1},
}

@Article{2016Sounakarxiv:1604.06243,
  author  = {Sounak Dey and Anguelos Nicolaou and Josep Llados and Umapada Pal},
  title   = {Evaluation of the Effect of Improper Segmentation on Word Spotting},
  journal = {arxiv:1604.06243},
  year    = {2016},
  url     = {http://arxiv.org/abs/1604.06243v1},
}

@Article{2016Yijiaarxiv:1604.05499,
  author  = {Yijia Liu and Wanxiang Che and Jiang Guo and Bing Qin and Ting Liu},
  title   = {Exploring Segment Representations for Neural Segmentation Models},
  journal = {arxiv:1604.05499},
  year    = {2016},
  url     = {http://arxiv.org/abs/1604.05499v1},
}

@Article{2016Ruey-Chengarxiv:1607.05822,
  author  = {Ruey-Cheng Chen},
  title   = {Incremental Learning for Fully Unsupervised Word Segmentation Using Penalized Likelihood and Model Selection},
  journal = {arxiv:1607.05822},
  year    = {2016},
  url     = {http://arxiv.org/abs/1607.05822v2},
}

@Article{2016Nanyunarxiv:1608.02689,
  author  = {Nanyun Peng and Mark Dredze},
  title   = {Multi-task Domain Adaptation for Sequence Tagging},
  journal = {arxiv:1608.02689},
  year    = {2016},
  url     = {http://arxiv.org/abs/1608.02689v2},
}

@Article{2016Yossiarxiv:1610.07918,
  author  = {Yossi Adi and Joseph Keshet and Emily Cibelli and Matthew Goldrick},
  title   = {Sequence Segmentation Using Joint RNN and Structured Prediction Models},
  journal = {arxiv:1610.07918},
  year    = {2016},
  url     = {http://arxiv.org/abs/1610.07918v1},
}

@Article{2016Qingrongarxiv:1608.01448,
  author  = {Qingrong Xia and Zhenghua Li and Jiayuan Chao and Min Zhang},
  title   = {Word Segmentation on Micro-blog Texts with External Lexicon and Heterogeneous Data},
  journal = {arxiv:1608.01448},
  year    = {2016},
  url     = {http://arxiv.org/abs/1608.01448v2},
}

@Article{2015Weiarxiv:1506.04111,
  author  = {Wei Wang and Kenneth Shirley},
  title   = {Breaking Bad: Detecting malicious domains using word segmentation},
  journal = {arxiv:1506.04111},
  year    = {2015},
  url     = {http://arxiv.org/abs/1506.04111v1},
}

@Article{2015Ricoarxiv:1508.07909,
  author  = {Rico Sennrich and Barry Haddow and Alexandra Birch},
  title   = {Neural Machine Translation of Rare Words with Subword Units},
  journal = {arxiv:1508.07909},
  year    = {2015},
  url     = {http://arxiv.org/abs/1508.07909v5},
}

@Article{2015Lingpengarxiv:1511.06018,
  author  = {Lingpeng Kong and Chris Dyer and Noah A. Smith},
  title   = {Segmental Recurrent Neural Networks},
  journal = {arxiv:1511.06018},
  year    = {2015},
  url     = {http://arxiv.org/abs/1511.06018v2},
}

@Article{2014Wangarxiv:1404.6866,
  author  = {Wang Liang and Zhao KaiYong},
  title   = {Detecting "protein words" through unsupervised word segmentation},
  journal = {arxiv:1404.6866},
  year    = {2014},
  url     = {http://arxiv.org/abs/1404.6866v6},
}

@Article{2013Grzegorzarxiv:1309.4628,
  author  = {Grzegorz Chrupała},
  title   = {Text segmentation with character-level text embeddings},
  journal = {arxiv:1309.4628},
  year    = {2013},
  url     = {http://arxiv.org/abs/1309.4628v1},
}

@Article{2012Peiyouarxiv:1206.4958,
  author  = {Peiyou Song and Anhei Shu and Anyu Zhou and Dan Wallach and Jedidiah R. Crandall},
  title   = {A Pointillism Approach for Natural Language Processing of Social Media},
  journal = {arxiv:1206.4958},
  year    = {2012},
  url     = {http://arxiv.org/abs/1206.4958v1},
}

@Article{2011Jerryarxiv:1105.6162,
  author  = {Jerry R. Van Aken},
  title   = {A statistical learning algorithm for word segmentation},
  journal = {arxiv:1105.6162},
  year    = {2011},
  url     = {http://arxiv.org/abs/1105.6162v2},
}

@Article{2010Satadalarxiv:1002.4048,
  author  = {Satadal Saha and Subhadip Basu and Mita Nasipuri and Dipak Kr. Basu},
  title   = {A Hough Transform based Technique for Text Segmentation},
  journal = {arxiv:1002.4048},
  year    = {2010},
  url     = {http://arxiv.org/abs/1002.4048v1},
}

@Article{2004A.arxiv:cs/0412083,
  author  = {A. Marcolino and Vitorino Ramos and Mario Ramalho and J. R. Caldas Pinto},
  title   = {Line and Word Matching in Old Documents},
  journal = {arxiv:cs/0412083},
  year    = {2004},
  url     = {http://arxiv.org/abs/cs/0412083v1},
}

@Article{2001Raduarxiv:cs/0107021,
  author  = {Radu Florian and Grace Ngai},
  title   = {Multidimensional Transformation-Based Learning},
  journal = {arxiv:cs/0107021},
  year    = {2001},
  url     = {http://arxiv.org/abs/cs/0107021v1},
}

@Article{1995Antonioarxiv:cmp-lg/9507004,
  author  = {Antonio Moreno and José M. Goñi},
  title   = {GRAMPAL: A Morphological Processor for Spanish implemented in Prolog},
  journal = {arxiv:cmp-lg/9507004},
  year    = {1995},
  url     = {http://arxiv.org/abs/cmp-lg/9507004v1},
}

@Article{doanalise,
  author = {do Vale, Rafaella Ferreira},
  title  = {An{\'a}lise comparativa de m{\'e}todos de simplifica{\c{c}}{\~a}o de senten{\c{c}}as para sumariza{\c{c}}{\~a}o de textos},
}

@Article{silvaavaliaccao,
  author = {SILVA, PRISCILLA DE SOUZA},
  title  = {AVALIA{\c{C}}{\~A}O DO DESEMPENHO DE M{\'E}TODOS DE AN{\'A}LISE DE SENTIMENTOS NA PRESEN{\c{C}}A DAS FIGURAS DE LINGUAGEM SARCASMO E IRONIA},
}

@Article{meiraclassificando,
  author  = {Meira, S{\'a}vio S and Martins, Claudia A and Figueiredo, Josiel M and Bonfante, Andreia G and J{\'u}nior, Vicente A Concei{\c{c}}{\~a}o and Arinos, Victor F},
  title   = {Classificando patentes utilizando a arquitetura GATE},
  journal = {Anais da VI Escola Regional de Inform{\'a}tica da Sociedade Brasileira de Computa{\c{c}}{\~a}o (SBC)-Regional de Mato Grosso},
  pages   = {84},
}

@Article{campanoferramenta,
  author = {Campano, Maurilio Martins and Sachs, Cinthyan Renata and de Barbosa, Camerlengo},
  title  = {Ferramenta para o aux{\'\i}lio no ensino-aprendizagem de pragas na cultura da soja},
}

@Article{garciainterfaces,
  author = {Garcia, Laura S},
  title  = {Interfaces em Linguagem Natural Orientadas por Menus: uma An{\'a}lise Cr{\'\i}tica do Ambiente},
}

@Article{barbosa1introduccao,
  author  = {Barbosa, Jardeson Leandro Nascimento and Vieira, Jo{\~a}o Paulo Albuquerque and Santos, Roney Lira de Sales and Junior, Gilvan Veras Magalh{\~a}es and MUNIZ, Mariana dos Santos and Moura, Raimundo Santos},
  title   = {Introdu{\c{c}}{\~a}o ao Processamento de Linguagem Natural usando Python},
  journal = {III ESCOLA REGIONAL DE INFORM{\'A}TICA DO PIAU{\'I}},
  volume  = {1},
  pages   = {336--360},
}

@Article{moreira11uso,
  author  = {Moreira, Leonard Barreto and Tamariz, Annabell Del Real and Fettermann, Joyce Vieira},
  title   = {O uso da minera{\c{c}}{\~a}o de textos no suporte a corre{\c{c}}{\~o}es de quest{\~o}es discursivas em uma institui{\c{c}}{\~a}o de educa{\c{c}}{\~a}o superior/The use of texts mining in the support to corrections of discursive questions in a higher education institution},
  journal = {Texto Livre: Linguagem e Tecnologia},
  volume  = {11},
  number  = {3},
  pages   = {213--227},
}

@Article{do13saussure,
  author  = {do Nascimento, Lucas},
  title   = {SAUSSURE: A SEMIOLOGIA E O L{\'E}XICO NO ENSINO DE L{\'I}NGUA PORTUGUESA NO BRASIL},
  journal = {Cadernos do CNLF},
  volume  = {13},
  number  = {04},
}

@Article{silva2019analise,
  author = {Silva, Diogo Borges da},
  title  = {An{\'a}lise da plataforma de comunica{\c{c}}{\~a}o slack para descoberta de informa{\c{c}}{\~o}es de processos de neg{\'o}cio},
  year   = {2019},
}

@Article{teixeira2019analise,
  author = {Teixeira, Carolina Barcelos},
  title  = {An{\'a}lise de sentimento dos usu{\'a}rios do twitter em rela{\c{c}}{\~a}o {\`a} atual situa{\c{c}}{\~a}o pol{\'\i}tica do Brasil.},
  year   = {2019},
}

@InProceedings{roque2019sistema,
  author       = {Roque, Carolinne and Junior, Maurilio Martins Campano and de Barbosa, Cinthyan Renata Sachs C and others},
  title        = {Sistema de Apoio {\`a} Decis{\~a}o por PLN para consultas de Pragas na Cultura da Soja},
  booktitle    = {Anais do XLVI Semin{\'a}rio Integrado de Software e Hardware},
  year         = {2019},
  pages        = {45--56},
  organization = {SBC},
}

@Article{de2018classificaccao,
  author  = {de Abreu Batista, Rodrigo and Bagatini, Daniela DS and Frozza, Rejane},
  title   = {Classifica{\c{c}}{\~a}o Autom{\'a}tica de C{\'o}digos NCM Utilizando o Algoritmo Na{\"\i}ve Bayes},
  journal = {iSys-Revista Brasileira de Sistemas de Informa{\c{c}}{\~a}o},
  year    = {2018},
  volume  = {11},
  number  = {2},
  pages   = {4--29},
}

@Article{alles2018construccao,
  author = {Alles, Vanderlei Jandir},
  title  = {Constru{\c{c}}{\~a}o de um corpus para extrair entidades nomeadas do Di{\'a}rio Oficial da Uni{\~a}o utilizando aprendizado supervisionado},
  year   = {2018},
}

@Article{boito2018unsupervised,
  author = {Boito, Marcely Zanon},
  title  = {Unsupervised word discovery using attentional encoder decoder models},
  year   = {2018},
}

@Article{batista2017classificaccao,
  author = {Batista, Rodrigo de Abreu},
  title  = {Classifica{\c{c}}{\~a}o autom{\'a}tica de c{\'o}digos NCM utilizando o algoritmo Na{\"\i}ve Bayes.},
  year   = {2017},
}

@Article{silva2017mineraccao,
  author = {Silva, Matheus Moreira},
  title  = {Minera{\c{c}}{\~a}o de dados no Twitter: uma ferramenta pr{\'a}tica para extra{\c{c}}{\~a}o e an{\'a}lise dos resultados.},
  year   = {2017},
}

@MastersThesis{almeida2017portservice,
  author = {ALMEIDA, WR},
  title  = {PortService-Br: Uma Plataforma para Processamento de Linguagem Natural da L{\'\i}ngua Portuguesa},
  school = {Universidade Federal do Esp{\'\i}rito Santo},
  year   = {2017},
}

@PhdThesis{santos2015classificaccao,
  author = {Santos, Cedric Michael dos},
  title  = {Classifica{\c{c}}{\~a}o de documentos com processamento de linguagem natural},
  year   = {2015},
}

@Article{almeida2014classificadores,
  author = {Almeida, Filipe Guedes de Oliveira},
  title  = {Classificadores de polaridade de not{\'\i}cias utilizando ferramentas de machine learning: o caso da Vale SA},
  year   = {2014},
}

@Article{abreu2014extraccao,
  author    = {Abreu, Sandra Collovini de and others},
  title     = {Extra{\c{c}}{\~a}o de Rela{\c{c}}{\~o}es do Dom{\'\i}nio de Organiza{\c{c}}{\~o}es para o Portugu{\^e}s},
  year      = {2014},
  publisher = {Pontif{\'\i}cia Universidade Cat{\'o}lica do Rio Grande do Sul},
}

@Article{carvalho2014mineraccao,
  author = {CARVALHO FILHO, JOS{\'E} ADAIL},
  title  = {MINERA{\c{C}}{\~A}O DE TEXTOS: AN{\'A}LISE DE SENTIMENTO UTILIZANDO TWEETS REFERENTES {\`A} COPA DO MUNDO 2014},
  year   = {2014},
}

@Book{de2014psicopatia,
  title     = {Psicopatia e Linguagem},
  publisher = {Chiado Editorial},
  year      = {2014},
  author    = {de Almeida Brites, Jos{\'e}},
}

@PhdThesis{carregosa2014sistema,
  author = {Carregosa, Felipe Borda},
  title  = {SISTEMA WEB INTERATIVO E ADAPTATIVO DE RESPOSTA AUTOM{\'A}TICA},
  school = {Universidade Federal do Rio de Janeiro},
  year   = {2014},
}

@PhdThesis{bonadio2013desenvolvimento,
  author = {Bonadio, {\'I}gor and Durham, Alan Mitchell},
  title  = {Desenvolvimento de um arcabou{\c{c}}o probabil{\'\i}stico para implementa{\c{c}}{\~a}o de campos aleat{\'o}rios condicionais},
  school = {Universidade de S{\~a}o Paulo},
  year   = {2013},
}

@Article{castro2013extraccao,
  author    = {CASTRO, Robinson Santos},
  title     = {Extra{\c{c}}{\~a}o de informa{\c{c}}{\~a}o: conceitos, plataformas e sistemas},
  year      = {2013},
  publisher = {Universidade Federal do Maranh{\~a}o},
}

@PhdThesis{gomes2013text,
  author = {Gomes, Helder Joaquim Carvalheira},
  title  = {Text Mining: an{\'a}lise de sentimentos na classifica{\c{c}}{\~a}o de not{\'\i}cias},
  year   = {2013},
}

@MastersThesis{oliveira2013metodo,
  author = {Oliveira, Hil{\'a}rio Tomaz Alves de},
  title  = {Um m{\'e}todo n{\~a}o supervisionado para o povoamento de ontologias na web},
  school = {Universidade Federal de Pernambuco},
  year   = {2013},
}

@Article{da2010uso,
  author = {DA SILVA, ANA CRISTINA CUNHA},
  title  = {O USO DE REDES NEURAIS AUTO-ORGANIZ{\'A}VEIS PARA A AN{\'A}LISE DO CONHECIMENTO ACENTUAL EM APRENDIZES BRASILEIROS DE L{\'I}NGUA INGLESA \_},
  year   = {2010},
}

@Article{pacheco2009morphomap,
  author    = {Pacheco, Edson Jos{\'e}},
  title     = {MorphoMap: mapeamento autom{\'a}tico de narrativas cl{\'\i}nicas para uma terminologia m{\'e}dica},
  year      = {2009},
  publisher = {Universidade Tecnol{\'o}gica Federal do Paran{\'a}},
}

@PhdThesis{barbas2008extracccao,
  author = {Barbas, J{\'u}lio Fl{\'a}vio Tavares},
  title  = {Extrac{\c{c}}{\~a}o autom{\'a}tica de informa{\c{c}}{\~a}o e conhecimento em textos no {\^a}mbito B2B},
  school = {FCT-UNL},
  year   = {2008},
}

@Article{borincross,
  author = {Borin, Lars},
  title  = {Is a cross-linguistic typology of multiword expressions useful or even possible?},
}

@Article{vareman,
  author = {Vare, Kadri and Tavast, Arvi and Fi{\v{s}}el, Mark and Zupping, Sirli and Luts, Martin},
  title  = {Man against Machine: Qualitative Comparison of Original, Translated and Post-edited Wikipedia Articles},
}

@Misc{neubignlp,
  author = {Neubig, Graham},
  title  = {NLP Programming Tutorial 8-Phrase Structure Parsing},
}

@InProceedings{peng2019web,
  author       = {Peng, Yu and Junju, Liu and Jian, Wang},
  title        = {A Web Service Matchmaking Approach based on Topic Models},
  booktitle    = {2019 IEEE 9th International Conference on Electronics Information and Emergency Communication (ICEIEC)},
  year         = {2019},
  pages        = {604--607},
  organization = {IEEE},
}

@Article{benlahbib2019unsupervised,
  author    = {Benlahbib, Abdessamad and Nfaoui, El Habib},
  title     = {An Unsupervised Approach for Reputation Generation},
  journal   = {Procedia computer science},
  year      = {2019},
  volume    = {148},
  pages     = {80--86},
  publisher = {Elsevier},
}

@Article{xia2019generalized,
  author  = {Xia, Mengzhou and Kong, Xiang and Anastasopoulos, Antonios and Neubig, Graham},
  title   = {Generalized Data Augmentation for Low-Resource Translation},
  journal = {arXiv preprint arXiv:1906.03785},
  year    = {2019},
}

@InProceedings{briakou2019university,
  author    = {Briakou, Eleftheria and Carpuat, Marine},
  title     = {The University of Maryland’s Kazakh-English Neural Machine Translation System at WMT19},
  booktitle = {Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)},
  year      = {2019},
  pages     = {134--140},
}

@Article{stymne2018parser,
  author  = {Stymne, Sara and de Lhoneux, Miryam and Smith, Aaron and Nivre, Joakim},
  title   = {Parser Training with Heterogeneous Treebanks},
  journal = {arXiv preprint arXiv:1805.05089},
  year    = {2018},
}

@InProceedings{manrique2018performance,
  author       = {Manrique, B and Solari, M and Zapata, CM},
  title        = {Performance analysis of a text processing architecture for knowledge acquisition in requirements engineering},
  booktitle    = {Proceedings of the Euro American Conference on Telematics and Information Systems},
  year         = {2018},
  pages        = {31},
  organization = {ACM},
}

@InProceedings{weissweiler2017developing,
  author       = {Weissweiler, Leonie and Fraser, Alexander},
  title        = {Developing a Stemmer for German Based on a Comparative Analysis of Publicly Available Stemmers},
  booktitle    = {International Conference of the German Society for Computational Linguistics and Language Technology},
  year         = {2017},
  pages        = {81--94},
  organization = {Springer},
}

@InProceedings{ma2017event,
  author       = {Ma, Meng and Liu, Ling and Lin, Yangxin and Pan, Disheng and Wang, Ping},
  title        = {Event Description and Detection in Cyber-Physical Systems: An Ontology-Based Language and Approach},
  booktitle    = {2017 IEEE 23rd International Conference on Parallel and Distributed Systems (ICPADS)},
  year         = {2017},
  pages        = {1--8},
  organization = {IEEE},
}

@Article{trommer2017inflectional,
  author    = {Trommer, Jochen and Bank, Sebastian},
  title     = {Inflectional learning as local optimization},
  journal   = {Morphology},
  year      = {2017},
  volume    = {27},
  number    = {3},
  pages     = {383--422},
  publisher = {Springer},
}

@InProceedings{zhang2017latent,
  author       = {Zhang, Pei and Zanni-Merk, Cecilia and Cavallucci, Denis},
  title        = {Latent semantic indexing for capitalizing experience in inventive design},
  booktitle    = {International Conference on Sustainable Design and Manufacturing},
  year         = {2017},
  pages        = {37--47},
  organization = {Springer},
}

@InProceedings{monteleone2017nooj,
  author       = {Monteleone, Mario and Guarasci, Raffaele and Maisto, Alessandro},
  title        = {NooJ Morphological Grammars for Stenotype Writing},
  booktitle    = {International Conference on Automatic Processing of Natural-Language Electronic Texts with NooJ},
  year         = {2017},
  pages        = {200--212},
  organization = {Springer},
}

@InProceedings{sheshasaayee2016ascertaining,
  author       = {Sheshasaayee, Ananthi and VR, Angela Deepa},
  title        = {Ascertaining the morphological components of Tamil language using unsupervised approach},
  booktitle    = {2016 Online International Conference on Green Engineering and Technologies (IC-GET)},
  year         = {2016},
  pages        = {1--6},
  organization = {IEEE},
}

@InProceedings{iqbal2016multi,
  author       = {Iqbal, Mohammad and Riesvicky, Hifshan and Rasjid, Hasma and Charli, Yulia},
  title        = {Multi Level Filtering to Classify and Block Undesirable Explicit Material in Website},
  booktitle    = {Proceedings of Second International Conference on Electrical Systems, Technology and Information 2015 (ICESTI 2015)},
  year         = {2016},
  pages        = {553--563},
  organization = {Springer},
}

@Article{sato2016operation,
  author  = {Sato, Toshinori and Hashimoto, Taiichi and Okumura, Manabu},
  title   = {Operation of a word segmentation dictionary generation system called NEologd},
  journal = {Information Processing Society of Japan, Special Interest Group on Natural Language Processing (IPSJ-SIGNL)},
  year    = {2016},
}

@InProceedings{burlot2016unsupervised,
  author = {Burlot, Franck and Yvon, Fran{\c{c}}ois},
  title  = {Unsupervised learning of morphology in the USSR},
  year   = {2016},
}

@Article{nishiguchi2015learning,
  author  = {Nishiguchi, Sumiyo},
  title   = {A Learning Model of Lexicon},
  journal = {情報処理学会第 77 回全国大会},
  year    = {2015},
  volume  = {3},
  pages   = {01},
}

@InProceedings{linzen2015model,
  author    = {Linzen, Tal and O’Donnell, Timothy},
  title     = {A model of rapid phonotactic generalization},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  year      = {2015},
  pages     = {1126--1131},
}

@Article{rajput2015survey,
  author  = {Rajput, Brajendra Singh and NilayKhare, A},
  title   = {A survey of stemming algorithms for information retrieval},
  journal = {IOSR Journal of Computer Engineering},
  year    = {2015},
  volume  = {17},
  number  = {3},
  pages   = {76--80},
}

@Article{kumar2015enhancing,
  author    = {Kumar, Suresh and Goel, Shivani},
  title     = {Enhancing Text Classification by Stochastic Optimization method and Support Vector Machine},
  journal   = {International Journal of Computer Science and Information Technologies},
  year      = {2015},
  volume    = {6},
  number    = {4},
  publisher = {Citeseer},
}

@InProceedings{liu2014annotation,
  author    = {Liu, Qun},
  title     = {Annotation Adaptation and Language Adaptation in NLP},
  booktitle = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  year      = {2014},
}

@Article{gondse2014main,
  author  = {Gondse, Ms Pranjali G and Raut, A},
  title   = {Main content extraction from web page using DOM},
  journal = {International Journal of Advanced Research in Computer and Communication Engineering},
  year    = {2014},
  volume  = {3},
  pages   = {5302},
}

@InProceedings{may2014particle,
  author    = {May, Chandler and Clemmer, Alex and Van Durme, Benjamin},
  title     = {Particle filter rejuvenation and latent dirichlet allocation},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  year      = {2014},
  pages     = {446--451},
}

@InProceedings{clouet2014splitting,
  author = {Clouet, Elizaveta Loginova and Daille, B{\'e}atrice},
  title  = {Splitting of compound terms in non-prototypical compounding languages},
  year   = {2014},
}

@InProceedings{du2014topic,
  author       = {Du, Lan and Pate, John K and Johnson, Mark},
  title        = {Topic models with topic ordering regularities for topic segmentation},
  booktitle    = {2014 IEEE International Conference on Data Mining},
  year         = {2014},
  pages        = {803--808},
  organization = {IEEE},
}

@Article{roshani2014unsupervised,
  author = {Roshani, Asra},
  title  = {Unsupervised segmentation of sequences using harmony search and hierarchical clustering techniques},
  year   = {2014},
}

@Article{simoes2013labeled,
  author  = {Sim{\~o}es, Gon{\c{c}}alo and Galhardas, Helena and Matos, David},
  title   = {A Labeled Graph Kernel for Relationship Extraction},
  journal = {arXiv preprint arXiv:1302.4874},
  year    = {2013},
}

@InProceedings{bando2013automatic,
  author       = {Bando, Takashi and Takenaka, Kazuhito and Nagasaka, Shogo and Taniguchi, Tadairo},
  title        = {Automatic drive annotation via multimodal latent topic model},
  booktitle    = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year         = {2013},
  pages        = {2744--2749},
  organization = {IEEE},
}

@InProceedings{demidova2013efficient,
  author       = {Demidova, Elena and Zhou, Xuan and Nejdl, Wolfgang},
  title        = {Efficient query construction for large scale data},
  booktitle    = {Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval},
  year         = {2013},
  pages        = {573--582},
  organization = {ACM},
}

@InProceedings{wang2013ontology,
  author       = {Wang, Shi Liu and Zhang, Gong Jie},
  title        = {Ontology based Domain Resource Semantic Retrieval Model},
  booktitle    = {Applied Mechanics and Materials},
  year         = {2013},
  volume       = {347},
  pages        = {2804--2808},
  organization = {Trans Tech Publ},
}

@InProceedings{yun2013sub,
  author       = {Yun, Xiao Yan and Teng, Wei},
  title        = {Sub-topic Segmentation in Multi-document},
  booktitle    = {Advanced Materials Research},
  year         = {2013},
  volume       = {756},
  pages        = {2958--2961},
  organization = {Trans Tech Publ},
}

@Article{pande2013suffix,
  author  = {Pande, BP and Tamta, Pawan and Dhami, HS},
  title   = {Suffix Stripping Problem as an Optimization Problem},
  journal = {arXiv preprint arXiv:1312.6802},
  year    = {2013},
}

@InProceedings{bando2013unsupervised,
  author       = {Bando, Takashi and Takenaka, Kazuhito and Nagasaka, Shogo and Taniguchi, Tadairo},
  title        = {Unsupervised drive topic finding from driving behavioral data},
  booktitle    = {2013 IEEE Intelligent Vehicles Symposium (IV)},
  year         = {2013},
  pages        = {177--182},
  organization = {IEEE},
}

@Article{levy2013grammar,
  author = {Levy, Roger},
  title  = {Why Grammar is Probabilistic},
  year   = {2013},
}

@Article{petic2012annotation,
  author  = {Petic, Mircea},
  title   = {Annotation on PhD Thesis},
  journal = {Computer Science},
  year    = {2012},
  volume  = {20},
  number  = {1},
  pages   = {58},
}

@Article{mircea2012automation,
  author = {Mircea, Petic},
  title  = {Automation of the process of computational linguistic resources creation},
  year   = {2012},
}

@InProceedings{hermann2012learning,
  author       = {Hermann, Karl Moritz and Blunsom, Phil and Dyer, Chris and Pulman, Stephen},
  title        = {Learning semantics and selectional preference of adjective-noun pairs},
  booktitle    = {Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation},
  year         = {2012},
  pages        = {70--74},
  organization = {Association for Computational Linguistics},
}

@InProceedings{zhu2012semantic,
  author       = {Zhu, Xinyan and Li, Ming and Guo, Wei and Zhang, Xia},
  title        = {Semantic-based user demand modeling for remote sensing images retrieval},
  booktitle    = {2012 IEEE International Geoscience and Remote Sensing Symposium},
  year         = {2012},
  pages        = {2902--2905},
  organization = {IEEE},
}

@InProceedings{mendez2012testing,
  author    = {M{\'e}ndez-Cruz, Carlos-Francisco and Soriano-Morales, Edmundo-Pavel and Medina-Urrea, Alfonso},
  title     = {Testing a statistical word stemmer based on affixality measurements in inex 2012 tweet contextualization track},
  booktitle = {Copyright cG2012 remains with the author/owner (s). The unreviewed pre-proceedings are collections of work submitted before the December workshops. They are not peer reviewed, are not quality controlled, and contain known errors in content and editing. The proceedings, published after the Workshop, is the authoritative reference for the work done at INEX.},
  year      = {2012},
  pages     = {194},
}

@InProceedings{ozkan2012turkish,
  author       = {{\"O}zkan, Sava{\c{s}}},
  title        = {Turkish document semantic categorization using web-based encyclopedia article association},
  booktitle    = {2012 International Symposium on Innovations in Intelligent Systems and Applications},
  year         = {2012},
  pages        = {1--5},
  organization = {IEEE},
}

@InProceedings{sadrzadeh2011compositional,
  author       = {Sadrzadeh, Mehrnoosh and Grefenstette, Edward},
  title        = {A compositional distributional semantics, two concrete constructions, and some experimental evaluations},
  booktitle    = {International Symposium on Quantum Interaction},
  year         = {2011},
  pages        = {35--47},
  organization = {Springer},
}

@Article{goldwater2011book,
  author  = {Goldwater, Sharon},
  title   = {Book Reviews: Computational Modeling of Human Language Acquisition by Afra Alishahi},
  journal = {Computational Linguistics},
  year    = {2011},
  volume  = {37},
  number  = {3},
}

@Article{goldberg2011computational,
  author = {Goldberg, Yoav},
  title  = {Computational Linguistics+ NLP},
  year   = {2011},
}

@Misc{alishahi2011computational,
  author = {Alishahi, Afra},
  title  = {Computational Modeling of Human Language Acquisition},
  year   = {2011},
}

@Article{reviewer2011computational,
  author    = {Reviewer-Goldwater, Sharon},
  title     = {Computational modeling of human language acquisition afra alishahi morgan \& claypool (synthesis lectures on human language technologies, edited by graeme hirst, volume 11), 2010, xiv+ 93 pp; paperbound, isbn 978-1-60845-339-9, $40.00; ebook, isbn 978-1-60845-340-5, $30.00 or by subscription},
  journal   = {Computational Linguistics},
  year      = {2011},
  volume    = {37},
  number    = {3},
  pages     = {627--629},
  publisher = {MIT Press},
}

@InProceedings{wang2011improving,
  author    = {Wang, Y and Kazama, J and Tsuruoka, Y and Chen, W and Zhang, Y and Torisawa, K},
  title     = {Improving word segmentationChinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data},
  booktitle = {Proceedings of 5th International Joint Conference on Natural Language Processing},
  year      = {2011},
  pages     = {309--317},
}

@InProceedings{ganjisaffar2011qspell,
  author    = {Ganjisaffar, Yasser and Zilio, Andrea and Javanmardi, Sara and Cetindil, Inci and Sikka, Manik and Katumalla, Sandeep and Khatib, Narges and Li, Chen and Lopes, Cristina},
  title     = {qspell: Spelling correction of web search queries using ranking models and iterative correction},
  booktitle = {Spelling Alteration for Web Search Workshop},
  year      = {2011},
  pages     = {15},
}

@InProceedings{peng2011research,
  author       = {Peng, Feifei and Qian, Xu and Meng, Hui and Zhou, Dan and Li, Gaoren},
  title        = {Research on algorithm of extracting micro-blog's hot topics},
  booktitle    = {2011 International Conference on Electronics, Communications and Control (ICECC)},
  year         = {2011},
  pages        = {986--989},
  organization = {IEEE},
}

@InProceedings{galiotou2011towards,
  author    = {Galiotou, Eleni},
  title     = {Towards the Preservation and Availability of Historical Books and Manuscripts: A Case Study},
  booktitle = {Symposium on Information and Knowledge Management},
  year      = {2011},
}

@InProceedings{goldwater2011unsupervised,
  author       = {Goldwater, Sharon},
  title        = {Unsupervised NLP and human language acquisition: making connections to make progress},
  booktitle    = {Proceedings of the First workshop on Unsupervised Learning in NLP},
  year         = {2011},
  pages        = {1--1},
  organization = {Association for Computational Linguistics},
}

@InProceedings{litvak2010new,
  author       = {Litvak, Marina and Last, Mark and Friedman, Menahem},
  title        = {A new approach to improving multilingual summarization using a genetic algorithm},
  booktitle    = {Proceedings of the 48th annual meeting of the association for computational linguistics},
  year         = {2010},
  pages        = {927--936},
  organization = {Association for Computational Linguistics},
}

@Article{sun2010decoding,
  author  = {Sun, Xu and Tsujii, Jun’ichi},
  title   = {Decoding in Latent Conditional Models: A Practically Fast Solution for an NP-hard Problem},
  journal = {CRF},
  year    = {2010},
  volume  = {2},
  pages   = {x1},
}

@InProceedings{dai2010event,
  author       = {Dai, Xiangying and Sun, Yunlian},
  title        = {Event identification within news topics},
  booktitle    = {2010 International Conference on Intelligent Computing and Integrated Systems},
  year         = {2010},
  pages        = {498--502},
  organization = {IEEE},
}

@InProceedings{seaghdha2010latent,
  author    = {S{\'e}aghdha, Diarmuid O},
  title     = {Latent variable models of selectional preference},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  year      = {2010},
  pages     = {435--444},
}

@Misc{fibla_bilingual_nodate,
  author   = {Fibla, Laia},
  title    = {Bilingual {Word} {Segmentation}},
  abstract = {This project aims to model balanced bilinguals and monolinguals of 3 different languages, with language switching happening every other sentence, or every 100 sentences.

We test the performance of different algorithms on word segmentation that represent different coginitve strategies that infants could be brining into the word segmentation task.

This repository contains the collected corpora from English, Catalan and Spanish which are also freely available in CHILDES https://childes.talkbank.org As well as all the steps to process the data wich are descrived in the recipes of each language.},
  url      = {https://github.com/laiafr/bilingual_wordseg},
  urldate  = {2019-09-19},
}

@Misc{baziotis_ekphrasis_nodate,
  author   = {Baziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos},
  title    = {Ekphrasis},
  abstract = {Ekphrasis is a text processing tool, geared towards text from social networks, such as Twitter or Facebook. Ekphrasis performs tokenization, word normalization, word segmentation (for splitting hashtags) and spell correction, using word statistics from 2 big corpora (english Wikipedia, twitter - 330mil english tweets).},
  url      = {https://github.com/cbaziotis/ekphrasis},
  urldate  = {2019-09-19},
}

@Misc{garbe_fast_nodate,
  author   = {Garbe, Wolf},
  title    = {Fast {Word} {Segmentation} of {Noisy} {Text}},
  abstract = {A string can be divided in several ways. Each distinct segmentation variant is called a composition. This article evaluates different types of word segmentation and propose several algorithms for each one of the basics concepts},
  language = {Inglês},
  url      = {https://towardsdatascience.com/fast-word-segmentation-for-noisy-text-2c2c41f9e8da},
  urldate  = {2019-09-19},
}

@Misc{lanxiaowei_ik-analyzer_nodate,
  author   = {Lanxiaowei},
  title    = {{IK}-{Analyzer}},
  abstract = {The source code of IK-Analyzer,Supported Arabic numerals and Chinese characters, Chinese figures and Chinese characters and Arabic Numbers and English letters of the word segmentation},
  url      = {https://github.com/yida-lxw/IK},
  urldate  = {2019-09-19},
}

@Misc{jenks_python_nodate,
  author   = {Jenks, Grant},
  title    = {Python {Word} {Segmentation}},
  abstract = {Based on code from the chapter "Natural Language Corpus Data" by Peter Norvig from the book "Beautiful Data" (Segaran and Hammerbacher, 2009). 
Data files are derived from the Google Web Trillion Word Corpus, as described by Thorsten Brants and Alex Franz, and distributed by the Linguistic Data Consortium. This module contains only a subset of that data. The unigram data includes only the most common 333,000 words. Similarly, bigram data includes only the most common 250,000 phrases. Every word and phrase is lowercased with punctuation removed.},
  url      = {https://github.com/grantjenks/python-wordsegment},
  urldate  = {2019-09-19},
}

@Misc{carvalho_shenmeci_nodate,
  author   = {Carvalho, Rodolfo},
  title    = {shenmeci},
  abstract = {Chinese word segmentation and Chinese-English online dictionary},
  url      = {https://github.com/rhcarvalho/shenmeci},
  urldate  = {2019-09-19},
}

@TechReport{norvig_statistical_nodate,
  author   = {Norvig, Peter},
  title    = {Statistical {Natural} {Language} {Processing} in {Python}. or {How} {To} {Do} {Things} {With} {Words}. {And} {Counters}. or {Everything} {I} {Needed} to {Know} {About} {NLP} {I} learned {From} {Sesame} {Street}. {Except} {Kneser}-{Ney} {Smoothing}. {The} {Count} {Didn}'t {Cover} {That}.},
  type     = {Relaçtório {Técnico}},
  abstract = {In this notebook Peter Norvig show us how to work with statistical natural language processing. With a theorical/pratical approach, Norvig shows the fundamentals about word segmentation using python.},
  language = {Inglês},
  url      = {https://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb},
  urldate  = {2019-09-19},
}

@Misc{vittal_text-reconstruction_nodate,
  author   = {Vittal, Sujay},
  title    = {Text-{Reconstruction}},
  abstract = {This project involves two tasks - word segmentation and vowel insertion. Word segmentation often comes up when processing many non-English languages, in which words might not be flanked by spaces on either end, such as written Chinese or long compound German words. Vowel insertion is relevant for languages like Arabic or Hebrew, where modern script eschews notations for vowel sounds and the human reader infers them from context. The goal of Vowel Insertion is to insert vowels back into segmented words in a way that maximizes sentence fluency (i.e., minimizes sentence cost). A bigram cost function is used.},
  url      = {https://github.com/sujay-vittal/Text-Reconstruction},
  urldate  = {2019-09-19},
}

@Misc{li_uninlp-phd_nodate,
  author   = {Li, Xinxin},
  title    = {uninlp-phd},
  abstract = {Java codes for basic natural language processing tasks, including Pinyin-to-Character Conversion, Chinese word segmentation, Part-of-Speech tagging, English chunking, dependency parsing},
  url      = {https://github.com/xxli/uninlp-phd},
  urldate  = {2019-09-19},
}

@Misc{frcchang_zpar_nodate,
  author   = {frcchang},
  title    = {{ZPar}},
  abstract = {ZPar statistical parser. Universal language support (depending on the availability of training data), with language-specific features for Chinese and English. Currently support word segmentation, POS tagging, dependency and phrase-structure parsing.},
  url      = {https://github.com/frcchang/zpar},
  urldate  = {2019-09-19},
}

@InProceedings{8682764,
  author    = {M. {Vetter} and S. {Sakti} and S. {Nakamura}},
  title     = {Cross-lingual Speech-based Tobi Label Generation Using Bidirectional Lstm},
  booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2019},
  pages     = {6620-6624},
  month     = {May},
  abstract  = {In this paper we investigate the automatic generation of ToBI-style prosody labels. The work is motivated by the idea of using prosodic information to facilitate the automatic lexicon discovery for unseen and under-resourced languages for which sufficient training data is not available. Specifically, the prosodic boundaries are meant to serve as additional top-down information in the word segmentation step. To this end we attempt to apply the trained Japanese models cross-lingually on a language not seen in training (English). We generate break index labels, using only the speech signal as input, with no additional information given at test time in the form of transcripts or prior word segmentations. The labels are generated using bidirectional LSTMs trained on spontaneous Japanese speech. We evaluate the quality of these labels using established metrics, with an F1 score of 0.55 for cross-lingual prosodic break detection (given a tolerance of 80 ms).},
  doi       = {10.1109/ICASSP.2019.8682764},
  keywords  = {natural language processing;speech recognition;speech synthesis;cross-lingual speech-based tobi label generation;ToBI-style prosody labels;prosodic boundaries;word segmentation step;speech signal;spontaneous Japanese speech;cross-lingual prosodic break detection;Japanese models;bidirectional LSTM;word segmentations;Training;Indexes;Task analysis;Data models;Neural networks;Speech recognition;Labeling;Prosody detection;ToBI label generation;cross-lingual speech processing;word segmentation},
}

@Article{8811494,
  author   = {Q. {Xie} and X. {Zhou} and J. {Wang} and X. {Gao} and X. {Chen} and C. {Liu}},
  title    = {Matching Real-World Facilities to Building Information Modeling Data Using Natural Language Processing},
  journal  = {IEEE Access},
  year     = {2019},
  volume   = {7},
  pages    = {119465-119475},
  abstract = {Building Information Modeling (BIM) is a promising technology for building informatics. Currently, an increasing number of applications adopt BIM to improve the building operations and facility management. In these applications, matching real-world facilities to the corresponding BIM items is a fundamental yet challenging task. This study addresses this issue using Natural Language Processing. Firstly, a novel BIM hierarchy tree (HiTree) is proposed to model the original spatial structure relationships of a BIM. Then, the locations of facilities are extracted from natural language through processes of word segmentation, keyword extraction, and semantic disambiguation. Thirdly, an algorithm that matches real-world facilities to the BIM data is developed using the HiTree and the extracted locations. Finally, a concrete case for a 35,000 m2 library is presented to verify the effectiveness of the proposed solution. BIM has become a common paradigm in the construction industry, and our scheme can facilitate more applications of BIM in building operations and facility management. One of the most representative applications is integrating the BIM data and information within IoT (Internet of Things) system intelligently by matching the BIM data to real-world facilities.},
  doi      = {10.1109/ACCESS.2019.2937219},
  keywords = {Facilities management;Natural language processing;Semantics;Data mining;Architecture;Building information modeling (BIM);facility;natural language processing (NLP);facility management},
}

@InProceedings{8704816,
  author    = {L. {Mzamo} and A. {Helberg} and S. {Bosch}},
  title     = {Towards an unsupervised morphological segmenter for isiXhosa},
  booktitle = {2019 Southern African Universities Power Engineering Conference/Robotics and Mechatronics/Pattern Recognition Association of South Africa (SAUPEC/RobMech/PRASA)},
  year      = {2019},
  pages     = {166-170},
  month     = {Jan},
  abstract  = {In this paper, branching entropy techniques and isiXhosa language heuristics are adapted to develop unsupervised morphological segmenters for isiXhosa. An overview of isiXhosa segmentation issues is given, followed by a discussion on previous work in automated segmentation, and segmentation of isiXhosa in particular. Two unsupervised isiXhosa segmenters are presented and compared to a random minimum baseline and Morfessor-Baseline, a standard in unsupervised word segmentation. Morfessor-Baseline outperforms both isiXhosa segmenters at 79.10% boundary identification accuracy. The IsiXhosa Branching Entropy Segmenter (XBES) performance varies depending on the segmentation mode used, with a maximum of 73.39%. The IsiXhosa Heuristic Maximum Likelihood Segmenter (XHMLS) achieves 72.42%. The study suggests that unsupervised isiXhosa morphological segmentation is feasible with better optimization of the current attempts.},
  doi       = {10.1109/RoboMech.2019.8704816},
  keywords  = {entropy;image segmentation;maximum likelihood estimation;natural language processing;unsupervised learning;segmentation mode;unsupervised isiXhosa morphological segmentation;unsupervised morphological segmenter;isiXhosa language heuristics;isiXhosa segmentation issues;automated segmentation;unsupervised isiXhosa segmenters;random minimum baseline;Morfessor-Baseline;unsupervised word segmentation;IsiXhosa branching entropy segmenter performance;IsiXhosa heuristic maximum likelihood segmenter;natural language processing;unsupervised machine learning;morphological segmentation;isiXhosa},
}

@InProceedings{8639515,
  author    = {A. {Thual} and C. {Dancette} and J. {Karadayi} and J. {Benjumea} and E. {Dupoux}},
  title     = {A K-Nearest Neighbours Approach To Unsupervised Spoken Term Discovery},
  booktitle = {2018 IEEE Spoken Language Technology Workshop (SLT)},
  year      = {2018},
  pages     = {491-497},
  month     = {Dec},
  abstract  = {The following topics are dealt with: speech recognition; neural nets; speech processing; learning (artificial intelligence); natural language processing; recurrent neural nets; speaker recognition; speech synthesis; feature extraction; and text analysis.},
  doi       = {10.1109/SLT.2018.8639515},
  keywords  = {natural language processing;speech processing;speech recognition;neural nets;speech processing;learning (artificial intelligence);natural language processing;recurrent neural nets;speaker recognition;speech synthesis;feature extraction;text analysis;Pipelines;Acoustics;Approximation algorithms;Indexes;Feature extraction;Measurement;Clustering methods;spoken term discovery;word discovery;unsupervised;word segmentation},
}

@InProceedings{8461545,
  author    = {L. {Ondel} and P. {Godard} and L. {Besacier} and E. {Larsen} and M. {Hasegawa-Johnson} and O. {Scharenborg} and E. {Dupoux} and L. {Burget} and F. {Yvon} and S. {Khudanpur}},
  title     = {Bayesian Models for Unit Discovery on a Very Low Resource Language},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2018},
  pages     = {5939-5943},
  month     = {April},
  abstract  = {Developing speech technologies for low-resource languages has become a very active research field over the last decade. Among others, Bayesian models have shown some promising results on artificial examples but still lack of in situ experiments. Our work applies state-of-the-art Bayesian models to unsupervised Acoustic Unit Discovery (AUD) in a real low-resource language scenario. We also show that Bayesian models can naturally integrate information from other resourceful languages by means of informative prior leading to more consistent discovered units. Finally, discovered acoustic units are used, either as the I-best sequence or as a lattice, to perform word segmentation. Word segmentation results show that this Bayesian approach clearly outperforms a Segmental-DTW baseline on the same corpus.},
  doi       = {10.1109/ICASSP.2018.8461545},
  keywords  = {acoustic signal processing;Bayes methods;natural language processing;speech processing;low-resource language scenario;resourceful languages;discovered acoustic units;Bayesian approach;speech technologies;low-resource languages;active research field;Bayesian Models;Very Low Resource Language;unsupervised acoustic unit discovery;AUD;word segmentation;Segmental-DTW baseline;Hidden Markov models;Bayes methods;Data models;Mel frequency cepstral coefficient;Lattices;Measurement;Acoustic Unit Discovery;Low-Resource ASR;Bayesian Model;Informative Prior},
}

@InProceedings{8629163,
  author    = {C. {Ma} and J. {Yang}},
  title     = {Burmese Word Segmentation Method and Implementation Based on CRF},
  booktitle = {2018 International Conference on Asian Language Processing (IALP)},
  year      = {2018},
  pages     = {340-343},
  month     = {Nov},
  abstract  = {Burmese belongs to the languages whose writing system has no delimiters to mark word boundaries. However, related works on Burmese word segmentation are still at the initial stage. This paper aims to fill the blank by employing CRF model to the task. The performance of the CRF method is evaluated with confidence, precision of segmentation. We prepared an experimental database of 5,000 sentences, which were manually segmented by Burmese experts. After the 6-fold cross-validation of the experimental data set, the experimental results show that the average confidence level of the CRF method is 93.4%, which is greater than the threshold, and the average value of the F1 is 93.0%. Therefore, the CRF segmentation method satisfies the requirements for developing a Burmese speech synthesis system.},
  doi       = {10.1109/IALP.2018.8629163},
  keywords  = {natural language processing;speech synthesis;statistical analysis;word processing;CRF model;experimental database;experimental data;CRF segmentation method;Burmese speech synthesis system;Burmese word segmentation method;Burmese language;Encoding;Writing;Machine learning;Speech synthesis;Compounds;Tagging;Information science;Burmese language;word segmentation;CRF;confidence},
}

@InProceedings{8691202,
  author    = {T. {Zhao} and L. {Li} and Y. {Xie} and Y. {Lv}},
  title     = {Data-driven Risk Assessment for Peer-to-Peer Network Lending Agencies},
  booktitle = {2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems (CCIS)},
  year      = {2018},
  pages     = {799-803},
  month     = {Nov},
  abstract  = {With the rapid development of Peer-to-Peer(P2P) network lending in the financial field, more data of lending agencies have appeared. P2P agencies also have problems such as absconded with ill-gotten gains and out of business. Therefore, it is necessary to assess their risks based on P2P company data. This paper proposes a framework of Data-driven Risk Assessment for P2P(DRAP2P) network lending agencies based on unstructured natural language data. First, use the natural language processing technology, such as word segmentation, keyword, LDA topic model, word2vec and doc2vec, to process and extract features of company profile which reflect its business status. Then, seven machine learning classifiers and three deep learning models are used for analysis. Since keywords show good performance in machine learning models, we improve Convolutional Neural Network(CNN) with keywords and propose two CNN+Keyword models, namely CNN+Keyword(static+BP) and CNN+Keyword (Expand word embedding). Experiments have shown that CNN+Keyword(static+BP) can achieve the best performance. Finally, we use the method of meta-learning to integrate CNN+Keyword(static+BP) and logistic regression classifier to further strengthen the performance.},
  doi       = {10.1109/CCIS.2018.8691202},
  keywords  = {convolutional neural nets;feature extraction;financial data processing;learning (artificial intelligence);natural language processing;pattern classification;peer-to-peer computing;regression analysis;convolutional neural network;data-driven risk assessment;peer-to-peer network;machine learning classifiers;DRAP2P network lending agencies;peer-to-peer network lending agencies;logistic regression classifier;financial field;doc2vec;word2vec;natural language processing technology;unstructured natural language data;CNN+Keyword models;machine learning models;deep learning models;Data-driven;P2P risk assessment;Machine learning;Deep learning;Meta-learning},
}

@InProceedings{8462264,
  author    = {S. {Bhati} and H. {Kamper} and K. {Sri Rama Murty}},
  title     = {Phoneme Based Embedded Segmental K-Means for Unsupervised Term Discovery},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2018},
  pages     = {5169-5173},
  month     = {April},
  abstract  = {Identifying and grouping the frequently occurring word-like patterns from raw acoustic waveforms is an important task in the zero resource speech processing. Embedded segmental K-means (ES-KMeans) discovers both the word boundaries and the word types from raw data. Starting from an initial set of subword boundaries, the ES-Kmeans iteratively eliminates some of the boundaries to arrive at frequently occurring longer word patterns. Notice that the initial word boundaries will not be adjusted during the process. As a result, the performance of the ES-Kmeans critically depends on the initial subword boundaries. Originally, syllable boundaries were used to initialize ES-Kmeans. In this paper, we propose to use a phoneme segmentation method that produces boundaries closer to true boundaries for ES-KMeans initialization. The use of shorter units increases the number of initial boundaries which leads to a significant increment in the computational complexity. To reduce the computational cost, we extract compact lower dimensional embeddings from an auto-encoder. The proposed algorithm is benchmarked on Zero Resource 2017 challenge, which consists of 70 hours of unlabeled data across three languages, viz. English, French, and Mandarin. The proposed algorithm outperforms the baseline system without any language-specific parameter tuning.},
  doi       = {10.1109/ICASSP.2018.8462264},
  keywords  = {computational complexity;natural language processing;pattern clustering;speech processing;speech recognition;unsupervised learning;unsupervised term discovery;frequently occurring word-like patterns;raw acoustic waveforms;zero resource speech processing;word types;initial subword boundaries;syllable boundaries;phoneme segmentation method;ES-KMeans initialization;compact lower dimensional embeddings;embedded segmental K-means;computational complexity;auto-encoder;language-specific parameter tuning;Clustering algorithms;Speech processing;Kernel;Mel frequency cepstral coefficient;System performance;Task analysis;Zero Resource speech processing;unsupervised learning;spoken term discovery;word segmentation},
}

@InProceedings{8462002,
  author    = {Y. {Wang} and H. {Lee} and L. {Lee}},
  title     = {Segmental Audio Word2Vec: Representing Utterances as Sequences of Vectors with Applications in Spoken Term Detection},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2018},
  pages     = {6269-6273},
  month     = {April},
  abstract  = {While Word2Vec represents words (in text) as vectors carrying semantic information, audio Word2Vec was shown to be able to represent signal segments of spoken words as vectors carrying phonetic structure information. Audio Word2Vec can be trained in an unsupervised way from an unlabeled corpus, except the word boundaries are needed. In this paper, we extend audio Word2Vec from word-level to utterance-level by proposing a new segmental audio Word2Vec, in which unsupervised spoken word boundary segmentation and audio Word2Vec are jointly learned and mutually enhanced, so an utterance can be directly represented as a sequence of vectors carrying phonetic structure information. This is achieved by a segmental sequence-to-sequence autoencoder (SSAE), in which a segmentation gate trained with reinforcement learning is inserted in the encoder. Experiments on English, Czech, French and German show very good performance in both unsupervised spoken word segmentation and spoken term detection applications (significantly better than frame-based DTW).},
  doi       = {10.1109/ICASSP.2018.8462002},
  keywords  = {audio signal processing;natural language processing;recurrent neural nets;speech processing;speech recognition;text analysis;unsupervised learning;vectors;segmental audio word2vec;representing utterances;vectors sequences;spoken term detection;semantic information;phonetic structure information;segmental sequence-to-sequence autoencoder;SSAE;reinforcement learning;natural language processing;unsupervised spoken word boundary segmentation;word-level;Logic gates;Decoding;Training;Erbium;Phonetics;Learning (artificial intelligence);Guidelines;recurrent neural network;autoencoder;reinforcement learning;policy gradient},
}

@InProceedings{8661146,
  author    = {M. {Pourreza} and R. {Derakhshan} and H. {Fayyazi} and M. {Sabokrou}},
  title     = {Sub-word based Persian OCR Using Auto-Encoder Features and Cascade Classifier},
  booktitle = {2018 9th International Symposium on Telecommunications (IST)},
  year      = {2018},
  pages     = {481-485},
  month     = {Dec},
  abstract  = {In Persian text, unlike English, the letters are stick together and their shapes varies depending on where they are in the word. This makes it difficult to distinguish letters in Persian OCR. One way to overcome this problem is to recognize the sub-words, not the letters. In this paper with the help of a complete sub-word image dictionary, a new approach for Persian OCR is presented. For sub-word image recognition, two cascade SVM classifiers that are trained with the features extracted by Auto-Encoder, are exploited. The extracted sub-word texts form the words based on the results of a pre-process word segmentation step. The resulted text is enhanced using a fast post-process algorithm which uses a word dictionary.},
  doi       = {10.1109/ISTEL.2018.8661146},
  keywords  = {feature extraction;image classification;image segmentation;natural language processing;optical character recognition;support vector machines;text analysis;pre-process word segmentation;sub-word based Persian OCR;feature extraction;word dictionary;extracted sub-word texts;cascade SVM classifiers;sub-word image recognition;complete sub-word image dictionary;Persian text;cascade classifier;auto-encoder features;Feature extraction;Optical character recognition software;Dictionaries;Support vector machines;Image segmentation;Image recognition;Clustering algorithms;Persian OCR;Sub-Word Image Dictionary;Auto-Encoder;Clustering;Cascade Classifier;Post-Process},
}

@InProceedings{8629215,
  author    = {D. {Tanaya} and M. {Adriani}},
  title     = {Word Segmentation for Javanese Character Using Dictionary, SVM, and CRF},
  booktitle = {2018 International Conference on Asian Language Processing (IALP)},
  year      = {2018},
  pages     = {240-243},
  month     = {Nov},
  abstract  = {Word segmentation method based on dictionary for Javanese character still cannot overcome the problem of ambiguity, derivational word segmentation, and unknown words identification. We propose a new approach for Javanese character segmentation, i.e. a machine learning based method using CRF and SVM with a set feature of Javanese character's categorization and its combination with dictionary-based method. Through several experiments, it is proven that combination of dictionary-based method and CRF performs best than word segmentation using CRF, SVM, and traditional dictionary-based word segmentation.},
  doi       = {10.1109/IALP.2018.8629215},
  keywords  = {character recognition;dictionaries;image segmentation;learning (artificial intelligence);natural language processing;random processes;support vector machines;dictionary-based method;CRF;SVM;word segmentation method;derivational word segmentation;unknown words identification;Javanese character segmentation;dictionary-based word segmentation;Support vector machines;Machine learning algorithms;Dictionaries;Machine learning;Learning systems;Task analysis;Feature extraction;Javanese character;word segmentation;conditional random field;support vector machine},
}

@InProceedings{8269008,
  author    = {H. {Kamper} and K. {Livescu} and S. {Goldwater}},
  title     = {An embedded segmental K-means model for unsupervised segmentation and clustering of speech},
  booktitle = {2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  year      = {2017},
  pages     = {719-726},
  month     = {Dec},
  abstract  = {Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most approaches lie at methodological extremes: some use probabilistic Bayesian models with convergence guarantees, while others opt for more efficient heuristic techniques. Despite competitive performance in previous work, the full Bayesian approach is difficult to scale to large speech corpora. We introduce an approximation to a recent Bayesian model that still has a clear objective function but improves efficiency by using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental K-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. We first compare ES-KMeans to previous approaches on common English and Xitsonga data sets (5 and 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being 5 times faster with fewer hyperparameters. However, its clusters are less pure than those of the other models. We then show that ES-KMeans scales to larger corpora by applying it to the 5 languages of the Zero Resource Speech Challenge 2017 (up to 45 hours), where it performs competitively compared to the challenge baseline.},
  doi       = {10.1109/ASRU.2017.8269008},
  keywords  = {Bayes methods;linguistics;natural language processing;pattern clustering;speech processing;speech recognition;unsupervised learning;competitive performance;Bayesian approach;speech corpora;hard clustering;Bayesian inference;fixed-dimensional acoustic word embeddings;word segmentation;ES-KMeans scales;zero-resource speech processing;convergence guarantees;heuristic techniques;probabilistic Bayesian models;Xitsonga data sets;English data sets;time 5.0 hour;time 2.5 hour;time 45.0 hour;Speech;Bayes methods;Clustering algorithms;Acoustics;Standards;Speech processing;Probabilistic logic;Zero-resource speech processing;word segmentation;unsupervised learning;language acquisition},
}

@InProceedings{8286379,
  author    = {N. D. {Londhe} and G. B. {Kshirsagar}},
  title     = {Continuous speech recognition system for chhattisgarhi},
  booktitle = {2017 International Conference on Communication and Signal Processing (ICCSP)},
  year      = {2017},
  pages     = {0365-0369},
  month     = {April},
  abstract  = {The next step of isolated word recognition will be the sentence or continuous speech recognition. In this paper, an algorithm to estimates maximum posterior probabilities from input chhattisgarhi speech has proposed. To compute the accurate and precise maximum likelihoods, one need to extract the features from segmented words accurately. An implemented algorithm for automatic word segmentation gives up to 96.00% of average word boundary detection accuracy. Mel filter cepstral coefficients, delta cepstral and delta-delta cepstral speech characteristics have been extracted from the signals. Automatic segmented words from the speech have added and created a vocabulary. Further, 3-gram language model has implemented to resolve the ambiguity of similar pronunciation of the words. The last stage of the system is to compute maximum log likelihoods and maximum posterior probabilities of the word sequence. The experiments have been carried out on self-recorded 220 continuous chhattisgarhi speech, which consist of 2640 sentences and 330,000 words. We have used multilayer feedforward neural network (MLP) and Hidden Markov model (HMM) for maximum likelihood estimation (MLE). The details of experimental analysis have presented in the paper.},
  doi       = {10.1109/ICCSP.2017.8286379},
  keywords  = {cepstral analysis;feature extraction;feedforward neural nets;filtering theory;hidden Markov models;maximum likelihood estimation;multilayer perceptrons;natural language processing;probability;speech recognition;maximum posterior probabilities;continuous chhattisgarhi speech;feature extraction;Mel filter cepstral coefficients;delta cepstral speech characteristics;multilayer feedforward neural network;hidden Markov model;HMM;isolated word recognition;continuous speech recognition system;maximum likelihood estimation;word sequence;3-gram language model;delta-delta cepstral speech characteristics;average word boundary detection accuracy;automatic word segmentation;segmented words;precise maximum likelihoods;input chhattisgarhi speech;Speech recognition;Speech;Hidden Markov models;Mel frequency cepstral coefficient;Feature extraction;Continuous speech recognition;Posterior probabilities;maximum log-likelihoods;Automatic word segmentation;Speech characteristics;MLP;HMM},
}

@InProceedings{8313765,
  author    = {K. {Thangairulappan} and K. {Mohan}},
  title     = {Efficient segmentation of printed tamil script into characters using projection and structure},
  booktitle = {2017 Fourth International Conference on Image Information Processing (ICIIP)},
  year      = {2017},
  pages     = {1-6},
  month     = {Dec},
  abstract  = {Segmenting text lines and touching characters remain a problem. But this paper segments the printed touching lines and characters of Tamil script into lines, words and characters. Standard horizontal projection and vertical projection methods cannot segment the touched lines and characters. The proposed method solves the problem of touching lines and touching characters of Tamil Script based on the structural properties of the characters and projection. The proposed method is implemented on different set of documents collected from different Tamil literary periodicals with different sizes and fonts. Experimental results are compared with the projection profile based technique and connected component labelling technique. Results shown that the proposed method segment the documents into lines, words and characters with good accuracy for the regular fonts with any size even though the lines and characters are of touching nature. This method can be applied in preparing OCR system and in document analysis and recognition system when the printed documents are with line and character overlapping.},
  doi       = {10.1109/ICIIP.2017.8313765},
  keywords  = {document image processing;image segmentation;natural language processing;optical character recognition;text analysis;text lines;touching characters;standard horizontal projection;vertical projection methods;touched lines;character overlapping;document segmentation;printed tamil script segmentation;printed touching lines;Tamil literary periodicals;projection profile based technique;connected component labelling technique;OCR system;document analysis;document recognition system;Image segmentation;Shape;Information processing;Character recognition;Smoothing methods;Computed tomography;Computer science;Tamil document;touching lines;line-segmentation;word-segmentation;touching-character segmentation;projection profile},
}

@InProceedings{8389951,
  author    = {A. {Mehta} and A. {Gor}},
  title     = {Multifont multisize Gujarati OCR with style identification},
  booktitle = {2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)},
  year      = {2017},
  pages     = {275-281},
  month     = {Aug},
  abstract  = {In recent years, OCR (Optical Character Recognition) technology has been applied throughout the entire spectrum of industries, revolutionizing the document management process. Document Analysis (DA) is a pre-requisite in every OCR task. Detection of style is one of the most important issues in DA. Style identification is the cause for accuracy of the text recognition system in the OCR. An approach is introduced for multifont, multisize OCR with style identification for printed Gujarati script. Here, scanned image is segmented into lines and words followed by style identification (Bold, Italic, or Normal) on segmented words. Then characters are segmented from words. For identifying characters uniquely Histogram of Oriented Gradients (HOG) and Chain Code Histogram (CCH) are used. Individual classifiers are used for recognizing bold, italic and normal style characters using SVM classifier. At the end post processing is carried out to finalize the class label. Approach is tested on various documents of different fonts (LMG-Arun, Gujarati-saral, thesis fonts) and sizes (12, 14, and 16). Total 34 consonants(to), 3 vowels, 6 modifiers and combination of modifiers with consonant are consider for recognition purpose. Overall recognition accuracy of all types of document is comparable.},
  doi       = {10.1109/ICECDS.2017.8389951},
  keywords  = {character sets;document image processing;gradient methods;image classification;image segmentation;natural language processing;optical character recognition;support vector machines;text analysis;multisize Gujarati OCR;style identification;OCR technology;Optical Character Recognition;document management process;Document Analysis;OCR task;text recognition system;multifont OCR;multisize OCR;printed Gujarati script;word segmentation;normal style character recognition;italic style character recognition;bold style character recognition;support vector machines;Histogram of Oriented Gradients;HOG;Chain Code Histogram;CCH;Image segmentation;Histograms;Optical character recognition software;Character recognition;Feature extraction;Support vector machines;Data analysis;optical character recognition;style identification;stroke width;zone identification;orientation angle;histogram of oriented gradients;chain code histogram;one vs all SVM;post processing},
}

@Article{7451222,
  author   = {J. {Nishihara} and T. {Nakamura} and T. {Nagai}},
  title    = {Online Algorithm for Robots to Learn Object Concepts and Language Model},
  journal  = {IEEE Transactions on Cognitive and Developmental Systems},
  year     = {2017},
  volume   = {9},
  number   = {3},
  pages    = {255-268},
  month    = {Sep.},
  abstract = {Humans form concept of objects by classifying them into categories, and acquire language by simultaneously interacting with others. Thus, the meaning of a word can be learned by connecting a recognized word to its corresponding concept. We consider this ability important for robots to flexibly develop knowledge of language and concepts. In this paper, we propose an online algorithm for robots to acquire knowledge of natural language and learn object concepts. A robot learns the language model from word sequences, which are obtained by the segmentation of phoneme sequences provided by a user, by using unsupervised word segmentation each time it is provided with a new object. Moreover, the robot acquires object concepts using these word sequences as well as multimodal information obtained by observing objects. The crucial aspect of our model is the interdependence of words and concepts: there is a high probability that the same words will be uttered to describe objects in the same category. By taking this relationship into account, our proposed method enables robots to acquire a more accurate language model and object concepts online. Experimental results verify this.},
  doi      = {10.1109/TCDS.2016.2552579},
  keywords = {natural language processing;robot programming;unsupervised learning;word processing;online algorithm;robot learning;object concepts;language model;natural language knowledge;word sequences;phoneme sequence segmentation;unsupervised word segmentation;Speech recognition;Speech;Robot sensing systems;Visualization;Feedback loop;Speech processing;Language acquisition;multimodal categorization;multimodal latent Dirichlet allocation (MLDA);object concepts;online learning;unsupervised learning},
}

@InProceedings{7916560,
  author    = {W. A. {Jamro}},
  title     = {Sindhi Language Processing: A survey},
  booktitle = {2017 International Conference on Innovations in Electrical Engineering and Computational Technologies (ICIEECT)},
  year      = {2017},
  pages     = {1-8},
  month     = {April},
  abstract  = {In this era of information technology, natural language processing (NLP) has become volatile field because of digital reliance of today's communities. The growth of Internet usage bringing the communities, cultures and languages online. In this regard much of the work has been done of the European and east Asian languages, in the result these languages have reached mature level in terms of computational processing. Despite the great importance of NLP science, still most of the South Asian languages are under developing phase. Sindhi language is one of them, which stands among the most ancient languages in the world. The Sindhi language has a great influence on the large community in Sindh province of Pakistan and some states of India and other countries. But unfortunately, it is at infant level in terms of computational processing, because it has not received such attention of language engineering community, due to its complex morphological structure and scarcity of language resources. Therefore, this study has been carried out in order to summarize the existing work on Sindhi Language Processing (SLP) and to explore future research opportunities, also some potential research problems. This paper will be helpful for the researchers in order to find all the information regarding SLP at one place in a unique way.},
  doi       = {10.1109/ICIEECT.2017.7916560},
  keywords  = {natural language processing;word processing;Sindhi language processing;natural language processing;Internet usage;European languages;east Asian languages;computational processing;NLP science;South Asian languages;Sindh province;Pakistan;India;language engineering community;morphological structure;language resources;SLP;Compounds;Tagging;Writing;Tools;Morphology;Optical character recognition software;Sindhi Language Processing;SLP;Morphological Analysis;Word Segmentation;Parts-of-Speech Tagging;Diacritization;technique and approaches},
}

@InProceedings{8359551,
  author    = {Y. {Zhai} and L. {Liu} and W. {Song} and C. {Du} and X. {Zhao}},
  title     = {The application of natural language processing in compiler principle system},
  booktitle = {2017 International Conference on Progress in Informatics and Computing (PIC)},
  year      = {2017},
  pages     = {245-248},
  month     = {Dec},
  abstract  = {Compiling principle is an important course of computer science major, which mainly introduces general principles and basic methods of the construction of compiling programs mainly. Due to high demands of the logic analysis ability, the course bring abstract and unintelligible experience to many students. Thus it is quite difficult for students to master the main points of this course within the limited class time. Based on the requirement above, this paper mainly proposed a method of making use of natural language processing in the research and application of compiling process, which utilizes Maximum Probability Word Segmentation algorithm during the process of lexical analysis and syntax analysis, to offer more effective interface between human and computer. The proposed method can provide students with intuitive and profound knowledge concept in the process of learning how to compile, makes it easier and quicker for students to understand the principle of computer compiling.},
  doi       = {10.1109/PIC.2017.8359551},
  keywords  = {computer science education;educational courses;natural language processing;probability;program compilers;important course;general principles;basic methods;compiling programs;logic analysis ability;natural language processing;compiling process;lexical analysis;syntax analysis;computer compiling;compiler principle system;compiling principle;computer science major;human computer interface;maximum probability word segmentation algorithm;natural language processing;compiling Principles;Maximum Probability Word Segmentation algorithm},
}

@InProceedings{7916723,
  author    = {A. {Sheshasaayee} and {Angela Deepa. V.R}},
  title     = {Ascertaining the morphological components of Tamil language using unsupervised approach},
  booktitle = {2016 Online International Conference on Green Engineering and Technologies (IC-GET)},
  year      = {2016},
  pages     = {1-6},
  month     = {Nov},
  abstract  = {The learning of morphological components in a natural language by means of segmentation of words to identify the prime chores of stems and affixes lead to effective morphological analysis. Morphological segmentation is an important step in the analysis of natural languages to identify its intricate properties. Remarkable approaches been used in order to construct an effective morphological segmentation framework to study the highly agglutinative Tamil language. This paper focus on unsupervised means of segmenting Tamil lexicons by using a novel algorithm across various parameters. The proposed work shows a promising result in favour to the identification of morphemes with their suffixes.},
  doi       = {10.1109/GET.2016.7916723},
  keywords  = {natural language processing;unsupervised learning;morphological component learning;Tamil language;unsupervised approach;natural language analysis;word segmentation;effective morphological analysis;effective morphological segmentation framework;Tamil lexicon segmentation;morpheme identification;Pragmatics;Morphology;Computer science;Algorithm design and analysis;Computational modeling;Measurement;Morphological segmentation;unsupervised;machine translation;agglutinative;morphemes;suffix},
}

@InProceedings{7939534,
  author    = {S. {Gupta}},
  title     = {Efficient malicious domain detection using word segmentation and BM pattern matching},
  booktitle = {2016 International Conference on Recent Advances and Innovations in Engineering (ICRAIE)},
  year      = {2016},
  pages     = {1-6},
  month     = {Dec},
  abstract  = {On the World Wide Web, the malicious links are highly problematic in the dissemination channels as a source code to the malware broadcasting. These suspicious malicious links gives full access to the web attackers as an instrument of web pages on internet. It is easily affected by the results of attackers on the system of victim where system is utilized easily for performing the cyber-attacks such as stealing the financial credentials, phishing-spamming, hacking and many more such web attacks. The developed system must be accurate and fast enough to detect these types of such cyber-attacks by observing the ability to find new developed malicious URLs or malicious source code contents. It is the critical task to detect the malicious contents in network of the web pages over the World Wide Web. The various malicious cyber-attacks like spamming, code phishing are done by using the malicious URLs to mount these types of cyber-attacks. Internet unlawful activities are found in Malicious Web sites as cornerstone of the Malicious URLs. The main threat is to identify these attacks so that the suspicious URLs can be easily resolved as malicious URLs along with its source code of the web pages. In this paper, a method has been proposed which is highly useful in the field of World Wide Web networking of domains for detecting the malicious URLs by using BM (Boyer-Moore) [1] string pattern matching algorithm based on word segmentation approach [2]. The nature of attacks is identified as a malicious URL or source code of the web sites questioned on the World Wide Web. The proposed approach is based on the real time system for getting suspicious URL from the DNS server followed by the detection on the basis of word segmentation of source code. The discriminative features of this system are verified by using the proposed method which gives a variety of properties including text and link in the source code as highly powerful and novel approach in the detection of suspicious URLs.},
  doi       = {10.1109/ICRAIE.2016.7939534},
  keywords  = {Internet;natural language processing;real-time systems;security of data;string matching;Web sites;malicious domain detection;word segmentation;World Wide Web;dissemination channels;malware broadcasting;suspicious malicious links;Web attackers;Web pages;cyber-attacks;malicious URL;malicious source code contents;malicious content detection;Internet unlawful activities;malicious Web sites;BM string pattern matching algorithm;Boyer-Moore algorithm;real time system;DNS server;Uniform resource locators;Web pages;Pattern matching;Databases;Servers;Algorithm design and analysis;BM (Boyer-Moore) Pattern;Malicious Web Pages;Classification Module;Web-Based Attacks},
}

@InProceedings{7440524,
  author    = {W. {Sukfong} and S. {Nakkrasae} and P. {Panpipat}},
  title     = {Motor insurance knowledge management in AIML format using tree structured dictionary mechanism and conceptual graph},
  booktitle = {2016 8th International Conference on Knowledge and Smart Technology (KST)},
  year      = {2016},
  pages     = {158-163},
  month     = {Feb},
  abstract  = {Nowadays, information providers of car insurance through the site are not effective enough for consumers needs. Thus, this paper presents a motor insurance knowledge management based on Artificial Intelligence Markup Language (AIML). In the part of knowledge management, tree structure dictionary mechanism method is used for word segmentation and the conversion technique to correct grammatical sentences (Pattern Normalization) utilizes conceptual graphs. Motor insurance enquiry intelligent system via mobile devices is then implemented. It is analyzed and designed by using object-oriented approach with UML (Unified Modeling Language). The result shows that the developed system can be used for answer questions and works more efficiently.},
  doi       = {10.1109/KST.2016.7440524},
  keywords  = {automobiles;insurance data processing;knowledge based systems;knowledge management;natural language processing;tree data structures;Unified Modeling Language;motor insurance knowledge management;AIML format;tree structured dictionary mechanism;conceptual graph;car insurance information providers;Artificial Intelligence Markup Language;word segmentation;conversion technique;grammatical sentence correction;pattern normalization;motor insurance enquiry intelligent system;mobile devices;object-oriented approach;UML;Unified Modeling Language;Insurance;Knowledge based systems;Dictionaries;Knowledge management;Markup languages;Unified modeling language;Word Segmentation;Dictionary Mechanism;Pattern Normalization;Conceptual Graphs;AIML (Artificial Intelligence Markup Language);CORPUS;Object Oriented Analysis and Design;UML},
}

@Article{7467531,
  author   = {A. {Taniguchi} and T. {Taniguchi} and T. {Inamura}},
  title    = {Spatial Concept Acquisition for a Mobile Robot That Integrates Self-Localization and Unsupervised Word Discovery From Spoken Sentences},
  journal  = {IEEE Transactions on Cognitive and Developmental Systems},
  year     = {2016},
  volume   = {8},
  number   = {4},
  pages    = {285-297},
  month    = {Dec},
  abstract = {In this paper, we propose a novel unsupervised learning method for the lexical acquisition of words related to places visited by robots, from human continuous speech signals. We address the problem of learning novel words by a robot that has no prior knowledge of these words except for a primitive acoustic model. Furthermore, we propose a method that allows a robot to effectively use the learned words and their meanings for self-localization tasks. The proposed method is nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates the generative model for self-localization and the unsupervised word segmentation in uttered sentences via latent variables related to the spatial concept. We implemented the proposed method SpCoA on SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile robot in a real environment. Further, we conducted experiments for evaluating the performance of SpCoA. The experimental results showed that SpCoA enabled the robot to acquire the names of places from speech sentences. They also revealed that the robot could effectively utilize the acquired spatial concepts and reduce the uncertainty in self-localization.},
  doi      = {10.1109/TCDS.2016.2565542},
  keywords = {Bayes methods;mobile robots;natural language processing;nonparametric statistics;sensor fusion;speech processing;uncertain systems;unsupervised learning;mobile robot;unsupervised word discovery;spoken sentences;unsupervised learning;lexical word acquisition;human continuous speech signals;learned words usage;self-localization tasks;nonparametric Bayesian spatial concept acquisition method;SpCoA;unsupervised word segmentation;uttered sentences;latent variables;SIGVerse;TurtleBot2;spatial concepts;uncertainty reduction;Robot sensing systems;Robot kinematics;Speech;Speech recognition;Vocabulary;Unsupervised learning;Learning place names;lexical acquisition;self-localization;spatial concept},
}

@InProceedings{7974624,
  author    = {X. {Cong} and L. {Li}},
  title     = {UGC quality evaluation based on meta-learning and content feature analysis},
  booktitle = {2016 IEEE International Conference on Network Infrastructure and Digital Content (IC-NIDC)},
  year      = {2016},
  pages     = {495-499},
  month     = {Sep.},
  abstract  = {With the fast development of Social Networking Services, there has been increasingly vast amount of information published by massive network users. Given this information explosion, how to analyze the quality of User Generated Contents (UGC) automatically becomes a challenging task for researchers. To solve the problem, we need to build an effective UGC quality evaluation system. In the light of our experience, we believe that the textual content of UGC is the key factor for its quality. Hence, we focus on textual content based quality evaluation and classification instead of using UGC publishing related data, such as times being commented and forwarded in this paper. We extract various features of the textual contents based on natural language processing technologies firstly, such as word segmentation, keywords, topic model, sentence parsing, distributed word representation etc. Secondly, we build several base-learning classifiers with different features and different machine learning algorithms to assign UGC contents with four different quality labels. Then, we create the global meta-learning model based on these base classifiers to generate the final quality labels for UGC contents. We have also implemented a series of experiments based on realistic data collected from Tianya Forum and use 10-fold cross-validation to test the model. Results have shown that our proposed meta-learning model performs much better.},
  doi       = {10.1109/ICNIDC.2016.7974624},
  keywords  = {feature extraction;learning (artificial intelligence);natural language processing;pattern classification;social networking (online);text analysis;UGC quality evaluation system;metalearning model;content feature analysis;social networking services;user generated contents;UGC textual content based quality evaluation;UGC textual content classification;feature extraction;natural language processing;base-learning classifiers;machine learning;Tianya Forum;Quality of service;User Generated Contents (UGC);meta-learning;feature analysis;quality evaluation;multiple classifier fusion},
}

@InProceedings{7490128,
  author    = {M. {Kulkarni} and S. S. {Karande} and S. {Lodha}},
  title     = {Unsupervised Word Clustering Using Deep Features},
  booktitle = {2016 12th IAPR Workshop on Document Analysis Systems (DAS)},
  year      = {2016},
  pages     = {263-268},
  month     = {April},
  abstract  = {Digitization is crucial especially in the Indian context. OCR engines fail on Indian scripts mainly because character segmentation is non-trivial. Even word based recognition approaches suffer from the issues such as time degradations, word segmentation errors, font style/size variations. In this paper, we propose a deep learning architecture based approach for unsupervised word clustering. An edge responsive untrained Convolutional Neural Network (CNN) is used as a feature extractor. Graph connected component analysis is applied on the similarity graph computed from the word features. Our approach inherently detects similar shape patterns at word level and hence, it is language agnostic. We validated our approach against multiple state of art word matching techniques. Experimental results show that our approach significantly outperforms all of them on variety of data sets. In addition, the approach is observed to be robust to word segmentation errors, font style/size variations.},
  doi       = {10.1109/DAS.2016.14},
  keywords  = {convolution;document image processing;feature extraction;graph theory;image segmentation;learning (artificial intelligence);natural language processing;neural nets;optical character recognition;pattern clustering;unsupervised word clustering;deep features;digitization;OCR engines;Indian scripts;character segmentation;word based recognition;word segmentation errors;font style variations;font size variations;deep learning architecture;edge responsive untrained convolutional neural network;feature extraction;graph connected component analysis;similarity graph;Feature extraction;Convolution;Machine learning;Computer architecture;Neural networks;Object recognition;Training;Deep learning;word clustering},
}

@InProceedings{7976613,
  author    = {X. {Li} and F. {He}},
  title     = {Word Segmentation in Japanese for Construction of Scientific Japanese Corpus},
  booktitle = {2016 8th International Conference on Information Technology in Medicine and Education (ITME)},
  year      = {2016},
  pages     = {864-868},
  month     = {Dec},
  abstract  = {Japanese word segmentation in the specialized field is the difficulty of building the Scientific Japanese Corpus. This paper deeply studied the basic problem of the word segmentation. They are ambiguity and words without login. In this paper we tried to use the ART network and immune principle to solve the problem of Japanese word segmentation, more than 90% of the words are correctly separated by our method.},
  doi       = {10.1109/ITME.2016.0199},
  keywords  = {natural language processing;text analysis;word processing;scientific Japanese corpus;Japanese word segmentation;ART network;Neurons;Immune system;Subspace constraints;Dictionaries;Adaptive systems;Pathogens;Vocabulary;Japanese Corpus;Word Gegmentation;ARTNetwork;Adaptive Network},
}

@InProceedings{7333755,
  author    = {M. {Javed} and P. {Nagabhushan} and B. B. {Chaudhuri}},
  title     = {A direct approach for word and character segmentation in run-length compressed documents with an application to word spotting},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
  year      = {2015},
  pages     = {216-220},
  month     = {Aug},
  abstract  = {Segmentation of a text document into lines, words and characters is an important objective in application like OCR and related analytics. However in today's scenario, the documents are compressed for archival and transmission efficiency. Text segmentation in compressed documents warrants decompression, and needs additional computing resources. In this backdrop, the paper proposes a method for text segmentation directly in run-length compressed, printed English text documents. Line segmentation is done using the projection profile technique. Further segmentation into words and characters is accomplished by tracing the white runs along the base region of the text line. During the process, a run based region growing technique is applied in the spatial neighborhood of the white runs to trace the vertical space between the characters. After detecting the character spaces in the entire text line, the decision of word space and character space is made by computing the average character space. Subsequently based on the spatial position of the detected words and characters, their respective compressed segments are extracted. The proposed algorithm is tested with 1083 compressed text lines, and F-measure of 97.93% and 92.86% respectively for word and character segmentation are obtained. Finally an application of word spotting is also presented.},
  doi       = {10.1109/ICDAR.2015.7333755},
  keywords  = {data compression;decision making;document image processing;image segmentation;natural language processing;optical character recognition;text detection;word processing;character segmentation;run-length compressed documents;word spotting;direct approach;word segmentation;text document segmentation;OCR;transmission efficiency;archival efficiency;compressed document warrant decompression;computing resources;text segmentation;printed English text documents;line segmentation;projection profile technique;run based region growing technique;character space detection;projection profile technique;word space decision;character space decision;compressed segment extraction;F-measure;Optical character recognition software;Image coding;Adaptation models;Compressed text document segmentation;compressed word segmentation;compressed character segmentation;word spotting in compressed domain},
}

@InProceedings{7490720,
  author    = {{Chunxiang Zhang} and {Shan He} and {Xueyao Gao}},
  title     = {A word sense disambiguation system based on bayesian model},
  booktitle = {2015 4th International Conference on Computer Science and Network Technology (ICCSNT)},
  year      = {2015},
  volume    = {01},
  pages     = {124-127},
  month     = {Dec},
  abstract  = {Research on word sense disambiguation (WSD) is of great importance in natural language processing fields. In this paper, a novel word sense disambiguation system is designed in which bayesian theory is applied to determine correct sense of an ambiguous word. Morphology knowledge in word unit is mined to guide WSD process. Neighboring morphology knowledge of an ambiguous word is used as feature for constructing WSD classifier. Word segmentation tool is integrated into this system and browser/server (B/S) framework is adopted. Experimental results show that the performance of WSD system is good.},
  doi       = {10.1109/ICCSNT.2015.7490720},
  keywords  = {Bayes methods;data mining;Internet;natural language processing;online front-ends;pattern classification;word sense disambiguation;natural language processing fields;Bayesian model;word unit;morphology knowledge mining;WSD process;ambiguous word;WSD classifier;word segmentation tool;browser-server framework;B/S framework;WSD system;Semantics;Feature extraction;Bayes methods;Computational modeling;Vocabulary;Unified modeling language;Browsers;word sense disambiguation;morphology knowledge;word segmentation tool;browser/server},
}

@Article{7188527,
  author   = {H. {Wang} and X. {Han} and L. {Liu} and W. {Song} and M. {Yuan}},
  title    = {An improved unsupervised approach to word segmentation},
  journal  = {China Communications},
  year     = {2015},
  volume   = {12},
  number   = {7},
  pages    = {82-95},
  month    = {July},
  abstract = {ESA is an unsupervised approach to word segmentation previously proposed by Wang, which is an iterative process consisting of three phases: Evaluation, Selection and Adjustment. In this article, we propose ExESA, the extension of ESA. In ExESA, the original approach is extended to a 2-pass process and the ratio of different word lengths is introduced as the third type of information combined with cohesion and separation. A maximum strategy is adopted to determine the best segmentation of a character sequence in the phrase of Selection. Besides, in Adjustment, ExESA re-evaluates separation information and individual information to overcome the overestimation frequencies. Additionally, a smoothing algorithm is applied to alleviate sparseness. The experiment results show that ExESA can further improve the performance and is time-saving by properly utilizing more information from un-annotated corpora. Moreover, the parameters of ExESA can be predicted by a set of empirical formulae or combined with the minimum description length principle.},
  doi      = {10.1109/CC.2015.7188527},
  keywords = {iterative methods;natural language processing;smoothing methods;text analysis;unsupervised learning;improved unsupervised approach;word segmentation;iterative process;ExESA;word length;cohesion;character sequence segmentation;separation information;overestimation frequency;smoothing algorithm;minimum description length principle;Entropy;Smoothing methods;Length measurement;Frequency measurement;Uncertainty;Prediction algorithms;Accuracy;word segmentation;character sequence;smoothing algorithm;maximum strategy},
}

@InProceedings{7179088,
  author    = {F. {Stahlberg} and T. {Schlippe} and S. {Vogel} and T. {Schultz}},
  title     = {Cross-lingual lexical language discovery from audio data using multiple translations},
  booktitle = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2015},
  pages     = {5823-5827},
  month     = {April},
  abstract  = {Zero-resource Automatic Speech Recognition (ZR ASR) addresses target languages without given pronunciation dictionary, transcribed speech, and language model. Lexical discovery for ZR ASR aims to extract word-like chunks from speech. Lexical discovery benefits from the availability of written translations in another source language. In this paper, we improve lexical discovery even more by combining multiple source languages. We present a novel method for combining noisy word segmentations resulting in up to 11.2% relative F-score gain. When we extract word pronunciations from the combined segmentations to bootstrap an ASR system, we improve accuracy by 9.1% relative compared to the best system with only one translation, and by 50.1% compared to monolingual lexical discovery.},
  doi       = {10.1109/ICASSP.2015.7179088},
  keywords  = {natural language processing;speech recognition;cross lingual lexical language discovery;audio data;multiple language translation;zero resource automatic speech recognition;word-like chunks;lexical discovery;multiple source language;noisy word segmentation;Speech;Zirconium;Dictionaries;Acoustics;Automatic speech recognition;Computational modeling;Lexical language discovery;zero-resource automatic speech recognition;word-to-phoneme alignment;non-written languages},
}

@InProceedings{7230422,
  author    = {M. {Ablimit} and A. {Hamdulla} and A. {Pattar}},
  title     = {Reducing morpho-phonetic confusion in sub-word based Uyghur ASR},
  booktitle = {2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)},
  year      = {2015},
  pages     = {348-352},
  month     = {July},
  abstract  = {Sub-word units like morphemes are selected as the lexicon for highly inflectional languages, as they can provide better coverage and a smaller vocabulary size. However, short units shrink the context of statistical models, prone to morpho-phonetic changes, and not always outperform the word based model. When sequence of units are merged or split, unit boundaries are phonetically harmonized in the speech which reflects as the morpho-phonetic changes in the text. This paper investigates morpho-phonetic confusions in the sub-word segmentation of Uyghur text, and phonetic reasons which affect automatic speech recognition (ASR) accuracy. An optimal lexicon set is obtained by comparing ASR results of different layers of lexica, which avoids phonetic confusions in the frequently misrecognized morpheme sequences. This optimal lexicon, which is obtained totally from a HMM based acoustic model, outperformed all the baseline linguistic units. And when all these units are directly incorporated a deep neural network (DNN) based acoustic model, without changing the training corpora and language models, the optimal lexicon not only drastically improved the ASR accuracy but also outperformed other units as a proof of the generality of our approach. Experimental results demonstrate that the optimal lexicon obtained by reducing morpho-phonetic confusions exhibits better ASR accuracy and robustness.},
  doi       = {10.1109/ChinaSIP.2015.7230422},
  keywords  = {hidden Markov models;natural language processing;neural nets;speech processing;speech recognition;morpho-phonetic confusion;Uyghur ASR;morpheme;inflectional language;vocabulary size;statistical model;morpho-phonetic change;sub-word segmentation;Uyghur text;phonetic reason;automatic speech recognition;ASR accuracy;optimal lexicon set;HMM based acoustic model;baseline linguistic unit;deep neural network;DNN based acoustic model;training corpora;language model;Hidden Markov models;Accuracy;Acoustics;Speech;Vocabulary;Training;Surface morphology;ASR;Uyghur;phonetic;morphology;DNN},
}

@InProceedings{7453382,
  author    = {S. {Puri} and S. P. {Singh}},
  title     = {Sentence Detection and Extraction in machine printed imaged document using matching technique},
  booktitle = {2015 2nd International Conference on Recent Advances in Engineering Computational Sciences (RAECS)},
  year      = {2015},
  pages     = {1-6},
  month     = {Dec},
  abstract  = {Sentence extraction is a new, challenging and critical step in the printed scanned imaged documents. In this paper, an efficient 4-layered Sentence Detection and Extraction System (SDES) model is proposed which is designed to detect and extract sentences from machine printed imaged document. Its internal details and architecture clearly show that how it processes an image to find out the underlying sentences. The basic idea is to first preprocess the imaged document for noise removal and skew correction, and then textual entities are detected and segmented at page, line and word levels. Firstly, the horizontal and vertical projection profiles are taken to segment and separate the lines and words. After skew correction, two stage Character Based and Word Based Leveled matching and testing are performed, which verify and identify the correct character and word by searching for similar textual characters and words in Character Set Storage (CSS) and Word Pseudo Thesaurus (WPT). If any word pattern is not matched and identified by WPT, then it is stored in the Unmatched Word Storage (UWS) for the future reference. Such testing and verification are used at two levels to increase the accuracy% of SDES, and thereby, reducing the errors. It increases the system performance greatly. Finally, all the sentences of imaged document are extracted. Experimental results are found at the word, character and sentence levels. Their accuracy% results are good which show the high system performance and efficiency.},
  doi       = {10.1109/RAECS.2015.7453382},
  keywords  = {document image processing;feature extraction;image denoising;image matching;natural language processing;machine printed imaged document;matching technique;4-layered sentence detection and extraction system;SDES model;noise removal;skew correction;character based leveled matching;word based leveled matching;character set storage;CSS;word pseudo thesaurus;WPT;unmatched word storage;UWS;Image segmentation;Feature extraction;Layout;Data mining;Testing;Cascading style sheets;Hidden Markov models;projection profile;line segmentation;word segmentation;character recognition;storage bins;sentence extraction;natural language processing},
}

@InProceedings{7451528,
  author    = {A. {Kumar} and L. {Padró} and A. {Oliver}},
  title     = {Unsupervised learning of agglutinated morphology using nested Pitman-Yor process based morpheme induction algorithm},
  booktitle = {2015 International Conference on Asian Language Processing (IALP)},
  year      = {2015},
  pages     = {45-48},
  month     = {Oct},
  abstract  = {In this paper we describe a method of morphologically segment highly agglutinating and inflectional languages from the Dravidian family. We use the nested Pitman-Yor process to segment long agglutinated words into their basic components, and use a corpus based morpheme induction algorithm to perform morpheme segmentation. We test our method on two languages, Malayalam and Kannada and compare the results with Morfessor-baseline.},
  doi       = {10.1109/IALP.2015.7451528},
  keywords  = {natural language processing;text analysis;unsupervised learning;unsupervised learning;agglutinated morphology;nested Pitman-Yor process-based morpheme induction algorithm;Dravidian family;long-agglutinated word segmentation;corpus-based morpheme induction algorithm;morpheme segmentation;Malayalam language;Kannada language;Morfessor-baseline;morphologically segmented highly-agglutinating-inflectional languages;Computational modeling;Morphology;Biological system modeling;Nested Pitman-Yor Process;Agglutinated mor-phology;Indian languages},
}

@InProceedings{7333852,
  author    = {E. {Kavallieratou}},
  title     = {Word segmentation using Wigner-Ville distribution},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
  year      = {2015},
  pages     = {701-705},
  month     = {Aug},
  abstract  = {In this paper, a novel technique for Word-segmentation is presented, based on Wigner-Ville distribution. The technique does not require training, while it is adapted to the writing style of the document image. Evaluation is performed on a standard dataset and the results are promising.},
  doi       = {10.1109/ICDAR.2015.7333852},
  keywords  = {document image processing;image segmentation;natural language processing;word segmentation;Wigner-Ville distribution;writing style;document image;standard dataset;Frequency modulation;word segmentation;Wigner-Ville distribution;Document image},
}

@InProceedings{6754849,
  author    = {S. A. {Angadi} and M. M. {Kodabagi}},
  title     = {A Robust Segmentation Technique for Line, Word and Character Extraction from Kannada Text in Low Resolution Display Board Images},
  booktitle = {2014 Fifth International Conference on Signal and Image Processing},
  year      = {2014},
  pages     = {42-49},
  month     = {Jan},
  abstract  = {Reliable extraction/segmentation of text lines, words and characters is one of the very important steps for development of automated systems for understanding the text in low resolution display board images. In this paper, a new approach for segmentation of text lines, words and characters from Kannada text in low resolution display board images is presented. The proposed method uses projection profile features and on pixel distribution statistics for segmentation of text lines. The method also detects text lines containing consonant modifiers and merges them with corresponding text lines, and efficiently separates overlapped text lines as well. The character extraction process computes character boundaries using vertical profile features for extracting character images from every text line. Further, the word segmentation process uses k-means clustering to group inter character gaps into character and word cluster spaces, which are used to compute thresholds for extracting words. The method also takes care of variations in character and word gaps. The proposed methodology is evaluated on a data set of 1008 low resolution images of display boards containing Kannada text captured from 2 mega pixel cameras on mobile phones at various sizes 240x320, 600x800 and 900x1200. The method achieves text line segmentation accuracy of 97.17%, word segmentation accuracy of 97.54% and character extraction accuracy of 99.09%. The proposed method is tolerant to font variability, spacing variations between characters and words, absence of free segmentation path due to consonant and vowel modifiers, noise and other degradations. The experimentation with images containing overlapped text lines has given promising results.},
  doi       = {10.1109/ICSIP.2014.11},
  keywords  = {character recognition;feature extraction;image resolution;image segmentation;natural language processing;pattern clustering;statistics;text analysis;robust segmentation technique;line extraction;word extraction;character extraction;Kannada text;low resolution display board images;reliable extraction;pixel distribution statistics;vertical profile features;character image extraction;k-means clustering;Image segmentation;Feature extraction;Image resolution;Accuracy;Algorithm design and analysis;Vectors;Equations;Segmentation;K-Means Clustering;Projection Profile Features;Low Resolution Images;Display Boards},
}

@InProceedings{7036013,
  author    = {E. {Bekbulatov} and A. {Kartbayev}},
  title     = {A study of certain morphological structures of Kazakh and their impact on the machine translation quality},
  booktitle = {2014 IEEE 8th International Conference on Application of Information and Communication Technologies (AICT)},
  year      = {2014},
  pages     = {1-5},
  month     = {Oct},
  abstract  = {This paper describes a morphological analysis of the Kazakh language for Kazakh-English statistical machine translation through changing the compound words of Kazakh language, and explores the effect of using the modified input on translation quality with a large number of training sentences. Word alignment problem would become more serious for translation from morphologically rich language such as Kazakh to morphologically simple one such as English, due to the problem of data sparseness on translation word forms in many different morphological variants. We present our investigations on unsupervised Kazakh morphological segmentation over newspaper corpus and compare unsupervised segmentation against rule-based language processing tools. In our experiments, the results show that our proposed method can improve word alignment and translation quality.},
  doi       = {10.1109/ICAICT.2014.7036013},
  keywords  = {computational linguistics;language translation;natural language processing;word processing;morphological structure;machine translation quality;morphological analysis;Kazakh language;Kazakh-English statistical machine translation;compound word;training sentence;word alignment problem;data sparseness;translation word;unsupervised Kazakh morphological segmentation;newspaper corpus;unsupervised segmentation;rule-based language processing tool;Morphology;Training;Automata;Pragmatics;Instruments;Smoothing methods;computational linguistics;kazakh morphology;word segmentation;machine translation},
}

@InProceedings{7019590,
  author    = {T. {Shreekanth} and V. {Udayashankara}},
  title     = {A two stage Braille Character segmentation approach for embossed double sided Hindi Devanagari Braille documents},
  booktitle = {2014 International Conference on Contemporary Computing and Informatics (IC3I)},
  year      = {2014},
  pages     = {533-538},
  month     = {Nov},
  abstract  = {The Optical Braille Character Recognition (OBR) system is in significant need in order to preserve the Braille documents to make them available in future for the large section of visually impaired people and also to make the bi-directional communication between the sighted people and the visually impaired people feasible. The recognition and transcribing the double sided Braille document into its corresponding natural text is a challenging task. This difficulty is due to the overlapping of the front side dots (Recto) with that of the back side dots (Verso) in the Inter-point Braille document. In such cases, the usual method of template matching to distinguish recto and verso dots is not efficient. In this paper a new system for double sided Braille dot recognition is proposed, which employs a two-stage highly efficient and an adaptive technique to differentiate the recto and verso dots from an inter-point Braille using the projection profile method. In this paper we present (i) a horizontal projection profile for Braille line segmentation, (ii) vertical projection profile for Braille word segmentation and (iii) Integration of horizontal and vertical projection profiles along with distance thresholding for Braille character segmentation. We demonstrate the effectiveness of this segmentation technique on a large dataset consisting of 754 words from Hindi Devanagari Braille documents with varying image resolution and with different word patterns. A recognition rate of 96.9% has been achieved.},
  doi       = {10.1109/IC3I.2014.7019590},
  keywords  = {handicapped aids;image resolution;image segmentation;natural language processing;optical character recognition;two stage Braille character segmentation approach;embossed double sided Hindi Devanagari Braille documents;optical Braille character recognition system;OBR;bidirectional communication;sighted people;visually impaired people;natural text;front side dots;recto;back side dots;verso;interpoint Braille document;double sided Braille dot recognition;image resolution;Image segmentation;Character recognition;Informatics;Image resolution;Lighting;Optical character recognition software;Optical imaging;Hindi Devanagari Braille;OBR;Projection profile;Recto;Verso},
}

@InProceedings{7083900,
  author    = {S. {Lushanthan} and A. R. {Weerasinghe} and D. L. {Herath}},
  title     = {Morphological analyzer and generator for Tamil Language},
  booktitle = {2014 14th International Conference on Advances in ICT for Emerging Regions (ICTer)},
  year      = {2014},
  pages     = {190-196},
  month     = {Dec},
  abstract  = {Morphological analysis is an essential component in Natural Language Processing (NLP) applications ranging from spell checker to machine translation. When performing a morphological analysis it leads to segmentation of a word into morphemes, combined with an analysis of the attachments of these morphemes. In English language the complexity of the formation of words is not much higher compared with Indic languages. Hence, Tamil language too does have its complexities when building up a NLP application. The morphemes in the language, the rules how these morphemes are connected and the changes occur when they attach together are the important factors that need to be considered when building up a Morphological Analyzer for any language. Our “Morphological Analyzer and Generator for Tamil Language” will be generating the word forms of a stem/ root, given a particular context and at the same time, a surface form in Tamil language should get analyzed into its proper context. This model tries to cover only the nouns and verbs in the Tamil language. This paper illustrates how the lexicon and the orthographic rules of Tamil language have been written as regular expressions using only finite state operations and how this approach has been implemented to develop a morphological analyzer/generator. This model is built using the Xerox toolkit, which uses “Two-level Morphology”, and almost 2000 noun stems and 96 verb stems have been incorporated into the network. A noun stem now produces about 40 different forms and a verb stem produces up to 240 forms. We have also defined our own transliteration scheme for this purpose.},
  doi       = {10.1109/ICTER.2014.7083900},
  keywords  = {finite state machines;language translation;natural language processing;morphological analyzer;morphological generator;Tamil language;natural language processing;NLP;spell checker;machine translation;word segmentation;morphemes;lexicon;orthographic rules;regular expressions;finite state operations;Xerox toolkit;two-level morphology;transliteration scheme;Tamil Morphological Analyzer and Generator;Morphology;Finite State Transducer;Regular Expressions},
}

@Article{6717132,
  author   = {M. {Doi} and H. {Lei}},
  title    = {STARS: Word Processing for the Japanese Language [Scanning Our Past]},
  journal  = {Proceedings of the IEEE},
  year     = {2014},
  volume   = {102},
  number   = {2},
  pages    = {222-228},
  month    = {Feb},
  abstract = {Toshiba started the automatic kanato-kanji conversion project in 1971. Kenichi Mori's research group solved the homonym problem using usage frequency information and interactive learning. In text editing, a user selects one of the homonyms displayed in the order of the usage frequency information. This information was based on the usage frequency in newspapers. A user's selection is automatically learned and used to update the usage frequency information: using the algorithm of last used homonym is the first out next time. Use of the last-in-first-out algorithm for selection of the homonym was crucial to the success of the conversion technology. In September 1978, Toshiba exhibited the JW-10, the first Japanese language word processor. During the 1990s, Japanese word processors became obsolete because of the growing popularity of PCs and word-processing software that incorporate kana-to-kanji conversion using automatic word segmentation. Programs like Microsoft Word and Justsystems' Ichitaro (1985) made this conversion a basic function of PCs. By 1999, major Japanese companies had exited the Japanese word-processor business.},
  doi      = {10.1109/JPROC.2013.2295876},
  keywords = {natural language processing;word processing;automatic kanato-kanji conversion project;homonym problem;usage frequency information;interactive learning;text editing;newspapers;user selection;last-in-first-out algorithm;Toshiba;JW-10;Japanese language word processor;PCs;word-processing software;kana-to-kanji conversion;automatic word segmentation;Microsoft Word;Justsystems' Ichitaro;Natural language processing;Japan;Text processing;History},
}

@Article{6549105,
  author   = {J. J. {Weinman} and Z. {Butler} and D. {Knoll} and J. {Feild}},
  title    = {Toward Integrated Scene Text Reading},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2014},
  volume   = {36},
  number   = {2},
  pages    = {375-387},
  month    = {Feb},
  abstract = {The growth in digital camera usage combined with a worldly abundance of text has translated to a rich new era for a classic problem of pattern recognition, reading. While traditional document processing often faces challenges such as unusual fonts, noise, and unconstrained lexicons, scene text reading amplifies these challenges and introduces new ones such as motion blur, curved layouts, perspective projection, and occlusion among others. Reading scene text is a complex problem involving many details that must be handled effectively for robust, accurate results. In this work, we describe and evaluate a reading system that combines several pieces, using probabilistic methods for coarsely binarizing a given text region, identifying baselines, and jointly performing word and character segmentation during the recognition process. By using scene context to recognize several words together in a line of text, our system gives state-of-the-art performance on three difficult benchmark data sets.},
  doi      = {10.1109/TPAMI.2013.126},
  keywords = {document image processing;image motion analysis;image recognition;image segmentation;image sensors;probability;integrated scene text reading;digital camera usage;worldly text abundance;pattern recognition;document processing;unconstrained lexicons;motion blur;curved layouts;perspective projection;occlusion;probabilistic methods;character segmentation;word segmentation;Image segmentation;Character recognition;Text recognition;Probabilistic logic;Hidden Markov models;Noise;Robustness;Scene text recognition;cropped word recognition;character recognition;discriminative semi-Markov model;image binarization;skew detection;baseline estimation;text guidelines;word normalization;word segmentation;Algorithms;Artificial Intelligence;Data Interpretation, Statistical;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Natural Language Processing;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Systems Integration},
}

@InProceedings{6559585,
  author    = {A. {Srithirath} and P. {Seresangtakul}},
  title     = {A hybrid approach to Lao word segmentation using longest syllable level matching with named entities recognition},
  booktitle = {2013 10th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology},
  year      = {2013},
  pages     = {1-5},
  month     = {May},
  abstract  = {The Lao language is written without words delimiter which makes it extremely difficult to process. The development of automatic word segmentation for natural language processing for the Lao language is an essential but challenging task. This paper proposes a longest syllable level match with named entities recognition approach for Lao word segmentation. Syllables were first extracted from the input text and then longest matching was applied. This is one of the techniques in the Dictionary Based approach with named entities recognition being used to combine them to form the words. The performance result obtained from this approach, in precision and recall, was 85.21% and 92.36%, respectively.},
  doi       = {10.1109/ECTICon.2013.6559585},
  keywords  = {dictionaries;natural language processing;pattern matching;Lao word segmentation;longest syllable level matching;Lao language;automatic word segmentation;natural language processing;named entities recognition approach;syllable extraction;dictionary based approach;Dictionaries;Indexes;Nickel;Educational institutions;Natural language processing;Lao word segmentation;tokenization;syllable extraction;longest matching;dictionary based;named entities recognition},
}

@InProceedings{6818181,
  author    = {W. {Wang} and T. {Zhao} and C. {Zhang}},
  title     = {Bilingual seed lexicon adaptation for entity translation extraction},
  booktitle = {2013 Ninth International Conference on Natural Computation (ICNC)},
  year      = {2013},
  pages     = {1309-1313},
  month     = {July},
  abstract  = {Bilingual seed lexicon, which is considered as a bridge between two languages, is one of the main resources used for entity translation extraction tasks from comparable corpora. However, little attention has been paid to this lexicon except its coverage. In fact, the quality of the seed lexicon is one of the key factors that affect the accuracy of entity translation extraction. In this paper, we propose a new self-adaptive model. We use a word segmentation technique to adapt segmented corpora and then propose two strategies of weight allocation and corresponding filter. Experiments demonstrate that our technique significantly outperforms the standard approach.},
  doi       = {10.1109/ICNC.2013.6818181},
  keywords  = {language translation;linguistics;natural language processing;text analysis;bilingual seed lexicon adaptation;entity translation extraction;self-adaptive model;word segmentation technique;segmented corpora;weight allocation;Resource management;Context;Noise;Vectors;Correlation;Standards;Radio spectrum management;adaptation;seed lexicon;comparable corpora;entity translation extraction},
}

@InProceedings{6639221,
  author    = {S. {Sitaram} and S. {Palkar} and Y. {Chen} and A. {Parlikar} and A. W. {Black}},
  title     = {Bootstrapping Text-to-Speech for speech processing in languages without an orthography},
  booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  year      = {2013},
  pages     = {7992-7996},
  month     = {May},
  abstract  = {Speech synthesis technology has reached the stage where given a well-designed corpus of audio and accurate transcription an at least understandable synthesizer can be built without necessarily resorting to new innovations. However many languages do not have a well-defined writing system but such languages could still greatly benefit from speech systems. In this paper we consider the case where we have a (potentially large) single speaker database but have no transcriptions and no standardized way to write transcriptions. To address this scenario we propose a method that allows us to bootstrap synthetic voices purely from speech data. We use a novel combination of automatic speech recognition and automatic word segmentation for the bootstrapping. Our experimental results on speech corpora in two languages, English and German, show that synthetic voices that are built using this method are close to understandable. Our method is language-independent and can thus be used to build synthetic voices from a speech corpus in any new language.},
  doi       = {10.1109/ICASSP.2013.6639221},
  keywords  = {bootstrapping;natural language processing;speech processing;speech recognition;speech synthesis;German language;English language;automatic word segmentation;automatic speech recognition;speech data;text-to-speech bootstrapping;speech processing;speech synthesis technology;well-defined writing system;single speaker database;synthetic voices;Speech;Data models;Speech recognition;Decoding;Synthesizers;Speech processing;Speech Synthesis;Synthesis without Text;Languages without an Orthography},
}

@InProceedings{6765459,
  author    = {Y. {Noguchi} and M. {Kondo} and S. {Kogure} and T. {Konishi} and Y. {Itoh} and A. {Takagi} and H. {Asoh} and I. {Kobayashi}},
  title     = {Generating a variety of expressions from visual information and user-designated viewpoints},
  booktitle = {2013 International Joint Conference on Awareness Science and Technology Ubi-Media Computing (iCAST 2013 UMEDIA 2013)},
  year      = {2013},
  pages     = {322-328},
  month     = {Nov},
  abstract  = {This paper reports the development and evaluation of a natural language generation system which generates a variety of language expressions from visual information taken by a CCD camera. The feature of this system is to generate a variety of language expressions from combinations of different syntactic structures and different sets of vocabulary, while managing the generation process based on the user-designated viewpoints. The system converts the visual information into a concept dependency structure using a semantic representation framework proposed by Takagi and Itoh. The system then transforms the structure and divides it into a set of words, deriving a word dependency structure, which is later arranged into a sentence. The transformation of a concept dependency structure and the variation in word segmentation allow the system to generate a variety of sentences from the same visual information. In this paper, we employ user-designated viewpoints to scenes containing more than one object. We designed the parameters of the user-designated viewpoints which enable the system to manage the generation process and to generate a variety of expressions. An evaluation has confirmed that the system generates certain variations according to parameter values set by the user. The variations include expressions referring to attribute values of the objects in the scenes and relative expressions denoting the relations between the targeted object and others.},
  doi       = {10.1109/ICAwST.2013.6765459},
  keywords  = {CCD image sensors;natural language processing;word processing;visual scene;word segmentation;word dependency structure;semantic representation framework;concept dependency structure;vocabulary;syntactic structure combination;CCD camera;natural language generation system;user-designated viewpoint;visual information;language expression generation;Semantics;Visualization;Natural languages;Image color analysis;Standards;Educational institutions;Prototypes;natural language generation;viewpoints;relative expressions;visual scene},
}

@InProceedings{6720529,
  author    = {S. {Somsap} and P. {Seresangtakul}},
  title     = {Isarn Dharma word segmentation},
  booktitle = {2013 International Conference on Control, Automation and Information Sciences (ICCAIS)},
  year      = {2013},
  pages     = {53-57},
  month     = {Nov},
  abstract  = {This paper presents Isarn Dhama word segmentation based on the Isarn Dharma writing system and dictionary. In this study, input text is segmented into sequences of Isarn Dharma Character Clusters (IDCCs). Each IDCC represents a group of inseparable Isarn Dharma characters based on the Isarn Dharma writing system. The sequence of IDCCs will be considered as input in order to look for the most suitable segmentation word from the dictionary using the IDCC longest matching algorithm. Grouping rules were then used to group adjacent remaining IDCCs that do not match an Isarn word in the dictionary. In order to evaluate the efficiency of the proposed technique, Isarn literature, Jataka, legend and Buddha foretell were used as the testing data to test the proposed system; comparing with longest matching and a hybrid of the IDCC longest matching. The experiment results showed that the F-measures are 80.15%, 85.06% and 86.07% for the longest matching, the IDCC longest matching algorithm, and the proposed method, respectively.},
  doi       = {10.1109/ICCAIS.2013.6720529},
  keywords  = {dictionaries;natural language processing;pattern clustering;text analysis;grouping rules;efficiency evaluation;Isarn literature;Jataka;Buddha foretell;testing data;F-measures;legend foretell;IDCC longest-matching algorithm;inseparable Isarn Dharma characters;IDCC sequences;Isarn Dharma character cluster sequences;input text segmentation;dictionary;Isarn Dharma writing system;Isarn Dharma word segmentation;Dictionaries;Clustering algorithms;Educational institutions;Computer science;Accuracy;Information technology},
}

@InProceedings{6645984,
  title     = {Table of contents},
  booktitle = {2013 International Conference on Asian Language Processing},
  year      = {2013},
  pages     = {v-x},
  month     = {Aug},
  abstract  = {The following topics are dealt with: Asian language processing; discourse analysis; information extraction; information retrieval; linguistics; language study; machine translation; sentiment analysis; spoken language processing; syntactic analysis; semantic analysis; text classification; text clustering; word segmentation; and POS tagging.},
  doi       = {10.1109/IALP.2013.4},
  keywords  = {information retrieval;language translation;linguistics;natural language processing;pattern classification;pattern clustering;text analysis;Asian language processing;discourse analysis;information extraction;information retrieval;linguistics;language study;machine translation;sentiment analysis;spoken language processing;syntactic analysis;semantic analysis;text classification;text clustering;word segmentation;POS tagging},
}

@InProceedings{6743944,
  author    = {R. {Sun} and W. {Zhou} and Z. {Liu}},
  title     = {Using language rules to improve the performance of word segmentation},
  booktitle = {2013 6th International Congress on Image and Signal Processing (CISP)},
  year      = {2013},
  volume    = {03},
  pages     = {1665-1669},
  month     = {Dec},
  abstract  = {Due to the disadvantage of the word segmentation tool in the aspect of the accuracy of word segmentation and speech tagging, it has harmful effects to the further research work. So this paper proposes a method based on the results of word segmentation to utilize language rules, which has been defined and described in Event Ontology, to verify and correct them. Experimental results show that compared with the method of only using word segmentation tool, the method of using language rules has a better performance.},
  doi       = {10.1109/CISP.2013.6743944},
  keywords  = {natural language processing;ontologies (artificial intelligence);text analysis;language rules;word segmentation tool;speech tagging;event ontology;Ontologies;Hidden Markov models;Accuracy;Tagging;Computers;Sun;Dictionaries;Language Rule;Word Segmentation;Performance;Event;Event Ontology},
}

@InProceedings{6376786,
  author    = {Y. {Dai} and X. {Ren}},
  title     = {A hybrid method to segment words},
  booktitle = {2012 International Conference on Audio, Language and Image Processing},
  year      = {2012},
  pages     = {1131-1134},
  month     = {July},
  abstract  = {Word segmentation is the foundations of machine translation, text classification and information searching. A method is proposed which combines word segmentation based on dictionary with reverse maximum matching and word segmentation based on statistic with suffix array. The input texts are segmented using the reserve maximum matching method based on dictionary, and a two-way suffix arrays are constructed, longest common prefix are computed, candidate words are filtered out by setting the threshold, the candidate words are filtered using mutual information in order to the true words. The texts that are ambiguity are filtered using information entropy. It is showed that the accuracy of word segmentation may achieve above 97% in the experiment.},
  doi       = {10.1109/ICALIP.2012.6376786},
  keywords  = {language translation;natural language processing;pattern classification;text analysis;hybrid method;word segmentation;machine translation;text classification;information searching;suffix array;input texts;common prefix;information entropy;Arrays;Dictionaries;Accuracy;Information filters;Sorting;Matched filters},
}

@InProceedings{6299310,
  author    = {H. {Yamamoto} and M. {Tanaka} and Y. {Kondo}},
  title     = {Diachronic Corpus and Linguistic Space: New Methods for the Analysis of Language Change},
  booktitle = {2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing},
  year      = {2012},
  pages     = {381-384},
  month     = {Aug},
  abstract  = {The project, design and development of the diachronic corpus of Japanese began in 2009 at the Department of Corpus Study, the National Institute of Japanese Language and Linguistics, Japan (NINJAL), as a collaborative research project by linguists and literature scholars of NINJAL and the University of Oxford. Its focus is on collecting representative Japanese literary works and classical documents from the tenth century to the nineteenth century. We are currently working on the development of the prototype version of the diachronic Japanese corpus: i.e. selection of materials, digitization of texts, addition of alternative texts (containing different orthography) to original texts, compilation of a basic thesaurus that differentiates between different spellings, and word segmentation. This paper addresses the discussion of the basic concepts encountered during our work on the project: synchronic and diachronic analysis, which led us to the design of a serial comparison model which allows us to examine language change between documents or literary works with respect to time.},
  doi       = {10.1109/SNPD.2012.104},
  keywords  = {linguistics;natural language processing;research and development;text analysis;linguistic space;language change analysis;national institute of Japanese language and linguistics;NINJAL;collaborative research project;University of Oxford;representative Japanese literary works;classical documents;diachronic Japanese corpus;thesaurus;word segmentation;diachronic analysis;synchronic analysis;Pragmatics;Analytical models;Educational institutions;Thesauri;Collaboration;Materials;History;diachronic;synchronic;language change;history of Japanese;serial comparison model;differential of lexical component},
}

@InProceedings{6189683,
  author    = {M. {Mishra} and V. K. {Mishra} and H. R. {Sharma}},
  title     = {Leveraging knowledge based question answer technology to address user-interactive short domain question in natural language},
  booktitle = {2012 2nd National Conference on Computational Intelligence and Signal Processing (CISP)},
  year      = {2012},
  pages     = {86-90},
  month     = {March},
  abstract  = {With the rapid growth of the Internet and database technologies in recent years, question answering systems (QAS) have emerged as important applications. Although many QAS have been implemented, little work has been done on the development of a user-centered evaluation for QAS. User-centered evaluation is used to understand a user's needs and identify important dimensions and factors in the development of an information system in order to improve its acceptance. This paper presents the application of understanding of short domain question in natural language to data query. The word segmentation tool, IK Analyzer, which is extended with the obtained domain dictionary, is used to segment the short text questions. From the segmentation results, the keywords are extracted to obtain query target and query requirement of the question and to generate a SQL statement for data query. The method proposed in this paper can be applied to question-answering system based on database and to develop a user-centered evaluation model for QAS from the user's perspective for enhancing the user satisfaction and acceptance of QAS.},
  doi       = {10.1109/NCCISP.2012.6189683},
  keywords  = {database management systems;natural language processing;query processing;question answering (information retrieval);user interfaces;knowledge based question answer technology;user-interactive short domain question;natural language;Internet;database technology;user-centered evaluation;user need;information system;data query;word segmentation tool;domain dictionary;short text question segmentation;query target;query requirement;SQL statement;Structured Query Language;user satisfaction;user acceptance;Databases;Natural languages;Dictionaries;Data mining;Information systems;Analytical models;Data models;Evaluation;Measurement;Question answering system;User satisfaction;domain dictionary;segmentation tool},
}

@InProceedings{6473732,
  author    = {Y. {Qin}},
  title     = {Machine Transliteration Based on Error-Driven Learning},
  booktitle = {2012 International Conference on Asian Language Processing},
  year      = {2012},
  pages     = {205-208},
  month     = {Nov},
  abstract  = {Transliteration is a common translation method when named entities are introduced into another language. Direct orthographical mapping (DOM) approach is successfully applied in machine transliteration by segmenting a word according to syllables and then mapping them directly into target language without considering its pronunciation. The paper studies the performance of two-stage machine transliteration based on Conditional Random Fields. To reduce the amount of computation in model training, we propose an error-driven learning by dividing the training data into several groups and training the transliteration model step by step based on the error prediction data until the performance doesnât increase or the limitation of the computer. Experiments on data of NEWS2011 show that error-driven model training reduces computational complexity and saves the time of model training. Compared to the combining transliteration model, our transliteration system increases the accuracy of top-1 output with 0.06, reaching 0.652.},
  doi       = {10.1109/IALP.2012.48},
  keywords  = {computational complexity;information retrieval;language translation;learning (artificial intelligence);natural language processing;random processes;two-stage machine transliteration;error-driven learning;translation method;direct orthographical mapping;DOM;conditional random fields;error-driven model training;computational complexity reduction;cross-lingual information retrieval;word segmentation;Training;Computational modeling;Training data;Data models;Conferences;Accuracy;Testing;Machine transliteration;Conditional Random Fields;Model training;Error-driven},
}

@InProceedings{6385812,
  author    = {T. {Araki} and T. {Nakamura} and T. {Nagai} and S. {Nagasaka} and T. {Taniguchi} and N. {Iwahashi}},
  title     = {Online learning of concepts and words using multimodal LDA and hierarchical Pitman-Yor Language Model},
  booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year      = {2012},
  pages     = {1623-1630},
  month     = {Oct},
  abstract  = {In this paper, we propose an online algorithm for multimodal categorization based on the autonomously acquired multimodal information and partial words given by human users. For multimodal concept formation, multimodal latent Dirichlet allocation (MLDA) using Gibbs sampling is extended to an online version. We introduce a particle filter, which significantly improve the performance of the online MLDA, to keep tracking good models among various models with different parameters. We also introduce an unsupervised word segmentation method based on hierarchical Pitman-Yor Language Model (HPYLM). Since the HPYLM requires no predefined lexicon, we can make the robot system that learns concepts and words in completely unsupervised manner. The proposed algorithms are implemented on a real robot and tested using real everyday objects to show the validity of the proposed system.},
  doi       = {10.1109/IROS.2012.6385812},
  keywords  = {educational robots;human-robot interaction;learning systems;natural language processing;online learning;multimodal LDA;hierarchical Pitman-Yor language model;online algorithm;multimodal categorization;multimodal information;partial words;multimodal concept formation;multimodal latent Dirichlet allocation;Gibbs sampling;particle filter;unsupervised word segmentation;predefined lexicon;robot system;Robot sensing systems;Humans;Vectors;Haptic interfaces;Data models;Predictive models},
}

@InProceedings{6204792,
  author    = {E. {Şahin} and H. {Adıgüzel} and P. {Duygulu} and M. {Kalpaklı}},
  title     = {OTAP ottoman archives internet interface},
  booktitle = {2012 20th Signal Processing and Communications Applications Conference (SIU)},
  year      = {2012},
  pages     = {1-2},
  month     = {April},
  abstract  = {Within Ottoman Text Archive Project a web interface to aid in uploading, binarization, line and word segmentation, labeling, recognition and testing of the Ottoman Turkish texts has been developed. It became possible to retrieve expert knowledge of scholars working with Ottoman archives through this interface, and apply this knowledge in developing further technologies in transliteration of historical manuscripts.},
  doi       = {10.1109/SIU.2012.6204792},
  keywords  = {expert systems;information retrieval systems;Internet;natural language interfaces;natural language processing;text analysis;OTAP ottoman;Internet interface;ottoman text archive project;Web interface;text uploading;text binarization;line segmentation;word segmentation;text labeling;text recognition;text testing;Ottoman Turkish texts;expert knowledge;Ottoman archives;transliteration;historical manuscripts;Internet;Abstracts;Text recognition;Testing;Educational institutions;Radio access networks;Transform coding},
}

@InProceedings{6176832,
  author    = {K. {Ghosh} and K. S. {Rao}},
  title     = {Subword based approach for grapheme-to-phoneme conversion in Bengali text-to-speech synthesis system},
  booktitle = {2012 National Conference on Communications (NCC)},
  year      = {2012},
  pages     = {1-5},
  month     = {Feb},
  abstract  = {In this paper, we propose a subword based approach for grapheme-to-phoneme (G2P) conversion in a text-to-speech (TTS) synthesis system. The proposed method resolves the problems present in both the manual and rule-based approaches for G2P conversion. The subword method uses a segmentation procedure which chops a word into its main part (root word) and subword part (suffix). By proper segmentation of a word, this method efficiently offers insight into the basic morphological information of the word. The proposed method reduces the size of the pronunciation dictionary without affecting the vocabulary coverage. For the rule-based approach, the proposed method segments the word into two parts and predicts the pronunciation for both the word segments separately. The final pronunciation is achieved by concatenating pronunciations for both the word segments. The subword method improves the accuracy of the rule-based approach by resolving the ambiguity especially in case of the inflected or compound words.},
  doi       = {10.1109/NCC.2012.6176832},
  keywords  = {natural language processing;speech synthesis;text analysis;vocabulary;subword based approach;grapheme-to-phoneme conversion;Bengali text-to-speech synthesis system;G2P conversion;TTS synthesis system;manual based approach;rule-based approach;word segmentation;root word;morphological information;pronunciation dictionary;vocabulary coverage;pronunciation concatenation;inflected word;compound word;Dictionaries;Manuals;Speech;Accuracy;Databases;Compounds;Information technology;Subword method;stemming;grapheme-to-phoneme conversion;text-to-speech synthesis},
}

@InProceedings{6424202,
  author    = {F. {Stahlberg} and T. {Schlippe} and S. {Vogel} and T. {Schultz}},
  title     = {Word segmentation through cross-lingual word-to-phoneme alignment},
  booktitle = {2012 IEEE Spoken Language Technology Workshop (SLT)},
  year      = {2012},
  pages     = {85-90},
  month     = {Dec},
  abstract  = {We present our new alignment model Model 3P for cross-lingual word-to-phoneme alignment, and show that unsupervised learning of word segmentation is more accurate when information of another language is used. Word segmentation with cross-lingual information is highly relevant to bootstrap pronunciation dictionaries from audio data for Automatic Speech Recognition, bypass the written form in Speech-to-Speech Translation or build the vocabulary of an unseen language, particularly in the context of under-resourced languages. Using Model 3P for the alignment between English words and Spanish phonemes outperforms a state-of-the-art monolingual word segmentation approach [1] on the BTEC corpus [2] by up to 42% absolute in F-Score on the phoneme level and a GIZA++ alignment based on IBM Model 3 by up to 17%.},
  doi       = {10.1109/SLT.2012.6424202},
  keywords  = {natural language processing;speech recognition;unsupervised learning;word segmentation;cross lingual word-to-phoneme alignment;unsupervised learning;cross lingual information;bootstrap pronunciation dictionaries;automatic speech recognition;speech-to-speech translation;English words;Spanish phonemes;Hidden Markov models;Error analysis;Grammar;Dictionaries;Vocabulary;Training data;Vectors;alignment model;word segmentation;under-resourced language;speech-to-speech translation},
}

@InProceedings{6108899,
  author    = {K. K. {Sita} and S. {Suhasini} and Z. P. {Shaik Mohd.}},
  title     = {Statistical estimation of emotions in speech notes by featured term analogy},
  booktitle = {2011 International Conference on Image Information Processing},
  year      = {2011},
  pages     = {1-6},
  month     = {Nov},
  abstract  = {Human Being is the only creature in the World who can express his emotions in various forms. Capturing the extent of emotions in a particular speech notes through quantification of verbal expressions is undoubtedly a challenging area to study. Either positive or negative, whatever be the emotions are in the notes, if we succeed in assessing the degree of positiveness or negativeness, the impact of the notes while addressing to the intended people can be pre-estimated easily. This paper presents a brief idea of how we can statistically estimate the emotional characteristics of a speech notes by analyzing the content in the speech notes through Information Extraction. Since it is mandatory for any sentence to have atleast a verb, we primarily concentrate on the verbs as featured terms in each sentence and there by statistically estimate the rigorousness of the verb on the entire speech note. The strength of the verb in its severity can be measured by comparing it with strong, medium, light corpus of emotions developed specially. This enables to judge the effectiveness of a speech notes in various emotions.},
  doi       = {10.1109/ICIIP.2011.6108899},
  keywords  = {emotion recognition;information retrieval;natural language processing;statistical analysis;word processing;statistical emotion estimation;speech note;term analogy feature;verbal expression quantification;information extraction;verb;Speech;Feature extraction;Tagging;Information processing;Speech processing;Estimation;Encoding;Emotion Estimator;Feature Term Identification (FTI);Inflectional morphemes;Normalization;POSTagging;Word Segmentation;Term Weight},
}

@InProceedings{6121461,
  author    = {T. {Laga} and X. {Zhao}},
  title     = {Theoretical Framework of Mongolian Word Segmentation Specification for Information Processing},
  booktitle = {2011 International Conference on Asian Language Processing},
  year      = {2011},
  pages     = {23-25},
  month     = {Nov},
  abstract  = {The establishment of Contemporary Mongolian word segmentation specification for information processing has a great significance in the standardization of information processing, the compatibleness of different systems, the sharing of corpus, grammatical analysis, and POS tagging. The present paper studies the framework of Mongolian word segmentation including guidelines, formulating principles, styles, scopes of segmentation units, establishment foundation, structure of the specification and so on, and lays the theoretical foundation for this specification.},
  doi       = {10.1109/IALP.2011.45},
  keywords  = {formal specification;natural language processing;text analysis;word processing;Mongolian word segmentation specification;information processing;corpus sharing;grammatical analysis;POS tagging;Information processing;Guidelines;Pragmatics;Educational institutions;Compounds;Grammar;Contemporary Mongolian word segmentation specification for information processing;theoretical framework;guidelines},
}

@InProceedings{6037316,
  author    = {T. {Armstrong} and S. {Antetomaso}},
  title     = {Unsupervised discovery of phoneme boundaries in multi-speaker continuous speech},
  booktitle = {2011 IEEE International Conference on Development and Learning (ICDL)},
  year      = {2011},
  volume    = {2},
  pages     = {1-5},
  month     = {Aug},
  abstract  = {Children rapidly learn the inventory of phonemes used in their native tongues. Computational approaches to learning phoneme boundaries from speech data do not yet reach the level of human performance. We present an algorithm that operates on, qualitatively, similar data to those children receive: natural language utterances from multiple speakers. Our algorithm is unsupervised and discovers phoneme boundary positions in speech. The approach draws inspiration from the word and text segmentation literature. To demonstrate the efficacy of our algorithm on speech data, we present empirical results of our method using the TIMIT data set. Our method achieves F-measure scores in the 0.68 - 0.73 range for locating phoneme boundary positions.},
  doi       = {10.1109/DEVLRN.2011.6037316},
  keywords  = {natural language processing;speech processing;unsupervised discovery;phoneme boundaries;multispeaker continuous speech;speech data;human performance;natural language utterances;multiple speakers;word segmentation;text segmentation;Manuals;Entropy;Feature extraction;Gold;Speech},
}

@InProceedings{5579805,
  author    = {{Aye Myat Mon} and {Soe Lai Phyue} and {Myint Myint Thein} and {Su Su Htay} and {Thinn Thinn Win}},
  title     = {Analysis of Myanmar Word boundary and segmentation by using Statistical Approach},
  booktitle = {2010 3rd International Conference on Advanced Computer Theory and Engineering(ICACTE)},
  year      = {2010},
  volume    = {5},
  pages     = {V5-233-V5-237},
  month     = {Aug},
  abstract  = {This paper proposed a unified approach for Myanmar Word analysis using Finite State Automata (FSA), Rule Based Heuristic Approach and Statistical Approach. Myanmar has no inter-word space and it make the tokenizing task difficulties. Therefore, to recognize the word, we implement with FSA. Segmentation is a major problem because of no delimiter. If there were errors in segmentation, this will cause subsequence failure in further NLP processes. Segmentation is also an essential preprocessing task for Natural Language Processing, such as Machine Translation, Information Retrieval etc. In this system, the Rule Based Heuristic Approach and Statistical Approach are used with corpus based dictionary. Evaluation results showed that the method is very effective for the Myanmar language.},
  doi       = {10.1109/ICACTE.2010.5579805},
  keywords  = {finite state machines;natural language processing;statistical analysis;word processing;word boundary;word segmentation;statistical approach;finite state automata;rule based heuristic approach;natural language processing;corpus based dictionary;Myanmar language;Entropy;Merging;Natural Language Processing;FSA;Segmentation;Syllable Merging;Statistical approach},
}

@InProceedings{5607428,
  author    = {R. {Zhang} and Q. {Zeng} and S. {Feng}},
  title     = {Data query using short domain question in natural language},
  booktitle = {2010 IEEE 2nd Symposium on Web Society},
  year      = {2010},
  pages     = {351-354},
  month     = {Aug},
  abstract  = {This paper presents the application of understanding of short domain question in natural language to data query. A domain dictionary can be obtained through extracting schema of all tables and short text data from database by ODBC API. The word segmentation tool, IK Analyzer, which is extended with the obtained domain dictionary, is used to segment the short text questions. From the segmentation results, the keywords are extracted to obtain query target and query requirement of the question and to generate a SQL statement for data query. The method proposed in this paper can be applied to question-answering system based on database.},
  doi       = {10.1109/SWS.2010.5607428},
  keywords  = {dictionaries;information retrieval systems;natural language processing;query processing;SQL;word processing;data query;short domain question;natural language;domain dictionary;ODBC API;word segmentation tool;IK analyzer;short text questions;query target;query requirement;SQL statement;question-answering system;Databases;Natural languages;Dictionaries;Data mining;Books;Arrays;Vocabulary},
}

@InProceedings{5555259,
  author    = {L. {Yuan}},
  title     = {Improvement for the automatic part-of-speech tagging based on hidden Markov model},
  booktitle = {2010 2nd International Conference on Signal Processing Systems},
  year      = {2010},
  volume    = {1},
  pages     = {V1-744-V1-747},
  month     = {July},
  abstract  = {In this paper, the Markov Family Models, a kind of statistical Models was firstly introduced. Under the assumption that the probability of a word depends both on its own tag and previous word, but its own tag and previous word are independent if the word is known, we simplify the Markov Family Model and use for part-of-speech tagging successfully. Experimental results show that this part-of-speech tagging method based on Markov Family Model has greatly improved the precision comparing the conventional POS tagging method based on Hidden Markov Model under the same testing conditions. The Markov Family Model is also very useful in other natural language processing technologies such as word segmentation, statistical parsing, text-to-speech, optical character recognition, etc.},
  doi       = {10.1109/ICSPS.2010.5555259},
  keywords  = {hidden Markov models;natural language processing;probability;speech processing;statistics;automatic part-of-speech tagging;hidden Markov model;statistical models;probability;natural language processing;Hidden Markov models;Markov processes;Tagging;Biological system modeling;Training;Natural language processing;Markov Family model;Part-of-Speech tagging;Hidden Markov model;Viterbi algorithm},
}

@InProceedings{5565148,
  author    = {A. {Agarwal} and A. {Jain} and N. {Prakash} and S. S. {Agrawal}},
  title     = {Word based emotion conversion in Hindi language},
  booktitle = {2010 3rd International Conference on Computer Science and Information Technology},
  year      = {2010},
  volume    = {9},
  pages     = {419-423},
  month     = {July},
  abstract  = {The main system to communicate with computers is the use of natural language in text form. To add naturalness and intelligibility, the speech form is becoming an important method to communicate with computers and other machines. Human-machine and human-robot dialogues in the next generation will be conquered by natural speech, which is fully impulsive and thus obsessed by emotion. Emotion adds expressiveness to the natural language speech. There is a great zeal to research in this field. In this paper we have proposed an algorithm for word based emotion conversion of neutral speech into emotional speech like `happy' and `sad' for Hindi language. This emotion conversion algorithm is based on the segmentation of the spoken utterance into words and the pitch differences of these words between different emotions. The segmentation of words in the spoken utterance is done using another algorithm of word boundary detection, which gives details of start of the word, end of the word and the no. of words in a sentence. This algorithm of word boundary detection is devised for Hindi language and is based on two main prosodic features `pitch' and `intensity'.},
  doi       = {10.1109/ICCSIT.2010.5565148},
  keywords  = {emotion recognition;human computer interaction;human-robot interaction;natural language processing;speech intelligibility;speech processing;word based emotion conversion;Hindi language;speech intelligibility;speech form;human-machine dialogue;human-robot dialogue;natural speech;natural language speech;neutral speech;emotional speech;emotion conversion algorithm;spoken utterance;pitch differences;word segmentation;word boundary detection;Databases;Word boundary detection;Emotion conversion;Prosody;Pitch;Intensity},
}

@InProceedings{5423133,
  author    = {K. K. {Zin} and N. L. {Thein}},
  title     = {Part of speech Tagging for Myanmar using Hidden Markov Model},
  booktitle = {2009 International Conference on the Current Trends in Information Technology (CTIT)},
  year      = {2009},
  pages     = {1-6},
  month     = {Dec},
  abstract  = {Part-Of-Speech (POS) Tagging is the process of assigning the words with their categories that best suits the definition of the word as well as the context of the sentence in which it is used. In this paper, we describe a machine learning algorithm for Myanmar Tagging using a corpus-based approach. In order to tag Myanmar language, we need to take part word segmentation, part of speech tagging using HMM and several Tag-sets. Thus, this paper deals with a combination of supervised and un-supervised learning which use pre-tagged and untagged corpus respectively. To assign to each word with the correct tag, we describe Supervised POS Tagging by using the class labels in terms of predictor features on manually tagged corpus and also describe Unsupervised POS Tagging for automatically training without using a manually tagged corpus. By experiments, the best configuration is investigated on different amount of training data and the accuracy is 97.56%.},
  doi       = {10.1109/CTIT.2009.5423133},
  keywords  = {hidden Markov models;natural language processing;speech processing;unsupervised learning;hidden Markov model;part of speech tagging;machine learning algorithm;Myanmar Tagging;corpus based approach;Myanmar language;word segmentation;HMM;unsupervised learning;pretagged corpus;untagged corpus;unsupervised POS tagging;Tagging;Hidden Markov models;Natural languages;Speech processing;Machine learning algorithms;Training data;Natural language processing;White spaces;Speech analysis;Testing},
}

@InProceedings{5313753,
  author    = {G. {Zhang} and Y. {Gao} and D. {Ji} and X. {Ren}},
  title     = {Research on Katakana phrase translation based on bi-directional integration},
  booktitle = {2009 International Conference on Natural Language Processing and Knowledge Engineering},
  year      = {2009},
  pages     = {1-6},
  month     = {Sep.},
  abstract  = {In order to solve the problem of Katakana reduced to English in Japanese-English translation, we employ the phrase-based statistical machine translation model to perform Katakana phrase (or word) translation from Japanese to English. The Katakana phrase is segmented into words by CRF, and then Japanese-English and English-Japanese bi-directional integration translation is carried out on those segmented results. The translated results of all the segmented words are comprehensively scored to obtain the best English phrase translation result. The experimental results indicate that the Katakana phrase translation precision reaches 76%, effectively addresses the problem of the Katakana reduced to English.},
  doi       = {10.1109/NLPKE.2009.5313753},
  keywords  = {computational linguistics;language translation;natural language processing;statistical analysis;text analysis;Katakana phrase translation;bidirectional integration;Japanese-English translation;phrase-based statistical machine translation;Katakana word translation;word segmentation;Bidirectional control;Knowledge engineering;Aerospace engineering;Dictionaries;Terminology;Natural languages;Katakana;Bi-directional Integration;Phrase-based Statistical Machine Translation},
}

@InProceedings{5395511,
  author    = {V. K. {Koppula} and N. {Atul} and U. {Garain}},
  title     = {Robust Text Line, Word And Character Extraction from Telugu Document Image},
  booktitle = {2009 Second International Conference on Emerging Trends in Engineering Technology},
  year      = {2009},
  pages     = {269-272},
  month     = {Dec},
  abstract  = {Designing an OCR system for Indian languages in general is more complex than those of European languages due the linguistic complexity. Efforts are on the way for the development of efficient OCR systems for Indian languages, especially for Telugu, a popular South Indian language. In this paper, we proposed a method for reliable extraction of text line, word and character from document images of Telugu scripts. In the text line segmentation, first we establish the relationship between the connected components and then cluster the connected components of a line using vertical spatial relation and nearest neighbor algorithm. In word segmentation, the space between two adjacent characters is computed and clustered into word space and character space. Consonant and vowel modifiers are segregated from the word image and segment the characters.},
  doi       = {10.1109/ICETET.2009.196},
  keywords  = {document image processing;feature extraction;image segmentation;natural language processing;optical character recognition;text analysis;word extraction;character extraction;Telugu document image;OCR system;Indian languages;linguistic complexity;text line segmentation;vertical spatial relation;nearest neighbor algorithm;word segmentation;text line extraction;Robustness;Image segmentation;Optical character recognition software;Nearest neighbor searches;Clustering algorithms;Carbon capture and storage;Educational institutions;Computational Intelligence Society;Computer vision;Character recognition},
}

@Article{5175425,
  author   = {G. {Tambouratzis}},
  title    = {Using an Ant Colony Metaheuristic to Optimize Automatic Word Segmentation for Ancient Greek},
  journal  = {IEEE Transactions on Evolutionary Computation},
  year     = {2009},
  volume   = {13},
  number   = {4},
  pages    = {742-753},
  month    = {Aug},
  abstract = {Given a text or collection of texts involving unconstrained language, a basic task in a multitude of applications is the identification of stems and endings for each word form, which is termed morphological analysis. In this paper, the use of an ant colony optimization (ACO) metaheuristic is proposed for a linguistic task that involves the automated morphological segmentation of Ancient Greek word forms into stem and ending. The task of morphological analysis is essential for implementing text-processing applications such as semantic analysis and information retrieval. The difficulty of the morphological analysis task differs depending on the language chosen, being hardest in the case of highly-inflectional languages, where each stem may be associated with a large number of different endings. In this paper, focus is placed on the morphological analysis of ancient Greek, which has been shown to be a particularly hard task. To perform this task, a system for the automated morphological processing has been proposed, which implements the morphological analysis of words by coupling an iterative pattern-recognition algorithm with a modest amount of linguistic knowledge, expressed via a set of interactions associated with weights. In an earlier version of the system, these weights were determined by combining the input from specialized scientists with a lengthy manual optimization process. In this paper, the ACO metaheuristic is applied to the task of defining near-optimal system weights using an automated process based on a set of training data. The experiments performed indicate that the segmentation quality achieved by ACO is equivalent to or in several cases substantially higher than that achieved using manually optimized weights.},
  doi      = {10.1109/TEVC.2009.2014363},
  keywords = {iterative methods;linguistics;natural language processing;optimisation;pattern recognition;text analysis;ant colony metaheuristic;optimize automatic word segmentation;ancient Greek;morphological analysis;semantic analysis;information retrieval;text processing application;highly-inflectional languages;automated morphological processing;iterative pattern-recognition algorithm;linguistic knowledge;manual optimization process;near-optimal system;segmentation quality;Ant colony optimization;Information analysis;Information retrieval;Data mining;Pattern analysis;Performance analysis;Algorithm design and analysis;Iterative algorithms;Training data;Text processing;Ancient Greek;ant colony optimization (ACO) metaheuristic;automated morphological analysis;heuristic function;text processing},
}

@InProceedings{4620976,
  author    = {{Hsien-Chang Wang} and {Yueh-Chin Chan}},
  title     = {On the abstraction and presentation of multi-source knowledge},
  booktitle = {2008 International Conference on Machine Learning and Cybernetics},
  year      = {2008},
  volume    = {6},
  pages     = {3307-3309},
  month     = {July},
  abstract  = {This paper proposed a knowledge abstraction and presentation system by information gathered Internet web pages. Documents gathered from different Websites are first segmented into different paragraphs according to their topics. The linguistic processing such as word segmentation, word tagging and word frequency evaluation are applied to these corpora first. Then two types of similarities are calculated in our study: the paragraph-based and sentence-based similarity.},
  doi       = {10.1109/ICMLC.2008.4620976},
  keywords  = {abstracting;information retrieval;Internet;natural language processing;word processing;multi-source knowledge;knowledge abstraction;knowledge presentation;Internet web pages;linguistic processing;word segmentation;word tagging;word frequency evaluation;paragraph-based similarity;sentence-based similarity;mean opinion score evaluation;Web pages;Knowledge engineering;Internet;Information filters;Data mining;Information filtering;Machine learning},
}

@InProceedings{4723042,
  author    = {W. {Bo} and L. {Yunqing}},
  title     = {Research on the Design of the Ontology-Based Automatic Question Answering System},
  booktitle = {2008 International Conference on Computer Science and Software Engineering},
  year      = {2008},
  volume    = {5},
  pages     = {871-874},
  month     = {Dec},
  abstract  = {Automatic question answering system is a hot issue in the field of natural language processing, and is playing an increasingly important role in the long-distance teaching through networks. This paper proposes an ontology-based automatic question answering system model, at first, build restricted area ontology, then take advantage of the accurate description of concept as well as the definition of the relationship of the concepts, to expand keywords and improve the accuracy and recall rates.},
  doi       = {10.1109/CSSE.2008.233},
  keywords  = {distance learning;information retrieval;knowledge based systems;natural language processing;ontologies (artificial intelligence);teaching;ontology-based automatic question answering system;natural language processing;long-distance teaching;knowledge base;Ontologies;Education;Educational institutions;Artificial intelligence;Natural language processing;Information analysis;Statistical analysis;Natural languages;Computer science;Software engineering;automatic question answering system;word segmentation;ontology;knowledge base},
}

@InProceedings{4579373,
  author    = {M. A. {Sattar} and K. {Mahmud} and H. {Arafat} and A. F. M. {Noor Uz Zaman}},
  title     = {Segmenting bangla text for optical recognition},
  booktitle = {2007 10th international conference on computer and information technology},
  year      = {2007},
  pages     = {1-6},
  month     = {Dec},
  abstract  = {One of the important reasons for poor recognition rate in optical character recognition (OCR) system is the error in character segmentation. Existence of different type of characters in the scanned documents is a major problem to design an effective character segmentation procedure. In this paper, a new technique is presented for identification and segmentation of Bengali printed characters. This paper focuses on the segmentation of printed Bengali characters for efficient recognition of the characters. Our Line segmentation success rate is 99.7 % for 1000 lines, we have tested. Our Word segmentation success rate is 99.8 % for 4900 words tested. From the experiment we noticed that isolated characters fall into isolated group in 99.50 % cases. Most of the errors come from connected characters and characters having tau in front of them as segmenting tau we take the help of width. From the experiment we noticed that most of the errors came from components having multi-touching points between two characters.},
  doi       = {10.1109/ICCITECHN.2007.4579373},
  keywords  = {natural language processing;optical character recognition;text analysis;optical character recognition;character segmentation procedure;Bengali printed characters;line segmentation;word segmentation;text segmentation;Bangla OCR;Bangla Text segmentation;Bangla Language Processing},
}

@InProceedings{4428144,
  author    = {Q. {Guo} and K. {Wu} and W. {Li}},
  title     = {The Research and Realization about Question Answer System based on Natural Language Processing},
  booktitle = {Second International Conference on Innovative Computing, Informatio and Control (ICICIC 2007)},
  year      = {2007},
  pages     = {502-502},
  month     = {Sep.},
  abstract  = {Automatic Question Answer System (QAS) is a kind of high-powered software system based on Internet. Its key technology is the interrelated technology based on natural language understanding, including the construction of knowledge base and corpus, the Word Segmentation and POS Tagging of text, the Grammatical Analysis and Semantic Analysis of sentences etc. This thesis dissertated mainly the denotation of knowledge-information based on semantic network in QAS, the stochastic syntax-parse model named LSF of knowledge-information in QAS, the structure and constitution of QAS. And the LSF model parameters were exercised; it proved that they are feasible. At the same time, through "the limited-domain QAS" which was exploited for banks by us, these technologies are proved effective and propagable.},
  doi       = {10.1109/ICICIC.2007.585},
  keywords  = {computational linguistics;grammars;information retrieval;information retrieval systems;natural language processing;semantic networks;automatic question answering system;natural language processing;high-powered software system;Internet;word segmentation;text POS tagging;sentence grammatical analysis;sentence semantic analysis;semantic network;LSF stochastic syntax-parse model;information retrieval;Natural language processing;Natural languages;Information analysis;Internet;Constitution;Web server;Databases;Software systems;Tagging;Stochastic processes},
}

@InProceedings{4410418,
  author    = {T. {Fukuda} and M. {Izumi} and T. {Miura}},
  title     = {Word Segmentation Using Domain Knowledge Based on Conditional Random Fields},
  booktitle = {19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)},
  year      = {2007},
  volume    = {2},
  pages     = {436-439},
  month     = {Oct},
  abstract  = {In this investigation, we propose an experimental approach for word segmentation in Japanese under domain-dependent situation. We apply Conditional Random Fields (CRF) to our issue. CRF learns several probabilistic parameters from training data with specific feature functions dependent on domains. Here we propose how to define domain specific feature functions.},
  doi       = {10.1109/ICTAI.2007.93},
  keywords  = {learning (artificial intelligence);natural language processing;probability;random processes;text analysis;Japanese word segmentation;conditional random field;probabilistic parameter;training data;domain specific feature function;domain knowledge;text processing;Natural languages;Artificial intelligence;Training data;Statistics;Stochastic processes;Pattern analysis;Speech;Dictionaries},
}

@InProceedings{953918,
  author    = {Y. {Ishitani}},
  title     = {Model-based information extraction method tolerant of OCR errors for document images},
  booktitle = {Proceedings of Sixth International Conference on Document Analysis and Recognition},
  year      = {2001},
  pages     = {908-915},
  month     = {Sep.},
  abstract  = {A new method for information extraction from document images is proposed in this paper as the basis for a document reader which can extract required keywords and their logical relationship from various printed documents. Such documents obtained from OCR results may have not only unknown words and compound words, but also incorrect words due to OCR errors. To cope with OCR errors, the proposed method adopts robust keyword matching which searches for a string pattern from two dimensional OCR results consisting of a set of possible character candidates. This keyword matching uses a keyword dictionary that includes incorrect words with typical OCR errors and segments of words to deal with the above difficulties. After keyword matching, a global document matching is carried out between keyword matching results in an input document and document models which consist of keyword models and their logical relationship. This global matching determines the most suitable model for the input document and solves word segmentation problems accurately even if the document has unknown words, compound words, or incorrect words. Experimental results obtained for 100 documents show that the method is robust and effective for various document structures.},
  doi       = {10.1109/ICDAR.2001.953918},
  keywords  = {information retrieval;optical character recognition;document image processing;image matching;dictionaries;image segmentation;string matching;model-based information extraction;OCR errors;document image processing;document reader;printed documents;keyword dictionary;global document matching;word segmentation;experimental results;unknown words;compound words;keyword matching;Data mining;Optical character recognition software;Robustness;Error analysis;Research and development;Pattern matching;Dictionaries;Natural language processing;Image segmentation;Information analysis},
}

@InProceedings{607161,
  author    = {A. {Ito} and M. {Kohda}},
  title     = {Language modeling by string pattern N-gram for Japanese speech recognition},
  booktitle = {Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96},
  year      = {1996},
  volume    = {1},
  pages     = {490-493 vol.1},
  month     = {Oct},
  abstract  = {This paper describes a new powerful statistical language model based on N-gram model for Japanese speech recognition. In English, a sentence is written word-by-word. On the other hand. A sentence in Japanese has no word boundary character. Therefore. A Japanese sentence requires word segmentation by morphemic analysis before the construction of word N-gram. We propose an N-gram based language model which requires no word segmentation. This model uses character string patterns as units of N-gram. The string patterns are chosen from the training text according to a statistical criterion. We carried out several experiments to compare perplexities of the proposed and the conventional models. which showed the advantage of our model. For many of the readers' interest, we applied this method to English text. As the result of a preliminary experiment, the proposed method got better performance than conventional word trigram.},
  doi       = {10.1109/ICSLP.1996.607161},
  keywords  = {speech recognition;natural languages;string pattern N-gram;Japanese speech recognition;language modeling;statistical language model;word segmentation;morphemic analysis;conventional word trigram;Natural languages;Speech recognition;Speech analysis;Spread spectrum communication;Information analysis;Information retrieval;Natural language processing;Probability;Dictionaries;Testing},
}

@Conference{Liu2019,
  author        = {Liu, D. and Su, J. and Song, L. and Qiu, Z.},
  title         = {Application of Internet segmentation research based on Natural Language Processing technology in enterprise public opinion risk monitoring},
  year          = {2019},
  volume        = {1187},
  number        = {4},
  note          = {cited By 0},
  abstract      = {With the advent of the mobile Internet era, the network has become a distribution center of various information such as media, entertainment, sports, economy, politics and so on. A large amount of information is generated and disappeared on the network every day. How to effectively extract and identify the relevant data, and judge and analyze them is an important part of the corporate public opinion control. This paper uses natural language processing technology to study the word segmentation of text information on the network, and applies it to the risk detection of corporate public opinion. © Published under licence by IOP Publishing Ltd.},
  art_number    = {042007},
  document_type = {Conference Paper},
  doi           = {10.1088/1742-6596/1187/4/042007},
  journal       = {Journal of Physics: Conference Series},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067651831&doi=10.1088%2f1742-6596%2f1187%2f4%2f042007&partnerID=40&md5=e3d2e39577932407e5d637d0a2726081},
}

@Conference{Zhao2019799,
  author        = {Zhao, T. and Li, L. and Xie, Y. and Lv, Y.},
  title         = {Data-driven Risk Assessment for Peer-to-Peer Network Lending Agencies},
  year          = {2019},
  pages         = {799-803},
  note          = {cited By 0},
  abstract      = {With the rapid development of Peer-to-Peer(P2P) network lending in the financial field, more data of lending agencies have appeared. P2P agencies also have problems such as absconded with ill-gotten gains and out of business. Therefore, it is necessary to assess their risks based on P2P company data. This paper proposes a framework of Data-driven Risk Assessment for P2P(DRAP2P) network lending agencies based on unstructured natural language data. First, use the natural language processing technology, such as word segmentation, keyword, LDA topic model, word2vec and doc2vec, to process and extract features of company profile which reflect its business status. Then, seven machine learning classifiers and three deep learning models are used for analysis. Since keywords show good performance in machine learning models, we improve Convolutional Neural Network(CNN) with keywords and propose two CNN+Keyword models, namely CNN+Keyword(static+BP) and CNN+Keyword (Expand word embedding). Experiments have shown that CNN+Keyword(static+BP) can achieve the best performance. Finally, we use the method of meta-learning to integrate CNN+Keyword(static+BP) and logistic regression classifier to further strengthen the performance. © 2018 IEEE.},
  art_number    = {8691202},
  document_type = {Conference Paper},
  doi           = {10.1109/CCIS.2018.8691202},
  journal       = {Proceedings of 2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems, CCIS 2018},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064990311&doi=10.1109%2fCCIS.2018.8691202&partnerID=40&md5=f46925816230dae8fa8ed053b74988cc},
}

@Article{Nguyen2019,
  author        = {Nguyen, H.T. and Duong, P.H. and Cambria, E.},
  title         = {Learning short-text semantic similarity with word embeddings and external knowledge sources},
  journal       = {Knowledge-Based Systems},
  year          = {2019},
  volume        = {182},
  note          = {cited By 0},
  abstract      = {We present a novel method based on interdependent representations of short texts for determining their degree of semantic similarity. The method represents each short text as two dense vectors: the former is built using the word-to-word similarity based on pre-trained word vectors, the latter is built using the word-to-word similarity based on external sources of knowledge. We also developed a preprocessing algorithm that chains coreferential named entities together and performs word segmentation to preserve the meaning of phrasal verbs and idioms. We evaluated the proposed method on three popular datasets, namely Microsoft Research Paraphrase Corpus, STS2015 and P4PIN, and obtained state-of-the-art results on all three without using prior knowledge of natural language, e.g., part-of-speech tags or parse tree, which indicates the interdependent representations of short text pairs are effective and efficient for semantic textual similarity tasks. © 2019 Elsevier B.V.},
  art_number    = {104842},
  document_type = {Article},
  doi           = {10.1016/j.knosys.2019.07.013},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069875471&doi=10.1016%2fj.knosys.2019.07.013&partnerID=40&md5=79164e89f20b11d4c4cf48b954deaae2},
}

@Conference{Shen2019660,
  author        = {Shen, H. and Long, C. and Wan, W. and Li, J. and Qin, Y. and Fu, Y. and Song, X.},
  title         = {Log Layering Based on Natural Language Processing},
  year          = {2019},
  volume        = {2019-February},
  pages         = {660-663},
  note          = {cited By 0},
  abstract      = {With the increasing number and variety of logs, the requirement of storage space is growing rapidly. Meantime, the speed and accuracy of querying in massive logs are becoming increasingly important. Although the well-built distributed storage technique solves the problem of mass storage and fast query, the cost is too high. As logs are created as the method to trace the historical operation, the requirement for query rate is not high. To balance the storage cost and query rate, this paper proposes a real-time log layering storage technique based on natural language processing. According to the characteristics of the log data, this technique is combined with the text language processing technique. It compresses the real-time log data effectively while considering the query efficiency. Firstly, the method extracts the feature of each log that flows in, which will be the type name of the log. Then, the method performs word segmentation on the log and encodes each word to store the key value pairs. Finally, the key value pairs of the log are stored in the memory, and the code of each log is stored in the database. Experiments show that this method can ensure the integrity of the data effectively, decompression time dropped to 40%, compression rate down to 35%. © 2019 Global IT Research Institute (GIRI).},
  art_number    = {8702019},
  document_type = {Conference Paper},
  doi           = {10.23919/ICACT.2019.8702019},
  journal       = {International Conference on Advanced Communication Technology, ICACT},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065643105&doi=10.23919%2fICACT.2019.8702019&partnerID=40&md5=c381ecc920d99ffe0e61b386ef3cb024},
}

@Article{Zuters2019225,
  author        = {Zuters, J. and Strazds, G. and Ļeonova, V.},
  title         = {Morphology-inspired word segmentation for neural machine translation},
  journal       = {Frontiers in Artificial Intelligence and Applications},
  year          = {2019},
  volume        = {315},
  pages         = {225-239},
  note          = {cited By 0},
  abstract      = {This paper proposes the Prefix-Root-Postfix-Encoding (PRPE) algorithm, which performs close-to-morphological segmentation of words as part of text pre-processing in machine translation. PRPE is a cross-language algorithm requiring only minor tweaking to adapt it for any particular language, a property which makes it potentially useful for morphologically rich languages with no morphological analysers available. As a key part of the proposed algorithm we introduce the ‘Root alignment’ principle to extract potential sub-words from a corpus, as well as a special technique for constructing words from potential sub-words. In addition, we supplemented the algorithm with specific processing for named-entities based on transliteration. We conducted experiments with two different neural machine translation systems, training them on parallel corpora for English-Latvian and Latvian-English translation. Evaluation of translation quality showed improvements in BLEU scores when the data were pre-processed using the proposed algorithm, compared to a couple of baseline word segmentation algorithms. Although we were able to demonstrate improvements in both translation directions and for both NMT systems, they were relatively minor, and our experiments show that machine translation with inflected languages remains challenging, especially with translation direction towards a highly inflected language. © 2019 The authors and IOS Press.},
  document_type = {Conference Paper},
  doi           = {10.3233/978-1-61499-941-6-225},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063356974&doi=10.3233%2f978-1-61499-941-6-225&partnerID=40&md5=f492e09e511a1872d8b05ca83773ab74},
}

@Conference{Moreau20191119,
  author        = {Moreau, E. and Vogel, C.},
  title         = {Multilingual word segmentation: Training many language-specific tokenizers smoothly thanks to the universal dependencies corpus},
  year          = {2019},
  pages         = {1119-1127},
  note          = {cited By 1},
  abstract      = {This paper describes how a tokenizer can be trained from any dataset in the Universal Dependencies 2.1 corpus (UD2) (Nivre et al., 2017). A software tool, which relies on Elephant (Evang et al., 2013) to perform the training, is also made available. Beyond providing the community with a large choice of language-specific tokenizers, we argue in this paper that: (1) tokenization should be considered as a supervised task; (2) language scalability requires a streamlined software engineering process across languages. © LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.},
  document_type = {Conference Paper},
  journal       = {LREC 2018 - 11th International Conference on Language Resources and Evaluation},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059903761&partnerID=40&md5=0d85521a7ec28ece3d5e35165d639dda},
}

@Conference{Mzamo2019166,
  author        = {Mzamo, L. and Helberg, A. and Bosch, S.},
  title         = {Towards an unsupervised morphological segmenter for isiXhosa},
  year          = {2019},
  pages         = {166-170},
  note          = {cited By 0},
  abstract      = {In this paper, branching entropy techniques and isiXhosa language heuristics are adapted to develop unsupervised morphological segmenters for isiXhosa. An overview of isiXhosa segmentation issues is given, followed by a discussion on previous work in automated segmentation, and segmentation of isiXhosa in particular. Two unsupervised isiXhosa segmenters are presented and compared to a random minimum baseline and Morfessor-Baseline, a standard in unsupervised word segmentation. Morfessor-Baseline outperforms both isiXhosa segmenters at 79.10% boundary identification accuracy. The IsiXhosa Branching Entropy Segmenter (XBES) performance varies depending on the segmentation mode used, with a maximum of 73.39%. The IsiXhosa Heuristic Maximum Likelihood Segmenter (XHMLS) achieves 72.42%. The study suggests that unsupervised isiXhosa morphological segmentation is feasible with better optimization of the current attempts. © 2019 IEEE.},
  art_number    = {8704816},
  document_type = {Conference Paper},
  doi           = {10.1109/RoboMech.2019.8704816},
  journal       = {Proceedings - 2019 Southern African Universities Power Engineering Conference/Robotics and Mechatronics/Pattern Recognition Association of South Africa, SAUPEC/RobMech/PRASA 2019},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065817284&doi=10.1109%2fRoboMech.2019.8704816&partnerID=40&md5=88adfed9927ac18eafb96aaa7e70bbb9},
}

@Article{Teahan2018,
  author        = {Teahan, W.J.},
  title         = {A compression-based toolkit for modelling and processing natural language text},
  journal       = {Information (Switzerland)},
  year          = {2018},
  volume        = {9},
  number        = {12},
  note          = {cited By 1},
  abstract      = {A novel compression-based toolkit for modelling and processing natural language text is described. The design of the toolkit adopts an encoding perspective-applications are considered to be problems in searching for the best encoding of different transformations of the source text into the target text. This paper describes a two phase 'noiseless channel model' architecture that underpins the toolkit which models the text processing as a lossless communication down a noise-free channel. The transformation and encoding that is performed in the first phase must be both lossless and reversible. The role of the verification and decoding second phase is to verify the correctness of the communication of the target text that is produced by the application. This paper argues that this encoding approach has several advantages over the decoding approach of the standard noisy channel model. The concepts abstracted by the toolkit's design are explained together with details of the library calls. The pseudo-code for a number of algorithms is also described for the applications that the toolkit implements including encoding, decoding, classification, training (model building), parallel sentence alignment, word segmentation and language segmentation. Some experimental results, implementation details, memory usage and execution speeds are also discussed for these applications. © 2018 by the authors.},
  art_number    = {294},
  document_type = {Article},
  doi           = {10.3390/info9120294},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058994088&doi=10.3390%2finfo9120294&partnerID=40&md5=c332a4ee5417f7acc966c9f13201ca3a},
}

@Article{Wang2018466,
  author        = {Wang, X. and Gao, C. and Cao, J. and Lin, K. and Du, W. and Yang, Z.},
  title         = {ALTAS: An intelligent text analysis system based on knowledge graphs},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2018},
  volume        = {10987 LNCS},
  pages         = {466-470},
  note          = {cited By 0},
  abstract      = {This paper presents an intelligent text analysis system, called ALTAS, to support various text analysis tasks such as statistics analysis, sentiment analysis, text classification, and text clustering. The system contains four main components: knowledge graphs, text processing, text analysis and intelligent report. First, the system has built a semantic-rich knowledge base using several knowledge graph resources. A novel text processing and analysis framework based on knowledge graphs is developed and implemented. Given a text dataset, the text processing phase will do data cleaning, word segmentation and feature extraction for it. With the extracted features, the text analysis phase allows users to select a text mining task. We have implemented the proposed novel algorithm and several typical algorithms for each task. If users select multiple algorithms for the task, the intelligent report phase will automatically generate comparison results for users. Especially, the intelligent report phase also provides users a paper summary generating function on text mining problems. © Springer International Publishing AG, part of Springer Nature 2018.},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-319-96890-2_40},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050537718&doi=10.1007%2f978-3-319-96890-2_40&partnerID=40&md5=671150c35b5218bda2fe429115d446d7},
}

@Conference{Long201880,
  author        = {Long, P. and Boonjing, V.},
  title         = {Longest Matching and Rule-based Techniques for Khmer Word Segmentation},
  year          = {2018},
  pages         = {80-83},
  note          = {cited By 0},
  abstract      = {Word boundaries are the essential assignment to be done in natural language processing research. In most Asian languages, as well as Khmer language, many studies involved with word segmentation have been investigated. In Khmer Word Segmentation, several approaches related to segmenting words based on dictionary have been studied. There are only few researches about solving unknown word problem. This matter is a quite challenge task in word separation. In this research, Maximum Matching algorithm (MMA) together with Rule-based technique has been proposed. First, MMA and a Khmer manual corpus were used to make word boundaries in each sentence. Then the unknown words were then defined and solved by using 21 grammar rules created. We tested the segmentation with 2018 sentences from agriculture, magazine, newspaper, technology, health and history. With Maximum Matching alone, we could achieve the accuracy of 88.55% and along with Rule-based, the accuracy increased to 92.81%. © 2018 IEEE.},
  art_number    = {8426109},
  document_type = {Conference Paper},
  doi           = {10.1109/KST.2018.8426109},
  journal       = {2018 10th International Conference on Knowledge and Smart Technology: Cybernetics in the Next Decades, KST 2018},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052291916&doi=10.1109%2fKST.2018.8426109&partnerID=40&md5=a220d539ec45a5e0ac28980d7e0a3b26},
}

@Article{Liu2018558,
  author        = {Liu, N. and Su, X. and Gao, G. and Bao, F.},
  title         = {Mongolian word segmentation based on three character level Seq2Seq models},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2018},
  volume        = {11305 LNCS},
  pages         = {558-569},
  note          = {cited By 0},
  abstract      = {Mongolian word segmentation is splitting the Mongolian words into roots and suffixes. It plays an important role in Mongolian related natural language processing tasks. To improve performance and avoid the tedious work of rule-making and statistics over large-scale corpus in early methods, this work takes a Seq2Seq framework to realize Mongolian word segmentation. Since each Mongolian word consisted of several sequential characters, we map Mongolian word segmentation to character-level Seq2Seq task, and further propose three different models from three different prospective to achieve the segmentation goal. The three character-level Seq2Seq models are (1) translation model, (2) true and pseudo mapping model, (3) binary choice model. The main differences of these three models are the output sequences and the architectures of the RNNs in segmentation. We employ an improved beam search to optimize the second segmentation model and boost the segmentation process. All the models are trained on a limited dataset, and the second model achieved the state-of-the-art accuracy. © 2018, Springer Nature Switzerland AG.},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-030-04221-9_50},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059068784&doi=10.1007%2f978-3-030-04221-9_50&partnerID=40&md5=686c94482295131c3cdbbc0ac8d869cc},
}

@Article{Zhang20182927,
  author        = {Zhang, Z. and Bi, X.},
  title         = {Research and Experiment of Intelligent Natural Language Processing Algorithms},
  journal       = {Wireless Personal Communications},
  year          = {2018},
  volume        = {102},
  number        = {4},
  pages         = {2927-2939},
  note          = {cited By 0},
  abstract      = {Natural language processing is mainly divided into two parts: speech processing and word processing. The level of word processing is mainly studied. Natural language processing is divided into lexical analysis, syntax analysis and semantic analysis. Aiming at the scope of the language ambiguity and thesaurus in the field of smart home, the maximal matching algorithm is used to segment the natural language. Then, through the way of template matching, semantic comprehension finally forms the code form that can control the home node. In the system applied in this paper, the speech is processed into words through the existing voice input function of the mobile terminal. Then, the control instruction is obtained through the language processing method. The processed data is communicated to the server via socket. The server sends the data to the home node through the Zigbee protocol. Finally, control of home appliances is achieved. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
  document_type = {Article},
  doi           = {10.1007/s11277-018-5316-2},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040677725&doi=10.1007%2fs11277-018-5316-2&partnerID=40&md5=73b8734ca4c3841c9c0e63013dc69c05},
}

@Article{Guo201766,
  author        = {Guo, X. and Zhao, W. and Wang, J. and Wang, C. and Zhang, K. and Chen, L.},
  title         = {A Study of Knowledge Modeling and Retrieval Methods Oriented Towards Innovative Design of Manufacturing Planning},
  journal       = {Jixie Gongcheng Xuebao/Journal of Mechanical Engineering},
  year          = {2017},
  volume        = {53},
  number        = {15},
  pages         = {66-72},
  note          = {cited By 4},
  abstract      = {In order to further extend the knowledge of manufacturing planning innovative design, a knowledge model oriented towards process innovative design and relevant retrieval methods are proposed. Before the discussions of characteristics of process knowledge and its expression, this paper presented the goal-based knowledge constituents and the ontology-based logical structure for management, regarding knowledge features and the scope of it. A knowledge organization model of manufacturing planning design is established, in which the function ontology and flow ontology are constructed under the framework of "function+flow+case", and the ontological expression of functions such as abstract flow, workpiece characteristics, and manufacturing resources is achieved. Through the combination of innovative methods and methods of manufacturing planning cases, a goal-oriented knowledge semantic model of manufacturing planning design is proposed. Based on the appositive and hyponymy extension of ontological terms related to function and flow, a process knowledge retrieval model aiming for innovative design, along with a prototype system, are established by means of the extended algorithm, semantic retrieval, and word segmentation. © 2017 Journal of Mechanical Engineering.},
  document_type = {Article},
  doi           = {10.3901/JME.2017.15.066},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030655711&doi=10.3901%2fJME.2017.15.066&partnerID=40&md5=fdd055daca17ace090cafa5dfd1ed26d},
}

@Conference{Gao2017434,
  author        = {Gao, K. and Zhang, S.-S. and Su, S. and Li, M.},
  title         = {Modeling on evaluation object extraction in e-commerce corpus based on semantic feature},
  year          = {2017},
  pages         = {434-438},
  note          = {cited By 0},
  abstract      = {As a newly shopping tool, electronic commerce has been drawing more and more attention of researchers. According to the characteristics of comments diversity, it is necessary to extract evaluation object which is an important component of sentiment information. This paper explores Conditional Random Field (CRF) to do evaluation objects extraction. After observing generally used features in sentiment extraction, this paper conclude all the features into four categories, i.e. word Segmentation, Part-of-speech Tagging (POS), Dependency Parsing, Semantic Dependency Parsing. What's more, focusing on the introduction of new feature semantic dependency is a very vital item in our research. In the experiment, we examine the various features and combinations in the extraction task performance, and make a detailed comparative study. The experimental results confirm that adding the feature of semantic dependency has better performance in terms of the evaluation object extraction. © 2016 University of MEDEA, Algeria.},
  art_number    = {7804151},
  document_type = {Conference Paper},
  doi           = {10.1109/ICMIC.2016.7804151},
  journal       = {Proceedings of 2016 8th International Conference on Modelling, Identification and Control, ICMIC 2016},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011290552&doi=10.1109%2fICMIC.2016.7804151&partnerID=40&md5=65db6606157eeccd05cb6708dd9e10d9},
}

@Article{Song20171,
  author        = {Song, S. and Zhang, N. and Huang, H.},
  title         = {Named entity recognition based on conditional random fields},
  journal       = {Cluster Computing},
  year          = {2017},
  pages         = {1-12},
  note          = {cited By 3; Article in Press},
  abstract      = {Named entity recognition (NER) is one of the fundamental problems in many natural language processing applications and the study on NER has great significance. Combining words segmentation and parts of speech analysis, the paper proposes a new NER method based on conditional random fields considering the graininess of candidate entities. The recognition granularity can be divided into two levels: word-based and character-based. We use segmented text to extract characteristics according to the characteristic templates which had been trained in the training phase, and then calculate (Formula presented.) to get the best result from the input sequence. The paper valuates the algorithm for different graininess on large-scale corpus experimentally, and the results show that this method has high research value and feasibility. © 2017 Springer Science+Business Media, LLC},
  document_type = {Article in Press},
  doi           = {10.1007/s10586-017-1146-3},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029009712&doi=10.1007%2fs10586-017-1146-3&partnerID=40&md5=8cf127b4e560b9faaf5042dc71ed7239},
}

@Conference{Jamro2017,
  author        = {Jamro, W.A.},
  title         = {Sindhi Language Processing: A survey},
  year          = {2017},
  note          = {cited By 0},
  abstract      = {In this era of information technology, natural language processing (NLP) has become volatile field because of digital reliance of today's communities. The growth of Internet usage bringing the communities, cultures and languages online. In this regard much of the work has been done of the European and east Asian languages, in the result these languages have reached mature level in terms of computational processing. Despite the great importance of NLP science, still most of the South Asian languages are under developing phase. Sindhi language is one of them, which stands among the most ancient languages in the world. The Sindhi language has a great influence on the large community in Sindh province of Pakistan and some states of India and other countries. But unfortunately, it is at infant level in terms of computational processing, because it has not received such attention of language engineering community, due to its complex morphological structure and scarcity of language resources. Therefore, this study has been carried out in order to summarize the existing work on Sindhi Language Processing (SLP) and to explore future research opportunities, also some potential research problems. This paper will be helpful for the researchers in order to find all the information regarding SLP at one place in a unique way. © 2017 IEEE.},
  art_number    = {7916560},
  document_type = {Conference Paper},
  doi           = {10.1109/ICIEECT.2017.7916560},
  journal       = {ICIEECT 2017 - International Conference on Innovations in Electrical Engineering and Computational Technologies 2017, Proceedings},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019970443&doi=10.1109%2fICIEECT.2017.7916560&partnerID=40&md5=ad916689a8d425936f9855aedf784fd4},
}

@Conference{Zhai2017245,
  author        = {Zhai, Y. and Liu, L. and Song, W. and Du, C. and Zhao, X.},
  title         = {The application of natural language processing in compiler principle system},
  year          = {2017},
  pages         = {245-248},
  note          = {cited By 0},
  abstract      = {Compiling principle is an important course of computer science major, which mainly introduces general principles and basic methods of the construction of compiling programs mainly. Due to high demands of the logic analysis ability, the course bring abstract and unintelligible experience to many students. Thus it is quite difficult for students to master the main points of this course within the limited class time. Based on the requirement above, this paper mainly proposed a method of making use of natural language processing in the research and application of compiling process, which utilizes Maximum Probability Word Segmentation algorithm during the process of lexical analysis and syntax analysis, to offer more effective interface between human and computer. The proposed method can provide students with intuitive and profound knowledge concept in the process of learning how to compile, makes it easier and quicker for students to understand the principle of computer compiling. © 2017 IEEE.},
  document_type = {Conference Paper},
  doi           = {10.1109/PIC.2017.8359551},
  journal       = {Proceedings of 2017 International Conference on Progress in Informatics and Computing, PIC 2017},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048154138&doi=10.1109%2fPIC.2017.8359551&partnerID=40&md5=fb78099fe452eaf8aac978579c7b536b},
}

@Conference{Cong2017495,
  author        = {Cong, X. and Li, L.},
  title         = {UGC quality evaluation based on meta-learning and content feature analysis},
  year          = {2017},
  pages         = {495-499},
  note          = {cited By 0},
  abstract      = {With the fast development of Social Networking Services, there has been increasingly vast amount of information published by massive network users. Given this information explosion, how to analyze the quality of User Generated Contents (UGC) automatically becomes a challenging task for researchers. To solve the problem, we need to build an effective UGC quality evaluation system. In the light of our experience, we believe that the textual content of UGC is the key factor for its quality. Hence, we focus on textual content based quality evaluation and classification instead of using UGC publishing related data, such as times being commented and forwarded in this paper. We extract various features of the textual contents based on natural language processing technologies firstly, such as word segmentation, keywords, topic model, sentence parsing, distributed word representation etc. Secondly, we build several base-learning classifiers with different features and different machine learning algorithms to assign UGC contents with four different quality labels. Then, we create the global meta-learning model based on these base classifiers to generate the final quality labels for UGC contents. We have also implemented a series of experiments based on realistic data collected from Tianya Forum and use 10-fold cross-validation to test the model. Results have shown that our proposed meta-learning model performs much better. © 2016 IEEE.},
  art_number    = {7974624},
  document_type = {Conference Paper},
  doi           = {10.1109/ICNIDC.2016.7974624},
  journal       = {Proceedings of 2016 5th International Conference on Network Infrastructure and Digital Content, IEEE IC-NIDC 2016},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027534422&doi=10.1109%2fICNIDC.2016.7974624&partnerID=40&md5=b4ad7131223ca0dab310903972bc0146},
}

@Article{Tursun2016,
  author        = {Tursun, E. and Ganguly, D. and Osman, T. and Yang, Y.-T. and Abdukerim, G. and Zhou, J.-L. and Liu, Q.},
  title         = {A semisupervised tag-transition-based markovian model for Uyghur morphology analysis},
  journal       = {ACM Transactions on Asian and Low-Resource Language Information Processing},
  year          = {2016},
  volume        = {16},
  number        = {2},
  note          = {cited By 4},
  abstract      = {Morphological analysis, which includes analysis of part-of-speech (POS) tagging, stemming, and morpheme segmentation, is one of the key components in natural language processing (NLP), particularly for agglutinative languages. In this article, we investigate the morphological analysis of the Uyghur language, which is the native language of the people in the Xinjiang Uyghur autonomous region of western China. Morphological analysis of Uyghur is challenging primarily because of factors such as (1) ambiguities arising due to the likelihood of association of a multiple number of POS tags with a word stem or a multiple number of functional tags with a word suffix, (2) ambiguous morpheme boundaries, and (3) complex morphopholonogy of the language. Further, the unavailability of a manually annotated training set in the Uyghur language for the purpose of word segmentation makes Uyghur morphological analysis more difficult. In our proposed work, we address these challenges by undertaking a semisupervised approach of learning a Markov model with the help of a manually constructed dictionary of "suffix to tag" mappings in order to predict the most likely tag transitions in the Uyghur morpheme sequence. Due to the linguistic characteristics of Uyghur, we incorporate a prior belief in our model for favoring word segmentations with a lower number of morpheme units. Empirical evaluation of our proposed model shows an accuracy of about 82%. We further improve the effectiveness of the tag transition model with an active learning paradigm. In particular, we manually investigated a subset of words for which the model prediction ambiguity was within the top 20%. Manually incorporating rules to handle these erroneous cases resulted in an overall accuracy of 93.81%. © 2016 ACM.},
  art_number    = {8},
  document_type = {Article},
  doi           = {10.1145/2968410},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997079132&doi=10.1145%2f2968410&partnerID=40&md5=db00c5c84f2228db1d13149001917e43},
}

@Conference{Zhang2016124,
  author        = {Zhang, C. and He, S. and Gao, X.},
  title         = {A word sense disambiguation system based on Bayesian model},
  year          = {2016},
  pages         = {124-127},
  note          = {cited By 0},
  abstract      = {Research on word sense disambiguation (WSD) is of great importance in natural language processing fields. In this paper, a novel word sense disambiguation system is designed in which Bayesian theory is applied to determine correct sense of an ambiguous word. Morphology knowledge in word unit is mined to guide WSD process. Neighboring morphology knowledge of an ambiguous word is used as feature for constructing WSD classifier. Word segmentation tool is integrated into this system and browser/server (B/S) framework is adopted. Experimental results show that the performance of WSD system is good. © 2015 IEEE.},
  art_number    = {7490720},
  document_type = {Conference Paper},
  doi           = {10.1109/ICCSNT.2015.7490720},
  journal       = {Proceedings of 2015 4th International Conference on Computer Science and Network Technology, ICCSNT 2015},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979299389&doi=10.1109%2fICCSNT.2015.7490720&partnerID=40&md5=15ddb824faa10b6baa249d5d15533d20},
}

@Conference{Wilkinson20163086,
  author        = {Wilkinson, A. and Zhao, T. and Black, A.W.},
  title         = {Deriving phonetic transcriptions and discovering word segmentations for speech-to-speech translation in low-resource settings},
  year          = {2016},
  volume        = {08-12-September-2016},
  pages         = {3086-3090},
  note          = {cited By 1},
  abstract      = {We investigate speech-to-speech translation where one language does not have a well-defined written form. We use English-Spanish and Mandarin-English bitext corpora in order to provide both gold-standard text-based translations and experimental results for different levels of automatically derived symbolic representations from speech. We constrain our experiments such that the methods developed can be extended to low-resource languages. We derive different phonetic representations of the source texts in order to model the kinds of transcriptions that can be learned from low-resource-language speech data. We experiment with different methods of clustering the elements of the phonetic representations together into word-like units. We train MT models on the resulting texts, and report BLEU scores for the different representations and clustering methods in order to compare their effectiveness. Finally, we discuss our findings and suggest avenues for future research. Copyright © 2016 ISCA.},
  document_type = {Conference Paper},
  doi           = {10.21437/Interspeech.2016-1319},
  journal       = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994229363&doi=10.21437%2fInterspeech.2016-1319&partnerID=40&md5=e60074ad5b17c3015a839fde03b67724},
}

@Conference{Khan2016192,
  author        = {Khan, I.A. and Choi, J.-T.},
  title         = {Lexicon-corpus Based Korean Unknown Foreign Word Extraction and Updating Using Syllable Identification},
  year          = {2016},
  volume        = {154},
  pages         = {192-198},
  note          = {cited By 0},
  abstract      = {This paper presents an efficient text mining method focusing on extraction and updating of unknown words (unknown foreign words) to improve data classification and POS tags. Proposed methods can also help to improve the accuracy of mining frequent pattern and association rules from unstructured (textual) data. Many researches have been done by numerous scholars on estimation and segmentation for unknown words, but, they are limited to grammatical and linguistic rules with limited vocabulary. In our project we have consider the fact, that no language is free from the influence of foreign languages, especially, country like Korea where there is a rapid improvement in the area of culture and media and the frequent usage of these foreign languages, resulted in mixing up different languages, their style along with slangs and also abbreviated words in daily life and conversation. The main characteristic of our system is to find such unknown foreign words and update them to appropriate words, which depends on available information through dictionaries. We have also explained the essential natural language processing (NLP) tools used for data processing. Our proposed method used simple but efficient techniques, first it converts the data into structured form, using data preprocessing techniques. In this phase data passes through different stages, such as, cleaning, integration and selection of important data, and then it gets organized into databases structure for further analysis and processing. This database consists of different kinds of dictionaries, our system heavily based on dictionaries. We have manually created various kinds of dictionaries for different kinds of unknown foreign words processing and analysis with the help of our team members. Our proposed methods for discovering and updating foreign unknown word, first discovers the foreign word using morphological analysis with the help of automatically and manually created dictionaries, then suffix trimming and word segmentation, next our algorithm checks for its different written pattern using dictionaries according to its spelling and synonym word in native language (Korean) and also, updates the POS tags. We have tested on different collection of data from economics news, beauty & fashion and college student blogs, the results have shown great efficiency and improvement, and they were adequate enough to research further.},
  document_type = {Conference Paper},
  doi           = {10.1016/j.proeng.2016.07.445},
  journal       = {Procedia Engineering},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997830871&doi=10.1016%2fj.proeng.2016.07.445&partnerID=40&md5=ab520c5f80a1b76958b5ca824165bd28},
}

@Conference{Ye20166993,
  author        = {Ye, Z. and Jia, Z. and Huang, J. and Yin, H.},
  title         = {Part-of-speech tagging based on dictionary and statistical machine learning},
  year          = {2016},
  volume        = {2016-August},
  pages         = {6993-6998},
  note          = {cited By 3},
  abstract      = {Part-of-speech tagging is the basis of Natural Language Processing, and is widely used in information retrieval, text processing and machine translation fields. The traditional statistical machine learning methods of POS tagging rely on the high quality training data, but obtaining the training data is very time-consuming. The methods of POS tagging based on dictionaries ignore the context information, which lead to lower performance. This paper proposed a POS tagging approach which combines methods based on dictionaries and traditional statistical machine learning. The experimental results show that the approach not only can solve the problem that the training data are insufficient in statistical methods, but also can improve the performance of the methods based on dictionaries. The People's Daily corpus in January 1998 is used as testing data, and the accurate rate of POS tagging achieves 95.80%. For the ambiguity word POS tagging, the accuracy achieves 88%. © 2016 TCCT.},
  art_number    = {7554459},
  document_type = {Conference Paper},
  doi           = {10.1109/ChiCC.2016.7554459},
  journal       = {Chinese Control Conference, CCC},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987912398&doi=10.1109%2fChiCC.2016.7554459&partnerID=40&md5=34dc9eaadb4113424692fc24a4f920f2},
}

@Conference{Matsumoto2016473,
  author        = {Matsumoto, K. and Yoshida, M. and Kita, K.},
  title         = {Sensibility estimation method for youth slang by using sensibility co-occurrence feature vector obtained from microblog},
  year          = {2016},
  pages         = {473-478},
  note          = {cited By 0},
  abstract      = {Social networking sites such as Twitter provide more opportunities to express what people think or intend in short text. In short text, abbreviations such as "ASAP" or "joinus" and emoticons are often used. Because these expressions are not registered into the existing dictionaries, these are analyzed as unknown expressions. That can be a bottleneck for improving accuracy of reputation analysis in text mining. To use context for unknown word clustering is a major method, however, it usually requires word segmentation process and it has weakness for split errors of unknown expressions such as youth slang. In this paper, we proposed a method to obtain the appropriate context even though unknown expressions cause split errors and estimate sensibility expressed in the text. Because the dimensions of the obtained context vector were enormous, we also proposed a method to create a feature vector based on the co-occurrence of the sensibility words as simple expression with low dimension. As an evaluation experiment, the proposed method showed certain accuracy even with the small training data. © 2015 IEEE.},
  art_number    = {7387618},
  document_type = {Conference Paper},
  doi           = {10.1109/CompComm.2015.7387618},
  journal       = {Proceedings of 2015 IEEE International Conference on Computer and Communications, ICCC 2015},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963939887&doi=10.1109%2fCompComm.2015.7387618&partnerID=40&md5=4a1d165f66f914a0b90d4bd3b4765816},
}

@Conference{Puri2016,
  author        = {Puri, S. and Singh, S.P.},
  title         = {Sentence Detection and Extraction in machine printed imaged document using matching technique},
  year          = {2016},
  note          = {cited By 0},
  abstract      = {Sentence extraction is a new, challenging and critical step in the printed scanned imaged documents. In this paper, an efficient 4-layered Sentence Detection and Extraction System (SDES) model is proposed which is designed to detect and extract sentences from machine printed imaged document. Its internal details and architecture clearly show that how it processes an image to find out the underlying sentences. The basic idea is to first preprocess the imaged document for noise removal and skew correction, and then textual entities are detected and segmented at page, line and word levels. Firstly, the horizontal and vertical projection profiles are taken to segment and separate the lines and words. After skew correction, two stage Character Based and Word Based Leveled matching and testing are performed, which verify and identify the correct character and word by searching for similar textual characters and words in Character Set Storage (CSS) and Word Pseudo Thesaurus (WPT). If any word pattern is not matched and identified by WPT, then it is stored in the Unmatched Word Storage (UWS) for the future reference. Such testing and verification are used at two levels to increase the accuracy% of SDES, and thereby, reducing the errors. It increases the system performance greatly. Finally, all the sentences of imaged document are extracted. Experimental results are found at the word, character and sentence levels. Their accuracy% results are good which show the high system performance and efficiency. © 2015 IEEE.},
  art_number    = {7453382},
  document_type = {Conference Paper},
  doi           = {10.1109/RAECS.2015.7453382},
  journal       = {2015 2nd International Conference on Recent Advances in Engineering and Computational Sciences, RAECS 2015},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966267386&doi=10.1109%2fRAECS.2015.7453382&partnerID=40&md5=7134580c1fb530b6aa5794445cbba254},
}

@Conference{Qian20151837,
  author        = {Qian, T. and Zhang, Y. and Zhang, M. and Ren, Y. and Ji, D.},
  title         = {A transition-based model for joint segmentation, POS-tagging and normalization},
  year          = {2015},
  pages         = {1837-1846},
  note          = {cited By 11},
  abstract      = {We propose a transition-based model for joint word segmentation, POS tagging and text normalization. Different from previous methods, the model can be trained on standard text corpora, overcoming the lack of annotated microblog corpora. To evaluate our model, we develop an annotated corpus based on microblogs. Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02%, compared to the traditional approach. © 2015 Association for Computational Linguistics.},
  document_type = {Conference Paper},
  journal       = {Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959905712&partnerID=40&md5=00d6ac7fdc06bd2b83d3dfb4ec314f4a},
}

@Conference{Takahashi20151186,
  author        = {Takahashi, F. and Mori, S.},
  title         = {Keyboard logs as natural annotations for word segmentation},
  year          = {2015},
  pages         = {1186-1196},
  note          = {cited By 2},
  abstract      = {In this paper we propose a framework to improve word segmentation accuracy using input method logs. An input method is software used to type sentences in languages which have far more characters than the number of keys on a keyboard. The main contributions of this paper are: 1) an input method server that proposes word candidates which are not included in the vocabulary, 2) a publicly usable input method that logs user behavior (like typing and selection of word candidates), and 3) a method for improving word segmentation by using these logs. We conducted word segmentation experiments on tweets from Twitter, and showed that our method improves accuracy in this domain. Our method itself is domain-independent and only needs logs from the target domain. © 2015 Association for Computational Linguistics.},
  document_type = {Conference Paper},
  journal       = {Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959884109&partnerID=40&md5=b951aa277fd249c48fa0691e5b63d495},
}

@Article{Kartbayev2015421,
  author        = {Kartbayev, A.},
  title         = {Refining Kazakh word alignment using simulation modeling methods for statistical machine translation},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2015},
  volume        = {9362},
  pages         = {421-427},
  note          = {cited By 0},
  abstract      = {Word alignment play an important role in the training of statistical machine translation systems. We present a technique to refine word alignments at phrase level after the collection of sentences from the Kazakh-English parallel corpora. The estimation technique extracts the phrase pairs from the word alignment and then incorporates them into the translation system for further steps. Although it is a pretty important step in training procedure, an word alignment process often has practical concerns with agglutinative languages. We consider an approach, which is a step towards an improved statistical translation model that incorporates morphological information and has better translation performance. Our goal is to present a statistical model of the morphology dependent procedure, which was evaluated over the Kazakh-English language pair and has obtained an improved BLEU score over state-of-the-art models. © Springer International Publishing Switzerland 2015.},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-319-25207-0_38},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951267727&doi=10.1007%2f978-3-319-25207-0_38&partnerID=40&md5=acfd3ce218009c0298fc315946630f6c},
}

@Article{Nivre20153,
  author        = {Nivre, J.},
  title         = {Towards a universal grammar for natural language processing},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2015},
  volume        = {9041},
  pages         = {3-16},
  note          = {cited By 26},
  abstract      = {Universal Dependencies is a recent initiative to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. In this paper, I outline the motivation behind the initiative and explain how the basic design principles follow from these requirements. I then discuss the different components of the annotation standard, including principles for word segmentation, morphological annotation, and syntactic annotation. I conclude with some thoughts on the challenges that lie ahead. © Springer International Publishing Switzerland 2015.},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-319-18111-0_1},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942673144&doi=10.1007%2f978-3-319-18111-0_1&partnerID=40&md5=0c36bdb69403326297326f303bf15cc2},
}

@Conference{Ludusan2014560,
  author        = {Ludusan, B. and Versteegh, M. and Jansen, A. and Gravier, G. and Cao, X.-N. and Johnson, M. and Dupoux, E.},
  title         = {Bridging the gap between speech technology and natural language processing: An evaluation toolbox for term discovery systems},
  year          = {2014},
  pages         = {560-567},
  note          = {cited By 17},
  abstract      = {The unsupervised discovery of linguistic terms from either continuous phoneme transcriptions or from raw speech has seen an increasing interest in the past years both from a theoretical and a practical standpoint. Yet, there exists no common accepted evaluation method for the systems performing term discovery. Here, we propose such an evaluation toolbox, drawing ideas from both speech technology and natural language processing. We first transform the speech-based output into a symbolic representation and compute five types of evaluation metrics on this representation: the quality of acoustic matching, the quality of the clusters found, and the quality of the alignment with real words (type, token, and boundary scores). We tested our approach on two term discovery systems taking speech as input, and one using symbolic input. The latter was run using both the gold transcription and a transcription obtained from an automatic speech recognizer, in order to simulate the case when only imperfect symbolic information is available. The results obtained are analysed through the use of the proposed evaluation metrics and the implications of these metrics are discussed.},
  document_type = {Conference Paper},
  journal       = {Proceedings of the 9th International Conference on Language Resources and Evaluation, LREC 2014},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020779259&partnerID=40&md5=8d4e5fda6f19a9d80393dc250c6be7f7},
}

@Conference{Zhu201479,
  author        = {Zhu, L. and Li, H. and Wang, S. and Li, C.},
  title         = {Exploration and development of text knowledge extraction},
  year          = {2014},
  pages         = {79-83},
  note          = {cited By 0},
  abstract      = {Text knowledge extraction technology has been applied in many fields, but few practices in the field of petroleum exploration and development. In this paper,we comprehensively utilize a statistical and natural language understanding technology to extract knowledge from the articles in the field of petroleum exploration and development. First of all, we get the key words and the core sentences containing article process model. Then after the word segmentation and phrase recognition, we use semantic templates to match and extract semantic information from the key sentences. The experimental results show that this method achieves 70% accuracy rate in keywords spotting and process model extraction. © 2014 Taylor & Francis Group, London.},
  document_type = {Conference Paper},
  journal       = {Information Technology and Computer Application Engineering - Proceedings of the 2013 International Conference on Information Technology and Computer Application Engineering, ITCAE 2013},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900429048&partnerID=40&md5=2427896a84c7d70a080a58e189d138f1},
}

@Article{Sun2014563,
  author        = {Sun, X. and Li, W. and Wang, H. and Lu, Q.},
  title         = {Feature-frequency-adaptive on-line training for fast and accurate natural language processing},
  journal       = {Computational Linguistics},
  year          = {2014},
  volume        = {40},
  number        = {3},
  pages         = {563-586},
  note          = {cited By 8},
  abstract      = {Training speed and accuracy are two major concerns of large-scale natural language processing systems. Typically, we need to make a tradeoff between speed and accuracy. It is trivial to improve the training speed via sacrificing accuracy or to improve the accuracy via sacrificing speed. Nevertheless, it is nontrivial to improve the training speed and the accuracy at the same time, which is the target of this work. To reach this target, we present a new training method, featurefrequency-adaptive on-line training, for fast and accurate training of natural language processing systems. It is based on the core idea that higher frequency features should have a learning rate that decays faster. Theoretical analysis shows that the proposed method is convergent with a fast convergence rate. Experiments are conducted based on well-known benchmark tasks, including named entity recognition, word segmentation, phrase chunking, and sentiment analysis. These tasks consist of three structured classification tasks and one non-structured classification task, with binary features and real-valued features, respectively. Experimental results demonstrate that the proposed method is faster and at the same time more accurate than existing methods, achieving state-of-the-art scores on the tasks with different characteristics. © 2014 Association for Computational Linguistics.},
  document_type = {Article},
  doi           = {10.1162/COLI_a_00193},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910067653&doi=10.1162%2fCOLI_a_00193&partnerID=40&md5=0e9182cc0ab48126232f79eee985559a},
}

@Conference{Pate2014844,
  author        = {Pate, J.K. and Johnson, M.},
  title         = {Syllable weight encodes mostly the same information for english word segmentation as dictionary stress},
  year          = {2014},
  pages         = {844-853},
  note          = {cited By 0},
  abstract      = {Stress is a useful cue for English word segmentation. A wide range of computational models have found that stress cues enable a 2-10% improvement in segmentation accuracy, depending on the kind of model, by using input that has been annotated with stress using a pronouncing dictionary. However, stress is neither invariably produced nor unambiguously identifiable in real speech. Heavy syllables, i.e. those with long vowels or syllable codas, attract stress in English. We devise Adaptor Grammar word segmentation models that exploit either stress, or syllable weight, or both, and evaluate the utility of syllable weight as a cue to word boundaries. Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does. © 2014 Association for Computational Linguistics.},
  document_type = {Conference Paper},
  journal       = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961296056&partnerID=40&md5=cf2aa54e54fbb32050bfd4d0c6349bd0},
}

@Article{Weinman2014375,
  author        = {Weinman, J.J. and Butler, Z. and Knoll, D. and Feild, J.},
  title         = {Toward integrated scene text reading},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year          = {2014},
  volume        = {36},
  number        = {2},
  pages         = {375-387},
  note          = {cited By 47},
  abstract      = {The growth in digital camera usage combined with a worldly abundance of text has translated to a rich new era for a classic problem of pattern recognition, reading. While traditional document processing often faces challenges such as unusual fonts, noise, and unconstrained lexicons, scene text reading amplifies these challenges and introduces new ones such as motion blur, curved layouts, perspective projection, and occlusion among others. Reading scene text is a complex problem involving many details that must be handled effectively for robust, accurate results. In this work, we describe and evaluate a reading system that combines several pieces, using probabilistic methods for coarsely binarizing a given text region, identifying baselines, and jointly performing word and character segmentation during the recognition process. By using scene context to recognize several words together in a line of text, our system gives state-of-the-art performance on three difficult benchmark data sets. © 1979-2012 IEEE.},
  art_number    = {6549105},
  document_type = {Article},
  doi           = {10.1109/TPAMI.2013.126},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891621153&doi=10.1109%2fTPAMI.2013.126&partnerID=40&md5=aada740bcad8388e856a99389025870d},
}

@Article{Gao201426,
  author        = {Gao, H.-Y. and Nian, F.-X.},
  title         = {User reviews based product feature mining of mobile phones in E-commerce},
  journal       = {Beijing Ligong Daxue Xuebao/Transaction of Beijing Institute of Technology},
  year          = {2014},
  volume        = {34},
  pages         = {26-30},
  note          = {cited By 1},
  abstract      = {Product review mining aims to quickly extract useful information from massive comments published by users and adopt an intuitive way to help consumers make purchasing decisions. Fine-grained product feature mining is very important, however, the product characteristics semantics (upper and lower characteristics, synonymous features) analysis is inadequate on existing product reviews researches. Firstly, the ontology of mobile phone features was constructed based on mobile phone descriptions. Then crawling programs was employed to get product comments and followed by conducting words segmentation, part of speech tagging, getting rid of the repeats and other pretreatments. Using the Apriori algorithm, the appropriate product features from user's perspective were extracted. Combining with HowNet dictionary, semantic extension was carried to improve the ontology of product features, which will facilitate further accurate sentiment analysis of the product reviews. ©, 2014, Beijing Institute of Technology. All right reserved.},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922612126&partnerID=40&md5=9dd8c8cbddd927f65a1f5b4dc4de37bc},
}

@Article{Zeng2013190,
  author        = {Zeng, L. and Li, F.},
  title         = {A classification-based approach for implicit feature identification},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2013},
  volume        = {8202 LNAI},
  pages         = {190-202},
  note          = {cited By 16},
  abstract      = {In recent years, sentiment analysis and opinion mining has grown to be one of the most active research areas. Most of the existing researches on feature-level opinion mining are dedicated to extract explicitly appeared features and opinion words. However, among the numerous kinds of reviews on the web, there are a significant number of reviews that contain only opinion words which imply some product features. The identification of such implicit features is still one of the most challenge tasks in opinion mining. In this paper, we propose a classification-based approach to deal with the task of implicit feature identification. Firstly, by exploiting the word segmentation, part-of-speech(POS) tagging and dependency parsing, a rule based method to extract the explicit feature-opinion pairs is presented. Secondly, the feature-opinion pairs for each opinion word are clustered and the training documents for each clustered feature-opinion pair are then constructed. Finally, the identification of implicit features is formulated into a classification-based feature selection. Experiments demonstrate that our approach outperforms the existing methods significantly. © Springer-Verlag 2013.},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-41491-6_18},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893031086&doi=10.1007%2f978-3-642-41491-6_18&partnerID=40&md5=77380a4f99a8805341450d35c070b926},
}

@Conference{Elsner201342,
  author        = {Elsner, M. and Goldwater, S. and Feldman, N.H. and Wood, F.},
  title         = {A joint learning model of word segmentation, lexical acquisition, and phonetic variability},
  year          = {2013},
  pages         = {42-54},
  note          = {cited By 19},
  abstract      = {We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We define the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed first, our model is qualitatively more similar to human learners. On data with variable pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence. © 2013 Association for Computational Linguistics.},
  document_type = {Conference Paper},
  journal       = {EMNLP 2013 - 2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926303216&partnerID=40&md5=1bb60c0df570ef1356d6b59e11aae487},
}

@Article{Namer201392,
  author        = {Namer, F.},
  title         = {A Rule-Based Morphosemantic Analyzer for French for a Fine-Grained Semantic Annotation of Texts},
  journal       = {Communications in Computer and Information Science},
  year          = {2013},
  volume        = {380 CCIS},
  pages         = {92-114},
  note          = {cited By 0},
  abstract      = {We describe DériF, a rule-based morphosemantic analyzer developed for French. Unlike existing word segmentation tools, DériF provides derived and compound words with various sorts of semantic information: (1) a definition, computed from both the base meaning and the specificities of the morphological rule; (2) lexical-semantic features, inferred from general linguistic properties of derivation rules; (3) lexical relations (synonymy, (co-)hyponymy) with other, morphologically unrelated, words belonging to the same analyzed corpus. © Springer-Verlag Berlin Heidelberg 2013.},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-40486-3_6},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904608636&doi=10.1007%2f978-3-642-40486-3_6&partnerID=40&md5=39c06edc8d7ffbce4c228d77c841004d},
}

@Article{Wang20133039,
  author        = {Wang, N. and Xu, L. and Li, L.Y. and Xu, L.X.},
  title         = {Design and implementation of an automatic scoring subjective question system based on domain ontology},
  journal       = {Advanced Materials Research},
  year          = {2013},
  volume        = {753-755},
  pages         = {3039-3042},
  note          = {cited By 0},
  abstract      = {Automated assessment technology for subjective tests is one of the key techniques of exam systems. A model based on domain ontology is proposed in this paper, which can be used in exam systems to estimate subjective tests. After analysing the present research status of subjective automated assessment technology, the paper makes a study on the construction method of domain ontology by taking software engineering domain as an example. Semantic similarity calculation based on domain ontology is used for automatic assessment in this paper. The automatic assessment system can divide a sentence into a series of phrases by using the natural language processing technology and get the score by evaluating the semantic similarity of the student's answer. The experiments show that the results of the system which has certain valuable feasibility and applicability are credible and the scoring errors are acceptable. © (2013) Trans Tech Publications, Switzerland.},
  document_type = {Conference Paper},
  doi           = {10.4028/www.scientific.net/AMR.753-755.3039},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884861075&doi=10.4028%2fwww.scientific.net%2fAMR.753-755.3039&partnerID=40&md5=e9bb74c4f7d2180b7130df66ae8c1139},
}

@Article{Goldberg2013121,
  author        = {Goldberg, Y. and Elhadad, M.},
  title         = {Word segmentation, unknown-word resolution, and morphological agreement in a Hebrew parsing system},
  journal       = {Computational Linguistics},
  year          = {2013},
  volume        = {39},
  number        = {1},
  pages         = {121-160},
  note          = {cited By 8},
  abstract      = {We present a constituency parsing system for Modern Hebrew. The system is based on the PCFG-LA parsing method of Petrov et al. (2006), which is extended in various ways in order to accommodate the specificities of Hebrew as a morphologically rich language with a small treebank. We show that parsing performance can be enhanced by utilizing a language resource external to the treebank, specifically, a lexicon-based morphological analyzer. We present a computational model of interfacing the external lexicon and a treebank-based parser, also in the common case where the lexicon and the treebank follow different annotation schemes. We show that Hebrew word-segmentation and constituency-parsing can be performed jointly using CKY lattice parsing. Performing the tasks jointly is effective, and substantially outperforms a pipelinebased model. We suggest modeling grammatical agreement in a constituency-based parser as a filter mechanism that is orthogonal to the grammar, and present a concrete implementation of the method. Although the constituency parser does not make many agreement mistakes to begin with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make. These contributions extend outside of the scope of Hebrew processing, and are of general applicability to the NLP community. Hebrew is a specific case of a morphologically rich language, and ideas presented in this work are useful also for processing other languages, including English. The lattice-based parsingmethodology is useful in any case where the input is uncertain. Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant for any language with a small treebank. © 2013 Association for Computational Linguistics.},
  document_type = {Article},
  doi           = {10.1162/COLI_a_00137},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874618096&doi=10.1162%2fCOLI_a_00137&partnerID=40&md5=78e968695482a4747271d37e38b3ef3b},
}

@Article{Chang2012828,
  author        = {Chang, S.-F. and Yu, L.-C.},
  title         = {A multi-function MSN robot for campus information seeking},
  journal       = {Advanced Science Letters},
  year          = {2012},
  volume        = {9},
  pages         = {828-832},
  note          = {cited By 0},
  abstract      = {An MSN robot is an intelligent system that can chat with users to accomplish a specific task. In this paper, we present a framework to build an MSN robot that can chat with students to provide campus information. The MSN robot has two major functions; that is it can provide two kinds of information: curriculum information and living information. The curriculum information includes a road map of all courses so that students can register courses according to their interests and future plans. The living information includes the information of restaurants, houses, transportation, and scenic spots around the campus. The MSN robot consists of three components: word segmentation and POS tagging, natural language understanding, and response text generation. The chat between the robot and students starts with an input of free-text question about campus information. Once the question is received, the natural language understanding component determines the meaning of the input question using classification methods. Finally, the output component generates a text response using a set of predefined templates. The MSN robot is implemented using the Dot MSN, an open source package that can connect the MSN Messenger service. The experimental results show that among several classification methods the support vector machine (SVM) achieves the best performance in classifying users' input questions into curriculum information or living information. © 2012 American Scientific Publishers.},
  document_type = {Article},
  doi           = {10.1166/asl.2012.2643},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862902958&doi=10.1166%2fasl.2012.2643&partnerID=40&md5=4582590a11bfce38b51c8e993aca82e6},
}

@Conference{Wu20121713,
  author        = {Wu, X. and Fan, W. and Yu, Y.},
  title         = {Sembler: Ensembling crowd sequential labeling for improved quality},
  year          = {2012},
  volume        = {2},
  pages         = {1713-1719},
  note          = {cited By 7},
  abstract      = {Many natural language processing tasks, such as named entity recognition (NER), part of speech (POS) tagging, word segmentation, and etc., can be formulated as sequential data labeling problems. Building a sound labeler requires very large number of correctly labeled training examples, which may not always be possible. On the other hand, crowdsourcing provides an inexpensive yet efficient alternative to collect manual sequential labeling from non-experts. However the quality of crowd labeling cannot be guaranteed, and three kinds of errors are typical: (1) incorrect annotations due to lack of expertise (e.g., labeling gene names from plain text requires corresponding domain knowledge); (2) ignored or omitted annotations due to carelessness or low confidence; (3) noisy annotations due to cheating or vandalism. To correct these mistakes, we present Sembler, a statistical model for ensembling crowd sequential labelings. Sembler considers three types of statistical information: (1) the majority agreement that proves the correctness of an annotation; (2) correct annotation that improves the credibility of the corresponding annotator; (3) correct annotation that enhances the correctness of other annotations which share similar linguistic or contextual features. We evaluate the proposed model on a real Twitter and a synthetical biological data set, and find that Sembler is particularly accurate when more than half of annotators make mistakes. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.},
  document_type = {Conference Paper},
  journal       = {Proceedings of the National Conference on Artificial Intelligence},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868274778&partnerID=40&md5=056a981811f122859f050ce2dfaa4ab1},
}

@Article{Lehal2011136,
  author        = {Lehal, G.S. and Saini, T.S.},
  title         = {A transliteration based word segmentation system for Shahmukhi script},
  journal       = {Communications in Computer and Information Science},
  year          = {2011},
  volume        = {139 CCIS},
  pages         = {136-143},
  note          = {cited By 0},
  abstract      = {Word Segmentation is an important prerequisite for almost all Natural Language Processing (NLP) applications. Since word is a fundamental unit of any language, almost every NLP system first needs to segment input text into a sequence of words before further processing. In this paper, Shahmukhi word segmentation has been discussed in detail. The presented word segmentation module is part of Shahmukhi-Gurmukhi transliteration system. Shahmukhi script is usually written without short vowels leading to ambiguity. Therefore, we have designed a novel approach for Shahmukhi word segmentation in which we used target Gurmukhi script lexical resources instead of Shahmukhi resources. We employ a combination of techniques to investigate an effective algorithm by applying syntactical analysis process using Shahmukhi Gurmukhi dictionary, writing system rules and statistical methods based on n-grams models. © 2011 Springer-Verlag.},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-19403-0_22},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952050316&doi=10.1007%2f978-3-642-19403-0_22&partnerID=40&md5=114db1d7c3acc0aad18224e7bf0bdb99},
}

@Article{Kesidis2011131,
  author        = {Kesidis, A.L. and Galiotou, E. and Gatos, B. and Pratikakis, I.},
  title         = {A word spotting framework for historical machine-printed documents},
  journal       = {International Journal on Document Analysis and Recognition},
  year          = {2011},
  volume        = {14},
  number        = {2},
  pages         = {131-144},
  note          = {cited By 23},
  abstract      = {In this paper, we propose a word spotting framework for accessing the content of historical machine-printed documents without the use of an optical character recognition engine. A preprocessing step is performed in order to improve the quality of the document images, while word segmentation is accomplished with the use of two complementary segmentation methodologies. In the proposed methodology, synthetic word images are created from keywords, and these images are compared to all the words in the digitized documents. A user feedback process is used in order to refine the search procedure. The methodology has been evaluated in early Modern Greek documents printed during the seventeenth and eighteenth century. In order to improve the efficiency of accessing and search, natural language processing techniques have been addressed that comprise a morphological generator that enables searching in documents using only a base word-form for locating all the corresponding inflected word-forms and a synonym dictionary that further facilitates access to the semantic context of documents. © 2010 Springer-Verlag.},
  document_type = {Article},
  doi           = {10.1007/s10032-010-0134-4},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957479567&doi=10.1007%2fs10032-010-0134-4&partnerID=40&md5=2de08ceded8dd7dcf2411d64ea992e53},
}

@Conference{Hewlett2011540,
  author        = {Hewlett, D. and Cohen, P.},
  title         = {Fully unsupervised word segmentation with BVE and MDL},
  year          = {2011},
  volume        = {2},
  pages         = {540-545},
  note          = {cited By 9},
  abstract      = {Several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. However, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the Minimum Description Length (MDL) principle. Therefore, it is necessary to generate a set of candidate segmentations and select between them according to the MDL principle. We evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the Bootstrapped Voting Experts algorithm consistently outperforms other methods when paired with MDL. © 2011 Association for Computational Linguistics.},
  document_type = {Conference Paper},
  journal       = {ACL-HLT 2011 - Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859036614&partnerID=40&md5=3ec416230c3f540c6806480dbe8fb3a1},
}

@Article{Paul2011690,
  author        = {Paul, M. and Finch, A. and Sumita, E.},
  title         = {Integration of multiple bilingually-trained segmentation schemes into statistical machine translation},
  journal       = {IEICE Transactions on Information and Systems},
  year          = {2011},
  volume        = {E94-D},
  number        = {3},
  pages         = {690-697},
  note          = {cited By 3},
  abstract      = {This paper proposes an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches. The method can be applied to any language pair in which the source language is unsegmented and the target language segmentation is known. In the first step, an iterative bootstrap method is applied to learn multiple segmentation schemes that are consistent with the phrasal segmentations of an SMT system trained on the resegmented bitext. In the second step, multiple segmentation schemes are integrated into a single SMT system by characterizing the source language side and merging identical translation pairs of differently segmented SMT models. Experimental results translating five Asian languages into English revealed that the proposed method of integrating multiple segmentation schemes outperforms SMT models trained on any of the learned word segmentations and performs comparably to available monolingually built segmentation tools. © 2011 The Institute of Electronics, Information and Communication Engineers.},
  document_type = {Conference Paper},
  doi           = {10.1587/transinf.E94.D.690},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952129828&doi=10.1587%2ftransinf.E94.D.690&partnerID=40&md5=f02f8345f7fbdde304c3c697b3f3d408},
}

@Conference{Laga201123,
  author        = {Laga, T. and Zhao, X.},
  title         = {Theoretical framework of Mongolian word segmentation specification for information processing},
  year          = {2011},
  pages         = {23-25},
  note          = {cited By 0},
  abstract      = {The establishment of Contemporary Mongolian word segmentation specification for information processing has a great significance in the standardization of information processing, the compatibleness of different systems, the sharing of corpus, grammatical analysis, and POS tagging. The present paper studies the framework of Mongolian word segmentation including guidelines, formulating principles, styles, scopes of segmentation units, establishment foundation, structure of the specification and so on, and lays the theoretical foundation for this specification. © 2011 IEEE.},
  art_number    = {6121461},
  document_type = {Conference Paper},
  doi           = {10.1109/IALP.2011.45},
  journal       = {Proceedings - 2011 International Conference on Asian Language Processing, IALP 2011},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862939676&doi=10.1109%2fIALP.2011.45&partnerID=40&md5=0d469ef24f9f1ba2fe286255d9ad9685},
}

@Article{Xue2011,
  author        = {Xue, H. and Yang, Y. and Turghun, O. and Li, X. and Zhang, R.},
  title         = {Uyghur word segmentation using a combination of rules and statistics},
  journal       = {Advances in Information Sciences and Service Sciences},
  year          = {2011},
  volume        = {3},
  number        = {11},
  note          = {cited By 7},
  abstract      = {Rich morphology of Uyghur produces a large number of words and leads to high out of vocabulary (OOV) rates that can cause many errors in Uyghur natural language processing (NLP). Morphological word segmentation is the very important component to overcome this problem caused by Uyghur morphology. This paper depicts some morphological rules by analyzing the universal structure of Uyghur words and presents a partly supervised word segmentation method. In this method, the suffix corpus was utilized to give all the possible morphological word segmentations, from which the optimal word segmentation is selected by the MAP-based model. In addition, cascaded language model was used to improve the accuracy of word segmentation. The test set composed of 5000 words was collected and segmented by hand. The experiment on this test set was given and experimental results show that the proposed method was more effective.},
  document_type = {Article},
  doi           = {10.4156/AISS.vol3.issue11.13},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855209817&doi=10.4156%2fAISS.vol3.issue11.13&partnerID=40&md5=97b20d57922d4f990a0eb87f5410062c},
}

@Conference{Wang2011357,
  author        = {Wang, K. and Thrasher, C. and Hsu, B.-J.},
  title         = {Web scale NLP: A case study on URL word breaking},
  year          = {2011},
  pages         = {357-366},
  note          = {cited By 32},
  abstract      = {This paper uses the URL word breaking task as an example to elaborate what we identify as crucialin designingstatistical natural language processing (NLP) algorithmsfor Web scale applications: (1) rudimentary multilingual capabilities to cope with the global nature of the Web, (2) multi-style modeling to handle diverse language styles seen in the Web contents, (3) fast adaptation to keep pace with the dynamic changes of the Web, (4) minimal heuristic assumptions for generalizability and robustness, and (5) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost. We first show that the state-of-the-art word breaking techniquescan be unified and generalized under the Bayesian minimum risk (BMR) framework that, using a Web scale N-gram, can meet the first three requirements. We discuss how the existing techniques can be viewed asintroducing additional assumptions to the basic BMR framework, and describe a generic yet efficient implementation called word synchronous beam search.Testing the framework and its implementation on a series of large scale experiments reveals the following. First, the language style used to build the modelplays a critical role in the word breaking task, and the most suitable for the URL word breaking task appears to be that ofthe document title where the best performance is obtained. Models created from other language styles, such as from document body, anchor text, and even queries, exhibit varying degrees of mismatch. Although all styles benefit from increasing modeling power which, in our experiments, corresponds tothe use of a higher order N-gram, the gain is most recognizable for the title model. The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models, but are less effective and, in many cases,lead to poorer performance than the matched model with minimal assumptions. For the matched model based on document titles, an accuracy rate of 97.18% can already be achieved using simple trigram without any heuristics. Copyright © 2011 by the Association for Computing Machinery, Inc. (ACM).},
  document_type = {Conference Paper},
  doi           = {10.1145/1963405.1963457},
  journal       = {Proceedings of the 20th International Conference on World Wide Web, WWW 2011},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052123854&doi=10.1145%2f1963405.1963457&partnerID=40&md5=f0884c46e20c2b2cf89b5c9a1bb92dd2},
}

@Conference{Hewlett201139,
  author        = {Hewlett, D. and Cohen, P.},
  title         = {Word segmentation as general chunking},
  year          = {2011},
  pages         = {39-47},
  note          = {cited By 5},
  abstract      = {During language acquisition, children learn to segment speech into phonemes, syllables, morphemes, and words. We examine word segmentation specifically, and explore the possibility that children might have general purpose chunking mechanisms to perform word segmentation. The Voting Experts (VE) and Bootstrapped Voting Experts (BVE) algorithms serve as computational models of this chunking ability. VE finds chunks by searching for a particular information-theoretic signature: low internal entropy and high boundary entropy. BVE adds to VE the ability to incorporate information about word boundaries previously found by the algorithm into future segmentations. We evaluate the general chunking model on phonemically encoded corpora of child-directed speech, and show that it is consistent with empirical results in the developmental literature. We argue that it offers a parsimonious alternative to special purpose linguistic models. © 2011 Association for Computational Linguistics.},
  document_type = {Conference Paper},
  journal       = {CoNLL 2011 - Fifteenth Conference on Computational Natural Language Learning, Proceedings of the Conference},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862288892&partnerID=40&md5=26aaa7e1a5aecc23904240ec78ba7340},
}

@Conference{Weber2010,
  author        = {Weber, C. and Handl, J.},
  title         = {A base-form lexicon of content words for correct word segmentation and syntactic-semantic annotation},
  year          = {2010},
  note          = {cited By 0},
  abstract      = {One issue in natural language processing is the interaction between a rule-based computational morphology and a syntactic-semantic analysis system. This is because derivational and compound word forms raise the question of how to deal with ambiguities caused by the rule-based analyser, and how to add additional information like valency to a derivational or compound word form if its valency frames differ from those of its root word. In this paper we propose a lexicon design addressing both of these issues. We evaluate our design in the context of a large-scale morphological analysis system for German in which the lexicon serves as an interface between morphology and syntax. In doing so, we aim at enriching the wellformed analysis results with additional information so that an adequate syntactic-semantic analysis can be ensured.},
  document_type = {Conference Paper},
  journal       = {Semantic Approaches in Natural Language Processing - Proceedings of the Conference on Natural Language Processing 2010, KONVENS},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894110931&partnerID=40&md5=f6254b4cf0ee2665f59a6be8a53db773},
}

@Conference{Yuan2010,
  author        = {Yuan, L.},
  title         = {Improvement for the automatic part-of-speech tagging based on Hidden Markov Model},
  year          = {2010},
  volume        = {1},
  pages         = {V1744-V1747},
  note          = {cited By 6},
  abstract      = {In this paper, the Markov Family Models, a kind of statistical Models was firstly introduced. Under the assumption that the probability of a word depends both on its own tag and previous word, but its own tag and previous word are independent if the word is known, we simplify the Markov Family Model and use for part-of-speech tagging successfully. Experimental results show that this part-of-speech tagging method based on Markov Family Model has greatly improved the precision comparing the conventional POS tagging method based on Hidden Markov Model under the same testing conditions. The Markov Family Model is also very useful in other natural language processing technologies such as word segmentation, statistical parsing, text-to-speech, optical character recognition, etc. © 2010 IEEE.},
  art_number    = {5555259},
  document_type = {Conference Paper},
  doi           = {10.1109/ICSPS.2010.5555259},
  journal       = {ICSPS 2010 - Proceedings of the 2010 2nd International Conference on Signal Processing Systems},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957282060&doi=10.1109%2fICSPS.2010.5555259&partnerID=40&md5=0497a5691e41adad2300192f94dc2f6f},
}

@Article{Zheng2010181,
  author        = {Zheng, X.-L. and Zhou, C.-L. and Zeng, H.-L.},
  title         = {Song ci style automatic identification},
  journal       = {Journal of Donghua University (English Edition)},
  year          = {2010},
  volume        = {27},
  number        = {2},
  pages         = {181-184},
  note          = {cited By 1},
  abstract      = {To identify Song Ci style automatically, we put forward a novel stylistic text categorization approach based on words and their semantic in this paper. And a modified special word segmentation method, a new semantic relativity computing method based on HowNet along with the corresponding word sense disambiguation method are proposed to extract words and semantic features from Song Ci. Experiments are carried out and the results show that these methods are effective.},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649613712&partnerID=40&md5=16c5881f1157390f094abd8ef4e13de6},
}

@Conference{Cho200929,
  author        = {Cho, H.-C. and Lee, D.-G. and Lee, J.-T. and Stenetorp, P. and Tsujii, J. and Rim, H.-C.},
  title         = {A novel word segmentation approach for written languages with word boundary markers},
  year          = {2009},
  pages         = {29-32},
  note          = {cited By 0},
  abstract      = {Most NLP applications work under the assumption that a user input is error-free; thus, word segmentation (WS) for written languages that use word boundary markers (WBMs), such as spaces, has been regarded as a trivial issue. However, noisy real-world texts, such as blogs, e-mails, and SMS, may contain spacing errors that require correction before further processing may take place. For the Korean language, many researchers have adopted a traditional WS approach, which eliminates all spaces in the user input and re-inserts proper word boundaries. Unfortunately, such an approach often exacerbates the word spacing quality for user input, which has few or no spacing errors; such is the case, because a perfect WS model does not exist. In this paper, we propose a novel WS method that takes into consideration the initial word spacing information of the user input. Our method generates a better output than the original user input, even if the user input has few spacing errors. Moreover, the proposed method significantly outperforms a state-of-the-art Korean WS model when the user input initially contains less than 10% spacing errors, and performs comparably for cases containing more spacing errors. We believe that the proposed method will be a very practical pre-processing module. © 2009 ACL and AFNLP.},
  document_type = {Conference Paper},
  journal       = {ACL-IJCNLP 2009 - Joint Conf. of the 47th Annual Meeting of the Association for Computational Linguistics and 4th Int. Joint Conf. on Natural Language Processing of the AFNLP, Proceedings of the Conf.},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859908122&partnerID=40&md5=90cbf81c86ca010ee0a13a41a0957027},
}

@Conference{NoAuthor2009,
  title         = {ACL-IJCNLP 2009 - Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and 4th International Joint Conference on Natural Language Processing of the AFNLP, Proceedings of the Conference},
  year          = {2009},
  note          = {cited By 0},
  abstract      = {The proceedings contain 174 papers. The topics discussed include: unsupervised argument identification for semantic role labeling; exploiting heterogeneous treebanks for parsing; cross language dependency parsing using a bilingual lexicon; topological field parsing of german; unsupervised multilingual grammar induction; reinforcement learning for mapping instructions to actions; learning semantic correspondences with less supervision; bayesian unsupervised word segmentation with nested pitman-yor language modeling; knowing the unseen: estimating vocabulary size over unseen samples; a ranking approach to stress prediction for letter-to-phoneme conversion; reducing the annotation effort for letter-to-phoneme conversion; revisiting pivot language approach for machine translation; and efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.},
  document_type = {Conference Review},
  journal       = {ACL-IJCNLP 2009 - Joint Conf. of the 47th Annual Meeting of the Association for Computational Linguistics and 4th Int. Joint Conf. on Natural Language Processing of the AFNLP, Proceedings of the Conf.},
  page_count    = {602},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859912182&partnerID=40&md5=9d5951e8fa79ccdda280a2b72d6d8aca},
}

@Conference{Parlak2009782,
  author        = {Parlak, S. and Saraclar, M.},
  title         = {Spoken information retrieval for turkish broadcast news},
  year          = {2009},
  pages         = {782-783},
  note          = {cited By 2},
  abstract      = {Speech Retrieval systems utilize automatic speech recognition (ASR) to generate textual data for indexing. However, automatic transcriptions include errors, either because of out-of-vocabulary (OOV) words or due to ASR inaccuracy. In this work, we address spoken information retrieval in Turkish, a morphologically rich language where OOV rates are high. We apply several techniques, such as using subword units and indexing alternative hypotheses, to cope with the OOV problem and ASR inaccuracy. Experiments are performed on our Turkish Broadcast News (BN) Corpus which also incorporates a spoken IR collection. Results indicate that word segmentation is quite useful but the efficiency of indexing alternative hypotheses depends on retrieval type.},
  document_type = {Conference Paper},
  doi           = {10.1145/1571941.1572126},
  journal       = {Proceedings - 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449149712&doi=10.1145%2f1571941.1572126&partnerID=40&md5=f15d01d8f71b2fbcf688a299ff1270e8},
}

@Conference{Qinglin2008,
  author        = {Qinglin, G. and Kehe, W. and Wei, L.},
  title         = {The research and realization about question answer system based on natural language processing},
  year          = {2008},
  note          = {cited By 4},
  abstract      = {Automatic Question Answer System(QAS)is a kind of high-powered software system based on Internet. Its key technology is the interrelated technology based on natural language understanding, including the construction of knowledge base and corpus, the Word Segmentation and POS Tagging of text, the Grammatical Analysis and Semantic Analysis of sentences etc. This thesis dissertated mainly the denotation of knowledge-information based on semantic network in QAS, the stochastic syntax-parse model named LSF of knowledge-information in QAS, the structure and constitution of QAS. And the LSF model parameters were exercised; it proved that they are feasible. At the same time, through "the limited-domain QAS" which was exploited for banks by us, these technologies are proved effective andpropagable. © 2007 IEEE.},
  art_number    = {4428144},
  document_type = {Conference Paper},
  doi           = {10.1109/ICICIC.2007.585},
  journal       = {Second International Conference on Innovative Computing, Information and Control, ICICIC 2007},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-39049090472&doi=10.1109%2fICICIC.2007.585&partnerID=40&md5=67b63faf00518c605e9d82d8eeb17f0f},
}

@Conference{Gabay200861,
  author        = {Gabay, D. and Ziv, B.E. and Elhadad, M.},
  title         = {Using wikipedia links to construct word segmentation corpora},
  year          = {2008},
  volume        = {WS-08-15},
  pages         = {61-63},
  note          = {cited By 3},
  abstract      = {Tagged corpora are essential for evaluating and training natural language processing tools. The cost of constructing large enough manually tagged corpora is high, even when the annotation level is shallow. This article describes a simple method to automatically create a partially tagged corpus, using Wikipedia hyperlinks. The resulting corpus contains information about the correct segmentation of 523,599 non-consecutive words in 363,090 sentences. We used our method to construct a corpus of Modern Hebrew (which we have made available at http://www.cs.bgu.ac.il/-nlpproj). The method can also be applied to other languages where word segmentation is difficult to determine, such as East and South-East Asian languages. Copyright © 2008.},
  document_type = {Conference Paper},
  journal       = {AAAI Workshop - Technical Report},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449775446&partnerID=40&md5=ad7bc10a1c3ed07e582931513ab8ae88},
}

@Article{Lee200728,
  author        = {Lee, D.-G. and Rim, H.-C. and Yook, D.},
  title         = {Automatic word spacing using probabilistic models based on character n-grams},
  journal       = {IEEE Intelligent Systems},
  year          = {2007},
  volume        = {22},
  number        = {1},
  pages         = {28-35},
  note          = {cited By 16},
  abstract      = {Probabilistic models based on Hidden Markov models (HMM) for automatic word spacing that use characters n-grams, which is a sub-sequence of n characters in a given character sequence, are discussed. Automatic word spacing is a preprocessing techniques used for correcting boundaries between words in a sentence containing spacing errors. These model can be effectively applied to a natural language with a small character set, such as English, using character n-grams that are larger than trigrams. These models, which are language independent and can be effectively used for languages having word spacing, can also be used for word segmentation in the languages without explicit word spacing. These models, by generalizing the HMMs, can consider a broad context and estimate accurate probabilities.},
  document_type = {Article},
  doi           = {10.1109/MIS.2007.4},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847611998&doi=10.1109%2fMIS.2007.4&partnerID=40&md5=f49f0e501e9b8b12dedfb8fee532c736},
}

@Conference{Álvarez200735,
  author        = {Álvarez, M.A.M. and Pombo, J.O. and Barcala Rodríguez, F.M. and Gil, J.G.},
  title         = {Practical application of one-pass Viterbi algorithm in tokenization and part-of-speech tagging},
  year          = {2007},
  volume        = {2007-January},
  pages         = {35-40},
  note          = {cited By 0},
  abstract      = {Sentence word segmentation and Part-Of-Speech (POS) tagging are common preprocessing tasks for many Natural Language Processing (NLP) applications. This paper presents a practical application for POS tagging and segmentation disambiguation using an extension of the one-pass Viterbi algorithm called Viterbi-N. We introduce the internals of the developed system, which is based on lattices and a stochastic model built using second order Hidden Markov Models (HMMs). Also, we present the results of an evaluation process and the analysis of the error cases. The results achieved suggest that the Viterbi-N algorithm applied on lattices allows POS tagging and segmentation disambiguation to be accomplished in a common process. Although the tests were done for the Galician language, the solution proposed could be easily exported to other languages.},
  document_type = {Conference Paper},
  journal       = {International Conference Recent Advances in Natural Language Processing, RANLP},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959058631&partnerID=40&md5=94a0c3d0a722f66f83f956b81baf2923},
}

@Article{Sornlertlamvanich20071565,
  author        = {Sornlertlamvanich, V. and Charoenporn, T. and Tongchim, S. and Kruengkrai, C. and Isahara, H.},
  title         = {Statistical-based approach to non-segmented language processing},
  journal       = {IEICE Transactions on Information and Systems},
  year          = {2007},
  volume        = {E90-D},
  number        = {10},
  pages         = {1565-1573},
  note          = {cited By 0},
  abstract      = {Several approaches have been studied to cope with the exceptional features of non-segmented languages. When there is no explicit information about the boundary of a word, segmenting an input text is a formidable task in language processing. Not only the contemporary word list, but also usages of the words have to be maintained to cover the use in the current texts. The accuracy and efficiency in higher processing do heavily rely on this word boundary identification task. In this paper, we introduce some statistical based approaches to tackle the problem due to the ambiguity in word segmentation. The word boundary identification problem is then defined as a part of others for performing the unified language processing in total. To exhibit the ability in conducting the unified language processing, we selectively study the tasks of language identification, word extraction, and dictionary-less search engine. Copyright © 2007 The Institute of Electronics, Information and Communication Engineers.},
  document_type = {Article},
  doi           = {10.1093/ietisy/e90-d.10.1565},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-68249162367&doi=10.1093%2fietisy%2fe90-d.10.1565&partnerID=40&md5=b975a7c339586b8e49372a82a28f30a5},
}

@Conference{Guo200792,
  author        = {Guo, Q. and Li, C.},
  title         = {The research on the application of text clustering and natural language understanding in automatic abstracting},
  year          = {2007},
  volume        = {4},
  pages         = {92-96},
  note          = {cited By 4},
  abstract      = {A method of realization of Automatic Abstracting based on Text Clustering and Natural Language Understanding is brought forward, aimed at overcoming shortages of some current methods. The method makes use of text Clustering and can realize Automatic Abstracting of multi- documents. The algorithm of twice Word Segmentation based on the Title and FirstSentences in Paragraphs is brought forward. Its precision and recall is above 95% For a specific domain on plastics, an Automatic Abstracting system named TCAAS is implemented. The precision and recall of multidocument's Automatic Abstracting is above 75% And experiments do prove that it is feasible to use the method to develop a domain Automatic Abstracting System, which is valuable for further study in more depth. © 2007 IEEE.},
  art_number    = {4406353},
  document_type = {Conference Paper},
  doi           = {10.1109/FSKD.2007.584},
  journal       = {Proceedings - Fourth International Conference on Fuzzy Systems and Knowledge Discovery, FSKD 2007},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-44049094329&doi=10.1109%2fFSKD.2007.584&partnerID=40&md5=1786ee4735dd963c58059a2f015c195a},
}

@Article{Zhang2007962,
  author        = {Zhang, L. and Lu, R.-Z.},
  title         = {Understanding speech utterances in mandarin dialogue system},
  journal       = {WSEAS Transactions on Information Science and Applications},
  year          = {2007},
  volume        = {4},
  number        = {5},
  pages         = {962-967},
  note          = {cited By 0},
  abstract      = {In this paper, we present a mandarin spoken dialogue system - STRQS (Shanghai Traffic Route Querying System), which is used for querying best traffic route between any two locations in Shanghai. A series of language processing strategies is used to understand speech utterances. The understanding processing is done in three steps: First, word segmentation and part-of-speech tagging module splits the utterance into words and labels them with semantic categories. The second step is a robust partial parsing process. Parsing is based on Unification Grammar (UG). An augmented chart algorithm with feature computing is implemented. Finally, the parsed utterance is associated with a semantic interpreter by a frame module. Semantic based analysis method we developed can directly extract information from the output of a speech recognizer, which contains errors and ill-formed components. The testing results demonstrate the robustness of our approach.},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248332570&partnerID=40&md5=c838fc8d4a9a2cc1bce9422bb5a8ff3d},
}

@Article{Guo2005705,
  author        = {Guo, Q.-L. and Fan, X.-Z. and Liu, C.-A.},
  title         = {Automatic abstracting based on text clustering and natural language understanding},
  journal       = {Beijing Ligong Daxue Xuebao/Transaction of Beijing Institute of Technology},
  year          = {2005},
  volume        = {25},
  number        = {8},
  pages         = {705-709},
  note          = {cited By 0},
  abstract      = {A method of realization of automatic abstracting based on text clustering and natural language understanding is brought forward, aimed at overcoming shortages of some current methods. The method makes use of text clustering and can realize automatic abstracting of multi-documents. The algorithm of twice word segmentation based on the title and first-sentences in paragraphs is brought forward. Its precision and recall is above 95%. For a specific domain on plastics, an automatic abstracting system is implemented. The precision and recall of multi-document's automatic abstracting is above 75%. And experiments do prove that it is feasible to use the method to develop a domain automatic abstracting system, that is valuable for further study in more depth.},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-26644433687&partnerID=40&md5=8d770e215c7f48cd82a08bb2b43150a2},
}

@Article{VanDalen2005211,
  author        = {Van Dalen, R.C. and Wiggers, P. and Rothkrantz, L.J.M.},
  title         = {Modelling lexical stress},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2005},
  volume        = {3658 LNAI},
  pages         = {211-218},
  note          = {cited By 2},
  abstract      = {Human listeners use lexical stress for word segmentation and disambiguation. We look into using lexical stress for speech recognition by examining a Dutch-language corpus. We propose that different spectral features are needed for different phonemes and that, besides vowels, consonants should be taken into account. © Springer-Verlag Berlin Heidelberg 2005.},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646048041&partnerID=40&md5=fbc7d621761b047f9ed7d44cfb94fb6a},
}

@Conference{Chao200470,
  author        = {Chao, C. and Zhengrong, S.},
  title         = {A self-service digital virtual reference service system for a college libraries},
  year          = {2004},
  pages         = {70-74},
  note          = {cited By 1},
  abstract      = {In this article a virtual reference service system is described, which accomplishes several self-service reference functions with current digital techniques. The system consists of three knowledge databases: general FAQ database, academic FAQ database and academic resource navigation database, based on which the system is able to provide natural language reference service by word segmentation of natural language, to train the reference service according to reference choice feedbacks of readers, as well as to perfect its own knowledge database through the reference service librarians provided to readers.},
  document_type = {Conference Paper},
  journal       = {Proceedings of the Eighth IASTED International Conference on Internet and Multimedia Systems and Applications},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-10444271801&partnerID=40&md5=b5b7d14732bf6a975e18daa028e97480},
}

@Article{Boyce2002325,
  author        = {Boyce, B.R.},
  title         = {In this issue},
  journal       = {Journal of the American Society for Information Science and Technology},
  year          = {2002},
  volume        = {53},
  number        = {5},
  pages         = {325-326},
  note          = {cited By 0},
  document_type = {Editorial},
  doi           = {10.1002/asi.10086},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036501429&doi=10.1002%2fasi.10086&partnerID=40&md5=13ba1ab38d4d3d34f7b72c7d06917260},
}

@Conference{Kim2002320,
  author        = {Kim, S.H. and Jeong, C.B. and Kwag, H.K. and Suen, C.Y.},
  title         = {Word segmentation of printed text lines based on gap clustering and special symbol detection},
  year          = {2002},
  volume        = {16},
  number        = {2},
  pages         = {320-323},
  note          = {cited By 17},
  abstract      = {This paper proposes a word segmentation method for machine-printed text lines. It utilizes gaps and special symbols as delimiters between words. A gap clustering technique is used to identify the gaps between words regardless of the gap-size variations among different document images. Next a special symbol detection technique is applied to find two types of special symbols lying between words. An experiment with 1.675 text lines in 100 different English and Korean documents shows that the proposed method achieves a high accuracy of word segmentation. © 2002 IEEE.},
  document_type = {Conference Paper},
  journal       = {Proceedings - International Conference on Pattern Recognition},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751570122&partnerID=40&md5=ab405aaa302da18ac3a52e46e7eb577c},
}

@Article{Kazakov2001121,
  author        = {Kazakov, D. and Manandhar, S.},
  title         = {Unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming},
  journal       = {Machine Learning},
  year          = {2001},
  volume        = {43},
  number        = {1-2},
  pages         = {121-162},
  note          = {cited By 21},
  abstract      = {This article presents a combination of unsupervised and supervised learning techniques for the generation of word segmentation rules from a raw list of words. First, a language bias for word segmentation is introduced and a simple genetic algorithm is used in the search for a segmentation that corresponds to the best bias value. In the second phase, the words segmented by the genetic algorithm are used as an input for the first order decision list learner CLOG. The result is a set of first order rules which can be used for segmentation of unseen words. When applied on either the training data or unseen data, these rules produce segmentation which are linguistically meaningful, and to a large degree conforming to the annotation provided.},
  document_type = {Article},
  doi           = {10.1023/A:1007629103294},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035312598&doi=10.1023%2fA%3a1007629103294&partnerID=40&md5=eaae5dc95f7c91cc97525afdf2bb2c17},
}

@InProceedings{10.1007/978-3-319-23980-4_7,
  author    = {Janicki, Maciej},
  title     = {A Multi-purpose Bayesian Model for Word-Based Morphology},
  booktitle = {Systems and Frameworks for Computational Morphology},
  year      = {2015},
  editor    = {Mahlow, Cerstin and Piotrowski, Michael},
  pages     = {104--123},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {This paper introduces a probabilistic model of morphology based on a word-based morphological theory. Morphology is understood here as a system of rules that describe systematic correspondences between full word forms, without decomposing words into any smaller units. The model is formulated in the Bayesian learning framework and can be trained in both supervised and unsupervised setting. Evaluation is performed on tasks of generating unseen words, lemmatization and inflected form production.},
  isbn      = {978-3-319-23980-4},
}

@Article{Pretorius2015,
  author   = {Pretorius, Laurette and Viljoen, Biffie and Berg, Ansu and Pretorius, Rigardt},
  title    = {Tswana finite state tokenisation},
  journal  = {Language Resources and Evaluation},
  year     = {2015},
  volume   = {49},
  number   = {4},
  pages    = {831--856},
  month    = {Dec},
  issn     = {1574-0218},
  abstract = {Tswana, a Bantu language in the Sotho group, is characterised by an agglutinative morphology and a disjunctive orthography, which mainly affects the verb category. In particular, verbal prefixes are usually written disjunctively, while suffixes follow a conjunctive writing style. Therefore, Tswana tokenisation cannot be based solely on whitespace, as is the case in many alphabetic, segmented languages, including the conjunctively written Nguni group of South African Bantu languages. This paper shows how a combination of two finite state tokeniser transducers and a finite state morphological analyser are combined to solve the Tswana (verb) tokenisation problem. The approach has the important advantage of bringing the processing of Tswana, beyond the morphological analysis level, in line with what is appropriate for the Nguni languages. This means that the challenge of the disjunctive orthography is met at the tokenisation/morphological analysis level and does not in principle propagate to subsequent levels of analysis such as POS tagging and shallow parsing, etc. The tokenisation approach is novel and, when implemented and evaluated, yields an F1-score of 95 {\%} with respect to a hand tokenised gold standard.},
  day      = {01},
  doi      = {10.1007/s10579-014-9292-1},
  url      = {https://doi.org/10.1007/s10579-014-9292-1},
}

@InProceedings{10.1007/978-3-642-35828-9_33,
  author    = {Cutugno, Francesco and Origlia, Antonio and Seppi, Dino},
  title     = {EVALITA 2011: Forced Alignment Task},
  booktitle = {Evaluation of Natural Language and Speech Tools for Italian},
  year      = {2013},
  editor    = {Magnini, Bernardo and Cutugno, Francesco and Falcone, Mauro and Pianta, Emanuele},
  pages     = {305--311},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Forced alignment both for words and phones is a challenging and interesting task for automatic speech processing systems because the difficulties introduced by natural speech are many and hard to deal with. Furthermore, forced alignment approaches have been tested on Italian just in a few studies. In this task, the main goal was to evaluate the performance offered by the proposed systems on Italian and their robustness in presence of noisy data.},
  isbn      = {978-3-642-35828-9},
}

@InProceedings{10.1007/978-3-642-35828-9_36,
  author    = {Ludusan, Bogdan},
  title     = {UNINA System for the EVALITA 2011 Forced Alignment Task},
  booktitle = {Evaluation of Natural Language and Speech Tools for Italian},
  year      = {2013},
  editor    = {Magnini, Bernardo and Cutugno, Francesco and Falcone, Mauro and Pianta, Emanuele},
  pages     = {330--337},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {This paper presents the results obtained for the EVALITA 2011 Forced Alignment on Spontaneous Speech task, including some aspects explored for the generation of the lexicon. A classical system was used for the alignment and several tests were performed to determine the impact of frame shift size and the use of speaker adaptation on the accuracy of the alignment. Good segmentation results were obtained, the proposed system outperforming the other teams' systems. Furthermore, phonetic change rules were determined on the train set and employed in the alignment process, improving significantly the performance of the system.},
  isbn      = {978-3-642-35828-9},
}

@InProceedings{10.1007/978-3-642-22327-3_24,
  author    = {Fern{\'a}ndez, Antonio and D{\'i}az, Josval and Guti{\'e}rrez, Yoan and Mu{\~{n}}oz, Rafael},
  title     = {An Unsupervised Method to Improve Spanish Stemmer},
  booktitle = {Natural Language Processing and Information Systems},
  year      = {2011},
  editor    = {Mu{\~{n}}oz, Rafael and Montoyo, Andr{\'e}s and M{\'e}tais, Elisabeth},
  pages     = {221--224},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We evaluate the effectiveness of using our edit distances algorithm to improving an unsupervised language-independent stemming method. The main idea is to create morphological families through the automatic words grouping using our distance. Based on that grouping, we make a stemming process. The capacity of the edit distance algorithm in the task of words clustering and the ability of our method to generate the correct stem for Spanish was evaluated. A good result (98{\%} precision) for the morphological families' creation and also a remarkable 99.85{\%} of correct stemming was obtained.},
  isbn      = {978-3-642-22327-3},
}

@InProceedings{10.1007/978-3-540-85760-0_13,
  author    = {Pingali, Prasad and Tune, Kula Kekeba and Varma, Vasudeva},
  title     = {Improving Recall for Hindi, Telugu, Oromo to English CLIR},
  booktitle = {Advances in Multilingual and Multimodal Information Retrieval},
  year      = {2008},
  editor    = {Peters, Carol and Jijkoun, Valentin and Mandl, Thomas and M{\"u}ller, Henning and Oard, Douglas W. and Pe{\~{n}}as, Anselmo and Petras, Vivien and Santos, Diana},
  pages     = {103--110},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {This paper presents the Cross Language Information Retrieval (CLIR) experiments of the Language Technologies Research Centre (LTRC, IIIT-Hyderabad) as part of our participation in the ad-hoc track of CLEF 2007. We present approaches to improve recall of query translation by handling morphological and spelling variations in source language keywords. We also present experiments using query expansion in CLIR using a source language monolingual corpus for Hindi, Telugu and English. We also present the effect of using an Oromo stemmer in Oromo-English CLIR system and report results using the CLEF 2007 dataset.},
  isbn      = {978-3-540-85760-0},
}

@InProceedings{10.1007/978-3-540-85760-0_115,
  author    = {Monson, Christian and Carbonell, Jaime and Lavie, Alon and Levin, Lori},
  title     = {ParaMor: Finding Paradigms across Morphology},
  booktitle = {Advances in Multilingual and Multimodal Information Retrieval},
  year      = {2008},
  editor    = {Peters, Carol and Jijkoun, Valentin and Mandl, Thomas and M{\"u}ller, Henning and Oard, Douglas W. and Pe{\~{n}}as, Anselmo and Petras, Vivien and Santos, Diana},
  pages     = {900--907},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {ParaMor automatically learns morphological paradigms from unlabelled text, and uses them to annotate word forms with morpheme boundaries. ParaMor competed in the English and German tracks of Morpho Challenge 2007 (Kurimo et al., 2008). In English, ParaMor's balanced precision and recall outperform at F1 an already sophisticated baseline induction algorithm, Morfessor (Creutz, 2006). In German, ParaMor suffers from a low morpheme recall. But combining ParaMor's analyses with analyses from Morfessor results in a set of analyses that outperform either algorithm alone, and that place first in F1 among all algorithms submitted to Morpho Challenge 2007.},
  isbn      = {978-3-540-85760-0},
}

@InProceedings{10.1007/978-3-540-85760-0_112,
  author    = {Bernhard, Delphine},
  title     = {Simple Morpheme Labelling in Unsupervised Morpheme Analysis},
  booktitle = {Advances in Multilingual and Multimodal Information Retrieval},
  year      = {2008},
  editor    = {Peters, Carol and Jijkoun, Valentin and Mandl, Thomas and M{\"u}ller, Henning and Oard, Douglas W. and Pe{\~{n}}as, Anselmo and Petras, Vivien and Santos, Diana},
  pages     = {873--880},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {This paper describes a system for unsupervised morpheme analysis and the results it obtained at Morpho Challenge 2007. The system takes a plain list of words as input and returns a list of labelled morphemic segments for each word. Morphemic segments are obtained by an unsupervised learning process which can directly be applied to different natural languages. Results obtained at competition 1 (evaluation of the morpheme analyses) are better in English, Finnish and German than in Turkish. For information retrieval (competition 2), the best results are obtained when indexing is performed using Okapi (BM25) weighting for all morphemes minus those belonging to an automatic stop list made of the most common morphemes.},
  isbn      = {978-3-540-85760-0},
}

@InProceedings{10.1007/978-3-540-85760-0_113,
  author    = {Bordag, Stefan},
  title     = {Unsupervised and Knowledge-Free Morpheme Segmentation and Analysis},
  booktitle = {Advances in Multilingual and Multimodal Information Retrieval},
  year      = {2008},
  editor    = {Peters, Carol and Jijkoun, Valentin and Mandl, Thomas and M{\"u}ller, Henning and Oard, Douglas W. and Pe{\~{n}}as, Anselmo and Petras, Vivien and Santos, Diana},
  pages     = {881--891},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {This paper presents a revised version of an unsupervised and knowledge-free morpheme boundary detection algorithm based on letter successor variety (LSV) and a trie classifier [1]. Additional knowledge about relatedness of the found morphs is obtained from a morphemic analysis based on contextual similarity. For the boundary detection the challenge of increasing recall of found morphs while retaining a high precision is tackled by adding a compound splitter, iterating the LSV analysis and dividing the trie classifier into two distinctly applied clasifiers. The result is a significantly improved overall performance and a decreased reliance on corpus size. Further possible improvements and analyses are discussed.},
  isbn      = {978-3-540-85760-0},
}

@InProceedings{10.1007/978-3-540-70939-8_18,
  author    = {Nogueira dos Santos, C{\'i}cero and Milidi{\'u}, Ruy Luiz},
  title     = {Probabilistic Classifications with TBL},
  booktitle = {Computational Linguistics and Intelligent Text Processing},
  year      = {2007},
  editor    = {Gelbukh, Alexander},
  pages     = {196--207},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The classifiers produced by the Transformation Based error-driven Learning (TBL) algorithm do not produce uncertainty measures by default. Nevertheless, there are situations like active and semi-supervised learning where the application requires both the sample's classification and the classification confidence. In this paper, we present a novel method which enables a TBL classifier to generate a probability distribution over the class labels. To assess the quality of this probability distribution, we carry out four experiments: cross entropy, perplexity, rejection curve and active learning. These experiments allow us to compare our method with another one proposed in the literature, the TBLDT. Our method, despite being simple and straightforward, outperforms TBLDT in all four experiments.},
  isbn      = {978-3-540-70939-8},
}

@InProceedings{10.1007/11780885_7,
  author    = {Geyken, Alexander and Hanneforth, Thomas},
  title     = {TAGH: A Complete Morphology for German Based on Weighted Finite State Automata},
  booktitle = {Finite-State Methods and Natural Language Processing},
  year      = {2006},
  editor    = {Yli-Jyr{\"a}, Anssi and Karttunen, Lauri and Karhum{\"a}ki, Juhani},
  pages     = {55--66},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {TAGH is a system for automatic recognition of German word forms. It is based on a stem lexicon with allomorphs and a concatenative mechanism for inflection and word formation. Weighted FSA and a cost function are used in order to determine the correct segmentation of complex forms: the correct segmentation for a given compound is supposed to be the one with the least cost. TAGH is based on a large stem lexicon of almost 80.000 stems that was compiled within 5 years on the basis of large newspaper corpora and literary texts. The number of analyzable word forms is increased considerably by more than 1000 different rules for derivational and compositional word formation. The recognition rate of TAGH is more than 99{\%} for modern newspaper text and approximately 98.5{\%} for literary texts.},
  isbn      = {978-3-540-35469-7},
}

@InProceedings{10.1007/11671299_12,
  author    = {Medina-Urrea, Alfonso},
  title     = {Towards the Automatic Lemmatization of 16th Century Mexican Spanish: A Stemming Scheme for the CHEM},
  booktitle = {Computational Linguistics and Intelligent Text Processing},
  year      = {2006},
  editor    = {Gelbukh, Alexander},
  pages     = {101--104},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Two of the problems that should arise when developing a stemming scheme for diachronic corpora are: (1) morphological systems of natural languages may vary throughout time, and these changes are normally not documented sufficiently; and (2) they exhibit very diverse orthographic characteristics. In this short paper, a stemming strategy for a diachronic corpus of Mexican Spanish is briefly described, which partially faces up to these problems. Success rates of the method are contrasted to those of a Porter stemmer.},
  isbn      = {978-3-540-32206-1},
}

@Article{Yeh2005,
  author   = {Yeh, Alexander and Morgan, Alexander and Colosimo, Marc and Hirschman, Lynette},
  title    = {BioCreAtIvE Task 1A: gene mention finding evaluation},
  journal  = {BMC Bioinformatics},
  year     = {2005},
  volume   = {6},
  number   = {1},
  pages    = {S2},
  month    = {May},
  issn     = {1471-2105},
  abstract = {The biological research literature is a major repository of knowledge. As the amount of literature increases, it will get harder to find the information of interest on a particular topic. There has been an increasing amount of work on text mining this literature, but comparing this work is hard because of a lack of standards for making comparisons. To address this, we worked with colleagues at the Protein Design Group, CNB-CSIC, Madrid to develop BioCreAtIvE (Critical Assessment for Information Extraction in Biology), an open common evaluation of systems on a number of biological text mining tasks. We report here on task 1A, which deals with finding mentions of genes and related entities in text. ``Finding mentions'' is a basic task, which can be used as a building block for other text mining tasks. The task makes use of data and evaluation software provided by the (US) National Center for Biotechnology Information (NCBI).},
  day      = {24},
  doi      = {10.1186/1471-2105-6-S1-S2},
  url      = {https://doi.org/10.1186/1471-2105-6-S1-S2},
}

@InProceedings{10.1007/978-3-540-30222-3_30,
  author    = {Francopoulo, Gil},
  title     = {Pruning Texts with NLP and Expanding Queries with an Ontology: TagSearch},
  booktitle = {Comparative Evaluation of Multilingual Information Access Systems},
  year      = {2004},
  editor    = {Peters, Carol and Gonzalo, Julio and Braschler, Martin and Kluck, Michael},
  pages     = {319--321},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The basic lines of our system is first to use natural language processing to prune the texts and the query, and secondly to use an ontology to expand the queries.},
  isbn      = {978-3-540-30222-3},
}

@InProceedings{10.1007/3-540-44645-1_27,
  author    = {Goldsmith, John A. and Higgins, Derrick and Soglasnova, Svetlana},
  title     = {Automatic Language-Specific Stemming in Information Retrieval},
  booktitle = {Cross-Language Information Retrieval and Evaluation},
  year      = {2001},
  editor    = {Peters, Carol},
  pages     = {273--283},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We employ Automorphology, an MDL-based algorithm that determines the suffixes present in a language-sample with no prior knowledge of the language in question, and describe our experiments on the usefulness of this approach for Information Retrieval, employing this stemmer in a SMARTbased IR engine.},
  isbn      = {978-3-540-44645-3},
}

@Article{ref1,
  title   = {Guest Editorial},
  journal = {Machine Learning},
  year    = {2001},
  volume  = {43},
  number  = {1},
  pages   = {5--6},
  month   = {Apr},
  issn    = {1573-0565},
  day     = {01},
  doi     = {10.1023/A:1017394631519},
  url     = {https://doi.org/10.1023/A:1017394631519},
}

@InProceedings{10.1007/978-3-662-12146-7_4,
  author    = {Schott, G.},
  title     = {Automatische Kompositazerlegung mit Einem Minimalw{\"o}rterbuch zur Informationsgewinnung aus Beliebigen Fachtexten},
  booktitle = {Klartextverarbeitung},
  year      = {1978},
  editor    = {Wingert, Friedrich},
  pages     = {32--43},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Die Zerlegung von beliebigen Komposita (zusammengesetzte Substantive oder Adjektive) in richtige Komponenten unter Verwendung eines zahlenm{\"a}{\ss}ig geringen Wortschatzes soll dazu dienen, in Informationssystemen auch nach Teilbegriffen abfragen zu k{\"o}nnen, ohne eine Auflistung aller existenten und potentiellen Wortbildungen erforderlich zu machen. Dabei soll ein Minimum an W{\"o}rtern ein Maximum an bereits lexikalisierten und noch bildbaren Komposita abdecken. Ihre Zerlegung in richtige Komponenten ist ein die Linguistik wie die Informatik angehendes Problem, zumal Fachsprache und Alltagssprache nur anscheinend eine un{\"u}berwindliche Grenze haben, die definitorisch nicht so einfach festzulegen ist, wie es bisher oft den Anschein hatte [3].},
  isbn      = {978-3-662-12146-7},
}

@InProceedings{ISI:000481622601073,
  author            = {Liu, Di and Su, Jiangwen and Song, Lihua and Qiu, Zhen},
  title             = {{Application of Internet segmentation research based on Natural Language Processing technology in enterprise public opinion risk monitoring}},
  booktitle         = {{2018 INTERNATIONAL SYMPOSIUM ON POWER ELECTRONICS AND CONTROL ENGINEERING (ISPECE 2018)}},
  year              = {{2019}},
  volume            = {{1187}},
  series            = {{Journal of Physics Conference Series}},
  note              = {{International Symposium on Power Electronics and Control Engineering (ISPECE), Xian Univ Technol, Xian, PEOPLES R CHINA, DEC 28-30, 2018}},
  abstract          = {{With the advent of the mobile Internet era, the network has become a
   distribution center of various information such as media, entertainment,
   sports, economy, politics and so on. A large amount of information is
   generated and disappeared on the network every day. How to effectively
   extract and identify the relevant data, and judge and analyze them is an
   important part of the corporate public opinion control. This paper uses
   natural language processing technology to study the word segmentation of
   text information on the network, and applies it to the risk detection of
   corporate public opinion.}},
  article-number    = {{042007}},
  book-group-author = {{IOP}},
  doi               = {{10.1088/1742-6596/1187/4/042007}},
  eissn             = {{1742-6596}},
  issn              = {{1742-6588}},
  unique-id         = {{ISI:000481622601073}},
}

@Article{ISI:000470910200001,
  author               = {Shivakumar, Prashanth Gurunath and Georgiou, Panayiotis},
  title                = {{Confusion2Vec: towards enriching vector space word representations with representational ambiguities}},
  journal              = {{PEERJ COMPUTER SCIENCE}},
  year                 = {{2019}},
  month                = {{JUN 10}},
  issn                 = {{2376-5992}},
  abstract             = {{Word vector representations are a crucial part of natural language
   processing (NLP) and human computer interaction. In this paper, we
   propose a novel word vector representation, Confusion2Vec, motivated
   from the human speech production and perception that encodes
   representational ambiguity. Humans employ both acoustic similarity cues
   and contextual cues to decode information and we focus on a model that
   incorporates both sources of information. The representational ambiguity
   of acoustics, which manifests itself in word confusions, is often
   resolved by both humans and machines through contextual cues. A range of
   representational ambiguities can emerge in various domains further to
   acoustic perception, such as morphological transformations, word
   segmentation, paraphrasing for NLP tasks like machine translation, etc.
   In this work, we present a case study in application to automatic speech
   recognition (ASR) task, where the word representational
   ambiguities/confusions are related to acoustic similarity. We present
   several techniques to train an acoustic perceptual similarity
   representation ambiguity. We term this Confusion2Vec and learn on
   unsupervised-generated data from ASR confusion networks or lattice-like
   structures. Appropriate evaluations for the Confusion2Vec are formulated
   for gauging acoustic similarity in addition to semantic-syntactic and
   word similarity evaluations. The Confusion2Vec is able to model word
   confusions efficiently, without compromising on the semantic-syntactic
   word relations, thus effectively enriching the word vector space with
   extra task relevant ambiguity information. We provide an intuitive
   exploration of the two-dimensional Confusion2Vec space using principal
   component analysis of the embedding and relate to semantic
   relationships, syntactic relationships, and acoustic relationships. We
   show through this that the new space preserves the semantic/syntactic
   relationships while robustly encoding acoustic similarities. The
   potential of the new vector representation and its ability in the
   utilization of uncertainty information associated with the lattice is
   demonstrated through small examples relating to the task of ASR error
   correction.}},
  article-number       = {{e195}},
  doi                  = {{10.7717/peerj-cs.195}},
  orcid-numbers        = {{Georgiou, Panayiotis Panos/0000-0002-0790-7161}},
  researcherid-numbers = {{Georgiou, Panayiotis Panos/E-8387-2018}},
  unique-id            = {{ISI:000470910200001}},
}

@InProceedings{ISI:000470071700124,
  author            = {Shen, Hanji and Long, Chun and Wan, Wei and Li, Jun and Qin, Yakui and Fu, Yuhao and Song, Xiaofan},
  title             = {{Log Layering Based on Natural Language Processing}},
  booktitle         = {{2019 21ST INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY (ICACT): ICT FOR 4TH INDUSTRIAL REVOLUTION}},
  year              = {{2019}},
  series            = {{International Conference on Advanced Communication Technology}},
  pages             = {{660-663}},
  organization      = {{IEEE; IEEE Commun Soc; Global IT Res Inst; Natl Informat Soc Agcy; ETRI; Gangwon Convent \& Visitors Bur; Natl Univ, Inst Vietnam, Informat Technol; KICS; IEEK ComSoc; Korean Inst Informat Scientists \& Engineers; Open Standards \& Internet Assoc; Korea Inst Informat Secur \& Cryptol; IEEE Commun Soc Commun \& Informat Secur Tech Community; IEEE Commun Soc Optical Networking Tech Community; IEEE Commun Soc Int Conference Adv Commun Technol}},
  note              = {{21st International Conference on Advanced Communication Technology (ICACT), Pyeongchang, SOUTH KOREA, FEB 17-20, 2019}},
  abstract          = {{With the increasing number and variety of logs, the requirement of
   storage space is growing rapidly. Meantime, the speed and accuracy of
   querying in massive logs are becoming increasingly important. Although
   the well-built distributed storage technique solves the problem of mass
   storage and fast query, the cost is too high. As logs are created as the
   method to trace the historical operation, the requirement for query rate
   is not high. To balance the storage cost and query rate, this paper
   proposes a real-time log layering storage technique based on natural
   language processing. According to the characteristics of the log data,
   this technique is combined with the text language processing technique.
   It compresses the real-time log data effectively while considering the
   query efficiency. Firstly, the method extracts the feature of each log
   that flows in, which will be the type name of the log. Then, the method
   performs word segmentation on the log and encodes each word to store the
   key value pairs. Finally, the key value pairs of the log are stored in
   the memory, and the code of each log is stored in the database.
   Experiments show that this method can ensure the integrity of the data
   effectively, decompression time dropped to 40\%, compression rate down
   to 35\%.}},
  book-group-author = {{IEEE}},
  isbn              = {{979-11-88428-02-1}},
  issn              = {{1738-9445}},
  unique-id         = {{ISI:000470071700124}},
}

@InProceedings{ISI:000470064000028,
  author            = {Mzamo, Lulamile and Helberg, Albert and Bosch, Sonja},
  title             = {{Towards an unsupervised morphological segmenter for isiXhosa}},
  booktitle         = {{2019 SOUTHERN AFRICAN UNIVERSITIES POWER ENGINEERING CONFERENCE/ROBOTICS AND MECHATRONICS/PATTERN RECOGNITION ASSOCIATION OF SOUTH AFRICA (SAUPEC/ROBMECH/PRASA)}},
  year              = {{2019}},
  pages             = {{166-170}},
  organization      = {{IEEE; S African Inst Elect Engineers; Robot Assoc S Africa; Pattern Recognit Assoc S Africa; IEEE S Africa Sect; FESTO; ABB; Altair; Cobots; OPAL RT Technologies; HORNE Technologies}},
  note              = {{27th Southern African Universities Power Engineering Conference (SAUPEC) / 11th Robotics and Mechatronics Conference of South Africa (RobMech) / 29th Annual Symposium of Pattern-Recognition-Association-of-South-Africa (PRASA), Cent Univ Technol, Bloemfontein, SOUTH AFRICA, JAN 28-30, 2019}},
  abstract          = {{In this paper, branching entropy techniques and isiXhosa language
   heuristics are adapted to develop unsupervised morphological segmenters
   for isiXhosa. An overview of isiXhosa segmentation issues is given,
   followed by a discussion on previous work in automated segmentation, and
   segmentation of isiXhosa in particular. Two unsupervised isiXhosa
   segmenters are presented and compared to a random minimum baseline and
   Morfessor-Baseline, a standard in unsupervised word segmentation.
   Morfessor-Baseline outperforms both isiXhosa segmenters at 79.10\%
   boundary identification accuracy. The IsiXhosa Branching Entropy
   Segmenter (XBES) performance varies depending on the segmentation mode
   used, with a maximum of 73.39\%. The IsiXhosa Heuristic Maximum
   Likelihood Segmenter (XHMLS) achieves 72.42\%. The study suggests that
   unsupervised isiXhosa morphological segmentation is feasible with better
   optimization of the current attempts.}},
  book-group-author = {{IEEE}},
  isbn              = {{978-1-7281-0369-3}},
  unique-id         = {{ISI:000470064000028}},
}

@InProceedings{ISI:000468838000153,
  author            = {Zhao, Tianyuan and Li, Lei and Xie, Yang and Lv, Yue},
  title             = {{Data-driven Risk Assessment for Peer-to-Peer Network Lending Agencies}},
  booktitle         = {{PROCEEDINGS OF 2018 5TH IEEE INTERNATIONAL CONFERENCE ON CLOUD COMPUTING AND INTELLIGENCE SYSTEMS (CCIS)}},
  year              = {{2018}},
  series            = {{International Conference on Cloud Computing and Intelligence Systems}},
  pages             = {{799-803}},
  organization      = {{IEEE; IEEE Beijing Sect; Chinese Assoc Artificial Intelligence; Nanjing Univ Posts \& Telecommunicat; Shanghai Univ; Jiangsu Engineering Lab Big Data Anal \& Control Active Distribut Network; Nanjing Univ Sci \& Technol; Swinburne Univ Technol; Shanghai Key Lab Power Stn Automat Technol}},
  note              = {{5th IEEE International Conference on Cloud Computing and Intelligence Systems (CCIS), Nanjing, PEOPLES R CHINA, NOV 23-25, 2018}},
  abstract          = {{With the rapid development of Peer-to-Peer(P2P) network lending in the
   financial field, more data of lending agencies have appeared. P2P
   agencies also have problems such as absconded with ill-gotten gains and
   out of business. Therefore, it is necessary to assess their risks based
   on P2P company data. This paper proposes a framework of Data-driven Risk
   Assessment for P2P(DRAP2P) network lending agencies based on
   unstructured natural language data. First, use the natural language
   processing technology, such as word segmentation, keyword, LDA topic
   model, word2vec and doc2vec, to process and extract features of company
   profile which reflect its business status. Then, seven machine learning
   classifiers and three deep learning models are used for analysis. Since
   keywords show good performance in machine learning models, we improve
   Convolutional Neural Network(CNN) with keywords and propose two
   CNN+Keyword models, namely CNN+Keyword(static+BP) and CNN+Keyword
   (Expand word embedding). Experiments have shown that
   CNN+Keyword(static+BP) can achieve the best performance. Finally, we use
   the method of meta-learning to integrate CNN+Keyword(static+BP) and
   logistic regression classifier to further strengthen the performance.}},
  book-group-author = {{IEEE}},
  isbn              = {{978-1-5386-6005-8}},
  issn              = {{2376-5933}},
  unique-id         = {{ISI:000468838000153}},
}

@Article{ISI:000450597900037,
  author    = {Zhang, Zeliang and Bi, Xinwen},
  title     = {{Research and Experiment of Intelligent Natural Language Processing Algorithms}},
  journal   = {{WIRELESS PERSONAL COMMUNICATIONS}},
  year      = {{2018}},
  volume    = {{102}},
  number    = {{4}},
  pages     = {{2927-2939}},
  month     = {{OCT}},
  issn      = {{0929-6212}},
  abstract  = {{Natural language processing is mainly divided into two parts: speech
   processing and word processing. The level of word processing is mainly
   studied. Natural language processing is divided into lexical analysis,
   syntax analysis and semantic analysis. Aiming at the scope of the
   language ambiguity and thesaurus in the field of smart home, the maximal
   matching algorithm is used to segment the natural language. Then,
   through the way of template matching, semantic comprehension finally
   forms the code form that can control the home node. In the system
   applied in this paper, the speech is processed into words through the
   existing voice input function of the mobile terminal. Then, the control
   instruction is obtained through the language processing method. The
   processed data is communicated to the server via socket. The server
   sends the data to the home node through the Zigbee protocol. Finally,
   control of home appliances is achieved.}},
  doi       = {{10.1007/s11277-018-5316-2}},
  eissn     = {{1572-834X}},
  unique-id = {{ISI:000450597900037}},
}

@InProceedings{ISI:000405903400040,
  author            = {Jamro, Wazir Ali},
  title             = {{Sindhi Language Processing: A Survey}},
  booktitle         = {{2017 INTERNATIONAL CONFERENCE ON INNOVATIONS IN ELECTRICAL ENGINEERING AND COMPUTATIONAL TECHNOLOGIES (ICIEECT)}},
  year              = {{2017}},
  organization      = {{Indus Univ, Fac Engn Sci \& Technol; Inst Elect \& Elect Engineers; Pakistan Engn Council; NTC; IEEE Karachi Sect; LEJ Natl Sci Informat Ctr, Salimuzzaman Siddiqui Auditorium Int Ctr Chem \& Biol Sci, Higher Educ Commiss; K Elect; Natl Comp Educ \& Accreditat Council, Higher Educ Commiss; Higher Educ Commiss Islamabad; Pakistan Sci Fdn; VMWare; Habib Bank Ltd; Pakistan Telecommunicat Author}},
  note              = {{International Conference on Innovations in Electrical Engineering and Computational Technologies (ICIEECT), Karachi, PAKISTAN, APR 05-07, 2017}},
  abstract          = {{In this era of information technology, natural language processing (NLP)
   has become volatile field because of digital reliance of today's
   communities. The growth of Internet usage bringing the communities,
   cultures and languages online. In this regard much of the work has been
   done of the European and east Asian languages, in the result these
   languages have reached mature level in terms of computational
   processing. Despite the great importance of NLP science, still most of
   the South Asian languages are under developing phase. Sindhi language is
   one of them, which stands among the most ancient languages in the world.
   The Sindhi language has a great influence on the large community in
   Sindh province of Pakistan and some states of India and other countries.
   But unfortunately, it is at infant level in terms of computational
   processing, because it has not received such attention of language
   engineering community, due to its complex morphological structure and
   scarcity of language resources. Therefore, this study has been carried
   out in order to summarize the existing work on Sindhi Language
   Processing (SLP) and to explore future research opportunities, also some
   potential research problems. This paper will be helpful for the
   researchers in order to find all the information regarding SLP at one
   place in a unique way.}},
  book-group-author = {{IEEE}},
  isbn              = {{978-1-5090-3310-2}},
  unique-id         = {{ISI:000405903400040}},
}

@InProceedings{ISI:000464102900048,
  author       = {Zhai, Yujia and Liu, Lizhen and Song, Wei and Du, Chao and Zhao, Xinlei},
  title        = {{The Application of Natural Language Processing in Compiler Principle System}},
  booktitle    = {{PROCEEDINGS OF 2017 IEEE INTERNATIONAL CONFERENCE ON PROGRESS IN INFORMATICS AND COMPUTING (PIC 2017)}},
  year         = {{2017}},
  editor       = {{Xiao, L and Wang, Y}},
  series       = {{Proceedings of the IEEE International Conference on Progress in Informatics and Computing}},
  pages        = {{245-248}},
  organization = {{IEEE; Nanjing Univ Sci \& Technol; Shanghai Univ Finance \& Econ; IEEE Beijing Sect}},
  note         = {{5th IEEE International Conference on Progress in Informatics and Computing (PIC), Nanjing, PEOPLES R CHINA, DEC 15-17, 2017}},
  abstract     = {{Compiling principle is an important course of computer science major,
   which mainly introduces general principles and basic methods of the
   construction of compiling programs mainly. Due to high demands of the
   logic analysis ability, the course bring abstract and unintelligible
   experience to many students. Thus it is quite difficult for students to
   master the main points of this course within the limited class time.
   Based on the requirement above, this paper mainly proposed a method of
   making use of natural language processing in the research and
   application of compiling process, which utilizes Maximum Probability
   Word Segmentation algorithm during the process of lexical analysis and
   syntax analysis, to offer more effective interface between human and
   computer. The proposed method can provide students with intuitive and
   profound knowledge concept in the process of learning how to compile,
   makes it easier and quicker for students to understand the principle of
   computer compiling.}},
  isbn         = {{978-1-5386-1978-0}},
  issn         = {{2474-0209}},
  unique-id    = {{ISI:000464102900048}},
}

@Article{ISI:000391439200001,
  author         = {Tursun, Eziz and Ganguly, Debasis and Osman, Turghun and Yang, Ya-Ting and Abdukerim, Ghalip and Zhou, Jun-Lin and Liu, Qun},
  title          = {{A Semisupervised Tag-Transition-Based Markovian Model for Uyghur Morphology Analysis}},
  journal        = {{ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING}},
  year           = {{2016}},
  volume         = {{16}},
  number         = {{2}},
  month          = {{DEC}},
  issn           = {{2375-4699}},
  abstract       = {{Morphological analysis, which includes analysis of part-of-speech (POS)
   tagging, stemming, and morpheme segmentation, is one of the key
   components in natural language processing (NLP), particularly for
   agglutinative languages. In this article, we investigate the
   morphological analysis of the Uyghur language, which is the native
   language of the people in the Xinjiang Uyghur autonomous region of
   western China. Morphological analysis of Uyghur is challenging primarily
   because of factors such as (1) ambiguities arising due to the likelihood
   of association of a multiple number of POS tags with a word stem or a
   multiple number of functional tags with a word suffix, (2) ambiguous
   morpheme boundaries, and (3) complex morphopholonogy of the language.
   Further, the unavailability of a manually annotated training set in the
   Uyghur language for the purpose of word segmentation makes Uyghur
   morphological analysis more difficult. In our proposed work, we address
   these challenges by undertaking a semisupervised approach of learning a
   Markov model with the help of a manually constructed dictionary of
   ``suffix to tag{''} mappings in order to predict the most likely tag
   transitions in the Uyghur morpheme sequence. Due to the linguistic
   characteristics of Uyghur, we incorporate a prior belief in our model
   for favoring word segmentations with a lower number of morpheme units.
   Empirical evaluation of our proposed model shows an accuracy of about
   82\%. We further improve the effectiveness of the tag transition model
   with an active learning paradigm. In particular, we manually
   investigated a subset of words for which the model prediction ambiguity
   was within the top 20\%. Manually incorporating rules to handle these
   erroneous cases resulted in an overall accuracy of 93.81\%.}},
  article-number = {{8}},
  doi            = {{10.1145/2968410}},
  eissn          = {{2375-4702}},
  unique-id      = {{ISI:000391439200001}},
}

@InProceedings{ISI:000385793200026,
  author       = {Khan, Irfan Ajmal and Choi, Jin-Tak},
  title        = {{LEXICON-CORPUS BASED KOREAN UNKNOWN FOREIGN WORD EXTRACTION AND UPDATING USING SYLLABLE IDENTIFICATION}},
  booktitle    = {{12TH INTERNATIONAL CONFERENCE ON HYDROINFORMATICS (HIC 2016) - SMART WATER FOR THE FUTURE}},
  year         = {{2016}},
  editor       = {{Kim, JH and Kim, HS and Yoo, DG and Jung, D and Song, CG}},
  volume       = {{154}},
  series       = {{Procedia Engineering}},
  pages        = {{192-198}},
  organization = {{Incheon Metropolitan Govt; Korea Tourism Org; Smart Water Grid Res Grp}},
  note         = {{12th International Conference on Hydroinformatics (HIC) - Smart Water for the Future, SOUTH KOREA, AUG 21-26, 2016}},
  abstract     = {{This paper presents an efficient text mining method focusing on
   extraction and updating of unknown words (unknown foreign words) to
   improve data classification and POS tags. Proposed methods can also help
   to improve the accuracy of mining frequent pattern and association rules
   from unstructured (textual) data. Many researches have been done by
   numerous scholars on estimation and segmentation for unknown words, but,
   they are limited to grammatical and linguistic rules with limited
   vocabulary. In our project we have consider the fact, that no language
   is free from the influence of foreign languages, especially, country
   like Korea where there is a rapid improvement in the area of culture and
   media and the frequent usage of these foreign languages, resulted in
   mixing up different languages, their style along with slangs and also
   abbreviated words in daily life and conversation. The main
   characteristic of our system is to find such unknown foreign words and
   update them to appropriate words, which depends on available information
   through dictionaries. We have also explained the essential natural
   language processing (NLP) tools used for data processing. Our proposed
   method used simple but efficient techniques, first it converts the data
   into structured form, using data preprocessing techniques. In this phase
   data passes through different stages, such as, cleaning, integration and
   selection of important data, and then it gets organized into databases
   structure for further analysis and processing. This database consists of
   different kinds of dictionaries, our system heavily based on
   dictionaries. We have manually created various kinds of dictionaries for
   different kinds of unknown foreign words processing and analysis with
   the help of our team members. Our proposed methods for discovering and
   updating foreign unknown word, first discovers the foreign word using
   morphological analysis with the help of automatically and manually
   created dictionaries, then suffix trimming and word segmentation, next
   our algorithm checks for its different written pattern using
   dictionaries according to its spelling and synonym word in native
   language (Korean) and also, updates the POS tags. We have tested on
   different collection of data from economics news, beauty \& fashion and
   college student blogs, the results have shown great efficiency and
   improvement, and they were adequate enough to research further. (C) 2016
   Published by Elsevier Ltd.}},
  doi          = {{10.1016/j.proeng.2016.07.445}},
  issn         = {{1877-7058}},
  unique-id    = {{ISI:000385793200026}},
}

@InProceedings{ISI:000400282203053,
  author       = {Ye Zhonglin and Jia Zhen and Huang Junfu and Yin Hongfeng},
  title        = {{Part-of-speech Tagging Based on Dictionary and Statistical Machine Learning}},
  booktitle    = {{PROCEEDINGS OF THE 35TH CHINESE CONTROL CONFERENCE 2016}},
  year         = {{2016}},
  editor       = {{Chen, J and Zhao, Q}},
  series       = {{Chinese Control Conference}},
  pages        = {{6993-6998}},
  organization = {{Chinese Assoc Automat, Tech Comm Control Theory; Syst Engn Soc China; SW Jiaotong Univ; Chinese Acad Sci, Acad Math \& Syst Sci; China Soc Indu \& Appl Math; Univ Elect Sci \& Technol China; Sichuan Univ; Asian Control Assocn; IEEE Control Syst Soc; Inst Control Robot \& Syst; SocInstrument \& Control Engineers; Sichuan Soc Automat \& Instrument}},
  note         = {{35th Chinese Control Conference (CCC), Chengdu, PEOPLES R CHINA, JUL 27-29, 2016}},
  abstract     = {{Part-of-speech tagging is the basis of Natural Language Processing, and
   is widely used in information retrieval, text processing and machine
   translation fields. The traditional statistical machine learning methods
   of POS tagging rely on the high quality training data, but obtaining the
   training data is very time-consuming. The methods of POS tagging based
   on dictionaries ignore the context information, which lead to lower
   performance. This paper proposed a POS tagging approach which combines
   methods based on dictionaries and traditional statistical machine
   learning. The experimental results show that the approach not only can
   solve the problem that the training data are insufficient in statistical
   methods, but also can improve the performance of the methods based on
   dictionaries. The People's Daily corpus in January 1998 is used as
   testing data, and the accurate rate of POS tagging achieves 95.80\%. For
   the ambiguity word POS tagging, the accuracy achieves 88\%.}},
  isbn         = {{978-9-8815-6391-0}},
  issn         = {{2161-2927}},
  unique-id    = {{ISI:000400282203053}},
}

@InProceedings{ISI:000466782100100,
  author       = {Cong, Xiaoyue and Li, Lei},
  title        = {{UGC QUALITY EVALUATION BASED ON META-LEARNING AND CONTENT FEATURE ANALYSIS}},
  booktitle    = {{PROCEEDINGS OF 2016 5TH IEEE INTERNATIONAL CONFERENCE ON NETWORK INFRASTRUCTURE AND DIGITAL CONTENT (IEEE IC-NIDC 2016)}},
  year         = {{2016}},
  editor       = {{Guo, J and Yang, J and Wang, W and Zhang, L and Gao, S and Ma, Z and Lu, J and Liu, Z}},
  series       = {{IEEE International Conference on Network Infrastructure and Digital Content}},
  pages        = {{495-499}},
  organization = {{IEEE; IEEE Beijing Sect; Advanced Intelligence \& Network Serv 111 Project China; Tohoku Univ, Global Ctr Excellence; Hanyang Univ, Fus IT Educ Future Innovat Leaders, BK21 Plus Program; Norwegian Univ Sci \& Technol; Aalborg Univ, Ctr Teleinfrastruktur; Chinese Assoc Artificial Intelligence; Hanyang Univ, Creat Educ Program Software, BK21 Plus Program; Beijing Nat Sci Fdn; Inst Engn \& Technol; Hanyang Univ}},
  note         = {{5th IEEE International Conference on Network Infrastructure and Digital Content (IC-NIDC), Beijing Univ Posts \& Telecommunicat, Beijing, PEOPLES R CHINA, SEP 23-25, 2016}},
  abstract     = {{With the fast development of Social Networking Services, there has been
   increasingly vast amount of infonnation published by massive network
   users. Given this information explosion, how to analyze the quality of
   User Generated Contents (UGC) automatically becomes a challenging task
   for researchers. To solve the problem, we need to build an effective UGC
   quality evaluation system. In the light of our experience, we believe
   that the textual content of UGC is the key factor for its quality.
   Hence, we focus on textual content based quality evaluation and
   classification instead of using UGC publishing related data, such as
   times being commented and forwarded in this paper. We extract various
   features of the textual contents based on natural language processing
   technologies firstly, such as word segmentation, keywords, topic model,
   sentence parsing, distributed word representation etc. Secondly, we
   build several base-learning classifiers with different features and
   different machine learning algorithms to assign UGC contents with four
   different quality labels. Then, we create the global meta-learning model
   based on these base classifiers to generate the final quality labels for
   UGC contents. We have also implemented a series of experiments based on
   realistic data collected from Tianya Forum and use 10-fold
   cross-validation to test the model. Results have shown that our proposed
   meta-learning model performs much better.}},
  isbn         = {{978-1-5090-1246-6}},
  issn         = {{2374-0272}},
  unique-id    = {{ISI:000466782100100}},
}

@InProceedings{ISI:000393314500026,
  author            = {Zhang, Chunxiang and He, Shan and Gao, Xueyao},
  title             = {{A Word Sense Disambiguation System Based on Bayesian Model}},
  booktitle         = {{PROCEEDINGS OF 2015 4TH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND NETWORK TECHNOLOGY (ICCSNT 2015)}},
  year              = {{2015}},
  pages             = {{124-127}},
  organization      = {{Heilongjiang Univ; Dalian Jiaotong Univ; NE Normal Univ; Shaanxi Normal Univ; Harbin Inst Technol; IEEE; IEEE Harbin Sect}},
  note              = {{4th International Conference on Computer Science and Network Technology (ICCSNT), Harbin, PEOPLES R CHINA, DEC 19-20, 2015}},
  abstract          = {{Research on word sense disambiguation (WSD) is of great importance in
   natural language processing fields. In this paper, a novel word sense
   disambiguation system is designed in which bayesian theory is applied to
   determine correct sense of an ambiguous word. Morphology knowledge in
   word unit is mined to guide WSD process. Neighboring morphology
   knowledge of an ambiguous word is used as feature for constructing WSD
   classifier. Word segmentation tool is integrated into this system and
   browser/server (B/S) framework is adopted. Experimental results show
   that the performance of WSD system is good.}},
  book-group-author = {{IEEE}},
  isbn              = {{978-1-4673-8173-4}},
  unique-id         = {{ISI:000393314500026}},
}

@Article{ISI:000368651300005,
  author    = {Khan, Irfan Ajmal and Choi, Jin-Tak},
  title     = {{Efficient text mining method and simple tweaks for discovering and updating unknown foreign words and improving association rules extraction from textual data}},
  journal   = {{ASIA LIFE SCIENCES}},
  year      = {{2015}},
  number    = {{12}},
  pages     = {{71-88}},
  month     = {{DEC}},
  issn      = {{0117-3375}},
  abstract  = {{A text mining method for discovering hidden knowledge from unstructured
   (textual) data and also extraction of frequent patterns, association
   rules and unknown foreign words are presented in this study. Association
   rule mining is an important data mining model studied widely by the
   database and data mining community. Although various association rules
   mining techniques have been successfully used for market basket analysis
   but very few has applied on textual data. In this paper we have explains
   the method of mining association rules on Korean textual data. We have
   also explained the essential natural language processing (NLP) tools
   used. First we have converted unstructured data into something
   structured by passing textual data through Data Preprocessing stage.
   This process cleans and integrates data, select relevant data then
   transforms into database along with useful information which can help
   algorithm improve the mining process. These database(s) consists of
   different kinds of dictionaries. Second we have created different types
   of dictionaries for different processing stages. Our foreign unknown
   word discovering and updating method first discover the foreign word
   using morphological analysis and unknown words and foreign unknown word
   dictionaries. Next is suffix trimming and word segmentation, then these
   foreign words were fed to a process, where it looks for its different
   written pattern using dictionaries according to its spelling and synonym
   word in native language (Korean). Next step is to update the POS tags
   using rule base POS tagging. Then data mining techniques are used to
   extract hidden patterns. These patterns are evaluated by specific rules
   until we get the valid and satisfactory result. We have tested on Korean
   news corpus and results have shown that it has worked well, and the
   results were adequate enough to further research.}},
  unique-id = {{ISI:000368651300005}},
}

@Article{ISI:000368651300052,
  author    = {Khan, Irfan Ajmal and Seo, Ji-Hoon and Choi, Jin-Tak},
  title     = {{Efficient text mining method and simple tweaks for discovering and updating unknown foreign words and improving association rules extraction from textual data}},
  journal   = {{ASIA LIFE SCIENCES}},
  year      = {{2015}},
  number    = {{12}},
  pages     = {{663-680}},
  month     = {{DEC}},
  issn      = {{0117-3375}},
  abstract  = {{We present a text mining method for discovering hidden knowledge from
   unstructured (textual) data and also extraction of frequent patterns,
   association rules and unknown foreign words. Association rule mining is
   an important data mining model studied widely by the database and data
   mining community. Although various association rules mining techniques
   have been successfully used for market basket analysis but very few has
   applied on textual data. In this paper we have explains the method of
   mining association rules on Korean textual data. We have also explained
   the essential natural language processing (NLP) tools used. First we
   have converted unstructured data into something structured by passing
   textual data through Data Preprocessing stage. This process cleans and
   integrates data, select relevant data then transforms into database
   along with useful information which can help algorithm improve the
   mining process. These database(s) consists of different kinds of
   dictionaries. Second we have created different types of dictionaries for
   different processing stages. Our foreign unknown word discovering and
   updating method first discover the foreign word using morphological
   analysis and unknown words and foreign unknown word dictionaries. Next
   is suffix trimming and word segmentation, then these foreign words were
   fed to a process, where it looks for its different written pattern using
   dictionaries according to its spelling and synonym word in native
   language (Korean). Next step is to update the POS tags using rule base
   POS tagging. Then data mining techniques are used to extract hidden
   patterns. These patterns are evaluated by specific rules until we get
   the valid and satisfactory result. We have tested on Korean news corpus
   and results have shown that it has worked well, and the results were
   adequate enough to research further.}},
  unique-id = {{ISI:000368651300052}},
}

@InProceedings{ISI:000380557800109,
  author            = {Puri, Shalini and Singh, Satya Prakash},
  title             = {{Sentence Detection and Extraction in Machine Printed Imaged Document using Matching Technique}},
  booktitle         = {{2015 2nd International Conference on Recent Advances in Engineering \& Computational Sciences (RAECS)}},
  year              = {{2015}},
  note              = {{2nd International Conference on Recent Advances in Engineering, Chandigarh, INDIA, DEC 21-22, 2015}},
  abstract          = {{Sentence extraction is a new, challenging and critical step in the
   printed scanned imaged documents. In this paper, an efficient 4-layered
   Sentence Detection and Extraction System (SDES) model is proposed which
   is designed to detect and extract sentences from machine printed imaged
   document. Its internal details and architecture clearly show that how it
   processes an image to find out the underlying sentences. The basic idea
   is to first preprocess the imaged document for noise removal and skew
   correction, and then textual entities are detected and segmented at
   page, line and word levels. Firstly, the horizontal and vertical
   projection profiles are taken to segment and separate the lines and
   words. After skew correction, two stage Character Based and Word Based
   Leveled matching and testing are performed, which verify and identify
   the correct character and word by searching for similar textual
   characters and words in Character Set Storage (CSS) and Word Pseudo
   Thesaurus (WPT). If any word pattern is not matched and identified by
   WPT, then it is stored in the Unmatched Word Storage (UWS) for the
   future reference. Such testing and verification are used at two levels
   to increase the accuracy\% of SDES, and thereby, reducing the errors. It
   increases the system performance greatly. Finally, all the sentences of
   imaged document are extracted. Experimental results are found at the
   word, character and sentence levels. Their accuracy\% results are good
   which show the high system performance and efficiency.}},
  book-group-author = {{IEEE}},
  isbn              = {{978-1-4673-8253-3}},
  unique-id         = {{ISI:000380557800109}},
}

@InProceedings{ISI:000362441400001,
  author    = {Nivre, Joakim},
  title     = {{Towards a Universal Grammar for Natural Language Processing}},
  booktitle = {{COMPUTATIONAL LINGUISTICS AND INTELLIGENT TEXT PROCESSING (CICLING 2015), PT I}},
  year      = {{2015}},
  editor    = {{Gelbukh, A}},
  volume    = {{9041}},
  series    = {{Lecture Notes in Computer Science}},
  pages     = {{3-16}},
  note      = {{16th Annual Conference on Intelligent Text Processing and Computational Linguistics (CICLing), Nile Univ, Cairo, EGYPT, APR 14-20, 2015}},
  abstract  = {{Universal Dependencies is a recent initiative to develop
   cross-linguistically consistent treebank annotation for many languages,
   with the goal of facilitating multilingual parser development,
   cross-lingual learning, and parsing research from a language typology
   perspective. In this paper, I outline the motivation behind the
   initiative and explain how the basic design principles follow from these
   requirements. I then discuss the different components of the annotation
   standard, including principles for word segmentation, morphological
   annotation, and syntactic annotation. I conclude with some thoughts on
   the challenges that lie ahead.}},
  doi       = {{10.1007/978-3-319-18111-0\_1}},
  eissn     = {{1611-3349}},
  isbn      = {{978-3-319-18111-0; 978-3-319-18110-3}},
  issn      = {{0302-9743}},
  unique-id = {{ISI:000362441400001}},
}

@InProceedings{ISI:000355611002026,
  author        = {Ludusan, Bogdan and Versteegh, Maarten and Jansen, Aren and Gravier, Guillaume and Cao, Xuan-Nga and Johnson, Mark and Dupoux, Emmanuel},
  title         = {{Bridging the gap between speech technology and natural language processing: an evaluation toolbox for term discovery systems}},
  booktitle     = {{LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION}},
  year          = {{2014}},
  editor        = {{Calzolari, N and Choukri, K and Declerck, T and Loftsson, H and Maegaard, B and Mariani, J and Moreno, A and Odijk, J and Piperidis, S}},
  pages         = {{560-567}},
  organization  = {{Holmes Semant Solut; European Media Lab GmBH; EML; VoiceBox Technologies; KDICTIONARIES}},
  note          = {{9th International Conference on Language Resources and Evaluation (LREC), Reykjavik, ICELAND, MAY 26-31, 2014}},
  abstract      = {{The unsupervised discovery of linguistic terms from either continuous
   phoneme transcriptions or from raw speech has seen an increasing
   interest in the past years both from a theoretical and a practical
   standpoint. Yet, there exists no common accepted evaluation method for
   the systems performing term discovery. Here, we propose such an
   evaluation toolbox, drawing ideas from both speech technology and
   natural language processing. We first transform the speech-based output
   into a symbolic representation and compute five types of evaluation
   metrics on this representation: the quality of acoustic matching, the
   quality of the clusters found, and the quality of the alignment with
   real words (type, token, and boundary scores). We tested our approach on
   two term discovery systems taking speech as input, and one using
   symbolic input. The latter was run using both the gold transcription and
   a transcription obtained from an automatic speech recognizer, in order
   to simulate the case when only imperfect symbolic information is
   available. The results obtained are analysed through the use of the
   proposed evaluation metrics and the implications of these metrics are
   discussed.}},
  isbn          = {{978-2-9517408-8-4}},
  orcid-numbers = {{Dupoux, Emmanuel/0000-0002-7814-2952}},
  unique-id     = {{ISI:000355611002026}},
}

@Article{ISI:000341843500004,
  author        = {Sun, Xu and Li, Wenjie and Wang, Houfeng and Lu, Qin},
  title         = {{Feature-Frequency-Adaptive On-line Training for Fast and Accurate Natural Language Processing}},
  journal       = {{COMPUTATIONAL LINGUISTICS}},
  year          = {{2014}},
  volume        = {{40}},
  number        = {{3}},
  pages         = {{563-586}},
  month         = {{SEP}},
  issn          = {{0891-2017}},
  abstract      = {{Training speed and accuracy are two major concerns of large-scale
   natural language processing systems. Typically, we need to make a
   tradeoff between speed and accuracy. It is trivial to improve the
   training speed via sacrificing accuracy or to improve the accuracy via
   sacrificing speed. Nevertheless, it is nontrivial to improve the
   training speed and the accuracy at the same time, which is the target of
   this work. To reach this target, we present a new training method,
   feature-frequency-adaptive on-line training, for fast and accurate
   training of natural language processing systems. It is based on the core
   idea that higher frequency features should have a learning rate that
   decays faster. Theoretical analysis shows that the proposed method is
   convergent with a fast convergence rate. Experiments are conducted based
   on well-known benchmark tasks, including named entity recognition, word
   segmentation, phrase chunking, and sentiment analysis. These tasks
   consist of three structured classification tasks and one non-structured
   classification task, with binary features and real-valued features,
   respectively. Experimental results demonstrate that the proposed method
   is faster and at the same time more accurate than existing methods,
   achieving state-of-the-art scores on the tasks with different
   characteristics.}},
  doi           = {{10.1162/COLI\_a\_00193}},
  eissn         = {{1530-9312}},
  orcid-numbers = {{Li, Wenjie/0000-0002-7360-8864 Lu, Qin/0000-0002-9092-2476}},
  unique-id     = {{ISI:000341843500004}},
}

@InProceedings{ISI:000326049800121,
  author            = {Srithirath, Arounyadeth and Seresangtakul, Pusadee},
  title             = {{A Hybrid Approach to Lao Word Segmentation using Longest Syllable Level Matching with Named Entities Recognition}},
  booktitle         = {{2013 10TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING/ELECTRONICS, COMPUTER, TELECOMMUNICATIONS AND INFORMATION TECHNOLOGY (ECTI-CON)}},
  year              = {{2013}},
  organization      = {{IEEE; IEEE Thailand Sect}},
  note              = {{10th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON), Krabi, THAILAND, MAY 15-17, 2013}},
  abstract          = {{The Lao language is written without words delimiter which makes it
   extremely difficult to process. The development of automatic word
   segmentation for natural language processing for the Lao language is an
   essential but challenging task. This paper proposes a longest syllable
   level match with named entities recognition approach for Lao word
   segmentation. Syllables were first extracted from the input text and
   then longest matching was applied. This is one of the techniques in the
   Dictionary Based approach with named entities recognition being used to
   combine them to form the words. The performance result obtained from
   this approach, in precision and recall, was 85.21\% and 92.36\%,
   respectively.}},
  book-group-author = {{IEEE}},
  isbn              = {{978-1-4799-0546-1; 978-1-4799-0545-4}},
  unique-id         = {{ISI:000326049800121}},
}

@InProceedings{ISI:000329078800006,
  author       = {Namer, Fiammetta},
  title        = {{A Rule-Based Morphosemantic Analyzer for French for a Fine-Grained Semantic Annotation of Texts}},
  booktitle    = {{SYSTEMS AND FRAMEWORKS FOR COMPUTATIONAL MORPHOLOGY}},
  year         = {{2013}},
  editor       = {{Mahlow, C and Piotrowski, M}},
  volume       = {{380}},
  series       = {{Communications in Computer and Information Science}},
  pages        = {{92-114}},
  organization = {{German Soc Computat Linguist \& Language Technol; Humboldt Univ}},
  note         = {{3rd International Workshop on Systems and Frameworks for Computational Morphology, Humboldt Univ, Berlin, GERMANY, SEP 06, 2013}},
  abstract     = {{We describe DeriF, a rule-based morphosemantic analyzer developed for
   French. Unlike existing word segmentation tools, DeriF provides derived
   and compound words with various sorts of semantic information: (1) a
   definition, computed from both the base meaning and the specificities of
   the morphological rule; (2) lexical-semantic features, inferred from
   general linguistic properties of derivation rules; (3) lexical relations
   (synonymy, (co-)hyponymy) with other, morphologically unrelated, words
   belonging to the same analyzed corpus.}},
  eissn        = {{1865-0937}},
  isbn         = {{978-3-642-40485-6}},
  issn         = {{1865-0929}},
  unique-id    = {{ISI:000329078800006}},
}

@InProceedings{ISI:000336944701290,
  author       = {Wang, Na and Xu, Lin and Li, Liyao and Xu, Luxiong},
  title        = {{Design And Implementation of an Automatic Scoring Subjective Question System Based on Domain Ontology}},
  booktitle    = {{MATERIALS PROCESSING AND MANUFACTURING III, PTS 1-4}},
  year         = {{2013}},
  editor       = {{Sang, X and Kim, YH}},
  volume       = {{753-755}},
  series       = {{Advanced Materials Research}},
  pages        = {{3039-3042}},
  organization = {{Hebei Prov Key Lab Inorgan Nonmetall Mat; Korea Maritime Univ}},
  note         = {{3rd International Conference on Advanced Engineering Materials and Technology (AEMT 2013), Zhangjiajie, PEOPLES R CHINA, MAY 11-12, 2013}},
  abstract     = {{Automated assessment technology for subjective tests is one of the key
   techniques of exam systems. A model based on domain ontology is proposed
   in this paper, which can be used in exam systems to estimate subjective
   tests. After analysing the present research status of subjective
   automated assessment technology, the paper makes a study on the
   construction method of domain ontology by taking software engineering
   domain as an example. Semantic similarity calculation based on domain
   ontology is used for automatic assessment in this paper. The automatic
   assessment system can divide a sentence into a series of phrases by
   using the natural language processing technology and get the score by
   evaluating the semantic similarity of the student's answer. The
   experiments show that the results of the system which has certain
   valuable feasibility and applicability are credible and the scoring
   errors are acceptable.}},
  doi          = {{10.4028/www.scientific.net/AMR.753-755.3039}},
  isbn         = {{978-3-03785-764-9}},
  issn         = {{1022-6680}},
  unique-id    = {{ISI:000336944701290}},
}

@Article{ISI:000315648000007,
  author        = {Goldberg, Yoav and Elhadad, Michael},
  title         = {{Word Segmentation, Unknown-word Resolution, and Morphological Agreement in a Hebrew Parsing System}},
  journal       = {{COMPUTATIONAL LINGUISTICS}},
  year          = {{2013}},
  volume        = {{39}},
  number        = {{1}},
  pages         = {{121-160}},
  month         = {{MAR}},
  issn          = {{0891-2017}},
  abstract      = {{We present a constituency parsing system for Modern Hebrew. The system
   is based on the PCFG-LA parsing method of Petrov et al. (2006), which is
   extended in various ways in order to accommodate the specificities of
   Hebrew as a morphologically rich language with a small treebank. We show
   that parsing performance can be enhanced by utilizing a language
   resource external to the treebank, specifically, a lexicon-based
   morphological analyzer. We present a computational model of interfacing
   the external lexicon and a treebank-based parser, also in the common
   case where the lexicon and the treebank follow different annotation
   schemes. We show that Hebrew word-segmentation and constituency-parsing
   can be performed jointly using CKY lattice parsing. Performing the tasks
   jointly is effective, and substantially outperforms a pipeline-based
   model. We suggest modeling grammatical agreement in a constituency-based
   parser as a filter mechanism that is orthogonal to the grammar, and
   present a concrete implementation of the method. Although the
   constituency parser does not make many agreement mistakes to begin with,
   the filter mechanism is effective in fixing the agreement mistakes that
   the parser does make.
   These contributions extend outside of the scope of Hebrew processing,
   and are of general applicability to the NLP community. Hebrew is a
   specific case of a morphologically rich language, and ideas presented in
   this work are useful also for processing other languages, including
   English. The lattice-based parsing methodology is useful in any case
   where the input is uncertain. Extending the lexical coverage of a
   treebank-derived parser using an external lexicon is relevant for any
   language with a small treebank.}},
  doi           = {{10.1162/COLI\_a\_00137}},
  eissn         = {{1530-9312}},
  orcid-numbers = {{Elhadad, Michael/0000-0002-5629-2351}},
  unique-id     = {{ISI:000315648000007}},
}

@InProceedings{ISI:000306397600022,
  author    = {Lehal, Gurpreet Singh and Saini, Tejinder Singh},
  title     = {{A Transliteration Based Word Segmentation System for Shahmukhi Script}},
  booktitle = {{INFORMATION SYSTEMS FOR INDIAN LANGUAGES}},
  year      = {{2011}},
  editor    = {{Singh, C and Lehal, GS and Sengupta, J and Sharma, DV and Goyal, V}},
  volume    = {{139}},
  series    = {{Communications in Computer and Information Science}},
  pages     = {{136-143}},
  note      = {{International Conference on Infirmation Systems for Indian Languages (ICISIL 2011), Patiala, INDIA, MAR 09-11, 2011}},
  abstract  = {{Word Segmentation is an important prerequisite for almost all Natural
   Language Processing (NLP) applications. Since word is a fundamental unit
   of any language, almost every NLP system first needs to segment input
   text into a sequence of words before further processing. In this paper,
   Shahmukhi word segmentation has been discussed in detail. The presented
   word segmentation module is part of Shahmukhi-Gurmukhi transliteration
   system. Shahmukhi script is usually written without short vowels leading
   to ambiguity. Therefore, we have designed a novel approach for Shahmukhi
   word segmentation in which we used target Gurmukhi script lexical
   resources instead of Shahmukhi resources. We employ a combination of
   techniques to investigate an effective algorithm by applying syntactical
   analysis process using Shahmukhi Gurmukhi dictionary, writing system
   rules and statistical methods based on n-grams models.}},
  isbn      = {{978-3-642-19402-3}},
  issn      = {{1865-0929}},
  unique-id = {{ISI:000306397600022}},
}

@Article{ISI:000290801100004,
  author    = {Kesidis, A. L. and Galiotou, E. and Gatos, B. and Pratikakis, I.},
  title     = {{A word spotting framework for historical machine-printed documents}},
  journal   = {{INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION}},
  year      = {{2011}},
  volume    = {{14}},
  number    = {{2, SI}},
  pages     = {{131-144}},
  month     = {{JUN}},
  issn      = {{1433-2833}},
  abstract  = {{In this paper, we propose a word spotting framework for accessing the
   content of historical machine-printed documents without the use of an
   optical character recognition engine. A preprocessing step is performed
   in order to improve the quality of the document images, while word
   segmentation is accomplished with the use of two complementary
   segmentation methodologies. In the proposed methodology, synthetic word
   images are created from keywords, and these images are compared to all
   the words in the digitized documents. A user feedback process is used in
   order to refine the search procedure. The methodology has been evaluated
   in early Modern Greek documents printed during the seventeenth and
   eighteenth century. In order to improve the efficiency of accessing and
   search, natural language processing techniques have been addressed that
   comprise a morphological generator that enables searching in documents
   using only a base word-form for locating all the corresponding inflected
   word-forms and a synonym dictionary that further facilitates access to
   the semantic context of documents.}},
  doi       = {{10.1007/s10032-010-0134-4}},
  eissn     = {{1433-2825}},
  unique-id = {{ISI:000290801100004}},
}

@InProceedings{ISI:000257568400001,
  author       = {Xiao, Guozheng},
  title        = {{Constructing verb synsets for language reasoning based on synset-allolexeme theory}},
  booktitle    = {{RECENT ADVANCE OF CHINESE COMPUTING TECHNOLOGIES}},
  year         = {{2007}},
  editor       = {{He, YX and Xiao, GZ and Sun, MS}},
  pages        = {{3-10}},
  organization = {{Wuhan Univ, Ctr Study Language \& Informat; Chinese \& Oriental Language Informat Proc Soc Singapore; Soc Chinese Informat Proc China}},
  note         = {{7th International Conference of Chinese Computing (ICCC 2007), Wuhan, PEOPLES R CHINA, OCT 13-15, 2007}},
  abstract     = {{Natural language processing is marching toward a new goal: automatic
   semantic analysis, after word segmentation, part-of-speech tagging and
   syntactic analysis have been almost achieved. Separating and describing
   verb senses and constructing synsets are important tasks in reaching the
   goal. This paper introduces a new theory in constructing synsets:
   synset-allolexeme theory as well as corresponding syntactic-semantic
   component analysis by depicting semantic lexemes and semantic varieties
   of the original meaning of verbs.}},
  isbn         = {{978-981-08-0099-4}},
  unique-id    = {{ISI:000257568400001}},
}

@Comment{jabref-meta: databaseType:bibtex;}
