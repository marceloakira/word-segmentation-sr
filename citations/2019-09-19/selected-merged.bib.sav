% Encoding: UTF-8

@Article{shishkova_annotated_nodate,
  author = {Shishkova, Anna and Chernyak, Ekaterina},
  title  = {Annotated {Suffix} {Tree} {Method} for {German} {Compound} {Splitting}},
  note   = {08},
  file   = {Shishkova_Chernyak_Annotated Suffix Tree Method for German Compound Splitting.pdf:/home/akira/Zotero/storage/V2LHAIVT/Shishkova_Chernyak_Annotated Suffix Tree Method for German Compound Splitting.pdf:application/pdf},
}

@Article{ullman_paraphrasing_nodate,
  author = {Ullman, Edvin},
  title  = {Paraphrasing of {Swedish} {Compound} {Nouns}},
  note   = {52},
  annote = {Artigo tem um complemento (in Machine Learning) e mais um autor..  },
  file   = {Ullman - Paraphrasing of Swedish Compound Nouns.pdf:/home/akira/Zotero/storage/GZFEHG35/Ullman - Paraphrasing of Swedish Compound Nouns.pdf:application/pdf},
}

@Article{patel_machine_2019,
  author   = {Patel, R.N. and Pimpale, P.B. and Sasikumar, M.},
  title    = {Machine translation in {Indian} languages: {Challenges} and resolution},
  journal  = {Journal of Intelligent Systems},
  year     = {2019},
  volume   = {28},
  number   = {3},
  pages    = {437--445},
  note     = {44},
  abstract = {English to Indian language machine translation poses the challenge of structural and morphological divergence. This paper describes English to Indian language statistical machine translation using preordering and suffix separation. The preordering uses rules to transfer the structure of the source sentences prior to training and translation. This syntactic restructuring helps statistical machine translation to tackle the structural divergence and hence provides better translation quality. The suffix separation is used to tackle the morphological divergence between English and highly agglutinative Indian languages. We demonstrate that the use of preordering and suffix separation helps in improving the quality of English to Indian language machine translation. © 2019 Walter de Gruyter GmbH, Berlin/Boston.},
  annote   = {cited By 1},
  doi      = {10.1515/jisys-2018-0014},
  file     = {Patel et al. - 2019 - Machine translation in Indian languages Challenge.pdf:/home/akira/Zotero/storage/QTIAWNTP/Patel et al. - 2019 - Machine translation in Indian languages Challenge.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053124049&doi=10.1515%2fjisys-2018-0014&partnerID=40&md5=5b1e11decdb5614448ceda0e259a43a4},
}

@Article{altinok_demorphy_2018,
  author  = {Altinok, Duygu},
  title   = {{DEMorphy}, {German} {Language} {Morphological} {Analyzer}},
  journal = {arXiv preprint arXiv:1803.00902},
  year    = {2018},
  note    = {23},
  annote  = {https://github.com/DuyguA/DEMorphy},
  file    = {Altinok - 2018 - DEMorphy, German Language Morphological Analyzer.pdf:/home/akira/Zotero/storage/BY8AMSZI/Altinok - 2018 - DEMorphy, German Language Morphological Analyzer.pdf:application/pdf},
}

@InProceedings{sugisaki_german_2018,
  author    = {Sugisaki, K. and Tuggener, D.},
  title     = {German compound splitting using the compound productivity of morphemes},
  booktitle = {{KONVENS} 2018 - {Conference} on {Natural} {Language} {Processing} / {Die} {Konferenz} zur {Verarbeitung} {Naturlicher} {Sprache}},
  year      = {2018},
  pages     = {141--147},
  note      = {32},
  abstract  = {In this work, we present a novel compound splitting method for German by capturing the compound productivity of morphemes. We use a giga web corpus to create a lexicon and decompose noun compounds by computing the probabilities of compound elements as bound and free morphemes. Furthermore, we provide a uniformed evaluation of several unsupervised approaches and morphological analysers for the task. Our method achieved a high F1 score of 0.92, which was a comparable result to state-of-the-art methods. © KONVENS 2018 - Conference on Natural Language Processing / Die Konferenz zur Verarbeitung Naturlicher Sprache.All right reserved.},
  annote    = {cited By 0},
  file      = {Sugisaki and Tuggener - 2018 - German compound splitting using the compound produ.pdf:/home/akira/Zotero/storage/9DH3QEFK/Sugisaki and Tuggener - 2018 - German compound splitting using the compound produ.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064207728&partnerID=40&md5=9ad88e6eff661a660d155b41ada1fbb9},
}

@InProceedings{li_helpful_2018,
  author    = {Li, J. and Du, Q. and Shi, K. and He, Y. and Wang, X. and Xu, J.},
  title     = {Helpful or not? an investigation on the feasibility of identifier splitting via {CNN}-{BiLSTM}-{CRF}},
  booktitle = {Proceedings of the {International} {Conference} on {Software} {Engineering} and {Knowledge} {Engineering}, {SEKE}},
  year      = {2018},
  volume    = {2018-July},
  pages     = {175--181},
  note      = {37},
  abstract  = {We recently introduced a new technique to handle source code identifier splitting. The proposed technique, denoted as CNN-BiLSTM-CRF[a neural network composed of a convolutional neural network(CNN), bidirectional long short-Term memory networks(BiLSTM) and conditional random fields(CRFs)] enables us to obtain a model that splits identifiers correctly and effectively. This technique combines the use of a CNN layer with the mature BiLSTM-CRF model. The experimental results indicate that CNN-BiLSTM-CRF delivers outstanding performance on all four of the evaluation oracles. More importantly, we endeavored to provide insight into the practical feasibility of this technique by considering the aspects of generality, data size in demand and construction cost, etc. Finally, we reasoned out that CNN-BiLSTM-CRF should be helpful and improvable for identifier splitting in practical works in terms of the accuracy and feasibility. This was validated by multifaceted experiments. Index Terms-identifier splitting, source code mining, program comprehension, CNN, BiLSTM-CRF, feasibility investigation. © 2018 Universitat zu Koln. All rights reserved.},
  annote    = {cited By 0},
  doi       = {10.18293/SEKE2018-167},
  file      = {2018_Li et al_Helpful or not.pdf:/home/akira/Zotero/storage/28RJQ7B3/2018_Li et al_Helpful or not.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056829823&doi=10.18293%2fSEKE2018-167&partnerID=40&md5=da7e9e1bd5ebdb234108fb0e7e22f2f9},
}

@InProceedings{pinter_investigating_2018,
  author    = {Pintér, Gábor and Schielke, Mira and Petrick, Rico},
  title     = {Investigating {Word} {Segmentation} {Techniques} for {German} {Using} {Finite}-{State} {Transducers}},
  booktitle = {International {Conference} on {Speech} and {Computer}},
  year      = {2018},
  pages     = {511--521},
  publisher = {Springer},
  note      = {39},
}

@Article{hucka_nostril_2018,
  author  = {Hucka, Michael},
  title   = {Nostril: {A} nonsense string evaluator written in {Python}.},
  journal = {J. Open Source Software},
  year    = {2018},
  volume  = {3},
  number  = {25},
  pages   = {596},
  note    = {49},
  file    = {Hucka - 2018 - Nostril A nonsense string evaluator written in Py.pdf:/home/akira/Zotero/storage/BMFB5JDU/Hucka - 2018 - Nostril A nonsense string evaluator written in Py.pdf:application/pdf},
}

@Article{hucka_spiral_2018,
  author  = {Hucka, Michael},
  title   = {Spiral: splitters for identifiers in source code files.},
  journal = {J. Open Source Software},
  year    = {2018},
  volume  = {3},
  number  = {24},
  pages   = {653},
  note    = {66},
  file    = {Hucka - 2018 - Spiral splitters for identifiers in source code f.pdf:/home/akira/Zotero/storage/99LTJ6MN/Hucka - 2018 - Spiral splitters for identifiers in source code f.pdf:application/pdf},
}

@Article{markovtsev_splitting_2018,
  author  = {Markovtsev, Vadim and Long, Waren and Bulychev, Egor and Keramitas, Romain and Slavnov, Konstantin and Markowski, Gabor},
  title   = {Splitting source code identifiers using {Bidirectional} {LSTM} {Recurrent} {Neural} {Network}},
  journal = {arXiv preprint arXiv:1805.11651},
  year    = {2018},
  note    = {69},
  file    = {Markovtsev et al. - 2018 - Splitting source code identifiers using Bidirectio.pdf:/home/akira/Zotero/storage/5MUZLW7V/Markovtsev et al. - 2018 - Splitting source code identifiers using Bidirectio.pdf:application/pdf},
}

@InProceedings{jagfeld_evaluating_2017,
  author    = {Jagfeld, G. and Ziering, P. and Van Der Plas, L.},
  title     = {Evaluating compound splitters extrinsically with textual entailment},
  booktitle = {{ACL} 2017 - 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}, {Proceedings} of the {Conference} ({Long} {Papers})},
  year      = {2017},
  volume    = {2},
  pages     = {58--63},
  note      = {29},
  abstract  = {Traditionally, compound splitters are evaluated intrinsically on gold-standard data or extrinsically on the task of statistical machine translation. We explore a novel way for the extrinsic evaluation of compound splitters, namely recognizing textual entailment. Compound splitting has great potential for this novel task that is both transparent and well-defined. Moreover, we show that it addresses certain aspects that are either ignored in intrinsic evaluations or compensated for by task-internal mechanisms in statistical machine translation. We show significant improvements using different compound splitting methods on a German textual entailment dataset. © 2017 Association for Computational Linguistics.},
  annote    = {cited By 0},
  doi       = {10.18653/v1/P17-2010},
  file      = {2017_Jagfeld et al_Evaluating compound splitters extrinsically with textual entailment.pdf:/home/akira/Zotero/storage/ENN8C9AU/2017_Jagfeld et al_Evaluating compound splitters extrinsically with textual entailment.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040556164&doi=10.18653%2fv1%2fP17-2010&partnerID=40&md5=abad0047ea69c94b560a54599233b45f},
}

@InProceedings{weller-di_marco_simple_2017,
  author    = {Weller-Di Marco, Marion},
  title     = {Simple compound splitting for {German}},
  booktitle = {Proceedings of the 13th {Workshop} on {Multiword} {Expressions} ({MWE} 2017)},
  year      = {2017},
  pages     = {161--166},
  note      = {61},
  file      = {Weller-Di Marco - 2017 - Simple compound splitting for German.pdf:/home/akira/Zotero/storage/SIXYHTKM/Weller-Di Marco - 2017 - Simple compound splitting for German.pdf:application/pdf},
}

@InProceedings{fujinuma_substring_2017,
  author    = {Fujinuma, Yoshinari and Grissom II, Alvin},
  title     = {Substring {Frequency} {Features} for {Segmentation} of {Japanese} {Katakana} {Words} with {Unlabeled} {Corpora}},
  booktitle = {Proceedings of the {Eighth} {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
  year      = {2017},
  pages     = {74--79},
  note      = {71},
  file      = {Fujinuma and Grissom II - 2017 - Substring Frequency Features for Segmentation of J.pdf:/home/akira/Zotero/storage/G3ZHDCNT/Fujinuma and Grissom II - 2017 - Substring Frequency Features for Segmentation of J.pdf:application/pdf},
}

@MastersThesis{nordal_cross-lingual_2016,
  author = {Nordal, Martin},
  title  = {Cross-lingual information retrieval using compound word splitting},
  school = {NTNU},
  year   = {2016},
  note   = {20},
}

@InProceedings{rigouts_terryn_dutch_2016,
  author    = {Rigouts Terryn, Ayla and Macken, Lieve and Lefever, Els},
  title     = {Dutch hypernym detection: does decompounding help?},
  booktitle = {Joint {Second} {Workshop} on {Language} and {Ontology} \& {Terminology} and {Knowledge} {Structures} ({LangOnto}2+ {TermiKS})},
  year      = {2016},
  pages     = {74--78},
  publisher = {European Language Resources Association (ELRA)},
  note      = {27},
  file      = {Rigouts Terryn et al. - 2016 - Dutch hypernym detection does decompounding help.pdf:/home/akira/Zotero/storage/ILMSTGEF/Rigouts Terryn et al. - 2016 - Dutch hypernym detection does decompounding help.pdf:application/pdf},
}

@InProceedings{ma_letter_2016,
  author    = {Ma, Jianqiang and Henrich, Verena and Hinrichs, Erhard},
  title     = {Letter sequence labeling for compound splitting},
  booktitle = {Proceedings of the 14th {SIGMORPHON} {Workshop} on {Computational} {Research} in {Phonetics}, {Phonology}, and {Morphology}},
  year      = {2016},
  pages     = {76--81},
  note      = {41},
  file      = {Ma et al. - 2016 - Letter sequence labeling for compound splitting.pdf:/home/akira/Zotero/storage/747WM5IB/Ma et al. - 2016 - Letter sequence labeling for compound splitting.pdf:application/pdf},
}

@Article{reuter_segmenting_2016,
  author  = {Reuter, Jack and Pereira-Martins, Jhonata and Kalita, Jugal},
  title   = {Segmenting twitter hashtags},
  journal = {International Journal on Natural Language Computing},
  year    = {2016},
  volume  = {5},
  pages   = {23--36},
  note    = {57},
  file    = {Reuter et al. - 2016 - Segmenting twitter hashtags.pdf:/home/akira/Zotero/storage/YHVRZH5D/Reuter et al. - 2016 - Segmenting twitter hashtags.pdf:application/pdf},
}

@InProceedings{shapiro_splitting_2016,
  author    = {Shapiro, Naomi Tachikawa},
  title     = {Splitting compounds with ngrams},
  booktitle = {Proceedings of {COLING} 2016, the 26th {International} {Conference} on {Computational} {Linguistics}: {Technical} {Papers}},
  year      = {2016},
  pages     = {630--640},
  note      = {68},
  file      = {Shapiro - 2016 - Splitting compounds with ngrams.pdf:/home/akira/Zotero/storage/U3VIUJDQ/Shapiro - 2016 - Splitting compounds with ngrams.pdf:application/pdf},
}

@InProceedings{ziering_towards_2016,
  author    = {Ziering, P. and Van Der Plas, L.},
  title     = {Towards unsupervised and language-independent compound splitting using inflectional morphological transformations},
  booktitle = {2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL} {HLT} 2016 - {Proceedings} of the {Conference}},
  year      = {2016},
  pages     = {644--653},
  note      = {75},
  abstract  = {In this paper, we address the task of language-independent, knowledge-lean and unsupervised compound splitting, which is an essential component for many natural language processing tasks such as machine translation. Previous methods on statistical compound splitting either include language-specific knowledge (e.g., linking elements) or rely on parallel data, which results in limited applicability. We aim to overcome these limitations by learning compounding morphology from inflectional information derived from lemmatized monolingual corpora. In experiments for Germanic languages, we show that our approach significantly outperforms language-dependent stateof-the-art methods in finding the correct split point and that word inflection is a good approximation for compounding morphology. ©2016 Association for Computational Linguistics.},
  annote    = {cited By 7},
  file      = {Ziering and Van Der Plas - 2016 - Towards unsupervised and language-independent comp.pdf:/home/akira/Zotero/storage/SU3ILLRJ/Ziering and Van Der Plas - 2016 - Towards unsupervised and language-independent comp.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994128765&partnerID=40&md5=5b0b9b0941575eb4aac9f0edfea61654},
}

@InProceedings{riedl_unsupervised_2016,
  author    = {Riedl, M. and Biemann, C.},
  title     = {Unsupervised compound splitting with distributional semantics rivals supervised methods},
  booktitle = {2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL} {HLT} 2016 - {Proceedings} of the {Conference}},
  year      = {2016},
  pages     = {617--622},
  note      = {79},
  abstract  = {In this paper we present a word decompounding method that is based on distributional semantics. Our method does not require any linguistic knowledge and is initialized using a large monolingual corpus. The core idea of our approach is that parts of compounds (like "candle" and "stick") are semantically similar to the entire compound, which helps to exclude spurious splits (like "candles" and "tick"). We report results for German and Dutch: For German, our unsupervised method comes on par with the performance of a rule-based and a supervised method and significantly outperforms two unsupervised baselines. For Dutch, our method performs only slightly below a rule-based optimized compound splitter. ©2016 Association for Computational Linguistics.},
  annote    = {cited By 8},
  file      = {Riedl and Biemann - 2016 - Unsupervised compound splitting with distributiona.pdf:/home/akira/Zotero/storage/8E8LSAW5/Riedl and Biemann - 2016 - Unsupervised compound splitting with distributiona.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994120942&partnerID=40&md5=907e2881fa0d72e3de877cd94f519d2d},
}

@InProceedings{hirschmann_what_2016,
  author    = {Hirschmann, F. and Nam, J. and Fürnkranz, J.},
  title     = {What makes word-level neural machine translation hard: {A} case study on {English}-{German} translation},
  booktitle = {{COLING} 2016 - 26th {International} {Conference} on {Computational} {Linguistics}, {Proceedings} of {COLING} 2016: {Technical} {Papers}},
  year      = {2016},
  pages     = {3199--3208},
  note      = {85},
  abstract  = {Traditional machine translation systems often require heavy feature engineering and the combination of multiple techniques for solving different subproblems. In recent years, several end-to-end learning architectures based on recurrent neural networks have been proposed. Unlike traditional systems, Neural Machine Translation (NMT) systems learn the parameters of the model and require only minimal preprocessing. Memory and time constraints allow to take only a fixed number of words into account, which leads to the out-of-vocabulary (OOV) problem. In this work, we analyze why the OOV problem arises and why it is considered a serious problem in German. We study the effectiveness of compound word splitters for alleviating the OOV problem, resulting in a 2.5+ BLEU points improvement over a baseline on the WMT' 14 German-to-English translation task. For English-to-German translation, we use target-side compound splitting through a special syntax during training that allows the model to merge compound words and gain 0.2 BLEU points. © 1963-2018 ACL.},
  annote    = {cited By 3},
  file      = {Hirschmann et al. - 2016 - What makes word-level neural machine translation h.pdf:/home/akira/Zotero/storage/STNZCZKA/Hirschmann et al. - 2016 - What makes word-level neural machine translation h.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039147994&partnerID=40&md5=92b1d72c2067f0a3005d477c56d828af},
}

@Article{escartin_assessing_2015,
  author  = {Escartín, Carla Parra and Alonso, Héctor Martínez},
  title   = {Assessing {WordNet} for bilingual compound dictionary extraction},
  journal = {MULTI-WORD UNITS IN MACHINE TRANSLATION AND TRANSLATION TECHNOLOGIES MUMTTT2015},
  year    = {2015},
  pages   = {19},
  note    = {10},
  file    = {2015_Escartín_Alonso_Assessing WordNet for bilingual compound dictionary extraction.pdf:/home/akira/Zotero/storage/4W8HEFB8/2015_Escartín_Alonso_Assessing WordNet for bilingual compound dictionary extraction.pdf:application/pdf},
}

@Article{volk_compounds_2015,
  author = {Volk, Martin and Mascarell, Laura and Fishel, Mark},
  title  = {Compounds, {Coreferences} and {Multiword} {Translations}},
  year   = {2015},
  note   = {18},
  file   = {Volk et al_Compounds, Coreferences and Multiword Translations.pdf:/home/akira/Zotero/storage/3IUN8SHG/Volk et al_Compounds, Coreferences and Multiword Translations.pdf:application/pdf},
}

@Article{prestes_extracao_2015,
  author = {Prestes, Kassius Vargas},
  title  = {Extração multilíngue de termos multipalavra em corpora comparáveis},
  year   = {2015},
  note   = {30},
  file   = {Prestes - 2015 - Extração multilíngue de termos multipalavra em cor.pdf:/home/akira/Zotero/storage/C3HNWX9X/Prestes - 2015 - Extração multilíngue de termos multipalavra em cor.pdf:application/pdf},
}

@Article{carvalho_source_2015,
  author   = {Carvalho, Nuno Ramos and Almeida, José João and Henriques, Pedro Rangel and Varanda, Maria João},
  title    = {From {Source} {Code} {Identifiers} to {Natural} {Language} {Terms}},
  journal  = {J. Syst. Softw.},
  year     = {2015},
  volume   = {100},
  number   = {C},
  pages    = {117--128},
  issn     = {0164-1212},
  note     = {31},
  doi      = {10.1016/j.jss.2014.10.013},
  file     = {2015_Carvalho et al_From Source Code Identifiers to Natural Language Terms.pdf:/home/akira/Zotero/storage/Q9EFTF6R/2015_Carvalho et al_From Source Code Identifiers to Natural Language Terms.pdf:application/pdf},
  keywords = {Identifier splitting, Natural language processing, Program comprehension},
  url      = {http://dx.doi.org/10.1016/j.jss.2014.10.013},
}

@InProceedings{tiedemann_morphological_2015,
  author    = {Tiedemann, Jörg and Ginter, Filip and Kanerva, Jenna},
  title     = {Morphological segmentation and {OPUS} for {Finnish}-{English} machine translation},
  booktitle = {Proceedings of the {Tenth} {Workshop} on {Statistical} {Machine} {Translation}},
  year      = {2015},
  pages     = {177--183},
  note      = {48},
  file      = {Tiedemann et al. - 2015 - Morphological segmentation and OPUS for Finnish-En.pdf:/home/akira/Zotero/storage/SHFYM3JI/Tiedemann et al. - 2015 - Morphological segmentation and OPUS for Finnish-En.pdf:application/pdf},
}

@Article{bretschneider_semantic_2015,
  author   = {Bretschneider, C. and Zillner, S.},
  title    = {Semantic splitting of {German} medical compounds},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2015},
  volume   = {9302},
  pages    = {207--215},
  note     = {60},
  abstract = {Compounding is widespread in highly inflectional languages with a quarter of all nouns created by composition. In our field of study, the German medical language, the amount of compounds significantly outnumbers this figure with 64\%. Thus, their correct splitting is a high-impact preprocessing step for any NLP-based application. In this work we address two challenges of medical decomposition: First, we introduce the consideration of unknown constituents in order to split compounds that were not recognized as such so far. Second, our approach builds on the corpus-based approach of Koehn and Knight and adds semantic knowledge from domain ontologies to increase the accuracy during disambiguation of the various split options. Using this first-of-a-kind semantic approach in a study on decomposition of German medical compounds, we outperform the existing approaches by far. © Springer International Publishing Switzerland 2015.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-319-24033-6_24},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951778429&doi=10.1007%2f978-3-319-24033-6_24&partnerID=40&md5=b80f85ba2c958dce5bb50b1ba2869b01},
}

@Article{daiber_splitting_2015,
  author  = {Daiber, Joachim and Quiroz, Lautaro and Wechsler, Roger and Frank, Stella},
  title   = {Splitting compounds by semantic analogy},
  journal = {arXiv preprint arXiv:1509.04473},
  year    = {2015},
  note    = {67},
  file    = {Daiber et al. - 2015 - Splitting compounds by semantic analogy.pdf:/home/akira/Zotero/storage/6CPNK7HK/Daiber et al. - 2015 - Splitting compounds by semantic analogy.pdf:application/pdf},
}

@Article{hellwig_using_2015,
  author  = {Hellwig, Oliver},
  title   = {Using {Recurrent} {Neural} {Networks} for joint compound splitting and {Sandhi} resolution in {Sanskrit}},
  journal = {Proceedings of the 7th LTC},
  year    = {2015},
  pages   = {289--293},
  note    = {81},
  file    = {Hellwig - 2015 - Using Recurrent Neural Networks for joint compound.pdf:/home/akira/Zotero/storage/QYVWPWDD/Hellwig - 2015 - Using Recurrent Neural Networks for joint compound.pdf:application/pdf},
}

@InProceedings{van_huyssteen_taxonomy_2014,
  author    = {Van Huyssteen, Gerhard and Verhoeven, Ben},
  title     = {A {Taxonomy} for {Afrikaans} and {Dutch} compounds},
  booktitle = {Proceedings of the {First} {Workshop} on {Computational} {Approaches} to {Compound} {Analysis} ({ComAComA} 2014)},
  year      = {2014},
  pages     = {31--40},
  note      = {03},
  file      = {2014_Van Huyssteen_Verhoeven_A Taxonomy for Afrikaans and Dutch compounds.pdf:/home/akira/Zotero/storage/ZRGENZYE/2014_Van Huyssteen_Verhoeven_A Taxonomy for Afrikaans and Dutch compounds.pdf:application/pdf},
}

@Article{pecina_adaptation_2014-1,
  author   = {Pecina, Pavel and Dušek, Ondřej and Goeuriot, Lorraine and Hajič, Jan and Hlaváčová, Jaroslava and Jones, Gareth J. F. and Kelly, Liadh and Leveling, Johannes and Mareček, David and Novák, Michal and Popel, Martin and Rosa, Rudolf and Tamchyna, Aleš and Urešová, Zdeňka},
  title    = {Adaptation of {Machine} {Translation} for {Multilingual} {Information} {Retrieval} in the {Medical} {Domain}},
  journal  = {Artif. Intell. Med.},
  year     = {2014},
  volume   = {61},
  number   = {3},
  pages    = {165--185},
  month    = jul,
  issn     = {0933-3657},
  note     = {04},
  doi      = {10.1016/j.artmed.2014.01.004},
  file     = {2014_Pecina et al_Adaptation of Machine Translation for Multilingual Information Retrieval in the Medical Domain2.pdf:/home/akira/Zotero/storage/US4BVHRF/2014_Pecina et al_Adaptation of Machine Translation for Multilingual Information Retrieval in the Medical Domain2.pdf:application/pdf},
  keywords = {Compound splitting, Cross-language information retrieval, Domain adaptation of statistical machine translation, Intelligent training data selection for machine translation, Medical query translation, Statistical machine translation},
  url      = {http://dx.doi.org/10.1016/j.artmed.2014.01.004},
}

@Article{hill_empirical_2014-1,
  author   = {Hill, Emily and Binkley, David and Lawrie, Dawn and Pollock, Lori and Vijay-Shanker, K.},
  title    = {An {Empirical} {Study} of {Identifier} {Splitting} {Techniques}},
  journal  = {Empirical Softw. Engg.},
  year     = {2014},
  volume   = {19},
  number   = {6},
  pages    = {1754--1780},
  issn     = {1382-3256},
  note     = {06},
  doi      = {10.1007/s10664-013-9261-0},
  file     = {2014_Hill et al_An Empirical Study of Identifier Splitting Techniques.pdf:/home/akira/Zotero/storage/AWMTSPKF/2014_Hill et al_An Empirical Study of Identifier Splitting Techniques.pdf:application/pdf},
  keywords = {Program comprehension, Identifier names, Software engineering tools, Source code text analysis},
  url      = {http://dx.doi.org/10.1007/s10664-013-9261-0},
}

@Article{guerrouj_experimental_2014,
  author   = {Guerrouj, Latifa and Penta, Massimiliano and Guéhéneuc, Yann-Gaël and Antoniol, Giuliano},
  title    = {An {Experimental} {Investigation} on the {Effects} of {Context} on {Source} {Code} {Identifiers} {Splitting} and {Expansion}},
  journal  = {Empirical Softw. Engg.},
  year     = {2014},
  volume   = {19},
  number   = {6},
  pages    = {1706--1753},
  issn     = {1382-3256},
  note     = {07},
  doi      = {10.1007/s10664-013-9260-1},
  file     = {2014_Guerrouj et al_An Experimental Investigation on the Effects of Context on Source Code Identifiers Splitting and Expansion.pdf:/home/akira/Zotero/storage/BA4LYM4F/2014_Guerrouj et al_An Experimental Investigation on the Effects of Context on Source Code Identifiers Splitting and Expansion.pdf:application/pdf},
  keywords = {Identifier splitting and expansion, Program understanding, Task context},
  url      = {http://dx.doi.org/10.1007/s10664-013-9260-1},
}

@Article{kirkedal_automatic_2014,
  author = {Kirkedal, Andreas Søeborg},
  title  = {Automatic {Phonetic} {Transcription} for {Danish} {Speech} {Recognition}},
  year   = {2014},
  note   = {12},
}

@InProceedings{bick_constraint_2014,
  author    = {Bick, Eckhard},
  title     = {Constraint {Grammar}-{Based} {Swedish}-{Danish} {Machine} {Translation}},
  booktitle = {Advances in {Natural} {Language} {Processing}},
  year      = {2014},
  editor    = {Przepiórkowski, Adam and Ogrodniczuk, Maciej},
  pages     = {216--227},
  address   = {Cham},
  publisher = {Springer International Publishing},
  note      = {19},
  abstract  = {This paper describes and evaluates a grammar-based machine translation system for the Swedish-Danish language pair. Source-language structural analysis, polysemy resolution, syntactic movement rules and target-language agreement are based on Constraint Grammar morphosyntactic tags and dependency trees. Lexical transfer rules exploit dependency links to access contextual information, such as syntactic argument function, semantic type and quantifiers, or to integrate verbal features, e.g. diathesis and auxiliaries. Out-of-vocabulary words are handled by derivational and compound analysis with a combined coverage of 99.3\%, as well as systematic morpho-phonemic transliterations for the remaining cases. The system achieved BLEU scores of 0.65-0.8 depending on references and outperformed both STMT and RBMT competitors by a large margin.},
  file      = {2014_Bick_Constraint Grammar-Based Swedish-Danish Machine Translation.pdf:/home/akira/Zotero/storage/V8KTA7ZK/2014_Bick_Constraint Grammar-Based Swedish-Danish Machine Translation.pdf:application/pdf},
  isbn      = {978-3-319-10888-9},
}

@InProceedings{brun_decomposing_2014,
  author    = {Brun, Caroline and Roux, Claude},
  title     = {Decomposing {Hashtags} to {Improve} {Tweet} {Polarity} {Classification} ({Décomposition} des Ğ hash tags ğ pour l’amélioration de la classification en polarité des Ğ tweets ğ)[in {French}]},
  booktitle = {Proceedings of {TALN} 2014 ({Volume} 2: {Short} {Papers})},
  year      = {2014},
  pages     = {473--478},
  note      = {22},
  file      = {Brun and Roux - 2014 - Decomposing Hashtags to Improve Tweet Polarity Cla.pdf:/home/akira/Zotero/storage/TZ383J7W/Brun and Roux - 2014 - Decomposing Hashtags to Improve Tweet Polarity Cla.pdf:application/pdf},
}

@InProceedings{weller_distinguishing_2014,
  author    = {Weller, Marion and Cap, Fabienne and Müller, Stefan and im Walde, Sabine Schulte and Fraser, Alexander},
  title     = {Distinguishing degrees of compositionality in compound splitting for statistical machine translation},
  booktitle = {Proceedings of the {First} {Workshop} on {Computational} {Approaches} to {Compound} {Analysis} ({ComAComA} 2014)},
  year      = {2014},
  pages     = {81--90},
  note      = {26},
  file      = {Weller et al. - 2014 - Distinguishing degrees of compositionality in comp.pdf:/home/akira/Zotero/storage/3XCJKMY9/Weller et al. - 2014 - Distinguishing degrees of compositionality in comp.pdf:application/pdf},
}

@InProceedings{escartin_german_2014,
  author    = {Escartín, Carla Parra and Peitz, Stephan and Ney, Hermann},
  title     = {German {Compounds} and {Statistical} {Machine} {Translation}. {Can} they get along?},
  booktitle = {Proceedings of the 10th {Workshop} on {Multiword} {Expressions} ({MWE})},
  year      = {2014},
  pages     = {48--56},
  note      = {33},
  file      = {Escartín et al. - 2014 - German Compounds and Statistical Machine Translati.pdf:/home/akira/Zotero/storage/JJYPSB2C/Escartín et al. - 2014 - German Compounds and Statistical Machine Translati.pdf:application/pdf},
}

@InProceedings{fishel_handling_2014,
  author    = {Fishel, M. and Sennrich, R.},
  title     = {Handling technical {OOVs} in {SMT}},
  booktitle = {Proceedings of the 17th {Annual} {Conference} of the {European} {Association} for {Machine} {Translation}, {EAMT} 2014},
  year      = {2014},
  pages     = {159--162},
  note      = {36},
  abstract  = {We present a project on machine translation of software help desk tickets, a highly technical text domain. The main source of translation errors were out-of-vocabulary tokens (OOVs), most of which were either in-domain German compounds or technical token sequences that must be preserved verbatim in the output. We describe our efforts on compound splitting and treatment of non-translatable tokens, which lead to a significant translation quality gain. © 2014 The authors.},
  annote    = {cited By 0},
  file      = {Fishel and Sennrich - 2014 - Handling technical OOVs in SMT.pdf:/home/akira/Zotero/storage/4LLQZCAQ/Fishel and Sennrich - 2014 - Handling technical OOVs in SMT.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000716797&partnerID=40&md5=3634713662e062110d558fbc30f94db4},
}

@Article{ablimit_lexicon_2014,
  author  = {Ablimit, Mijit and Kawahara, Tatsuya and Hamdulla, Askar},
  title   = {Lexicon optimization based on discriminative learning for automatic speech recognition of agglutinative language},
  journal = {Speech communication},
  year    = {2014},
  volume  = {60},
  pages   = {78--87},
  note    = {42},
  file    = {Ablimit et al. - 2014 - Lexicon optimization based on discriminative learn.pdf:/home/akira/Zotero/storage/RN3QYC5F/Ablimit et al. - 2014 - Lexicon optimization based on discriminative learn.pdf:application/pdf},
}

@Article{verhoeven_proceedings_2014,
  author = {Verhoeven, Ben and Daelemans, Walter and van Zaanen, Menno and van Huyssteen, Gerhard},
  title  = {Proceedings of the {First} {Workshop} on {Computational} {Approaches} to {Compound} {Analysis} held at the 25th {International} {Conference} on {Computational} {Linguistics} ({COLING} 2014)},
  year   = {2014},
  note   = {53},
  file   = {Verhoeven et al. - 2014 - Proceedings of the First Workshop on Computational.pdf:/home/akira/Zotero/storage/IJGCDCNF/Verhoeven et al. - 2014 - Proceedings of the First Workshop on Computational.pdf:application/pdf},
}

@Article{wolk_real-time_2014,
  author   = {Wołk, K. and Marasek, K.},
  title    = {Real-time statistical speech translation},
  journal  = {Advances in Intelligent Systems and Computing},
  year     = {2014},
  volume   = {275 AISC},
  number   = {VOLUME 1},
  pages    = {107--113},
  note     = {55},
  abstract = {This research investigates the Statistical Machine Translation approaches to translate speech in real time automatically. Such systems can be used in a pipeline with speech recognition and synthesis software in order to produce a real-time voice communication system between foreigners. We obtained three main data sets from spoken proceedings that represent three different types of human speech. TED, Europarl, and OPUS parallel text corpora were used as the basis for training of language models, for developmental tuning and testing of the translation system. We also conducted experiments involving part of speech tagging, compound splitting, linear language model interpolation, TrueCasing and morphosyntactic analysis. We evaluated the effects of variety of data preparations on the translation results using the BLEU, NIST, METEOR and TER metrics and tried to give answer which metric is most suitable for PL-EN language pair. © Springer International Publishing Switzerland 2014.},
  annote   = {cited By 8},
  doi      = {10.1007/978-3-319-05951-8_11},
  file     = {Wołk and Marasek - 2014 - Real-time statistical speech translation.pdf:/home/akira/Zotero/storage/4IGT7AK8/Wołk and Marasek - 2014 - Real-time statistical speech translation.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904602811&doi=10.1007%2f978-3-319-05951-8_11&partnerID=40&md5=d9d3d16efc7102d26081575e8d464b8b},
}

@PhdThesis{leidig_single_2014,
  author = {Leidig, Sebastian and Schlippe, Dipl-Inform Tim and Schultz, Ing Tanja},
  title  = {Single and {Combined} {Features} for the {Detection} of {Anglicisms} in {German} and {Afrikaans}},
  school = {Bachelor’s Thesis, Karlsruhe Institute of Technology},
  year   = {2014},
  type   = {{PhD} {Thesis}},
  note   = {62},
  file   = {Leidig et al. - 2014 - Single and Combined Features for the Detection of .pdf:/home/akira/Zotero/storage/PWZ8XMTL/Leidig et al. - 2014 - Single and Combined Features for the Detection of .pdf:application/pdf},
}

@InProceedings{pimpale_smt_2014,
  author    = {Pimpale, Prakash B and Patel, Raj Nath and Sasikumar, M},
  title     = {{SMT} from {Agglutinative} {Languages}: {Use} of {Suffix} {Separation} and {Word} {Splitting}},
  booktitle = {Proceedings of the 11th {International} {Conference} on {Natural} {Language} {Processing}},
  year      = {2014},
  pages     = {2--10},
  note      = {63},
  file      = {Pimpale et al. - 2014 - SMT from Agglutinative Languages Use of Suffix Se.pdf:/home/akira/Zotero/storage/IJ8L8R4T/Pimpale et al. - 2014 - SMT from Agglutinative Languages Use of Suffix Se.pdf:application/pdf},
}

@InProceedings{junczys-dowmunt_smt_2014,
  author    = {Junczys-Dowmunt, Marcin and Pouliquen, Bruno},
  title     = {{SMT} of {German} {Patents} at {WIPO}: {Decompounding} and {Verb} {Structure} {Pre}-reordering},
  booktitle = {Proceedings of the 17th {Annual} {Conference} of the {European} {Association} for {Machine} {Translation} ({EAMT}2014)},
  year      = {2014},
  pages     = {217--220},
  note      = {64},
  file      = {Junczys-Dowmunt and Pouliquen - 2014 - SMT of German Patents at WIPO Decompounding and V.pdf:/home/akira/Zotero/storage/LU44R53C/Junczys-Dowmunt and Pouliquen - 2014 - SMT of German Patents at WIPO Decompounding and V.pdf:application/pdf},
}

@InProceedings{santos_using_2014,
  author    = {Santos, Pedro Bispo},
  title     = {Using compound lists for german decompounding in a back-off scenario},
  booktitle = {Workshop on {Computational}, {Cognitive}, and {Linguistic} {Approaches} to the {Analysis} of {Complex} {Words} and {Collocations} ({CCLCC} 2014)},
  year      = {2014},
  pages     = {51--55},
  note      = {80},
  file      = {Santos - 2014 - Using compound lists for german decompounding in a.pdf:/home/akira/Zotero/storage/WTA926Y6/Santos - 2014 - Using compound lists for german decompounding in a.pdf:application/pdf},
}

@InProceedings{owczarzak_wordsyoudontknow_2014,
  author    = {Owczarzak, Karolina and de Haan, Ferdinand and Krupka, George and Hindle, Don},
  title     = {Wordsyoudontknow: {Evaluation} of lexicon-based decompounding with unknown handling},
  booktitle = {Proceedings of the {First} {Workshop} on {Computational} {Approaches} to {Compound} {Analysis} ({ComAComA} 2014)},
  year      = {2014},
  pages     = {63--71},
  note      = {87},
  file      = {Owczarzak et al. - 2014 - Wordsyoudontknow Evaluation of lexicon-based deco.pdf:/home/akira/Zotero/storage/XRHBA9RL/Owczarzak et al. - 2014 - Wordsyoudontknow Evaluation of lexicon-based deco.pdf:application/pdf},
}

@InProceedings{ganguly_case_2013,
  author    = {Ganguly, Debasis and Leveling, Johannes and Jones, Gareth J. F.},
  title     = {A {Case} {Study} in {Decompounding} for {Bengali} {Information} {Retrieval}},
  booktitle = {Information {Access} {Evaluation}. {Multilinguality}, {Multimodality}, and {Visualization}},
  year      = {2013},
  editor    = {Forner, Pamela and Müller, Henning and Paredes, Roberto and Rosso, Paolo and Stein, Benno},
  pages     = {108--119},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  note      = {01},
  abstract  = {Decompounding has been found to improve information retrieval (IR) effectiveness for compounding languages such as Dutch, German, or Finnish. No previous studies, however, exist on the effect of decomposition of compounds in IR for Indian languages. In this case study, we investigate the effect of decompounding for Bengali, a highly agglutinative Indian language. The standard approach of decompounding for IR, i.e. indexing compound parts (constituents) in addition to compound words, has proven beneficial for European languages. Our experiments reported in this paper show that such a standard approach does not work particularly well for Bengali IR. Some unique characteristics of Bengali compounds are: i) only one compound constituent may be a valid word in contrast to the stricter requirement of both being so; and ii) the first character of the right constituent can be modified by the rules of Sandhi in contrast to simple concatenation. As a solution, we firstly propose a more relaxed decompounding where a compound word is decomposed into only one constituent if the other constituent is not a valid word, and secondly we perform selective decompounding by ensuring that constituents often co-occur with the compound word, which indicates how related the constituents and the compound are. We perform experiments on Bengali ad-hoc IR collections from FIRE 2008 to 2012. Our experiments show that both the relaxed decomposition and the co-occurrence-based constituent selection proves more effective than the standard frequency-based decomposition method, improving mean average precision (MAP) up to 2.72\% and recall up to 1.8\%, compared to not decompounding words.},
  annote    = {cited by 5  },
  file      = {2013_Ganguly et al_A Case Study in Decompounding for Bengali Information Retrieval.pdf:/home/akira/Zotero/storage/79DGE4TB/2013_Ganguly et al_A Case Study in Decompounding for Bengali Information Retrieval.pdf:application/pdf},
  isbn      = {978-3-642-40802-1},
}

@InProceedings{binkley_dataset_2013,
  author    = {Binkley, D. and Lawrie, D. and Pollock, L. and Hill, E. and Vijay-Shanker, K.},
  title     = {A dataset for evaluating identifier splitters},
  booktitle = {{IEEE} {International} {Working} {Conference} on {Mining} {Software} {Repositories}},
  year      = {2013},
  pages     = {401--404},
  note      = {02},
  abstract  = {Software engineering and evolution techniques have recently started to exploit the natural language information in source code. A key step in doing so is splitting identifiers into their constituent words. While simple in concept, identifier splitting raises several challenging issues, leading to a range of splitting techniques. Consequently, the research community would benefit from a dataset (i.e., a gold set) that facilitates comparative studies of identifier splitting techniques. A gold set of 2,663 split identifiers was constructed from 8,522 individual human splitting judgements and can be obtained from www.cs.loyola.edu/∼binkley/ludiso. This set's construction and observations aimed at its effective use are described. © 2013 IEEE.},
  annote    = {cited By 6},
  doi       = {10.1109/MSR.2013.6624055},
  file      = {2013_Binkley et al_A dataset for evaluating identifier splitters.pdf:/home/akira/Zotero/storage/98KGJGBE/2013_Binkley et al_A dataset for evaluating identifier splitters.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889056852&doi=10.1109%2fMSR.2013.6624055&partnerID=40&md5=20c554dc45e942680416227bea5a5bdf},
}

@Article{kpodjedo_madmatch_2013,
  author   = {Kpodjedo, S. and Ricca, F. and Galinier, P. and Antoniol, G. and Gueheneuc, Y.-G.},
  title    = {Madmatch: {Many}-to-many approximate diagram matching for design comparison},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2013},
  volume   = {39},
  number   = {8},
  pages    = {1090--1111},
  note     = {45},
  abstract = {Matching algorithms play a fundamental role in many important but difficult software engineering activities, especially design evolution analysis and model comparison. We present MADMatch, a fast and scalable many-to-many approximate diagram matching approach based on an error-tolerant graph matching (ETGM) formulation. Diagrams are represented as graphs, costs are assigned to possible differences between two given graphs, and the goal is to retrieve the cheapest matching. We address the resulting optimization problem with a tabu search enhanced by the novel use of lexical and structural information. Through several case studies with different types of diagrams and tasks, we show that our generic approach obtains better results than dedicated state-of-the-art algorithms, such as AURA, PLTSDiff, or UMLDiff, on the exact same datasets used to introduce (and evaluate) these algorithms. © 1976-2012 IEEE.},
  annote   = {cited By 13},
  doi      = {10.1109/TSE.2013.9},
  file     = {2013_Kpodjedo et al_Madmatch.pdf:/home/akira/Zotero/storage/QQRW8NHR/2013_Kpodjedo et al_Madmatch.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881038069&doi=10.1109%2fTSE.2013.9&partnerID=40&md5=9a548cfb32b924a02c660b079f2e1e6b},
}

@Article{guerrouj_tidier_2013,
  author   = {Guerrouj, L. and Di Penta, M. and Antoniol, G. and Guéh́eneuc, Y.-G.},
  title    = {{TIDIER}: {An} identifier splitting approach using speech recognition techniques},
  journal  = {Journal of software: Evolution and Process},
  year     = {2013},
  volume   = {25},
  number   = {6},
  pages    = {575--599},
  note     = {74},
  abstract = {The software engineering literature reports empirical evidence on the relation between various characteristics of a software system and its quality. Among other factors, recent studies have shown that a proper choice of identifiers influences understandability and maintainability. Indeed, identifiers are developers' main source of information and guide their cognitive processes during program comprehension when high-level documentation is scarce or outdated and when source code is not sufficiently commented. This paper proposes a novel approach to recognize words composing source code identifiers. The approach is based on an adaptation of Dynamic Time Warping used to recognize words in continuous speech. The approach overcomes the limitations of existing identifier-splitting approaches when naming conventions (e.g., Camel Case) are not used or when identifiers contain abbreviations. We apply the approach on a sample of more than 1000 identifiers extracted from 340 C programs and compare its results with a simple Camel Case splitter and with an implementation of an alternative identifier splitting approach, Samurai. Results indicate the capability of the novel approach: (i) to outperform the alternative ones, when using a dictionary augmented with domain knowledge or a contextual dictionary and (ii) to expand 48\% of a set of selected abbreviations into dictionary words. Copyright © 2011 John Wiley \& Sons, Ltd.},
  annote   = {cited By 21},
  doi      = {10.1002/smr.539},
  file     = {Guerrouj et al. - 2013 - TIDIER An identifier splitting approach using spe.pdf:/home/akira/Zotero/storage/SL7YMR5A/Guerrouj et al. - 2013 - TIDIER An identifier splitting approach using spe.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883654687&doi=10.1002%2fsmr.539&partnerID=40&md5=79227717373f850527ad7601246a17bb},
}

@InProceedings{corazza_linsen_2012,
  author    = {Corazza, A. and Martino, S. Di and Maggio, V.},
  title     = {{LINSEN}: {An} efficient approach to split identifiers and expand abbreviations},
  booktitle = {2012 28th {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM})},
  year      = {2012},
  pages     = {233--242},
  month     = sep,
  note      = {43},
  abstract  = {Information Retrieval (IR) techniques are being exploited by an increasing number of tools supporting Software Maintenance activities. Indeed the lexical information embedded in the source code can be valuable for tasks such as concept location, clustering or recovery of traceability links. The application of such IR-based techniques relies on the consistency of the lexicon available in the different artifacts, and their effectiveness can worsen if programmers introduce abbreviations (e.g: rect) and/or do not strictly follow naming conventions such as Camel Case (e.g: UTFtoASCII). In this paper we propose an approach to automatically split identifiers in their composing words, and expand abbreviations. The solution is based on a graph model and performs in linear time with respect to the size of the dictionary, taking advantage of an approximate string matching technique. The proposed technique exploits a number of different dictionaries, referring to increasingly broader contexts, in order to achieve a disambiguation strategy based on the knowledge gathered from the most appropriate domain. The approach has been compared to other splitting and expansion techniques, using freely available oracles for the identifiers extracted from 24 C/C++ and Java open source systems. Results show an improvement in both splitting and expanding performance, in addition to a strong enhancement in the computational efficiency.},
  doi       = {10.1109/ICSM.2012.6405277},
  file      = {Corazza et al. - 2012 - LINSEN An efficient approach to split identifiers.pdf:/home/akira/Zotero/storage/CLTIYFUB/Corazza et al. - 2012 - LINSEN An efficient approach to split identifiers.pdf:application/pdf},
  keywords  = {Context, information retrieval, abbreviation expansion, approximate string matching technique, Approximation algorithms, C-C++, C++ language, camel case, concept location, Conferences, Dictionaries, dictionary, disambiguation strategy, Expansion, identifier splitting, information retrieval techniques, IR-based techniques, Java, Java open source systems, lexical information, LINSEN, oracles, pattern clustering, Program Comprehension, public domain software, Software algorithms, software maintenance, Software maintenance, software maintenance activities, Source Code Identifiers, Splitting, traceability links clustering, traceability links recovery},
}

@InProceedings{srinivasan_segmenting_2012,
  author    = {Srinivasan, S. and Bhattacharya, S. and Chakraborty, R.},
  title     = {Segmenting web-domains and hashtags using length specific models},
  booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
  year      = {2012},
  pages     = {1113--1122},
  note      = {58},
  abstract  = {Segmentation of a string of English language characters into a sequence of words has many applications. Here, we study two applications in the internet domain. First application is the web domain segmentation which is crucial for monetization of broken URLs. Secondly, we propose and study a novel application of twitter hashtag segmentation for increasing recall on twitter searches. Existing methods for word segmentation use unsupervised language models. We find that when using multiple corpora, the joint probability model from multiple corpora performs significantly better than the individual corpora. Motivated by this, we propose weighted joint probability model, with weights specific to each corpus. We propose to train the weights in a supervised manner using max-margin methods. The supervised probability models improve segmentation accuracy over joint probability models. Finally, we observe that length of segments is an important parameter for word segmentation, and incorporate length-specific weights into our model. The length specific models further improve segmentation accuracy over supervised probability models. For all models proposed here, inference problem can be solved using the dynamic programming algorithm. We test our methods on five different datasets, two from web domains data, and three from news headlines data from an LDC dataset. The supervised length specific models show significant improvements over unsupervised single corpus and joint probability models. Cross-testing between the datasets confirm that supervised probability models trained on all datasets, and length specific models trained on news headlines data, generalize well. Segmentation of hashtags result in significant improvement in recall on searches for twitter trends. © 2012 ACM.},
  annote    = {cited By 13},
  doi       = {10.1145/2396761.2398410},
  file      = {Srinivasan et al. - 2012 - Segmenting web-domains and hashtags using length s.pdf:/home/akira/Zotero/storage/HLUIP69X/Srinivasan et al. - 2012 - Segmenting web-domains and hashtags using length s.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871061019&doi=10.1145%2f2396761.2398410&partnerID=40&md5=4a4ef08843f61ac76f3dbc0b0b0a71a5},
}

@InProceedings{sureka_source_2012,
  author    = {Sureka, A.},
  title     = {Source code identifier splitting using yahoo image and web search engine},
  booktitle = {Proceedings of the {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
  year      = {2012},
  pages     = {1--8},
  note      = {65},
  abstract  = {Source-code or program identifiers are sequence of characters consisting of one or more tokens representing domain concepts. Splitting or tokenizing identifiers that does not contain explicit markers or clues such as came-casing or using underscore as a token separatoris a technically challenging problem. In this paper, we present a technique for automatic tokenization and splitting of source-code identifiers using Yahoo web search and image search similarity distance. We present an algorithm that decides the split position based on various factors such as conceptual correlations and semantic relatedness between the left and right splits strings of a given identifier, popularity of the token and its length. The number of hits or search results returned by the web and image search engine serves as a proxy to measures such as term popularity and correlation. We perform a series of experiments to validate the proposed approach and present performance results.},
  annote    = {cited By 1},
  doi       = {10.1145/2384416.2384417},
  file      = {2012_Sureka_Source code identifier splitting using yahoo image and web search engine.pdf:/home/akira/Zotero/storage/8NJVVA7U/2012_Sureka_Source code identifier splitting using yahoo image and web search engine.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869760971&doi=10.1145%2f2384416.2384417&partnerID=40&md5=042160e97cea76d60ed1709ec697fb7f},
}

@InProceedings{biggers_effects_2012,
  author    = {Biggers, L.R.},
  title     = {The effects of identifier retention and stop word removal on a latent {Dirichlet} allocation based feature location technique},
  booktitle = {Proceedings of the {Annual} {Southeast} {Conference}},
  year      = {2012},
  pages     = {164--169},
  note      = {72},
  abstract  = {Feature location, an important task in program comprehension, occurs when the developer identifies the source code entity or entities responsible for implementing a functionality. Researchers have applied static analysis techniques to multiple software maintenance tasks, including feature localization. Static analysis techniques operate on a document corpus. Configuration and preprocessing decisions are required to build a suitable source code corpus for a static analysis technique. Currently, there is little guidance in the software engineering literature for making such configuration decisions. This paper focuses on two preprocessing methods for source code corpora, identifier splitting and stop word lists. We experiment on three open source Java test suites, i.e. Mylyn 1.0.1, Rhino 1.5R5, and Rhino 1.6R5. Our results indicate that identifier splitting and stop word list decisions do not significantly affect the performance of the LDA based feature location technique. © 2012 ACM.},
  annote    = {cited By 1},
  doi       = {10.1145/2184512.2184551},
  file      = {Biggers - 2012 - The effects of identifier retention and stop word .pdf:/home/akira/Zotero/storage/HATKDGVJ/Biggers - 2012 - The effects of identifier retention and stop word .pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862667183&doi=10.1145%2f2184512.2184551&partnerID=40&md5=596eddec20f80762f4b8b0c7f84166e1},
}

@InProceedings{guerrouj_tris_2012,
  author    = {Guerrouj, L. and Galinier, P. and Guéhéneuc, Y.-G. and Antoniol, G. and Di Penta, M.},
  title     = {{TRIS}: {A} fast and accurate identifiers splitting and expansion algorithm},
  booktitle = {Proceedings - {Working} {Conference} on {Reverse} {Engineering}, {WCRE}},
  year      = {2012},
  pages     = {103--112},
  note      = {76},
  abstract  = {Understanding source code identifiers, by identifying words composing them, is a necessary step for many program comprehension, reverse engineering, or redocumentation tasks. To this aim, researchers have proposed several identifier splitting and expansion approaches such as Samurai, TIDIER and more recently GenTest. The ultimate goal of such approaches is to help disambiguating conceptual information encoded in compound (or abbreviated) identifiers. This paper presents TRIS, TRee-based Identifier Splitter, a two-phases approach to split and expand program identifiers. First, TRIS pre-compiles transformed dictionary words into a tree representation, associating a cost to each transformation. In a second phase, it maps the identifier splitting/expansion problem into a minimization problem, i.e., the search of the shortest path (optimal split/expansion) in a weighted graph. We apply TRIS to a sample of 974 identifiers extracted from JHotDraw, 3,085 from Lynx, and to a sample of 489 identifiers extracted from 340 C programs. Also, we compare TRIS with GenTest on a set of 2,663 mixed Java, C and C++ identifiers. We report evidence that TRIS split (and expansion) is more accurate than state-of-the-art approaches and that it is also efficient in terms of computation time. © 2012 IEEE.},
  annote    = {cited By 4},
  doi       = {10.1109/WCRE.2012.20},
  file      = {2012_Guerrouj et al_TRIS.pdf:/home/akira/Zotero/storage/XXQU5ZW3/2012_Guerrouj et al_TRIS.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872297690&doi=10.1109%2fWCRE.2012.20&partnerID=40&md5=cde8887fb5e9ca9bd4f353e3c9bd9ad8},
}

@InProceedings{dit_can_2011-1,
  author    = {Dit, B. and Guerrouj, L. and Poshyvanyk, D. and Antoniol, G.},
  title     = {Can {Better} {Identifier} {Splitting} {Techniques} {Help} {Feature} {Location}?},
  booktitle = {2011 {IEEE} 19th {International} {Conference} on {Program} {Comprehension}},
  year      = {2011},
  pages     = {11--20},
  month     = jun,
  note      = {14},
  abstract  = {The paper presents an exploratory study of two feature location techniques utilizing three strategies for splitting identifiers: Camel Case, Samurai and manual splitting of identifiers. The main research question that we ask in this study is if we had a perfect technique for splitting identifiers, would it still help improve accuracy of feature location techniques applied in different scenarios and settings? In order to answer this research question we investigate two feature location techniques, one based on Information Retrieval and the other one based on the combination of Information Retrieval and dynamic analysis, for locating bugs and features using various configurations of preprocessing strategies on two open-source systems, Rhino and jEdit. The results of an extensive empirical evaluation reveal that feature location techniques using Information Retrieval can benefit from better preprocessing algorithms in some cases, and that their improvement in effectiveness while using manual splitting over state-of-the-art approaches is statistically significant in those cases. However, the results for feature location technique using the combination of Information Retrieval and dynamic analysis do not show any improvement while using manual splitting, indicating that any preprocessing technique will suffice if execution data is available. Overall, our findings outline potential benefits of putting additional research efforts into defining more sophisticated source code preprocessing techniques as they can still be useful in situations where execution information cannot be easily collected.},
  doi       = {10.1109/ICPC.2011.47},
  file      = {2011_Dit et al_Can Better Identifier Splitting Techniques Help Feature Location.pdf:/home/akira/Zotero/storage/K35KUHE5/2011_Dit et al_Can Better Identifier Splitting Techniques Help Feature Location.pdf:application/pdf},
  keywords  = {information retrieval, Manuals, Dictionaries, Samurai, Software, dynamic analysis, feature location, identifier splitting algorithms, Algorithm design and analysis, Accuracy, CamelCase, feature location technique, Gold, identifier splitting technique, jEdit, Large scale integration, open source system, preprocessing strategies, Rhino},
}

@InProceedings{henrich_determining_2011,
  author    = {Henrich, V. and Hinrichs, E.},
  title     = {Determining immediate constituents of compounds in {GermaNet}},
  booktitle = {International {Conference} {Recent} {Advances} in {Natural} {Language} {Processing}, {RANLP}},
  year      = {2011},
  pages     = {420--426},
  note      = {24},
  abstract  = {In order to be able to systematically link compounds in GermaNet to their constituent parts, compound splitting needs to be applied recursively and has to identify the immediate constituents at each level of analysis. Existing tools for compound splitting for German only offer an analysis of all component parts of a compound at once without any grouping of subconstituents. Thus, existing tools for splitting compounds were adapted to overcome this issue. Algorithms combining three heterogeneous kinds of compound splitters are developed to achieve better results. The best overall result with an accuracy of 92.42\% is achieved by a hybrid combined compound splitter that takes into account all knowledge provided by the individual compound splitters, and in addition some domain knowledge about German derivation morphology and compounding.},
  annote    = {cited By 11},
  file      = {Henrich and Hinrichs - 2011 - Determining immediate constituents of compounds in.pdf:/home/akira/Zotero/storage/5LC9LJZ3/Henrich and Hinrichs - 2011 - Determining immediate constituents of compounds in.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866877956&partnerID=40&md5=4e50f1d779c887840937a10cea5dcd74},
}

@InProceedings{macherey_language-independent_2011,
  author    = {Macherey, Klaus and Dai, Andrew M. and Talbot, David and Popat, Ashok C. and Och, Franz},
  title     = {Language-independent {Compound} {Splitting} with {Morphological} {Operations}},
  booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} - {Volume} 1},
  year      = {2011},
  series    = {{HLT} '11},
  pages     = {1395--1404},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  note      = {event-place: Portland, Oregon},
  copyright = {40},
  file      = {Macherey et al. - 2011 - Language-independent Compound Splitting with Morph.pdf:/home/akira/Zotero/storage/KBEJ83RS/Macherey et al. - 2011 - Language-independent Compound Splitting with Morph.pdf:application/pdf},
  isbn      = {978-1-932432-87-9},
  url       = {http://dl.acm.org/citation.cfm?id=2002472.2002644},
}

@InProceedings{wang_web_2011,
  author    = {Wang, K. and Thrasher, C. and Hsu, B.-J.},
  title     = {Web scale {NLP}: {A} case study on {URL} word breaking},
  booktitle = {Proceedings of the 20th {International} {Conference} on {World} {Wide} {Web}, {WWW} 2011},
  year      = {2011},
  pages     = {357--366},
  note      = {84},
  abstract  = {This paper uses the URL word breaking task as an example to elaborate what we identify as crucialin designingstatistical natural language processing (NLP) algorithmsfor Web scale applications: (1) rudimentary multilingual capabilities to cope with the global nature of the Web, (2) multi-style modeling to handle diverse language styles seen in the Web contents, (3) fast adaptation to keep pace with the dynamic changes of the Web, (4) minimal heuristic assumptions for generalizability and robustness, and (5) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost. We first show that the state-of-the-art word breaking techniquescan be unified and generalized under the Bayesian minimum risk (BMR) framework that, using a Web scale N-gram, can meet the first three requirements. We discuss how the existing techniques can be viewed asintroducing additional assumptions to the basic BMR framework, and describe a generic yet efficient implementation called word synchronous beam search.Testing the framework and its implementation on a series of large scale experiments reveals the following. First, the language style used to build the modelplays a critical role in the word breaking task, and the most suitable for the URL word breaking task appears to be that ofthe document title where the best performance is obtained. Models created from other language styles, such as from document body, anchor text, and even queries, exhibit varying degrees of mismatch. Although all styles benefit from increasing modeling power which, in our experiments, corresponds tothe use of a higher order N-gram, the gain is most recognizable for the title model. The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models, but are less effective and, in many cases,lead to poorer performance than the matched model with minimal assumptions. For the matched model based on document titles, an accuracy rate of 97.18\% can already be achieved using simple trigram without any heuristics. Copyright © 2011 by the Association for Computing Machinery, Inc. (ACM).},
  annote    = {cited By 32},
  doi       = {10.1145/1963405.1963457},
  file      = {Wang et al. - 2011 - Web scale NLP A case study on URL word breaking.pdf:/home/akira/Zotero/storage/RIG4JJ95/Wang et al. - 2011 - Web scale NLP A case study on URL word breaking.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052123854&doi=10.1145%2f1963405.1963457&partnerID=40&md5=f0884c46e20c2b2cf89b5c9a1bb92dd2},
}

@InProceedings{guerrouj_automatic_2010,
  author    = {Guerrouj, Latifa},
  title     = {Automatic {Derivation} of {Concepts} {Based} on the {Analysis} of {Source} {Code} {Identifiers}},
  booktitle = {Proceedings of the 2010 17th {Working} {Conference} on {Reverse} {Engineering}},
  year      = {2010},
  series    = {{WCRE} '10},
  pages     = {301--304},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  note      = {11},
  annote    = { Cited by 4},
  doi       = {10.1109/WCRE.2010.45},
  file      = {2010_Guerrouj_Automatic Derivation of Concepts Based on the Analysis of Source Code Identifiers.pdf:/home/akira/Zotero/storage/E4Q9KZDC/2010_Guerrouj_Automatic Derivation of Concepts Based on the Analysis of Source Code Identifiers.pdf:application/pdf},
  isbn      = {978-0-7695-4123-5},
  keywords  = {Program Comprehension, Linguistic Analysis, Identifier Splitting, Software Quality},
  url       = {http://dx.doi.org/10.1109/WCRE.2010.45},
}

@InProceedings{fritzinger_how_2010,
  author    = {Fritzinger, Fabienne and Fraser, Alexander},
  title     = {How to {Avoid} {Burning} {Ducks}: {Combining} {Linguistic} {Analysis} and {Corpus} {Statistics} for {German} {Compound} {Processing}},
  booktitle = {Proceedings of the {Joint} {Fifth} {Workshop} on {Statistical} {Machine} {Translation} and {MetricsMATR}},
  year      = {2010},
  series    = {{WMT} '10},
  pages     = {224--234},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  note      = {event-place: Uppsala, Sweden 38},
  file      = {Fritzinger and Fraser - 2010 - How to Avoid Burning Ducks Combining Linguistic A.pdf:/home/akira/Zotero/storage/7N3M7US2/Fritzinger and Fraser - 2010 - How to Avoid Burning Ducks Combining Linguistic A.pdf:application/pdf},
  isbn      = {978-1-932432-71-8},
  url       = {http://dl.acm.org/citation.cfm?id=1868850.1868884},
}

@Article{kumar_sanskrit_2010,
  author   = {Kumar, A. and Mittal, V. and Kulkarni, A.},
  title    = {Sanskrit compound processor},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2010},
  volume   = {6465 LNAI},
  pages    = {57--69},
  note     = {56},
  abstract = {Sanskrit is very rich in compound formation. Typically a compound does not code the relation between its components explicitly. To understand the meaning of a compound, it is necessary to identify its components, discover the relations between them and finally generate a paraphrase of the compound. In this paper, we discuss the automatic segmentation and type identification of a compound using simple statistics that results from the manually annotated data. © 2010 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 3},
  doi      = {10.1007/978-3-642-17528-2_5},
  file     = {2010_Kumar et al_Sanskrit compound processor.pdf:/home/akira/Zotero/storage/7D73P8XH/2010_Kumar et al_Sanskrit compound processor.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651063736&doi=10.1007%2f978-3-642-17528-2_5&partnerID=40&md5=ba8a099b10cf2e63862a66f42bd24e2f},
}

@Article{zeman_using_2010,
  author   = {Zeman, D.},
  title    = {Using {TectoMT} as a preprocessing tool for phrase-based statistical machine translation},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2010},
  volume   = {6231 LNAI},
  pages    = {216--223},
  note     = {83},
  abstract = {We present a systematic comparison of preprocessing techniques for two language pairs: English-Czech and English-Hindi. The two target languages, although both belonging to the Indo-European language family, show significant differences in morphology, syntax and word order. We describe how TectoMT, a successful framework for analysis and generation of language, can be used as preprocessor for a phrase-based MT system.We compare the two language pairs and the optimal sets of source-language transformations applied to them. The following transformations are examples of possible preprocessing steps: lemmatization; retokenization, compound splitting; removing/adding words lacking counterparts in the other language; phrase reordering to resemble the target word order; marking syntactic functions. TectoMT, as well as all other tools and data sets we use, are freely available on the Web. © 2010 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-642-15760-8_28},
  file     = {2010_Zeman_Using TectoMT as a preprocessing tool for phrase-based statistical machine translation.pdf:/home/akira/Zotero/storage/LGBJMYQH/2010_Zeman_Using TectoMT as a preprocessing tool for phrase-based statistical machine translation.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049263442&doi=10.1007%2f978-3-642-15760-8_28&partnerID=40&md5=d8df80e3e3bbfd901f36029213a636fe},
}

@InProceedings{khaitan_data-driven_2009,
  author    = {Khaitan, Sanjeet and Das, Arumay and Gain, Sandeep and Sampath, Adithi},
  title     = {Data-driven {Compound} {Splitting} {Method} for {English} {Compounds} in {Domain} {Names}},
  booktitle = {Proceedings of the 18th {ACM} {Conference} on {Information} and {Knowledge} {Management}},
  year      = {2009},
  series    = {{CIKM} '09},
  pages     = {207--214},
  address   = {New York, NY, USA},
  publisher = {ACM},
  note      = {event-place: Hong Kong, China 21},
  doi       = {10.1145/1645953.1645982},
  file      = {Khaitan et al. - 2009 - Data-driven Compound Splitting Method for English .pdf:/home/akira/Zotero/storage/9IF4T54W/Khaitan et al. - 2009 - Data-driven Compound Splitting Method for English .pdf:application/pdf},
  isbn      = {978-1-60558-512-3},
  keywords  = {compound splitting, domain names},
  url       = {http://doi.acm.org/10.1145/1645953.1645982},
}

@InProceedings{enslen_mining_2009,
  author    = {Enslen, E. and Hill, E. and Pollock, L. and Vijay-Shanker, K.},
  title     = {Mining source code to automatically split identifiers for software analysis},
  booktitle = {2009 6th {IEEE} {International} {Working} {Conference} on {Mining} {Software} {Repositories}},
  year      = {2009},
  pages     = {71--80},
  note      = {46},
  abstract  = {Automated software engineering tools (e.g., program search, concern location, code reuse, quality assessment, etc.) increasingly rely on natural language information from comments and identifiers in code. The first step in analyzing words from identifiers requires splitting identifiers into their constituent words. Unlike natural languages, where space and punctuation are used to delineate words, identifiers cannot contain spaces. One common way to split identifiers is to follow programming language naming conventions. For example, Java programmers often use camel case, where words are delineated by uppercase letters or non-alphabetic characters. However, programmers also create identifiers by concatenating sequences of words together with no discernible delineation, which poses challenges to automatic identifier splitting. In this paper, we present an algorithm to automatically split identifiers into sequences of words by mining word frequencies in source code. With these word frequencies, our identifier splitter uses a scoring technique to automatically select the most appropriate partitioning for an identifier. In an evaluation of over 8000 identifiers from open source Java programs, our Samurai approach outperforms the existing state of the art techniques.},
  doi       = {10.1109/MSR.2009.5069482},
  file      = {2009_Enslen et al_Mining source code to automatically split identifiers for software analysis.pdf:/home/akira/Zotero/storage/PVKTE3UL/2009_Enslen et al_Mining source code to automatically split identifiers for software analysis.pdf:application/pdf},
  keywords  = {Natural languages, data mining, Java, software maintenance, Software maintenance, program diagnostics, Frequency, automatic split identifier, Information analysis, Open source software, Programming profession, Quality assessment, software analysis, Software quality, Software tools, source code mining, word frequency mining},
}

@Article{braga_algoritmos_2008,
  author = {Braga, Daniela Filipa Macedo Moreira da and {others}},
  title  = {Algoritmos de processamento da linguagem natural para sistemas de conversao texto-fala em português},
  year   = {2008},
  note   = {05},
  file   = {2008_Braga_others_Algoritmos de processamento da linguagem natural para sistemas de conversao texto-fala em português.pdf:/home/akira/Zotero/storage/C4KAGR32/2008_Braga_others_Algoritmos de processamento da linguagem natural para sistemas de conversao texto-fala em português.pdf:application/pdf},
}

@InProceedings{stymne_german_2008,
  author    = {Stymne, Sara},
  title     = {German {Compounds} in {Factored} {Statistical} {Machine} {Translation}},
  booktitle = {Proceedings of the 6th {International} {Conference} on {Advances} in {Natural} {Language} {Processing}},
  year      = {2008},
  series    = {{GoTAL} '08},
  pages     = {464--475},
  address   = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  note      = {event-place: Gothenburg, Sweden 34},
  doi       = {10.1007/978-3-540-85287-2_44},
  file      = {2008_Stymne_German Compounds in Factored Statistical Machine Translation.pdf:/home/akira/Zotero/storage/X9MBUY4N/German-compounds-in-factored-statistical-machine-translationLecture-Notes-in-Computer-Science-including-subseries-Lecture-Notes-in-Artificial-Intelligence-and-Lecture-Notes-in-Bioinformatics.pdf:application/pdf},
  isbn      = {978-3-540-85286-5},
  url       = {http://dx.doi.org/10.1007/978-3-540-85287-2_44},
}

@InProceedings{alfonseca_german_2008,
  author    = {Alfonseca, Enrique and Bilac, Slaven and Pharies, Stefan},
  title     = {German {Decompounding} in a {Difficult} {Corpus}},
  booktitle = {Computational {Linguistics} and {Intelligent} {Text} {Processing}},
  year      = {2008},
  editor    = {Gelbukh, Alexander},
  pages     = {128--139},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  note      = {35},
  abstract  = {Splitting compound words has proved to be useful in areas such as Machine Translation, Speech Recognition or Information Retrieval (IR). In the case of IR systems, they usually have to cope with noisy data, as user queries are usually written quickly and submitted without review. This work attempts at improving the current approaches for German decompounding when applied to query keywords. The results show an increase of more than 10\% in accuracy compared to other state-of-the-art methods.},
  file      = {Alfonseca et al. - 2008 - German Decompounding in a Difficult Corpus.pdf:/home/akira/Zotero/storage/233AY5DW/Alfonseca et al. - 2008 - German Decompounding in a Difficult Corpus.pdf:application/pdf},
  isbn      = {978-3-540-78135-6},
}

@InProceedings{stymne_processing_2008,
  author    = {Stymne, S. and Holmqvist, M.},
  title     = {Processing of {Swedish} compounds for phrase-based statistical machine translation},
  booktitle = {Proceedings of the 12th {European} {Association} for {Machine} {Translation} {Conference}, {EAMT} 2008},
  year      = {2008},
  pages     = {182--191},
  note      = {54},
  abstract  = {We investigated the effects of processing Swedish compounds for phrase-based SMT between Swedish and English. Compounds were split in a pre-processing step using an unsupervised empirical method. After translation into Swedish, compounds were merged, using a novel merging algorithm. We investigated two ways of handling compound parts, by marking them as compound parts or by normalizing them to a canonical form. We found that compound splitting did improve translation into Swedish, according to automatic metrics. For translation into English the results were not consistent across automatic metrics. However, error analysis of compound translation showed a small improvement in the systems that used splitting. The number of untranslated words in the English output was reduced by 50\%.},
  annote    = {cited By 6},
  file      = {Stymne and Holmqvist - 2008 - Processing of Swedish compounds for phrase-based s.pdf:/home/akira/Zotero/storage/F57RPWWV/Stymne and Holmqvist - 2008 - Processing of Swedish compounds for phrase-based s.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857535918&partnerID=40&md5=11ef39e004bb3623703d713985652303},
}

@InProceedings{holz_unsupervised_2008,
  author    = {Holz, Florian and Biemann, Chris},
  title     = {Unsupervised and {Knowledge}-free {Learning} of {Compound} {Splits} and {Periphrases}},
  booktitle = {Proceedings of the 9th {International} {Conference} on {Computational} {Linguistics} and {Intelligent} {Text} {Processing}},
  year      = {2008},
  series    = {{CICLing}'08},
  pages     = {117--127},
  address   = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  note      = {event-place: Haifa, Israel 77},
  file      = {Holz and Biemann - 2008 - Unsupervised and Knowledge-free Learning of Compou.pdf:/home/akira/Zotero/storage/6K9L3PNP/Holz and Biemann - 2008 - Unsupervised and Knowledge-free Learning of Compou.pdf:application/pdf},
  isbn      = {3-540-78134-X 978-3-540-78134-9},
  url       = {http://dl.acm.org/citation.cfm?id=1787578.1787592},
}

@InProceedings{bordag_unsupervised_2007,
  author    = {Bordag, S.},
  title     = {Unsupervised and knowledge-free morpheme segmentation and analysis},
  booktitle = {{CEUR} {Workshop} {Proceedings}},
  year      = {2007},
  volume    = {1173},
  note      = {78},
  abstract  = {This paper presents a revised version of an unsupervised and knowledge-free morpheme boundary detection algorithm based on letter successor variety (LSV) and a trie classifier [5]. Additionally a morphemic analysis based on contextual similarity provides knowledge about relatedness of the found morphs. For the boundary detection the challenge of increasing recall of found morphs while retaining a high precision is tackled by adding a compound splitter, iterating the LSV analysis and dividing the trie classifier into two distinctly applied clasifiers. The result is a significantly improved overall performance and a decreased reliance on corpus size. Further possible improvements and analyses are discussed.},
  annote    = {cited By 0},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921960169&partnerID=40&md5=af8fc25f3c4af981dbe962e6e760ad24},
}

@Article{pedersen_using_2007,
  author   = {Pedersen, B.S.},
  title    = {Using shallow linguistic analysis to improve search on {Danish} compounds},
  journal  = {Natural Language Engineering},
  year     = {2007},
  volume   = {13},
  number   = {1},
  pages    = {75--90},
  note     = {82},
  abstract = {In this paper we focus on a specific search-related query expansion topic, namely search on Danish compounds and expansion to some of their synonymous phrases. Compounds constitute a specific issue in search, in particular in languages where they are written in one word, as is the case for Danish and the other Scandinavian languages. For such languages, expansion of the query compound into separate lemmas is a way of finding the often frequent alternative synonymous phrases in which the content of a compound can also be expressed. However, it is crucial to note that the number of irrelevant hits is generally very high when using this expansion strategy. The aim of this paper is therefore to examine how we can obtain better search results on split compounds, partly by looking at the internal structure of the original compound, partly by analyzing the context in which the split compound occurs. In this context, we pursue two hypotheses: (1) that some categories of compounds are more likely to have synonymous 'split' counterparts than others; and (2) that search results where both the search words (obtained by splitting the compound) occur in the same noun phrase, are more likely to contain a synonymous phrase to the original compound query. The search results from 410 enhanced compound queries are used as a test bed for our experiments. On these search results, we perform a shallow linguistic analysis and introduce a new, linguistically based threshold for retrieved hits. The results obtained by using this strategy demonstrate that compound splitting combined with a shallow linguistic analysis focusing on the argument structure of the compound head as well as on the recognition of NPs, can improve search by substantially bringing down the number of irrelevant hits. © 2006 Cambridge University Press.},
  annote   = {cited By 0},
  doi      = {10.1017/S1351324906004256},
  file     = {2007_Pedersen_Using shallow linguistic analysis to improve search on Danish compounds.pdf:/home/akira/Zotero/storage/I64LBPLS/2007_Pedersen_Using shallow linguistic analysis to improve search on Danish compounds.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847289690&doi=10.1017%2fS1351324906004256&partnerID=40&md5=b20e47119434e911c56c063423f9c5b1},
}

@Article{popovic_statistical_2006-1,
  author   = {Popović, M. and Stein, D. and Ney, H.},
  title    = {Statistical machine translation of german compound words},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2006},
  volume   = {4139 LNAI},
  pages    = {616--624},
  note     = {70},
  abstract = {German compound words pose special problems to statistical machine translation systems: the occurence of each of the components in the training data is not sufficient for successful translation. Even if the compound itself has been seen during training, the system may not be capable of translating it properly into two or more words. If German is the target language, the system might generate only separated components or may not be capable of choosing the correct compound. In this work, we investigate and compare different strategies for the treatment of German compound words in statistical machine translation systems. For translation from German, we compare linguistic-based and corpusbased compound splitting. For translation into German, we investigate splitting and rejoining German compounds, as well as joining English potential components. Additionaly, we investigate word alignments enhanced with knowledge about the splitting points of German compounds. The translation quality is consistently improved by all methods for both translation directions. © Springer-Verlag Berlin Heidelberg 2006.},
  annote   = {cited By 20},
  file     = {Popović et al. - 2006 - Statistical machine translation of german compound.pdf:/home/akira/Zotero/storage/VNCE4PST/Popović et al. - 2006 - Statistical machine translation of german compound.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749674758&partnerID=40&md5=5a2ba01a1897f9b2a062138b3e9f05d3},
}

@Article{airio_word_2006,
  author   = {Airio, Eija},
  title    = {Word normalization and decompounding in mono- and bilingual {IR}},
  journal  = {Information Retrieval},
  year     = {2006},
  volume   = {9},
  number   = {3},
  pages    = {249--271},
  month    = jun,
  issn     = {1573-7659},
  note     = {86},
  abstract = {The present research studies the impact of decompounding and two different word normalization methods, stemming and lemmatization, on monolingual and bilingual retrieval. The languages in the monolingual runs are English, Finnish, German and Swedish. The source language of the bilingual runs is English, and the target languages are Finnish, German and Swedish. In the monolingual runs, retrieval in a lemmatized compound index gives almost as good results as retrieval in a decompounded index, but in the bilingual runs differences are found: retrieval in a lemmatized decompounded index performs better than retrieval in a lemmatized compound index. The reason for the poorer performance of indexes without decompounding in bilingual retrieval is the difference between the source language and target languages: phrases are used in English, while compounds are used instead of phrases in Finnish, German and Swedish. No remarkable performance differences could be found between stemming and lemmatization.},
  doi      = {10.1007/s10791-006-0884-2},
  file     = {Airio - 2006 - Word normalization and decompounding in mono- and .pdf:/home/akira/Zotero/storage/VN6RWNQW/Airio - 2006 - Word normalization and decompounding in mono- and .pdf:application/pdf},
  url      = {https://doi.org/10.1007/s10791-006-0884-2},
}

@Article{hedlund_dictionary-based_2004,
  author   = {Hedlund, Turid and Airio, Eija and Keskustalo, Heikki and Lehtokangas, Raija and Pirkola, Ari and Järvelin, Kalervo},
  title    = {Dictionary-{Based} {Cross}-{Language} {Information} {Retrieval}: {Learning} {Experiences} from {CLEF} 2000–2002},
  journal  = {Information Retrieval},
  year     = {2004},
  volume   = {7},
  number   = {1},
  pages    = {99--119},
  month    = jan,
  issn     = {1573-7659},
  note     = {25},
  abstract = {In this study the basic framework and performance analysis results are presented for the three year long development process of the dictionary-based UTACLIR system. The tests expand from bilingual CLIR for three language pairs Swedish, Finnish and German to English, to six language pairs, from English to French, German, Spanish, Italian, Dutch and Finnish, and from bilingual to multilingual. In addition, transitive translation tests are reported. The development process of the UTACLIR query translation system will be regarded from the point of view of a learning process. The contribution of the individual components, the effectiveness of compound handling, proper name matching and structuring of queries are analyzed. The results and the fault analysis have been valuable in the development process. Overall the results indicate that the process is robust and can be extended to other languages. The individual effects of the different components are in general positive. However, performance also depends on the topic set and the number of compounds and proper names in the topic, and to some extent on the source and target language. The dictionaries used affect the performance significantly.},
  doi      = {10.1023/B:INRT.0000009442.34054.55},
  file     = {Hedlund et al. - 2004 - Dictionary-Based Cross-Language Information Retrie.pdf:/home/akira/Zotero/storage/RS5Z25KX/Hedlund et al. - 2004 - Dictionary-Based Cross-Language Information Retrie.pdf:application/pdf},
  url      = {https://doi.org/10.1023/B:INRT.0000009442.34054.55},
}

@Article{hollink_monolingual_2004,
  author   = {Hollink, V. and Kamps, J. and Monz, C. and De Rijke, M.},
  title    = {Monolingual document retrieval for {European} languages},
  journal  = {Information Retrieval},
  year     = {2004},
  volume   = {7},
  number   = {1-2},
  pages    = {33--52},
  note     = {47},
  abstract = {Recent years have witnessed considerable advances in information retrieval for European languages other than English. We give an overview of commonly used techniques and we analyze them with respect to their impact on retrieval effectiveness. The techniques considered range from linguistically motivated techniques, such as morphological normalization and compound splitting, to knowledge-free approaches, such as n-gram indexing. Evaluations are carried out against data from the CLEF campaign, covering eight European languages. Our results show that for many of these languages a modicum of linguistic techniques may lead to improvements in retrieval effectiveness, as can the use of language independent techniques.},
  annote   = {cited By 54},
  file     = {Hollink et al. - 2004 - Monolingual document retrieval for European langua.pdf:/home/akira/Zotero/storage/MSUD8LUB/Hollink et al. - 2004 - Monolingual document retrieval for European langua.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-3843105784&partnerID=40&md5=f9c951e94a8a59c020fd5a0c4b3bbf46},
}

@Article{coster_selective_2004,
  author   = {Cöster, R. and Sahlgren, M. and Karlgren, J.},
  title    = {Selective compound splitting of swedish queries for boolean combinations of truncated terms},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2004},
  volume   = {3237},
  pages    = {337--344},
  note     = {59},
  abstract = {In languages that use compound words such as Swedish, it is often neccessary to split compound words when indexing documents or queries. One of the problems is that it is difficult to find constituents that express a concept similar to that expressed by the compound. The approach taken here is to expand a query with the leading constituents of the compound words. Every query term is truncated so as to increase recall by hopefully finding other compounds with the leading constituent as prefix. This approach increases recall in a rather uncontrolled way, so we use a Boolean quorum-level search method to rank documents both according to a tf-idf factor but also to the number of matching Boolean combinations. The Boolean combinations performed relatively well, taking into consideration that the queries were very short (maximum of five search terms). Also included in this paper are the results of two other methods we are currently working on in our lab; one for re-ranking search results on the basis of stylistic analysis of documents, and one for dimensionality reduction using Random Indexing. © Springer-Verlag 2004.},
  annote   = {cited By 3},
  file     = {Cöster et al. - 2004 - Selective compound splitting of swedish queries fo.pdf:/home/akira/Zotero/storage/KV3Q2RSP/Cöster et al. - 2004 - Selective compound splitting of swedish queries fo.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048871540&partnerID=40&md5=f5b2b1834f7cad7abe59755b8a103d03},
}

@InProceedings{ordelman_compound_2003,
  author    = {Ordelman, R. and Van Hessen, A. and De Jong, F.},
  title     = {Compound decomposition in {Dutch} large vocabulary speech recognition},
  booktitle = {{EUROSPEECH} 2003 - 8th {European} {Conference} on {Speech} {Communication} and {Technology}},
  year      = {2003},
  pages     = {225--228},
  note      = {16},
  abstract  = {This paper addresses compound splitting for Dutch in the context of broadcast news transcription. Language models were created using original text versions and text versions that were decomposed using a data-driven compound splitting algorithm. Language model performances were compared in terms of outof- vocabulary rates and word error rates in a real-world broadcast news transcription task. It was concluded that compound splitting does improve ASR performance. Best results were obtained when frequent compounds were not decomposed.},
  annote    = {cited By 23},
  file      = {2003_Ordelman et al_Compound decomposition in Dutch large vocabulary speech recognition.pdf:/home/akira/Zotero/storage/NNID7XFS/2003_Ordelman et al_Compound decomposition in Dutch large vocabulary speech recognition.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009170957&partnerID=40&md5=9373eec435a757ed1ac5b17f6d94fcd8},
}

@Article{libben_compound_2003,
  author   = {Libben, Gary and Gibson, Martha and Yoon, Yeo Bom and Sandra, Dominiek},
  title    = {Compound fracture: {The} role of semantic transparency and morphological headedness},
  journal  = {Brain and Language},
  year     = {2003},
  volume   = {84},
  number   = {1},
  pages    = {50 -- 64},
  issn     = {0093-934X},
  note     = {17},
  abstract = {This paper explores the role of semantic transparency in the representation and processing of English compounds. We focus on the question of whether semantic transparency is best viewed as a property of the entire multimorphemic string or as a property of constituent morphemes. Accordingly, we investigated the processing of English compound nouns that were categorized in terms of the semantic transparency of each of their constituents. Fully transparent such as bedroom are those in which the meanings of each of the constituents are transparently represented in the meaning of the compound as a whole. These compounds were contrasted with compounds such as strawberry, in which only the second constituent is semantically transparent, jailbird, in which only the first constituent is transparent, and hogwash, in which neither constituent is semantically transparent. We propose that significant insights can be achieved through such analysis of the transparency of individual morphemes. The two experiments that we report present evidence that both semantically transparent compounds and semantically opaque compounds show morphological constituency. The semantic transparency of the morphological head (the second constituent in a morphologically right-headed language such as English) was found to play a significant role in overall lexical decision latencies, in patterns of decomposition, and in the effects of stimulus repetition within the experiment.},
  annote   = {Brain and Language Special Issue},
  doi      = {https://doi.org/10.1016/S0093-934X(02)00520-5},
  file     = {2003_Libben et al_Compound fracture.pdf:/home/akira/Zotero/storage/QZDR5734/2003_Libben et al_Compound fracture.pdf:application/pdf},
  url      = {http://www.sciencedirect.com/science/article/pii/S0093934X02005205},
}

@InProceedings{koehn_empirical_2003,
  author    = {Koehn, Philipp and Knight, Kevin},
  title     = {Empirical {Methods} for {Compound} {Splitting}},
  booktitle = {Proceedings of the {Tenth} {Conference} on {European} {Chapter} of the {Association} for {Computational} {Linguistics} - {Volume} 1},
  year      = {2003},
  series    = {{EACL} '03},
  pages     = {187--193},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  note      = {event-place: Budapest, Hungary 28},
  doi       = {10.3115/1067807.1067833},
  file      = {Koehn and Knight - 2003 - Empirical Methods for Compound Splitting.pdf:/home/akira/Zotero/storage/X53YTB2J/Koehn and Knight - 2003 - Empirical Methods for Compound Splitting.pdf:application/pdf},
  isbn      = {1-333-56789-0},
  url       = {https://doi.org/10.3115/1067807.1067833},
}

@Article{hedlund_aspects_2001-1,
  author   = {Hedlund, T. and Pirkola, A. and Järvelin, K.},
  title    = {Aspects of {Swedish} {Morphology} and {Semantics} from the {Perspective} of {Mono}- and {Cross}-{Language} {Information} {Retrieval}},
  journal  = {Information Processing and Management},
  year     = {2001},
  volume   = {37},
  number   = {1},
  pages    = {147--161},
  note     = {09},
  abstract = {This paper analyzes the features of the Swedish language from the viewpoint of mono- and cross-language information retrieval (CLIR). The study was motivated by the fact that Swedish is known poorly from the IR perspective. This paper shows that Swedish has unique features, in particular gender features, the use of fogemorphemes in the formation of compound words, and a high frequency of homographic words. Especially in dictionary-based CLIR, correct word normalization and compound splitting are essential. It was shown in this study, however, that publicly available morphological analysis tools used for normalization and compound splitting have pitfalls that might decrease the effectiveness of IR and CLIR. A comparative study was performed to test the degree of lexical ambiguity in Swedish, Finnish and English. The results suggest that part-of-speech tagging might be useful in Swedish IR due to the high frequency of homographic words.},
  annote   = {cited By 30},
  doi      = {10.1016/S0306-4573(00)00024-8},
  file     = {Hedlund et al. - 2001 - Aspects of Swedish Morphology and Semantics from t.pdf:/home/akira/Zotero/storage/QHHH9KGW/Hedlund et al. - 2001 - Aspects of Swedish Morphology and Semantics from t.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035152106&doi=10.1016%2fS0306-4573%2800%2900024-8&partnerID=40&md5=7a27a4aaed911888c61fa3e179606b73},
}

@InProceedings{hedlund_bilingual_2001,
  author    = {Hedlund, Turid and Keskustalo, Heikki and Pirkola, Ari and Sepponen, Mikko and Järvelin, Kalervo},
  title     = {Bilingual {Tests} with {Swedish}, {Finnish}, and {German} {Queries}: {Dealing} with {Morphology}, {Compound} {Words}, and {Query} {Structure}},
  booktitle = {Cross-{Language} {Information} {Retrieval} and {Evaluation}},
  year      = {2001},
  editor    = {Peters, Carol},
  pages     = {210--223},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  note      = {13},
  abstract  = {We designed, implemented and evaluated an automated method for query construction for CLIR from Finnish, Swedish and German to English. This method seeks to automatically extract topical information from request sentences written in one of the source languages and to create a target language query, based on translations given by a translation dictionary. We paid particular attention to morphology, compound words and query structure. we tested this approach in the bilingual track of CLEF. All the source languages are compound languages, i.e., languages rich in compound words. A compound word refers to a multi-word expression where the component words are written together. Because source language request words may appear in various inflected forms not included in a translation dictionary, morphological normalization was used to aid dictionary translation. The query resulting from this process may be structured according to the translation alternatives of each source language word or remain as an unstructured word list.},
  annote    = { Citado por 24},
  file      = {2001_Hedlund et al_Bilingual Tests with Swedish, Finnish, and German Queries.pdf:/home/akira/Zotero/storage/IQMIVI8I/2001_Hedlund et al_Bilingual Tests with Swedish, Finnish, and German Queries.pdf:application/pdf},
  isbn      = {978-3-540-44645-3},
}

@Book{greghi_o_2001,
  title     = {O {Processo} de {Desenvolvimento} da {BDL}-{NILC}},
  publisher = {Série de Relatórios do NILC, NILC-TR-01-7. São Carlos, Outubro, 57p},
  year      = {2001},
  author    = {Greghi, Juliana Galvani and Martins, Ronaldo Teixeira and Nunes, Maria das Graças Volpe},
  note      = {50},
  file      = {Greghi et al. - 2001 - O Processo de Desenvolvimento da BDL-NILC.pdf:/home/akira/Zotero/storage/ZYPZUH9J/Greghi et al. - 2001 - O Processo de Desenvolvimento da BDL-NILC.pdf:application/pdf},
}

@Article{ranchhod_o_2001,
  author  = {Ranchhod, Elisabete Marques},
  title   = {O uso de dicionários e de autómatos finitos na representação lexical das línguas naturais},
  journal = {Tratamento das Línguas por Computador. Uma Introdução à Linguística Computacional e suas Aplicações},
  year    = {2001},
  pages   = {13--47},
  note    = {51},
  file    = {Ranchhod - 2001 - O uso de dicionários e de autómatos finitos na rep.PDF:/home/akira/Zotero/storage/K89MX9EH/Ranchhod - 2001 - O uso de dicionários e de autómatos finitos na rep.PDF:application/pdf},
}

@InProceedings{monz_university_2001,
  author    = {Monz, C. and De Rijke, M.},
  title     = {The university of {Amsterdam} at {CLEF} 2001},
  booktitle = {{CEUR} {Workshop} {Proceedings}},
  year      = {2001},
  volume    = {1167},
  note      = {73},
  abstract  = {This paper describes the official runs of our team for CLEF-2001. We took part in the monolingual task, for Dutch, German, and Italian. The focus of our experiments was on the effects of morphological analyses such as stemming and compound splitting on retrieval effectiveness. Confirming earlier reports on retrieval in compound splitting languages such as Dutch and German, we found improvements to be around 25\% for German and as much as 55\% for Dutch. For Italian, lexicon-based stemming resulted in gains of up to 25\%. Copyright © 2001 for the individual papers by the papers' authors.},
  annote    = {cited By 1},
  file      = {Monz and De Rijke - 2001 - The university of Amsterdam at CLEF 2001.pdf:/home/akira/Zotero/storage/LUUWZEM5/Monz and De Rijke - 2001 - The university of Amsterdam at CLEF 2001.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921953688&partnerID=40&md5=85d483b1ce4dbd8f4a9881095044adb8},
}

@InProceedings{kraaij_comparing_1998,
  author    = {Kraaij, Wessel and Pohlmann, Renée},
  title     = {Comparing the {Effect} of {Syntactic} vs. {Statistical} {Phrase} {Indexing} {Strategies} for {Dutch}},
  booktitle = {Proceedings of the {Second} {European} {Conference} on {Research} and {Advanced} {Technology} for {Digital} {Libraries}},
  year      = {1998},
  series    = {{ECDL} '98},
  pages     = {605--617},
  address   = {London, UK, UK},
  publisher = {Springer-Verlag},
  note      = {15},
  file      = {1998_Kraaij_Pohlmann_Comparing the Effect of Syntactic vs.pdf:/home/akira/Zotero/storage/CWSNUTKG/1998_Kraaij_Pohlmann_Comparing the Effect of Syntactic vs.pdf:application/pdf},
  isbn      = {3-540-65101-2},
  url       = {http://dl.acm.org/citation.cfm?id=646631.696685},
}

@Book{fibla_bilingual_nodate-2,
  title    = {Bilingual {Word} {Segmentation}},
  author   = {Fibla, Laia},
  note     = {102},
  abstract = {This project aims to model balanced bilinguals and monolinguals of 3 different languages, with language switching happening every other sentence, or every 100 sentences. We test the performance of different algorithms on word segmentation that represent different coginitve strategies that infants could be brining into the word segmentation task. This repository contains the collected corpora from English, Catalan and Spanish which are also freely available in CHILDES https://childes.talkbank.org As well as all the steps to process the data wich are descrived in the recipes of each language.},
  url      = {https://github.com/laiafr/bilingual_wordseg},
  urldate  = {2019-09-19},
}

@Book{baziotis_ekphrasis_nodate-2,
  title    = {Ekphrasis},
  author   = {Baziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos},
  note     = {108},
  abstract = {Ekphrasis is a text processing tool, geared towards text from social networks, such as Twitter or Facebook. Ekphrasis performs tokenization, word normalization, word segmentation (for splitting hashtags) and spell correction, using word statistics from 2 big corpora (english Wikipedia, twitter - 330mil english tweets).},
  url      = {https://github.com/cbaziotis/ekphrasis},
  urldate  = {2019-09-19},
}

@Misc{garbe_fast_nodate-2,
  author   = {Garbe, Wolf},
  title    = {Fast {Word} {Segmentation} of {Noisy} {Text}},
  note     = {110},
  abstract = {A string can be divided in several ways. Each distinct segmentation variant is called a composition. This article evaluates different types of word segmentation and propose several algorithms for each one of the basics concepts},
  language = {Inglês},
  url      = {https://towardsdatascience.com/fast-word-segmentation-for-noisy-text-2c2c41f9e8da},
  urldate  = {2019-09-19},
}

@Book{lanxiaowei_ik-analyzer_nodate-2,
  title    = {{IK}-{Analyzer}},
  author   = {{Lanxiaowei}},
  note     = {112},
  abstract = {The source code of IK-Analyzer,Supported Arabic numerals and Chinese characters, Chinese figures and Chinese characters and Arabic Numbers and English letters of the word segmentation},
  url      = {https://github.com/yida-lxw/IK},
  urldate  = {2019-09-19},
}

@Book{jenks_python_nodate-2,
  title    = {Python {Word} {Segmentation}},
  author   = {Jenks, Grant},
  note     = {130},
  abstract = {Based on code from the chapter "Natural Language Corpus Data" by Peter Norvig from the book "Beautiful Data" (Segaran and Hammerbacher, 2009). Data files are derived from the Google Web Trillion Word Corpus, as described by Thorsten Brants and Alex Franz, and distributed by the Linguistic Data Consortium. This module contains only a subset of that data. The unigram data includes only the most common 333,000 words. Similarly, bigram data includes only the most common 250,000 phrases. Every word and phrase is lowercased with punctuation removed.},
  url      = {https://github.com/grantjenks/python-wordsegment},
  urldate  = {2019-09-19},
}

@TechReport{norvig_statistical_nodate-2,
  author   = {Norvig, Peter},
  title    = {Statistical {Natural} {Language} {Processing} in {Python}. or {How} {To} {Do} {Things} {With} {Words}. {And} {Counters}. or {Everything} {I} {Needed} to {Know} {About} {NLP} {I} learned {From} {Sesame} {Street}. {Except} {Kneser}-{Ney} {Smoothing}. {The} {Count} {Didn}'t {Cover} {That}.},
  type     = {Relaçtório {Técnico}},
  note     = {137},
  abstract = {In this notebook Peter Norvig show us how to work with statistical natural language processing. With a theorical/pratical approach, Norvig shows the fundamentals about word segmentation using python.},
  language = {Inglês},
  url      = {https://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb},
  urldate  = {2019-09-19},
}

@Book{vittal_text-reconstruction_nodate-2,
  title    = {Text-{Reconstruction}},
  author   = {Vittal, Sujay},
  note     = {140},
  abstract = {This project involves two tasks - word segmentation and vowel insertion. Word segmentation often comes up when processing many non-English languages, in which words might not be flanked by spaces on either end, such as written Chinese or long compound German words. Vowel insertion is relevant for languages like Arabic or Hebrew, where modern script eschews notations for vowel sounds and the human reader infers them from context. The goal of Vowel Insertion is to insert vowels back into segmented words in a way that maximizes sentence fluency (i.e., minimizes sentence cost). A bigram cost function is used.},
  url      = {https://github.com/sujay-vittal/Text-Reconstruction},
  urldate  = {2019-09-19},
}

@Book{frcchang_zpar_nodate-2,
  title    = {{ZPar}},
  author   = {{frcchang}},
  note     = {154},
  abstract = {ZPar statistical parser. Universal language support (depending on the availability of training data), with language-specific features for Chinese and English. Currently support word segmentation, POS tagging, dependency and phrase-structure parsing.},
  url      = {https://github.com/frcchang/zpar},
  urldate  = {2019-09-19},
}

@Article{zhu_systematic_2019-1,
  author   = {Zhu, Yi and Vulić, Ivan and Korhonen, Anna},
  title    = {A {Systematic} {Study} of {Leveraging} {Subword} {Information} for {Learning} {Word} {Representations}},
  journal  = {arxiv:1904.07994},
  year     = {2019},
  note     = {97},
  file     = {Full Text:/home/akira/Zotero/storage/3SR5YGZX/Zhu et al. - 2019 - A Systematic Study of Leveraging Subword Informati.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1904.07994v2},
}

@Article{nguyen_learning_2019-3,
  author   = {Nguyen, Hien T. and Duong, Phuc H. and Cambria, Erik},
  title    = {Learning short-text semantic similarity with word embeddings and external knowledge sources},
  journal  = {Knowledge-Based Systems},
  year     = {2019},
  volume   = {182},
  pages    = {104842},
  issn     = {0950-7051},
  note     = {121},
  abstract = {We present a novel method based on interdependent representations of short texts for determining their degree of semantic similarity. The method represents each short text as two dense vectors: the former is built using the word-to-word similarity based on pre-trained word vectors, the latter is built using the word-to-word similarity based on external sources of knowledge. We also developed a preprocessing algorithm that chains coreferential named entities together and performs word segmentation to preserve the meaning of phrasal verbs and idioms. We evaluated the proposed method on three popular datasets, namely Microsoft Research Paraphrase Corpus, STS2015 and P4PIN, and obtained state-of-the-art results on all three without using prior knowledge of natural language, e.g., part-of-speech tags or parse tree, which indicates the interdependent representations of short text pairs are effective and efficient for semantic textual similarity tasks.},
  doi      = {10/gf8qds},
  keywords = {Paraphrase identification, Semantic textual similarity, Sentence similarity, Short text similarity},
  url      = {http://www.sciencedirect.com/science/article/pii/S095070511930317X},
}

@Article{nguyen_learning_2019-4,
  author   = {Nguyen, H.T. and Duong, P.H. and Cambria, E.},
  title    = {Learning short-text semantic similarity with word embeddings and external knowledge sources},
  journal  = {Knowledge-Based Systems},
  year     = {2019},
  volume   = {182},
  note     = {120},
  abstract = {We present a novel method based on interdependent representations of short texts for determining their degree of semantic similarity. The method represents each short text as two dense vectors: the former is built using the word-to-word similarity based on pre-trained word vectors, the latter is built using the word-to-word similarity based on external sources of knowledge. We also developed a preprocessing algorithm that chains coreferential named entities together and performs word segmentation to preserve the meaning of phrasal verbs and idioms. We evaluated the proposed method on three popular datasets, namely Microsoft Research Paraphrase Corpus, STS2015 and P4PIN, and obtained state-of-the-art results on all three without using prior knowledge of natural language, e.g., part-of-speech tags or parse tree, which indicates the interdependent representations of short text pairs are effective and efficient for semantic textual similarity tasks. © 2019 Elsevier B.V.},
  annote   = {cited By 0},
  doi      = {10/gf8qds},
  file     = {Nguyen et al. - 2019 - Learning short-text semantic similarity with word .pdf:/home/akira/Zotero/storage/8295KYQ5/Nguyen et al. - 2019 - Learning short-text semantic similarity with word .pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069875471&doi=10.1016%2fj.knosys.2019.07.013&partnerID=40&md5=79164e89f20b11d4c4cf48b954deaae2},
}

@InProceedings{moreau_multilingual_2019-2,
  author    = {Moreau, E. and Vogel, C.},
  title     = {Multilingual word segmentation: {Training} many language-specific tokenizers smoothly thanks to the universal dependencies corpus},
  booktitle = {{LREC} 2018 - 11th {International} {Conference} on {Language} {Resources} and {Evaluation}},
  year      = {2019},
  pages     = {1119--1127},
  note      = {125},
  abstract  = {This paper describes how a tokenizer can be trained from any dataset in the Universal Dependencies 2.1 corpus (UD2) (Nivre et al., 2017). A software tool, which relies on Elephant (Evang et al., 2013) to perform the training, is also made available. Beyond providing the community with a large choice of language-specific tokenizers, we argue in this paper that: (1) tokenization should be considered as a supervised task; (2) language scalability requires a streamlined software engineering process across languages. © LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.},
  annote    = {cited By 1},
  file      = {Moreau and Vogel - 2019 - Multilingual word segmentation Training many lang.pdf:/home/akira/Zotero/storage/2I4HQFVQ/Moreau and Vogel - 2019 - Multilingual word segmentation Training many lang.pdf:application/pdf},
  keywords  = {⛔ No DOI found},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059903761&partnerID=40&md5=0d85521a7ec28ece3d5e35165d639dda},
}

@Article{smith_82_2018-1,
  author   = {Smith, Aaron and Bohnet, Bernd and Lhoneux, Miryam de and Nivre, Joakim and Shao, Yan and Stymne, Sara},
  title    = {82 {Treebanks}, 34 {Models}: {Universal} {Dependency} {Parsing} with {Multi}-{Treebank} {Models}},
  journal  = {arxiv:1809.02237},
  year     = {2018},
  note     = {88},
  file     = {Full Text:/home/akira/Zotero/storage/LQQQKTE7/Smith et al. - 2018 - 82 Treebanks, 34 Models Universal Dependency Pars.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1809.02237v1},
}

@Article{wang_altas_2018-2,
  author   = {Wang, X. and Gao, C. and Cao, J. and Lin, K. and Du, W. and Yang, Z.},
  title    = {{ALTAS}: {An} intelligent text analysis system based on knowledge graphs},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2018},
  volume   = {10987 LNCS},
  pages    = {466--470},
  note     = {99},
  abstract = {This paper presents an intelligent text analysis system, called ALTAS, to support various text analysis tasks such as statistics analysis, sentiment analysis, text classification, and text clustering. The system contains four main components: knowledge graphs, text processing, text analysis and intelligent report. First, the system has built a semantic-rich knowledge base using several knowledge graph resources. A novel text processing and analysis framework based on knowledge graphs is developed and implemented. Given a text dataset, the text processing phase will do data cleaning, word segmentation and feature extraction for it. With the extracted features, the text analysis phase allows users to select a text mining task. We have implemented the proposed novel algorithm and several typical algorithms for each task. If users select multiple algorithms for the task, the intelligent report phase will automatically generate comparison results for users. Especially, the intelligent report phase also provides users a paper summary generating function on text mining problems. © Springer International Publishing AG, part of Springer Nature 2018.},
  annote   = {cited By 0},
  doi      = {10/gf8qdz},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050537718&doi=10.1007%2f978-3-319-96890-2_40&partnerID=40&md5=671150c35b5218bda2fe429115d446d7},
}

@Article{doval_comparing_2018-1,
  author  = {Doval, Yerai and Gómez-Rodríguez, Carlos},
  title   = {Comparing neural- and {N}-gram-based language models for word segmentation},
  journal = {Journal of the Association for Information Science and Technology},
  year    = {2018},
  volume  = {70},
  number  = {2},
  pages   = {187--197},
  note    = {105},
  doi     = {10/gfs6rd},
  file    = {Full Text:/home/akira/Zotero/storage/KSU5WPLA/Doval e Gómez-Rodríguez - 2018 - Comparing neural- and N-gram-based language models.pdf:application/pdf},
  url     = {https://doi.org/10.1002%2Fasi.24082},
}

@Article{kawakami_learning_2018-1,
  author   = {Kawakami, Kazuya and Dyer, Chris and Blunsom, Phil},
  title    = {Learning to {Discover}, {Ground} and {Use} {Words} with {Segmental} {Neural} {Language} {Models}},
  journal  = {arxiv:1811.09353},
  year     = {2018},
  note     = {122},
  file     = {Full Text:/home/akira/Zotero/storage/RALC8BPS/Kawakami et al. - 2018 - Learning to Discover, Ground and Use Words with Se.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1811.09353v2},
}

@Article{machacek_morphological_2018-1,
  author   = {Macháček, Dominik and Vidra, Jonáš and Bojar, Ondřej},
  title    = {Morphological and {Language}-{Agnostic} {Word} {Segmentation} for {NMT}},
  journal  = {arxiv:1806.05482},
  year     = {2018},
  note     = {124},
  file     = {Full Text:/home/akira/Zotero/storage/8UX99ETR/Macháček et al. - 2018 - Morphological and Language-Agnostic Word Segmentat.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1806.05482v1},
}

@Article{zhang_research_2018-3,
  author   = {Zhang, Z. and Bi, X.},
  title    = {Research and {Experiment} of {Intelligent} {Natural} {Language} {Processing} {Algorithms}},
  journal  = {Wireless Personal Communications},
  year     = {2018},
  volume   = {102},
  number   = {4},
  pages    = {2927--2939},
  note     = {131},
  abstract = {Natural language processing is mainly divided into two parts: speech processing and word processing. The level of word processing is mainly studied. Natural language processing is divided into lexical analysis, syntax analysis and semantic analysis. Aiming at the scope of the language ambiguity and thesaurus in the field of smart home, the maximal matching algorithm is used to segment the natural language. Then, through the way of template matching, semantic comprehension finally forms the code form that can control the home node. In the system applied in this paper, the speech is processed into words through the existing voice input function of the mobile terminal. Then, the control instruction is obtained through the language processing method. The processed data is communicated to the server via socket. The server sends the data to the home node through the Zigbee protocol. Finally, control of home appliances is achieved. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
  annote   = {cited By 0},
  doi      = {10/gf8qdp},
  file     = {Zhang and Bi - 2018 - Research and Experiment of Intelligent Natural Lan.pdf:/home/akira/Zotero/storage/NLQBEYVD/Zhang and Bi - 2018 - Research and Experiment of Intelligent Natural Lan.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040677725&doi=10.1007%2fs11277-018-5316-2&partnerID=40&md5=73b8734ca4c3841c9c0e63013dc69c05},
}

@InProceedings{brito_self-organizing_2018-1,
  author    = {Brito, Raphael C. and Bassani, Hansenclever F.},
  title     = {Self-{Organizing} {Maps} with {Variable} {Input} {Length} for {Motif} {Discovery} and {Word} {Segmentation}},
  booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
  year      = {2018},
  month     = jul,
  publisher = {IEEE},
  note      = {132},
  doi       = {10/gf8qdn},
  file      = {Brito e Bassani - 2018 - Self-Organizing Maps with Variable Input Length fo.pdf:/home/akira/Zotero/storage/FEGR2L2Y/Brito e Bassani - 2018 - Self-Organizing Maps with Variable Input Length fo.pdf:application/pdf},
  url       = {https://doi.org/10.1109%2Fijcnn.2018.8489090},
}

@Article{shao_universal_2018-1,
  author   = {Shao, Yan and Hardmeier, Christian and Nivre, Joakim},
  title    = {Universal {Word} {Segmentation}: {Implementation} and {Interpretation}},
  journal  = {arxiv:1807.02974},
  year     = {2018},
  note     = {143},
  file     = {Full Text:/home/akira/Zotero/storage/NH2QJWD7/Shao et al. - 2018 - Universal Word Segmentation Implementation and In.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1807.02974v1},
}

@Article{godard_unsupervised_2018-1,
  author   = {Godard, Pierre and Zanon-Boito, Marcely and Ondel, Lucas and Berard, Alexandre and Yvon, François and Villavicencio, Aline and Besacier, Laurent},
  title    = {Unsupervised {Word} {Segmentation} from {Speech} with {Attention}},
  journal  = {arxiv:1806.06734},
  year     = {2018},
  note     = {147},
  file     = {Full Text:/home/akira/Zotero/storage/CICSYAQQ/Godard et al. - 2018 - Unsupervised Word Segmentation from Speech with At.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1806.06734v1},
}

@Article{shao_cross-lingual_2017-1,
  author   = {Shao, Yan},
  title    = {Cross-lingual {Word} {Segmentation} and {Morpheme} {Segmentation} as {Sequence} {Labelling}},
  journal  = {arxiv:1709.03756},
  year     = {2017},
  note     = {106},
  file     = {Full Text:/home/akira/Zotero/storage/9LDU4SXF/Shao - 2017 - Cross-lingual Word Segmentation and Morpheme Segme.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1709.03756v1},
}

@Article{williams_is_2017-1,
  author   = {Williams, Jake Ryland and Santia, Giovanni C.},
  title    = {Is space a word, too?},
  journal  = {arxiv:1710.07729},
  year     = {2017},
  note     = {117},
  file     = {Full Text:/home/akira/Zotero/storage/ZRG26WUV/Williams e Santia - 2017 - Is space a word, too.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1710.07729v1},
}

@InProceedings{gao_modeling_2017-2,
  author    = {Gao, K. and Zhang, S.-S. and Su, S. and Li, M.},
  title     = {Modeling on evaluation object extraction in e-commerce corpus based on semantic feature},
  booktitle = {Proceedings of 2016 8th {International} {Conference} on {Modelling}, {Identification} and {Control}, {ICMIC} 2016},
  year      = {2017},
  pages     = {434--438},
  note      = {123},
  abstract  = {As a newly shopping tool, electronic commerce has been drawing more and more attention of researchers. According to the characteristics of comments diversity, it is necessary to extract evaluation object which is an important component of sentiment information. This paper explores Conditional Random Field (CRF) to do evaluation objects extraction. After observing generally used features in sentiment extraction, this paper conclude all the features into four categories, i.e. word Segmentation, Part-of-speech Tagging (POS), Dependency Parsing, Semantic Dependency Parsing. What's more, focusing on the introduction of new feature semantic dependency is a very vital item in our research. In the experiment, we examine the various features and combinations in the extraction task performance, and make a detailed comparative study. The experimental results confirm that adding the feature of semantic dependency has better performance in terms of the evaluation object extraction. © 2016 University of MEDEA, Algeria.},
  annote    = {cited By 0},
  doi       = {10/gf8qdt},
  file      = {Gao et al. - 2017 - Modeling on evaluation object extraction in e-comm.pdf:/home/akira/Zotero/storage/TWYI7WVT/Gao et al. - 2017 - Modeling on evaluation object extraction in e-comm.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011290552&doi=10.1109%2fICMIC.2016.7804151&partnerID=40&md5=65db6606157eeccd05cb6708dd9e10d9},
}

@Article{song_named_2017-2,
  author   = {Song, S. and Zhang, N. and Huang, H.},
  title    = {Named entity recognition based on conditional random fields},
  journal  = {Cluster Computing},
  year     = {2017},
  pages    = {1--12},
  note     = {126},
  abstract = {Named entity recognition (NER) is one of the fundamental problems in many natural language processing applications and the study on NER has great significance. Combining words segmentation and parts of speech analysis, the paper proposes a new NER method based on conditional random fields considering the graininess of candidate entities. The recognition granularity can be divided into two levels: word-based and character-based. We use segmented text to extract characteristics according to the characteristic templates which had been trained in the training phase, and then calculate (Formula presented.) to get the best result from the input sequence. The paper valuates the algorithm for different graininess on large-scale corpus experimentally, and the results show that this method has high research value and feasibility. © 2017 Springer Science+Business Media, LLC},
  annote   = {cited By 3; Article in Press},
  doi      = {10/gf8qdr},
  file     = {Song et al. - 2017 - Named entity recognition based on conditional rand.pdf:/home/akira/Zotero/storage/P4Y8HTXI/Song et al. - 2017 - Named entity recognition based on conditional rand.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029009712&doi=10.1007%2fs10586-017-1146-3&partnerID=40&md5=8cf127b4e560b9faaf5042dc71ed7239},
}

@Article{yang_neural_2017-1,
  author   = {Yang, Jie and Zhang, Yue and Dong, Fei},
  title    = {Neural {Word} {Segmentation} with {Rich} {Pretraining}},
  journal  = {arxiv:1704.08960},
  year     = {2017},
  note     = {128},
  file     = {Full Text:/home/akira/Zotero/storage/L4N2EHEJ/Yang et al. - 2017 - Neural Word Segmentation with Rich Pretraining.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1704.08960v1},
}

@Article{chen_incremental_2016-1,
  author   = {Chen, Ruey-Cheng},
  title    = {Incremental {Learning} for {Fully} {Unsupervised} {Word} {Segmentation} {Using} {Penalized} {Likelihood} and {Model} {Selection}},
  journal  = {arxiv:1607.05822},
  year     = {2016},
  note     = {114},
  file     = {Full Text:/home/akira/Zotero/storage/JFMPUAYM/Chen - 2016 - Incremental Learning for Fully Unsupervised Word S.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1607.05822v2},
}

@InProceedings{matsumoto_sensibility_2016-2,
  author    = {Matsumoto, K. and Yoshida, M. and Kita, K.},
  title     = {Sensibility estimation method for youth slang by using sensibility co-occurrence feature vector obtained from microblog},
  booktitle = {Proceedings of 2015 {IEEE} {International} {Conference} on {Computer} and {Communications}, {ICCC} 2015},
  year      = {2016},
  pages     = {473--478},
  note      = {134},
  abstract  = {Social networking sites such as Twitter provide more opportunities to express what people think or intend in short text. In short text, abbreviations such as "ASAP" or "joinus" and emoticons are often used. Because these expressions are not registered into the existing dictionaries, these are analyzed as unknown expressions. That can be a bottleneck for improving accuracy of reputation analysis in text mining. To use context for unknown word clustering is a major method, however, it usually requires word segmentation process and it has weakness for split errors of unknown expressions such as youth slang. In this paper, we proposed a method to obtain the appropriate context even though unknown expressions cause split errors and estimate sensibility expressed in the text. Because the dimensions of the obtained context vector were enormous, we also proposed a method to create a feature vector based on the co-occurrence of the sensibility words as simple expression with low dimension. As an evaluation experiment, the proposed method showed certain accuracy even with the small training data. © 2015 IEEE.},
  annote    = {cited By 0},
  doi       = {10/gf8qdm},
  file      = {Matsumoto et al. - 2016 - Sensibility estimation method for youth slang by u.pdf:/home/akira/Zotero/storage/D5FACJLN/Matsumoto et al. - 2016 - Sensibility estimation method for youth slang by u.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963939887&doi=10.1109%2fCompComm.2015.7387618&partnerID=40&md5=4a1d165f66f914a0b90d4bd3b4765816},
}

@Article{xia_word_2016-1,
  author   = {Xia, Qingrong and Li, Zhenghua and Chao, Jiayuan and Zhang, Min},
  title    = {Word {Segmentation} on {Micro}-blog {Texts} with {External} {Lexicon} and {Heterogeneous} {Data}},
  journal  = {arxiv:1608.01448},
  year     = {2016},
  note     = {152},
  file     = {Full Text:/home/akira/Zotero/storage/MLA5NMUT/Xia et al. - 2016 - Word Segmentation on Micro-blog Texts with Externa.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1608.01448v2},
}

@InProceedings{qian_transition-based_2015-2,
  author    = {Qian, T. and Zhang, Y. and Zhang, M. and Ren, Y. and Ji, D.},
  title     = {A transition-based model for joint segmentation, {POS}-tagging and normalization},
  booktitle = {Conference {Proceedings} - {EMNLP} 2015: {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  year      = {2015},
  pages     = {1837--1846},
  note      = {98},
  abstract  = {We propose a transition-based model for joint word segmentation, POS tagging and text normalization. Different from previous methods, the model can be trained on standard text corpora, overcoming the lack of annotated microblog corpora. To evaluate our model, we develop an annotated corpus based on microblogs. Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02\%, compared to the traditional approach. © 2015 Association for Computational Linguistics.},
  annote    = {cited By 11},
  doi       = {10/gf8qd7},
  file      = {Qian et al. - 2015 - A transition-based model for joint segmentation, P.pdf:/home/akira/Zotero/storage/5AFV9JGI/Qian et al. - 2015 - A transition-based model for joint segmentation, P.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959905712&partnerID=40&md5=00d6ac7fdc06bd2b83d3dfb4ec314f4a},
}

@Article{wang_improved_2015-2,
  author   = {Wang, H. and Han, X. and Liu, L. and Song, W. and Yuan, M.},
  title    = {An improved unsupervised approach to word segmentation},
  journal  = {China Communications},
  year     = {2015},
  volume   = {12},
  number   = {7},
  pages    = {82--95},
  month    = jul,
  note     = {100},
  abstract = {ESA is an unsupervised approach to word segmentation previously proposed by Wang, which is an iterative process consisting of three phases: Evaluation, Selection and Adjustment. In this article, we propose ExESA, the extension of ESA. In ExESA, the original approach is extended to a 2-pass process and the ratio of different word lengths is introduced as the third type of information combined with cohesion and separation. A maximum strategy is adopted to determine the best segmentation of a character sequence in the phrase of Selection. Besides, in Adjustment, ExESA re-evaluates separation information and individual information to overcome the overestimation frequencies. Additionally, a smoothing algorithm is applied to alleviate sparseness. The experiment results show that ExESA can further improve the performance and is time-saving by properly utilizing more information from un-annotated corpora. Moreover, the parameters of ExESA can be predicted by a set of empirical formulae or combined with the minimum description length principle.},
  doi      = {10/gf8qd5},
  file     = {Wang et al. - 2015 - An improved unsupervised approach to word segmenta.pdf:/home/akira/Zotero/storage/YMJEGQIS/Wang et al. - 2015 - An improved unsupervised approach to word segmenta.pdf:application/pdf},
  keywords = {text analysis, natural language processing, Uncertainty, Accuracy, word segmentation, unsupervised learning, iterative methods, Entropy, Smoothing methods, character sequence, character sequence segmentation, cohesion, ExESA, Frequency measurement, improved unsupervised approach, iterative process, Length measurement, maximum strategy, minimum description length principle, overestimation frequency, Prediction algorithms, separation information, smoothing algorithm, smoothing methods, word length},
}

@Article{wang_breaking_2015-1,
  author   = {Wang, Wei and Shirley, Kenneth},
  title    = {Breaking {Bad}: {Detecting} malicious domains using word segmentation},
  journal  = {arxiv:1506.04111},
  year     = {2015},
  note     = {104},
  file     = {Full Text:/home/akira/Zotero/storage/NC3L4SL3/Wang e Shirley - 2015 - Breaking Bad Detecting malicious domains using wo.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1506.04111v1},
}

@InProceedings{takahashi_keyboard_2015-2,
  author    = {Takahashi, F. and Mori, S.},
  title     = {Keyboard logs as natural annotations for word segmentation},
  booktitle = {Conference {Proceedings} - {EMNLP} 2015: {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  year      = {2015},
  pages     = {1186--1196},
  note      = {118},
  abstract  = {In this paper we propose a framework to improve word segmentation accuracy using input method logs. An input method is software used to type sentences in languages which have far more characters than the number of keys on a keyboard. The main contributions of this paper are: 1) an input method server that proposes word candidates which are not included in the vocabulary, 2) a publicly usable input method that logs user behavior (like typing and selection of word candidates), and 3) a method for improving word segmentation by using these logs. We conducted word segmentation experiments on tweets from Twitter, and showed that our method improves accuracy in this domain. Our method itself is domain-independent and only needs logs from the target domain. © 2015 Association for Computational Linguistics.},
  annote    = {cited By 2},
  doi       = {10/gf8qd6},
  file      = {Takahashi and Mori - 2015 - Keyboard logs as natural annotations for word segm.pdf:/home/akira/Zotero/storage/I7RWW6WT/Takahashi and Mori - 2015 - Keyboard logs as natural annotations for word segm.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959884109&partnerID=40&md5=b951aa277fd249c48fa0691e5b63d495},
}

@Article{sennrich_neural_2015-1,
  author   = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  title    = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
  journal  = {arxiv:1508.07909},
  year     = {2015},
  note     = {127},
  file     = {Full Text:/home/akira/Zotero/storage/IHCFSY4E/Sennrich et al. - 2015 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1508.07909v5},
}

@Article{nivre_towards_2015-3,
  author   = {Nivre, J.},
  title    = {Towards a universal grammar for natural language processing},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2015},
  volume   = {9041},
  pages    = {3--16},
  note     = {142},
  abstract = {Universal Dependencies is a recent initiative to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. In this paper, I outline the motivation behind the initiative and explain how the basic design principles follow from these requirements. I then discuss the different components of the annotation standard, including principles for word segmentation, morphological annotation, and syntactic annotation. I conclude with some thoughts on the challenges that lie ahead. © Springer International Publishing Switzerland 2015.},
  annote   = {cited By 26},
  doi      = {10/f3mzxj},
  file     = {Nivre - 2015 - Towards a universal grammar for natural language p.pdf:/home/akira/Zotero/storage/DUAJFYVL/Nivre - 2015 - Towards a universal grammar for natural language p.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942673144&doi=10.1007%2f978-3-319-18111-0_1&partnerID=40&md5=0c36bdb69403326297326f303bf15cc2},
}

@Article{liang_detecting_2014-1,
  author   = {Liang, Wang and KaiYong, Zhao},
  title    = {Detecting "protein words" through unsupervised word segmentation},
  journal  = {arxiv:1404.6866},
  year     = {2014},
  note     = {107},
  file     = {Full Text:/home/akira/Zotero/storage/SQY5UMD9/Liang e KaiYong - 2014 - Detecting protein words through unsupervised wor.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1404.6866v6},
}

@InProceedings{zhu_exploration_2014-2,
  author    = {Zhu, L. and Li, H. and Wang, S. and Li, C.},
  title     = {Exploration and development of text knowledge extraction},
  booktitle = {Information {Technology} and {Computer} {Application} {Engineering} - {Proceedings} of the 2013 {International} {Conference} on {Information} {Technology} and {Computer} {Application} {Engineering}, {ITCAE} 2013},
  year      = {2014},
  pages     = {79--83},
  note      = {109},
  abstract  = {Text knowledge extraction technology has been applied in many fields, but few practices in the field of petroleum exploration and development. In this paper,we comprehensively utilize a statistical and natural language understanding technology to extract knowledge from the articles in the field of petroleum exploration and development. First of all, we get the key words and the core sentences containing article process model. Then after the word segmentation and phrase recognition, we use semantic templates to match and extract semantic information from the key sentences. The experimental results show that this method achieves 70\% accuracy rate in keywords spotting and process model extraction. © 2014 Taylor \& Francis Group, London.},
  annote    = {cited By 0},
  keywords  = {⛔ No DOI found},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900429048&partnerID=40&md5=2427896a84c7d70a080a58e189d138f1},
}

@InProceedings{clouet_splitting_2014-1,
  author = {Clouet, Elizaveta Loginova and Daille, Béatrice},
  title  = {Splitting of compound terms in non-prototypical compounding languages},
  year   = {2014},
  note   = {136},
  doi    = {10/gf8qdv},
  file   = {Clouet and Daille - 2014 - Splitting of compound terms in non-prototypical co.pdf:/home/akira/Zotero/storage/6N3S96V2/Clouet and Daille - 2014 - Splitting of compound terms in non-prototypical co.pdf:application/pdf},
}

@InProceedings{pate_syllable_2014-2,
  author    = {Pate, J.K. and Johnson, M.},
  title     = {Syllable weight encodes mostly the same information for english word segmentation as dictionary stress},
  booktitle = {{EMNLP} 2014 - 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {Proceedings} of the {Conference}},
  year      = {2014},
  pages     = {844--853},
  note      = {138},
  abstract  = {Stress is a useful cue for English word segmentation. A wide range of computational models have found that stress cues enable a 2-10\% improvement in segmentation accuracy, depending on the kind of model, by using input that has been annotated with stress using a pronouncing dictionary. However, stress is neither invariably produced nor unambiguously identifiable in real speech. Heavy syllables, i.e. those with long vowels or syllable codas, attract stress in English. We devise Adaptor Grammar word segmentation models that exploit either stress, or syllable weight, or both, and evaluate the utility of syllable weight as a cue to word boundaries. Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does. © 2014 Association for Computational Linguistics.},
  annote    = {cited By 0},
  doi       = {10/gf8qdw},
  file      = {Pate and Johnson - 2014 - Syllable weight encodes mostly the same informatio.pdf:/home/akira/Zotero/storage/HJ6EXAAJ/Pate and Johnson - 2014 - Syllable weight encodes mostly the same informatio.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961296056&partnerID=40&md5=cf2aa54e54fbb32050bfd4d0c6349bd0},
}

@Article{roshani_unsupervised_2014-1,
  author   = {Roshani, Asra},
  title    = {Unsupervised segmentation of sequences using harmony search and hierarchical clustering techniques},
  year     = {2014},
  note     = {146},
  file     = {Roshani - 2014 - Unsupervised segmentation of sequences using harmo.pdf:/home/akira/Zotero/storage/Y65UNZMF/Roshani - 2014 - Unsupervised segmentation of sequences using harmo.pdf:application/pdf},
  keywords = {⛔ No DOI found},
}

@Article{zeng_classification-based_2013-2,
  author   = {Zeng, L. and Li, F.},
  title    = {A classification-based approach for implicit feature identification},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2013},
  volume   = {8202 LNAI},
  pages    = {190--202},
  note     = {90},
  abstract = {In recent years, sentiment analysis and opinion mining has grown to be one of the most active research areas. Most of the existing researches on feature-level opinion mining are dedicated to extract explicitly appeared features and opinion words. However, among the numerous kinds of reviews on the web, there are a significant number of reviews that contain only opinion words which imply some product features. The identification of such implicit features is still one of the most challenge tasks in opinion mining. In this paper, we propose a classification-based approach to deal with the task of implicit feature identification. Firstly, by exploiting the word segmentation, part-of-speech(POS) tagging and dependency parsing, a rule based method to extract the explicit feature-opinion pairs is presented. Secondly, the feature-opinion pairs for each opinion word are clustered and the training documents for each clustered feature-opinion pair are then constructed. Finally, the identification of implicit features is formulated into a classification-based feature selection. Experiments demonstrate that our approach outperforms the existing methods significantly. © Springer-Verlag 2013.},
  annote   = {cited By 16},
  doi      = {10/gf8qd4},
  file     = {Zeng and Li - 2013 - A classification-based approach for implicit featu.pdf:/home/akira/Zotero/storage/GET7UVZ5/Zeng and Li - 2013 - A classification-based approach for implicit featu.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893031086&doi=10.1007%2f978-3-642-41491-6_18&partnerID=40&md5=77380a4f99a8805341450d35c070b926},
}

@InProceedings{elsner_joint_2013-2,
  author    = {Elsner, M. and Goldwater, S. and Feldman, N.H. and Wood, F.},
  title     = {A joint learning model of word segmentation, lexical acquisition, and phonetic variability},
  booktitle = {{EMNLP} 2013 - 2013 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {Proceedings} of the {Conference}},
  year      = {2013},
  pages     = {42--54},
  note      = {92},
  abstract  = {We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We define the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed first, our model is qualitatively more similar to human learners. On data with variable pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence. © 2013 Association for Computational Linguistics.},
  annote    = {cited By 19},
  file      = {Elsner et al. - 2013 - A joint learning model of word segmentation, lexic.pdf:/home/akira/Zotero/storage/UN3PGX45/Elsner et al. - 2013 - A joint learning model of word segmentation, lexic.pdf:application/pdf},
  keywords  = {⛔ No DOI found},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926303216&partnerID=40&md5=1bb60c0df570ef1356d6b59e11aae487},
}

@Article{namer_rule-based_2013-3,
  author   = {Namer, F.},
  title    = {A {Rule}-{Based} {Morphosemantic} {Analyzer} for {French} for a {Fine}-{Grained} {Semantic} {Annotation} of {Texts}},
  journal  = {Communications in Computer and Information Science},
  year     = {2013},
  volume   = {380 CCIS},
  pages    = {92--114},
  note     = {95},
  abstract = {We describe DériF, a rule-based morphosemantic analyzer developed for French. Unlike existing word segmentation tools, DériF provides derived and compound words with various sorts of semantic information: (1) a definition, computed from both the base meaning and the specificities of the morphological rule; (2) lexical-semantic features, inferred from general linguistic properties of derivation rules; (3) lexical relations (synonymy, (co-)hyponymy) with other, morphologically unrelated, words belonging to the same analyzed corpus. © Springer-Verlag Berlin Heidelberg 2013.},
  annote   = {cited By 0},
  doi      = {10/gf8qdx},
  file     = {Namer - 2013 - A Rule-Based Morphosemantic Analyzer for French fo.pdf:/home/akira/Zotero/storage/6D7L84UJ/Namer - 2013 - A Rule-Based Morphosemantic Analyzer for French fo.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904608636&doi=10.1007%2f978-3-642-40486-3_6&partnerID=40&md5=39c06edc8d7ffbce4c228d77c841004d},
}

@InProceedings{sun_using_2013-2,
  author    = {Sun, R. and Zhou, W. and Liu, Z.},
  title     = {Using language rules to improve the performance of word segmentation},
  booktitle = {2013 6th {International} {Congress} on {Image} and {Signal} {Processing} ({CISP})},
  year      = {2013},
  volume    = {03},
  pages     = {1665--1669},
  note      = {149},
  abstract  = {Due to the disadvantage of the word segmentation tool in the aspect of the accuracy of word segmentation and speech tagging, it has harmful effects to the further research work. So this paper proposes a method based on the results of word segmentation to utilize language rules, which has been defined and described in Event Ontology, to verify and correct them. Experimental results show that compared with the method of only using word segmentation tool, the method of using language rules has a better performance.},
  doi       = {10/gf8qdk},
  file      = {Sun et al. - 2013 - Using language rules to improve the performance of.pdf:/home/akira/Zotero/storage/CY645BDE/Sun et al. - 2013 - Using language rules to improve the performance of.pdf:application/pdf},
  keywords  = {Ontologies, text analysis, natural language processing, Dictionaries, Word Segmentation, ontologies (artificial intelligence), Accuracy, Computers, Event, event ontology, Event Ontology, Hidden Markov models, Language Rule, language rules, Performance, speech tagging, Sun, Tagging, word segmentation tool},
}

@InProceedings{dai_hybrid_2012-2,
  author    = {Dai, Y. and Ren, X.},
  title     = {A hybrid method to segment words},
  booktitle = {2012 {International} {Conference} on {Audio}, {Language} and {Image} {Processing}},
  year      = {2012},
  pages     = {1131--1134},
  month     = jul,
  note      = {91},
  abstract  = {Word segmentation is the foundations of machine translation, text classification and information searching. A method is proposed which combines word segmentation based on dictionary with reverse maximum matching and word segmentation based on statistic with suffix array. The input texts are segmented using the reserve maximum matching method based on dictionary, and a two-way suffix arrays are constructed, longest common prefix are computed, candidate words are filtered out by setting the threshold, the candidate words are filtered using mutual information in order to the true words. The texts that are ambiguity are filtered using information entropy. It is showed that the accuracy of word segmentation may achieve above 97\% in the experiment.},
  doi       = {10/gf8qd3},
  file      = {Dai and Ren - 2012 - A hybrid method to segment words.pdf:/home/akira/Zotero/storage/KATP7DN3/Dai and Ren - 2012 - A hybrid method to segment words.pdf:application/pdf},
  keywords  = {text analysis, natural language processing, Dictionaries, Arrays, pattern classification, Accuracy, text classification, word segmentation, language translation, machine translation, Information filters, common prefix, hybrid method, information entropy, information searching, input texts, Matched filters, Sorting, suffix array},
}

@Article{chang_multi-function_2012-2,
  author   = {Chang, S.-F. and Yu, L.-C.},
  title    = {A multi-function {MSN} robot for campus information seeking},
  journal  = {Advanced Science Letters},
  year     = {2012},
  volume   = {9},
  pages    = {828--832},
  note     = {93},
  abstract = {An MSN robot is an intelligent system that can chat with users to accomplish a specific task. In this paper, we present a framework to build an MSN robot that can chat with students to provide campus information. The MSN robot has two major functions; that is it can provide two kinds of information: curriculum information and living information. The curriculum information includes a road map of all courses so that students can register courses according to their interests and future plans. The living information includes the information of restaurants, houses, transportation, and scenic spots around the campus. The MSN robot consists of three components: word segmentation and POS tagging, natural language understanding, and response text generation. The chat between the robot and students starts with an input of free-text question about campus information. Once the question is received, the natural language understanding component determines the meaning of the input question using classification methods. Finally, the output component generates a text response using a set of predefined templates. The MSN robot is implemented using the Dot MSN, an open source package that can connect the MSN Messenger service. The experimental results show that among several classification methods the support vector machine (SVM) achieves the best performance in classifying users' input questions into curriculum information or living information. © 2012 American Scientific Publishers.},
  annote   = {cited By 0},
  doi      = {10/gf8qd2},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862902958&doi=10.1166%2fasl.2012.2643&partnerID=40&md5=4582590a11bfce38b51c8e993aca82e6},
}

@InProceedings{araki_online_2012-2,
  author    = {Araki, T. and Nakamura, T. and Nagai, T. and Nagasaka, S. and Taniguchi, T. and Iwahashi, N.},
  title     = {Online learning of concepts and words using multimodal {LDA} and hierarchical {Pitman}-{Yor} {Language} {Model}},
  booktitle = {2012 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
  year      = {2012},
  pages     = {1623--1630},
  note      = {129},
  abstract  = {In this paper, we propose an online algorithm for multimodal categorization based on the autonomously acquired multimodal information and partial words given by human users. For multimodal concept formation, multimodal latent Dirichlet allocation (MLDA) using Gibbs sampling is extended to an online version. We introduce a particle filter, which significantly improve the performance of the online MLDA, to keep tracking good models among various models with different parameters. We also introduce an unsupervised word segmentation method based on hierarchical Pitman-Yor Language Model (HPYLM). Since the HPYLM requires no predefined lexicon, we can make the robot system that learns concepts and words in completely unsupervised manner. The proposed algorithms are implemented on a real robot and tested using real everyday objects to show the validity of the proposed system.},
  doi       = {10/gf8qdq},
  file      = {Araki et al. - 2012 - Online learning of concepts and words using multim.pdf:/home/akira/Zotero/storage/ELCUQP2K/Araki et al. - 2012 - Online learning of concepts and words using multim.pdf:application/pdf},
  keywords  = {Data models, Predictive models, natural language processing, human-robot interaction, Humans, unsupervised word segmentation, Robot sensing systems, educational robots, Gibbs sampling, Haptic interfaces, hierarchical Pitman-Yor language model, learning systems, multimodal categorization, multimodal concept formation, multimodal information, multimodal latent Dirichlet allocation, multimodal LDA, online algorithm, online learning, partial words, particle filter, predefined lexicon, robot system, Vectors},
}

@InProceedings{wu_sembler_2012-2,
  author    = {Wu, X. and Fan, W. and Yu, Y.},
  title     = {Sembler: {Ensembling} crowd sequential labeling for improved quality},
  booktitle = {Proceedings of the {National} {Conference} on {Artificial} {Intelligence}},
  year      = {2012},
  volume    = {2},
  pages     = {1713--1719},
  note      = {133},
  abstract  = {Many natural language processing tasks, such as named entity recognition (NER), part of speech (POS) tagging, word segmentation, and etc., can be formulated as sequential data labeling problems. Building a sound labeler requires very large number of correctly labeled training examples, which may not always be possible. On the other hand, crowdsourcing provides an inexpensive yet efficient alternative to collect manual sequential labeling from non-experts. However the quality of crowd labeling cannot be guaranteed, and three kinds of errors are typical: (1) incorrect annotations due to lack of expertise (e.g., labeling gene names from plain text requires corresponding domain knowledge); (2) ignored or omitted annotations due to carelessness or low confidence; (3) noisy annotations due to cheating or vandalism. To correct these mistakes, we present Sembler, a statistical model for ensembling crowd sequential labelings. Sembler considers three types of statistical information: (1) the majority agreement that proves the correctness of an annotation; (2) correct annotation that improves the credibility of the corresponding annotator; (3) correct annotation that enhances the correctness of other annotations which share similar linguistic or contextual features. We evaluate the proposed model on a real Twitter and a synthetical biological data set, and find that Sembler is particularly accurate when more than half of annotators make mistakes. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.},
  annote    = {cited By 7},
  file      = {Wu et al. - 2012 - Sembler Ensembling crowd sequential labeling for .pdf:/home/akira/Zotero/storage/A2B775IV/Wu et al. - 2012 - Sembler Ensembling crowd sequential labeling for .pdf:application/pdf},
  keywords  = {⛔ No DOI found},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868274778&partnerID=40&md5=056a981811f122859f050ce2dfaa4ab1},
}

@InProceedings{stahlberg_word_2012-2,
  author    = {Stahlberg, F. and Schlippe, T. and Vogel, S. and Schultz, T.},
  title     = {Word segmentation through cross-lingual word-to-phoneme alignment},
  booktitle = {2012 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
  year      = {2012},
  pages     = {85--90},
  note      = {153},
  abstract  = {We present our new alignment model Model 3P for cross-lingual word-to-phoneme alignment, and show that unsupervised learning of word segmentation is more accurate when information of another language is used. Word segmentation with cross-lingual information is highly relevant to bootstrap pronunciation dictionaries from audio data for Automatic Speech Recognition, bypass the written form in Speech-to-Speech Translation or build the vocabulary of an unseen language, particularly in the context of under-resourced languages. Using Model 3P for the alignment between English words and Spanish phonemes outperforms a state-of-the-art monolingual word segmentation approach [1] on the BTEC corpus [2] by up to 42\% absolute in F-Score on the phoneme level and a GIZA++ alignment based on IBM Model 3 by up to 17\%.},
  doi       = {10/gf8qdj},
  file      = {Stahlberg et al. - 2012 - Word segmentation through cross-lingual word-to-ph.pdf:/home/akira/Zotero/storage/3QJFPCZD/Stahlberg et al. - 2012 - Word segmentation through cross-lingual word-to-ph.pdf:application/pdf},
  keywords  = {natural language processing, Dictionaries, word segmentation, Vocabulary, unsupervised learning, Hidden Markov models, Training data, Grammar, speech recognition, automatic speech recognition, Vectors, Error analysis, alignment model, bootstrap pronunciation dictionaries, cross lingual information, cross lingual word-to-phoneme alignment, English words, Spanish phonemes, speech-to-speech translation, under-resourced language},
}

@Article{aken_statistical_2011-1,
  author   = {Aken, Jerry R. Van},
  title    = {A statistical learning algorithm for word segmentation},
  journal  = {arxiv:1105.6162},
  year     = {2011},
  note     = {96},
  file     = {Full Text:/home/akira/Zotero/storage/AR72IWPS/Aken - 2011 - A statistical learning algorithm for word segmenta.pdf:application/pdf},
  keywords = {⛔ No DOI found},
  url      = {http://arxiv.org/abs/1105.6162v2},
}

@InProceedings{hewlett_fully_2011-2,
  author    = {Hewlett, Daniel and Cohen, Paul},
  title     = {Fully {Unsupervised} {Word} {Segmentation} with {BVE} and {MDL}},
  booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Short} {Papers} - {Volume} 2},
  year      = {2011},
  series    = {{HLT} '11},
  pages     = {540--545},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  note      = {111 event-place: Portland, Oregon},
  file      = {Full Text:/home/akira/Zotero/storage/X5H6EVSX/Hewlett e Cohen - 2011 - Fully Unsupervised Word Segmentation with BVE and .pdf:application/pdf},
  isbn      = {978-1-932432-88-6},
  url       = {http://dl.acm.org/citation.cfm?id=2002736.2002843},
}

@Article{paul_integration_2011-2,
  author   = {Paul, M. and Finch, A. and Sumita, E.},
  title    = {Integration of multiple bilingually-trained segmentation schemes into statistical machine translation},
  journal  = {IEICE Transactions on Information and Systems},
  year     = {2011},
  volume   = {E94-D},
  number   = {3},
  pages    = {690--697},
  note     = {116},
  abstract = {This paper proposes an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches. The method can be applied to any language pair in which the source language is unsegmented and the target language segmentation is known. In the first step, an iterative bootstrap method is applied to learn multiple segmentation schemes that are consistent with the phrasal segmentations of an SMT system trained on the resegmented bitext. In the second step, multiple segmentation schemes are integrated into a single SMT system by characterizing the source language side and merging identical translation pairs of differently segmented SMT models. Experimental results translating five Asian languages into English revealed that the proposed method of integrating multiple segmentation schemes outperforms SMT models trained on any of the learned word segmentations and performs comparably to available monolingually built segmentation tools. © 2011 The Institute of Electronics, Information and Communication Engineers.},
  annote   = {cited By 3},
  doi      = {10/bsbh8n},
  file     = {Full Text:/home/akira/Zotero/storage/YTEN8TNS/Paul et al. - 2011 - Integration of multiple bilingually-trained segmen.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952129828&doi=10.1587%2ftransinf.E94.D.690&partnerID=40&md5=f02f8345f7fbdde304c3c697b3f3d408},
}

@InProceedings{hewlett_word_2011-2,
  author    = {Hewlett, Daniel and Cohen, Paul},
  title     = {Word {Segmentation} {As} {General} {Chunking}},
  booktitle = {Proceedings of the {Fifteenth} {Conference} on {Computational} {Natural} {Language} {Learning}},
  year      = {2011},
  series    = {{CoNLL} '11},
  pages     = {39--47},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  note      = {151 event-place: Portland, Oregon},
  file      = {Full Text:/home/akira/Zotero/storage/NXKP7UHW/Hewlett e Cohen - 2011 - Word Segmentation As General Chunking.pdf:application/pdf},
  isbn      = {978-1-932432-92-3},
  url       = {http://dl.acm.org/citation.cfm?id=2018936.2018941},
}

@InProceedings{weber_base-form_2010-2,
  author    = {Weber, C. and Handl, J.},
  title     = {A base-form lexicon of content words for correct word segmentation and syntactic-semantic annotation},
  booktitle = {Semantic {Approaches} in {Natural} {Language} {Processing} - {Proceedings} of the {Conference} on {Natural} {Language} {Processing} 2010, {KONVENS}},
  year      = {2010},
  note      = {89},
  abstract  = {One issue in natural language processing is the interaction between a rule-based computational morphology and a syntactic-semantic analysis system. This is because derivational and compound word forms raise the question of how to deal with ambiguities caused by the rule-based analyser, and how to add additional information like valency to a derivational or compound word form if its valency frames differ from those of its root word. In this paper we propose a lexicon design addressing both of these issues. We evaluate our design in the context of a large-scale morphological analysis system for German in which the lexicon serves as an interface between morphology and syntax. In doing so, we aim at enriching the wellformed analysis results with additional information so that an adequate syntactic-semantic analysis can be ensured.},
  annote    = {cited By 0},
  file      = {Weber and Handl - 2010 - A base-form lexicon of content words for correct w.pdf:/home/akira/Zotero/storage/RXIRA7PG/Weber and Handl - 2010 - A base-form lexicon of content words for correct w.pdf:application/pdf},
  keywords  = {⛔ No DOI found},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894110931&partnerID=40&md5=f6254b4cf0ee2665f59a6be8a53db773},
}

@InProceedings{paul_integration_2010-1,
  author    = {Paul, Michael and Finch, Andrew and Sumita, Eiichiro},
  title     = {Integration of {Multiple} {Bilingually}-learned {Segmentation} {Schemes} into {Statistical} {Machine} {Translation}},
  booktitle = {Proceedings of the {Joint} {Fifth} {Workshop} on {Statistical} {Machine} {Translation} and {MetricsMATR}},
  year      = {2010},
  series    = {{WMT} '10},
  pages     = {400--408},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  note      = {115 event-place: Uppsala, Sweden},
  file      = {Full Text:/home/akira/Zotero/storage/9BHTDMQY/Paul et al. - 2010 - Integration of Multiple Bilingually-learned Segmen.pdf:application/pdf},
  isbn      = {978-1-932432-71-8},
  url       = {http://dl.acm.org/citation.cfm?id=1868850.1868910},
}

@InProceedings{cho_novel_2009-2,
  author    = {Cho, Han-Cheol and Lee, Do-Gil and Lee, Jung-Tae and Stenetorp, Pontus and Tsujii, Jun'ichi and Rim, Hae-Chang},
  title     = {A {Novel} {Word} {Segmentation} {Approach} for {Written} {Languages} with {Word} {Boundary} {Markers}},
  booktitle = {Proceedings of the {ACL}-{IJCNLP} 2009 {Conference} {Short} {Papers}},
  year      = {2009},
  series    = {{ACLShort} '09},
  pages     = {29--32},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  note      = {event-place: Suntec, Singapore},
  copyright = {94},
  doi       = {10/bqwx4g},
  file      = {Cho et al. - 2009 - A Novel Word Segmentation Approach for Written Lan.pdf:/home/akira/Zotero/storage/WXFWXQNS/Cho et al. - 2009 - A Novel Word Segmentation Approach for Written Lan.pdf:application/pdf},
  url       = {http://dl.acm.org/citation.cfm?id=1667583.1667594},
}

@InProceedings{ma_bilingually_2009-1,
  author    = {Ma, Yanjun and Way, Andy},
  title     = {Bilingually {Motivated} {Domain}-adapted {Word} {Segmentation} for {Statistical} {Machine} {Translation}},
  booktitle = {Proceedings of the 12th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
  year      = {2009},
  series    = {{EACL} '09},
  pages     = {549--557},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  note      = {103 event-place: Athens, Greece},
  file      = {Full Text:/home/akira/Zotero/storage/IIIGXMST/Ma e Way - 2009 - Bilingually Motivated Domain-adapted Word Segmenta.pdf:application/pdf},
  keywords  = {⛔ No DOI found},
  url       = {http://dl.acm.org/citation.cfm?id=1609067.1609128},
}

@InProceedings{johnson_improving_2009-1,
  author    = {Johnson, Mark and Goldwater, Sharon},
  title     = {Improving {Nonparameteric} {Bayesian} {Inference}: {Experiments} on {Unsupervised} {Word} {Segmentation} with {Adaptor} {Grammars}},
  booktitle = {Proceedings of {Human} {Language} {Technologies}: {The} 2009 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
  year      = {2009},
  series    = {{NAACL} '09},
  pages     = {317--325},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  note      = {113 event-place: Boulder, Colorado},
  file      = {Full Text:/home/akira/Zotero/storage/9GDKPWN3/Johnson e Goldwater - 2009 - Improving Nonparameteric Bayesian Inference Exper.pdf:application/pdf},
  isbn      = {978-1-932432-41-1},
  url       = {http://dl.acm.org/citation.cfm?id=1620754.1620800},
}

@InProceedings{paul_language_2009-1,
  author    = {Paul, Michael and Finch, Andrew and Sumita, Eiichiro},
  title     = {Language {Independent} {Word} {Segmentation} for {Statistical} {Machine} {Translation}},
  booktitle = {Proceedings of the 3rd {International} {Universal} {Communication} {Symposium}},
  year      = {2009},
  series    = {{IUCS} '09},
  pages     = {36--40},
  address   = {New York, NY, USA},
  publisher = {ACM},
  note      = {event-place: Tokyo, Japan},
  copyright = {119},
  doi       = {10/fqpwcc},
  file      = {Paul et al. - 2009 - Language Independent Word Segmentation for Statist.pdf:/home/akira/Zotero/storage/9PXBNIG3/Paul et al. - 2009 - Language Independent Word Segmentation for Statist.pdf:application/pdf},
  isbn      = {978-1-60558-641-0},
  url       = {http://doi.acm.org/10.1145/1667780.1667788},
}

@InProceedings{noauthor_session_2009-1,
  title     = {Session {Details}: {Word} {Segmentation} and {POS} {Tagging}},
  booktitle = {Proceedings of the {Joint} {Conference} of the 47th {Annual} {Meeting} of the {ACL} and the 4th {International} {Joint} {Conference} on {Natural} {Language} {Processing} of the {AFNLP}: {Volume} 1 - {Volume} 1},
  year      = {2009},
  series    = {{ACL} '09},
  pages     = {--},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  note      = {135 event-place: Suntec, Singapore},
  annote    = {Session Chair-Ng, Hwee Tou},
  isbn      = {978-1-932432-45-9},
  url       = {http://dl.acm.org/citation.cfm?id=1687878.3252572},
}

@Article{tambouratzis_using_2009-2,
  author   = {Tambouratzis, G.},
  title    = {Using an {Ant} {Colony} {Metaheuristic} to {Optimize} {Automatic} {Word} {Segmentation} for {Ancient} {Greek}},
  journal  = {IEEE Transactions on Evolutionary Computation},
  year     = {2009},
  volume   = {13},
  number   = {4},
  pages    = {742--753},
  note     = {148},
  abstract = {Given a text or collection of texts involving unconstrained language, a basic task in a multitude of applications is the identification of stems and endings for each word form, which is termed morphological analysis. In this paper, the use of an ant colony optimization (ACO) metaheuristic is proposed for a linguistic task that involves the automated morphological segmentation of Ancient Greek word forms into stem and ending. The task of morphological analysis is essential for implementing text-processing applications such as semantic analysis and information retrieval. The difficulty of the morphological analysis task differs depending on the language chosen, being hardest in the case of highly-inflectional languages, where each stem may be associated with a large number of different endings. In this paper, focus is placed on the morphological analysis of ancient Greek, which has been shown to be a particularly hard task. To perform this task, a system for the automated morphological processing has been proposed, which implements the morphological analysis of words by coupling an iterative pattern-recognition algorithm with a modest amount of linguistic knowledge, expressed via a set of interactions associated with weights. In an earlier version of the system, these weights were determined by combining the input from specialized scientists with a lengthy manual optimization process. In this paper, the ACO metaheuristic is applied to the task of defining near-optimal system weights using an automated process based on a set of training data. The experiments performed indicate that the segmentation quality achieved by ACO is equivalent to or in several cases substantially higher than that achieved using manually optimized weights.},
  doi      = {10/b2q8xs},
  file     = {Tambouratzis - 2009 - Using an Ant Colony Metaheuristic to Optimize Auto.pdf:/home/akira/Zotero/storage/T9VHSHSX/Tambouratzis - 2009 - Using an Ant Colony Metaheuristic to Optimize Auto.pdf:application/pdf},
  keywords = {information retrieval, Information retrieval, Data mining, text analysis, optimisation, Performance analysis, natural language processing, Algorithm design and analysis, Information analysis, ancient Greek, Ancient Greek, ant colony metaheuristic, Ant colony optimization, ant colony optimization (ACO) metaheuristic, automated morphological analysis, automated morphological processing, heuristic function, highly-inflectional languages, Iterative algorithms, iterative methods, iterative pattern-recognition algorithm, linguistic knowledge, linguistics, manual optimization process, morphological analysis, near-optimal system, optimize automatic word segmentation, Pattern analysis, pattern recognition, segmentation quality, semantic analysis, text processing, Text processing, text processing application, Training data},
}

@InProceedings{gabay_using_2008-2,
  author    = {Gabay, D. and Ziv, B.E. and Elhadad, M.},
  title     = {Using wikipedia links to construct word segmentation corpora},
  booktitle = {{AAAI} {Workshop} - {Technical} {Report}},
  year      = {2008},
  volume    = {WS-08-15},
  pages     = {61--63},
  note      = {150},
  abstract  = {Tagged corpora are essential for evaluating and training natural language processing tools. The cost of constructing large enough manually tagged corpora is high, even when the annotation level is shallow. This article describes a simple method to automatically create a partially tagged corpus, using Wikipedia hyperlinks. The resulting corpus contains information about the correct segmentation of 523,599 non-consecutive words in 363,090 sentences. We used our method to construct a corpus of Modern Hebrew (which we have made available at http://www.cs.bgu.ac.il/-nlpproj). The method can also be applied to other languages where word segmentation is difficult to determine, such as East and South-East Asian languages. Copyright © 2008.},
  annote    = {cited By 3},
  file      = {Gabay et al. - 2008 - Using wikipedia links to construct word segmentati.pdf:/home/akira/Zotero/storage/JVMZ39DY/Gabay et al. - 2008 - Using wikipedia links to construct word segmentati.pdf:application/pdf},
  keywords  = {⛔ No DOI found},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449775446&partnerID=40&md5=ad7bc10a1c3ed07e582931513ab8ae88},
}

@Article{lee_automatic_2007-2,
  author   = {Lee, D.-G. and Rim, H.-C. and Yook, D.},
  title    = {Automatic word spacing using probabilistic models based on character n-grams},
  journal  = {IEEE Intelligent Systems},
  year     = {2007},
  volume   = {22},
  number   = {1},
  pages    = {28--35},
  note     = {101},
  abstract = {Probabilistic models based on Hidden Markov models (HMM) for automatic word spacing that use characters n-grams, which is a sub-sequence of n characters in a given character sequence, are discussed. Automatic word spacing is a preprocessing techniques used for correcting boundaries between words in a sentence containing spacing errors. These model can be effectively applied to a natural language with a small character set, such as English, using character n-grams that are larger than trigrams. These models, which are language independent and can be effectively used for languages having word spacing, can also be used for word segmentation in the languages without explicit word spacing. These models, by generalizing the HMMs, can consider a broad context and estimate accurate probabilities.},
  annote   = {cited By 16},
  doi      = {10/c6b6z9},
  file     = {Lee et al. - 2007 - Automatic word spacing using probabilistic models .pdf:/home/akira/Zotero/storage/SQF3783E/Lee et al. - 2007 - Automatic word spacing using probabilistic models .pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847611998&doi=10.1109%2fMIS.2007.4&partnerID=40&md5=f49f0e501e9b8b12dedfb8fee532c736},
}

@InProceedings{guo_research_2007-3,
  author    = {Guo, Q. and Li, C.},
  title     = {The research on the application of text clustering and natural language understanding in automatic abstracting},
  booktitle = {Proceedings - {Fourth} {International} {Conference} on {Fuzzy} {Systems} and {Knowledge} {Discovery}, {FSKD} 2007},
  year      = {2007},
  volume    = {4},
  pages     = {92--96},
  note      = {141},
  abstract  = {A method of realization of Automatic Abstracting based on Text Clustering and Natural Language Understanding is brought forward, aimed at overcoming shortages of some current methods. The method makes use of text Clustering and can realize Automatic Abstracting of multi- documents. The algorithm of twice Word Segmentation based on the Title and FirstSentences in Paragraphs is brought forward. Its precision and recall is above 95\% For a specific domain on plastics, an Automatic Abstracting system named TCAAS is implemented. The precision and recall of multidocument's Automatic Abstracting is above 75\% And experiments do prove that it is feasible to use the method to develop a domain Automatic Abstracting System, which is valuable for further study in more depth. © 2007 IEEE.},
  annote    = {cited By 4},
  doi       = {10/cf5vpg},
  file      = {Guo and Li - 2007 - The research on the application of text clustering.pdf:/home/akira/Zotero/storage/YAZAQ8WQ/Guo and Li - 2007 - The research on the application of text clustering.pdf:application/pdf},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-44049094329&doi=10.1109%2fFSKD.2007.584&partnerID=40&md5=1786ee4735dd963c58059a2f015c195a},
}

@Article{creutz_unsupervised_2007-1,
  author   = {Creutz, Mathias and Lagus, Krista},
  title    = {Unsupervised {Models} for {Morpheme} {Segmentation} and {Morphology} {Learning}},
  journal  = {ACM Trans. Speech Lang. Process.},
  year     = {2007},
  volume   = {4},
  number   = {1},
  pages    = {3:1--3:34},
  issn     = {1550-4875},
  note     = {145},
  doi      = {10/bx99qh},
  file     = {Creutz and Lagus - 2007 - Unsupervised Models for Morpheme Segmentation and .pdf:/home/akira/Zotero/storage/GFXGSMQ2/Creutz and Lagus - 2007 - Unsupervised Models for Morpheme Segmentation and .pdf:application/pdf},
  keywords = {Efficient storage, highly inflecting and compounding languages, language independent methods, maximum a posteriori (MAP) estimation, morpheme lexicon and segmentation, unsupervised learning},
  url      = {http://doi.acm.org/10.1145/1187415.1187418},
}

@InProceedings{geyken_tagh_2006-2,
  author    = {Geyken, Alexander and Hanneforth, Thomas},
  title     = {{TAGH}: {A} {Complete} {Morphology} for {German} {Based} on {Weighted} {Finite} {State} {Automata}},
  booktitle = {Finite-{State} {Methods} and {Natural} {Language} {Processing}},
  year      = {2006},
  editor    = {Yli-Jyrä, Anssi and Karttunen, Lauri and Karhumäki, Juhani},
  pages     = {55--66},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  note      = {139},
  abstract  = {TAGH is a system for automatic recognition of German word forms. It is based on a stem lexicon with allomorphs and a concatenative mechanism for inflection and word formation. Weighted FSA and a cost function are used in order to determine the correct segmentation of complex forms: the correct segmentation for a given compound is supposed to be the one with the least cost. TAGH is based on a large stem lexicon of almost 80.000 stems that was compiled within 5 years on the basis of large newspaper corpora and literary texts. The number of analyzable word forms is increased considerably by more than 1000 different rules for derivational and compositional word formation. The recognition rate of TAGH is more than 99\% for modern newspaper text and approximately 98.5\% for literary texts.},
  file      = {Geyken and Hanneforth - 2006 - TAGH A Complete Morphology for German Based on We.pdf:/home/akira/Zotero/storage/C5ISUSPX/Geyken and Hanneforth - 2006 - TAGH A Complete Morphology for German Based on We.pdf:application/pdf},
  isbn      = {978-3-540-35469-7},
}

@Article{kazakov_unsupervised_2001-2,
  author   = {Kazakov, D. and Manandhar, S.},
  title    = {Unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming},
  journal  = {Machine Learning},
  year     = {2001},
  volume   = {43},
  number   = {1-2},
  pages    = {121--162},
  note     = {144},
  abstract = {This article presents a combination of unsupervised and supervised learning techniques for the generation of word segmentation rules from a raw list of words. First, a language bias for word segmentation is introduced and a simple genetic algorithm is used in the search for a segmentation that corresponds to the best bias value. In the second phase, the words segmented by the genetic algorithm are used as an input for the first order decision list learner CLOG. The result is a set of first order rules which can be used for segmentation of unseen words. When applied on either the training data or unseen data, these rules produce segmentation which are linguistically meaningful, and to a large degree conforming to the annotation provided.},
  annote   = {cited By 21},
  doi      = {10/fng8qb},
  file     = {Full Text:/home/akira/Zotero/storage/WQU2LPGV/Kazakov e Manandhar - 2001 - Unsupervised learning of word segmentation rules w.pdf:application/pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035312598&doi=10.1023%2fA%3a1007629103294&partnerID=40&md5=eaae5dc95f7c91cc97525afdf2bb2c17},
}

@Comment{jabref-meta: databaseType:bibtex;}
