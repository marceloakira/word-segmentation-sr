Scopus
EXPORT DATE: 19 September 2019

@ARTICLE{Nguyen2019,
author={Nguyen, H.T. and Duong, P.H. and Cambria, E.},
title={Learning short-text semantic similarity with word embeddings and external knowledge sources},
journal={Knowledge-Based Systems},
year={2019},
volume={182},
doi={10.1016/j.knosys.2019.07.013},
art_number={104842},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069875471&doi=10.1016%2fj.knosys.2019.07.013&partnerID=40&md5=79164e89f20b11d4c4cf48b954deaae2},
abstract={We present a novel method based on interdependent representations of short texts for determining their degree of semantic similarity. The method represents each short text as two dense vectors: the former is built using the word-to-word similarity based on pre-trained word vectors, the latter is built using the word-to-word similarity based on external sources of knowledge. We also developed a preprocessing algorithm that chains coreferential named entities together and performs word segmentation to preserve the meaning of phrasal verbs and idioms. We evaluated the proposed method on three popular datasets, namely Microsoft Research Paraphrase Corpus, STS2015 and P4PIN, and obtained state-of-the-art results on all three without using prior knowledge of natural language, e.g., part-of-speech tags or parse tree, which indicates the interdependent representations of short text pairs are effective and efficient for semantic textual similarity tasks. © 2019 Elsevier B.V.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liu2019,
author={Liu, D. and Su, J. and Song, L. and Qiu, Z.},
title={Application of Internet segmentation research based on Natural Language Processing technology in enterprise public opinion risk monitoring},
journal={Journal of Physics: Conference Series},
year={2019},
volume={1187},
number={4},
doi={10.1088/1742-6596/1187/4/042007},
art_number={042007},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067651831&doi=10.1088%2f1742-6596%2f1187%2f4%2f042007&partnerID=40&md5=e3d2e39577932407e5d637d0a2726081},
abstract={With the advent of the mobile Internet era, the network has become a distribution center of various information such as media, entertainment, sports, economy, politics and so on. A large amount of information is generated and disappeared on the network every day. How to effectively extract and identify the relevant data, and judge and analyze them is an important part of the corporate public opinion control. This paper uses natural language processing technology to study the word segmentation of text information on the network, and applies it to the risk detection of corporate public opinion. © Published under licence by IOP Publishing Ltd.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mzamo2019166,
author={Mzamo, L. and Helberg, A. and Bosch, S.},
title={Towards an unsupervised morphological segmenter for isiXhosa},
journal={Proceedings - 2019 Southern African Universities Power Engineering Conference/Robotics and Mechatronics/Pattern Recognition Association of South Africa, SAUPEC/RobMech/PRASA 2019},
year={2019},
pages={166-170},
doi={10.1109/RoboMech.2019.8704816},
art_number={8704816},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065817284&doi=10.1109%2fRoboMech.2019.8704816&partnerID=40&md5=88adfed9927ac18eafb96aaa7e70bbb9},
abstract={In this paper, branching entropy techniques and isiXhosa language heuristics are adapted to develop unsupervised morphological segmenters for isiXhosa. An overview of isiXhosa segmentation issues is given, followed by a discussion on previous work in automated segmentation, and segmentation of isiXhosa in particular. Two unsupervised isiXhosa segmenters are presented and compared to a random minimum baseline and Morfessor-Baseline, a standard in unsupervised word segmentation. Morfessor-Baseline outperforms both isiXhosa segmenters at 79.10% boundary identification accuracy. The IsiXhosa Branching Entropy Segmenter (XBES) performance varies depending on the segmentation mode used, with a maximum of 73.39%. The IsiXhosa Heuristic Maximum Likelihood Segmenter (XHMLS) achieves 72.42%. The study suggests that unsupervised isiXhosa morphological segmentation is feasible with better optimization of the current attempts. © 2019 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shen2019660,
author={Shen, H. and Long, C. and Wan, W. and Li, J. and Qin, Y. and Fu, Y. and Song, X.},
title={Log Layering Based on Natural Language Processing},
journal={International Conference on Advanced Communication Technology, ICACT},
year={2019},
volume={2019-February},
pages={660-663},
doi={10.23919/ICACT.2019.8702019},
art_number={8702019},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065643105&doi=10.23919%2fICACT.2019.8702019&partnerID=40&md5=c381ecc920d99ffe0e61b386ef3cb024},
abstract={With the increasing number and variety of logs, the requirement of storage space is growing rapidly. Meantime, the speed and accuracy of querying in massive logs are becoming increasingly important. Although the well-built distributed storage technique solves the problem of mass storage and fast query, the cost is too high. As logs are created as the method to trace the historical operation, the requirement for query rate is not high. To balance the storage cost and query rate, this paper proposes a real-time log layering storage technique based on natural language processing. According to the characteristics of the log data, this technique is combined with the text language processing technique. It compresses the real-time log data effectively while considering the query efficiency. Firstly, the method extracts the feature of each log that flows in, which will be the type name of the log. Then, the method performs word segmentation on the log and encodes each word to store the key value pairs. Finally, the key value pairs of the log are stored in the memory, and the code of each log is stored in the database. Experiments show that this method can ensure the integrity of the data effectively, decompression time dropped to 40%, compression rate down to 35%. © 2019 Global IT Research Institute (GIRI).},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhao2019799,
author={Zhao, T. and Li, L. and Xie, Y. and Lv, Y.},
title={Data-driven Risk Assessment for Peer-to-Peer Network Lending Agencies},
journal={Proceedings of 2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems, CCIS 2018},
year={2019},
pages={799-803},
doi={10.1109/CCIS.2018.8691202},
art_number={8691202},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064990311&doi=10.1109%2fCCIS.2018.8691202&partnerID=40&md5=f46925816230dae8fa8ed053b74988cc},
abstract={With the rapid development of Peer-to-Peer(P2P) network lending in the financial field, more data of lending agencies have appeared. P2P agencies also have problems such as absconded with ill-gotten gains and out of business. Therefore, it is necessary to assess their risks based on P2P company data. This paper proposes a framework of Data-driven Risk Assessment for P2P(DRAP2P) network lending agencies based on unstructured natural language data. First, use the natural language processing technology, such as word segmentation, keyword, LDA topic model, word2vec and doc2vec, to process and extract features of company profile which reflect its business status. Then, seven machine learning classifiers and three deep learning models are used for analysis. Since keywords show good performance in machine learning models, we improve Convolutional Neural Network(CNN) with keywords and propose two CNN+Keyword models, namely CNN+Keyword(static+BP) and CNN+Keyword (Expand word embedding). Experiments have shown that CNN+Keyword(static+BP) can achieve the best performance. Finally, we use the method of meta-learning to integrate CNN+Keyword(static+BP) and logistic regression classifier to further strengthen the performance. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Moreau20191119,
author={Moreau, E. and Vogel, C.},
title={Multilingual word segmentation: Training many language-specific tokenizers smoothly thanks to the universal dependencies corpus},
journal={LREC 2018 - 11th International Conference on Language Resources and Evaluation},
year={2019},
pages={1119-1127},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059903761&partnerID=40&md5=0d85521a7ec28ece3d5e35165d639dda},
abstract={This paper describes how a tokenizer can be trained from any dataset in the Universal Dependencies 2.1 corpus (UD2) (Nivre et al., 2017). A software tool, which relies on Elephant (Evang et al., 2013) to perform the training, is also made available. Beyond providing the community with a large choice of language-specific tokenizers, we argue in this paper that: (1) tokenization should be considered as a supervised task; (2) language scalability requires a streamlined software engineering process across languages. © LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zuters2019225,
author={Zuters, J. and Strazds, G. and Ļeonova, V.},
title={Morphology-inspired word segmentation for neural machine translation},
journal={Frontiers in Artificial Intelligence and Applications},
year={2019},
volume={315},
pages={225-239},
doi={10.3233/978-1-61499-941-6-225},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063356974&doi=10.3233%2f978-1-61499-941-6-225&partnerID=40&md5=f492e09e511a1872d8b05ca83773ab74},
abstract={This paper proposes the Prefix-Root-Postfix-Encoding (PRPE) algorithm, which performs close-to-morphological segmentation of words as part of text pre-processing in machine translation. PRPE is a cross-language algorithm requiring only minor tweaking to adapt it for any particular language, a property which makes it potentially useful for morphologically rich languages with no morphological analysers available. As a key part of the proposed algorithm we introduce the ‘Root alignment’ principle to extract potential sub-words from a corpus, as well as a special technique for constructing words from potential sub-words. In addition, we supplemented the algorithm with specific processing for named-entities based on transliteration. We conducted experiments with two different neural machine translation systems, training them on parallel corpora for English-Latvian and Latvian-English translation. Evaluation of translation quality showed improvements in BLEU scores when the data were pre-processed using the proposed algorithm, compared to a couple of baseline word segmentation algorithms. Although we were able to demonstrate improvements in both translation directions and for both NMT systems, they were relatively minor, and our experiments show that machine translation with inflected languages remains challenging, especially with translation direction towards a highly inflected language. © 2019 The authors and IOS Press.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Teahan2018,
author={Teahan, W.J.},
title={A compression-based toolkit for modelling and processing natural language text},
journal={Information (Switzerland)},
year={2018},
volume={9},
number={12},
doi={10.3390/info9120294},
art_number={294},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058994088&doi=10.3390%2finfo9120294&partnerID=40&md5=c332a4ee5417f7acc966c9f13201ca3a},
abstract={A novel compression-based toolkit for modelling and processing natural language text is described. The design of the toolkit adopts an encoding perspective-applications are considered to be problems in searching for the best encoding of different transformations of the source text into the target text. This paper describes a two phase 'noiseless channel model' architecture that underpins the toolkit which models the text processing as a lossless communication down a noise-free channel. The transformation and encoding that is performed in the first phase must be both lossless and reversible. The role of the verification and decoding second phase is to verify the correctness of the communication of the target text that is produced by the application. This paper argues that this encoding approach has several advantages over the decoding approach of the standard noisy channel model. The concepts abstracted by the toolkit's design are explained together with details of the library calls. The pseudo-code for a number of algorithms is also described for the applications that the toolkit implements including encoding, decoding, classification, training (model building), parallel sentence alignment, word segmentation and language segmentation. Some experimental results, implementation details, memory usage and execution speeds are also discussed for these applications. © 2018 by the authors.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang20182927,
author={Zhang, Z. and Bi, X.},
title={Research and Experiment of Intelligent Natural Language Processing Algorithms},
journal={Wireless Personal Communications},
year={2018},
volume={102},
number={4},
pages={2927-2939},
doi={10.1007/s11277-018-5316-2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040677725&doi=10.1007%2fs11277-018-5316-2&partnerID=40&md5=73b8734ca4c3841c9c0e63013dc69c05},
abstract={Natural language processing is mainly divided into two parts: speech processing and word processing. The level of word processing is mainly studied. Natural language processing is divided into lexical analysis, syntax analysis and semantic analysis. Aiming at the scope of the language ambiguity and thesaurus in the field of smart home, the maximal matching algorithm is used to segment the natural language. Then, through the way of template matching, semantic comprehension finally forms the code form that can control the home node. In the system applied in this paper, the speech is processed into words through the existing voice input function of the mobile terminal. Then, the control instruction is obtained through the language processing method. The processed data is communicated to the server via socket. The server sends the data to the home node through the Zigbee protocol. Finally, control of home appliances is achieved. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Long201880,
author={Long, P. and Boonjing, V.},
title={Longest Matching and Rule-based Techniques for Khmer Word Segmentation},
journal={2018 10th International Conference on Knowledge and Smart Technology: Cybernetics in the Next Decades, KST 2018},
year={2018},
pages={80-83},
doi={10.1109/KST.2018.8426109},
art_number={8426109},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052291916&doi=10.1109%2fKST.2018.8426109&partnerID=40&md5=a220d539ec45a5e0ac28980d7e0a3b26},
abstract={Word boundaries are the essential assignment to be done in natural language processing research. In most Asian languages, as well as Khmer language, many studies involved with word segmentation have been investigated. In Khmer Word Segmentation, several approaches related to segmenting words based on dictionary have been studied. There are only few researches about solving unknown word problem. This matter is a quite challenge task in word separation. In this research, Maximum Matching algorithm (MMA) together with Rule-based technique has been proposed. First, MMA and a Khmer manual corpus were used to make word boundaries in each sentence. Then the unknown words were then defined and solved by using 21 grammar rules created. We tested the segmentation with 2018 sentences from agriculture, magazine, newspaper, technology, health and history. With Maximum Matching alone, we could achieve the accuracy of 88.55% and along with Rule-based, the accuracy increased to 92.81%. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2018558,
author={Liu, N. and Su, X. and Gao, G. and Bao, F.},
title={Mongolian word segmentation based on three character level Seq2Seq models},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={11305 LNCS},
pages={558-569},
doi={10.1007/978-3-030-04221-9_50},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059068784&doi=10.1007%2f978-3-030-04221-9_50&partnerID=40&md5=686c94482295131c3cdbbc0ac8d869cc},
abstract={Mongolian word segmentation is splitting the Mongolian words into roots and suffixes. It plays an important role in Mongolian related natural language processing tasks. To improve performance and avoid the tedious work of rule-making and statistics over large-scale corpus in early methods, this work takes a Seq2Seq framework to realize Mongolian word segmentation. Since each Mongolian word consisted of several sequential characters, we map Mongolian word segmentation to character-level Seq2Seq task, and further propose three different models from three different prospective to achieve the segmentation goal. The three character-level Seq2Seq models are (1) translation model, (2) true and pseudo mapping model, (3) binary choice model. The main differences of these three models are the output sequences and the architectures of the RNNs in segmentation. We employ an improved beam search to optimize the second segmentation model and boost the segmentation process. All the models are trained on a limited dataset, and the second model achieved the state-of-the-art accuracy. © 2018, Springer Nature Switzerland AG.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2018466,
author={Wang, X. and Gao, C. and Cao, J. and Lin, K. and Du, W. and Yang, Z.},
title={ALTAS: An intelligent text analysis system based on knowledge graphs},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10987 LNCS},
pages={466-470},
doi={10.1007/978-3-319-96890-2_40},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050537718&doi=10.1007%2f978-3-319-96890-2_40&partnerID=40&md5=671150c35b5218bda2fe429115d446d7},
abstract={This paper presents an intelligent text analysis system, called ALTAS, to support various text analysis tasks such as statistics analysis, sentiment analysis, text classification, and text clustering. The system contains four main components: knowledge graphs, text processing, text analysis and intelligent report. First, the system has built a semantic-rich knowledge base using several knowledge graph resources. A novel text processing and analysis framework based on knowledge graphs is developed and implemented. Given a text dataset, the text processing phase will do data cleaning, word segmentation and feature extraction for it. With the extracted features, the text analysis phase allows users to select a text mining task. We have implemented the proposed novel algorithm and several typical algorithms for each task. If users select multiple algorithms for the task, the intelligent report phase will automatically generate comparison results for users. Especially, the intelligent report phase also provides users a paper summary generating function on text mining problems. © Springer International Publishing AG, part of Springer Nature 2018.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Song20171,
author={Song, S. and Zhang, N. and Huang, H.},
title={Named entity recognition based on conditional random fields},
journal={Cluster Computing},
year={2017},
pages={1-12},
doi={10.1007/s10586-017-1146-3},
note={cited By 3; Article in Press},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029009712&doi=10.1007%2fs10586-017-1146-3&partnerID=40&md5=8cf127b4e560b9faaf5042dc71ed7239},
abstract={Named entity recognition (NER) is one of the fundamental problems in many natural language processing applications and the study on NER has great significance. Combining words segmentation and parts of speech analysis, the paper proposes a new NER method based on conditional random fields considering the graininess of candidate entities. The recognition granularity can be divided into two levels: word-based and character-based. We use segmented text to extract characteristics according to the characteristic templates which had been trained in the training phase, and then calculate (Formula presented.) to get the best result from the input sequence. The paper valuates the algorithm for different graininess on large-scale corpus experimentally, and the results show that this method has high research value and feasibility. © 2017 Springer Science+Business Media, LLC},
document_type={Article in Press},
source={Scopus},
}

@ARTICLE{Guo201766,
author={Guo, X. and Zhao, W. and Wang, J. and Wang, C. and Zhang, K. and Chen, L.},
title={A Study of Knowledge Modeling and Retrieval Methods Oriented Towards Innovative Design of Manufacturing Planning},
journal={Jixie Gongcheng Xuebao/Journal of Mechanical Engineering},
year={2017},
volume={53},
number={15},
pages={66-72},
doi={10.3901/JME.2017.15.066},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030655711&doi=10.3901%2fJME.2017.15.066&partnerID=40&md5=fdd055daca17ace090cafa5dfd1ed26d},
abstract={In order to further extend the knowledge of manufacturing planning innovative design, a knowledge model oriented towards process innovative design and relevant retrieval methods are proposed. Before the discussions of characteristics of process knowledge and its expression, this paper presented the goal-based knowledge constituents and the ontology-based logical structure for management, regarding knowledge features and the scope of it. A knowledge organization model of manufacturing planning design is established, in which the function ontology and flow ontology are constructed under the framework of "function+flow+case", and the ontological expression of functions such as abstract flow, workpiece characteristics, and manufacturing resources is achieved. Through the combination of innovative methods and methods of manufacturing planning cases, a goal-oriented knowledge semantic model of manufacturing planning design is proposed. Based on the appositive and hyponymy extension of ontological terms related to function and flow, a process knowledge retrieval model aiming for innovative design, along with a prototype system, are established by means of the extended algorithm, semantic retrieval, and word segmentation. © 2017 Journal of Mechanical Engineering.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Cong2017495,
author={Cong, X. and Li, L.},
title={UGC quality evaluation based on meta-learning and content feature analysis},
journal={Proceedings of 2016 5th International Conference on Network Infrastructure and Digital Content, IEEE IC-NIDC 2016},
year={2017},
pages={495-499},
doi={10.1109/ICNIDC.2016.7974624},
art_number={7974624},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027534422&doi=10.1109%2fICNIDC.2016.7974624&partnerID=40&md5=b4ad7131223ca0dab310903972bc0146},
abstract={With the fast development of Social Networking Services, there has been increasingly vast amount of information published by massive network users. Given this information explosion, how to analyze the quality of User Generated Contents (UGC) automatically becomes a challenging task for researchers. To solve the problem, we need to build an effective UGC quality evaluation system. In the light of our experience, we believe that the textual content of UGC is the key factor for its quality. Hence, we focus on textual content based quality evaluation and classification instead of using UGC publishing related data, such as times being commented and forwarded in this paper. We extract various features of the textual contents based on natural language processing technologies firstly, such as word segmentation, keywords, topic model, sentence parsing, distributed word representation etc. Secondly, we build several base-learning classifiers with different features and different machine learning algorithms to assign UGC contents with four different quality labels. Then, we create the global meta-learning model based on these base classifiers to generate the final quality labels for UGC contents. We have also implemented a series of experiments based on realistic data collected from Tianya Forum and use 10-fold cross-validation to test the model. Results have shown that our proposed meta-learning model performs much better. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jamro2017,
author={Jamro, W.A.},
title={Sindhi Language Processing: A survey},
journal={ICIEECT 2017 - International Conference on Innovations in Electrical Engineering and Computational Technologies 2017, Proceedings},
year={2017},
doi={10.1109/ICIEECT.2017.7916560},
art_number={7916560},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019970443&doi=10.1109%2fICIEECT.2017.7916560&partnerID=40&md5=ad916689a8d425936f9855aedf784fd4},
abstract={In this era of information technology, natural language processing (NLP) has become volatile field because of digital reliance of today's communities. The growth of Internet usage bringing the communities, cultures and languages online. In this regard much of the work has been done of the European and east Asian languages, in the result these languages have reached mature level in terms of computational processing. Despite the great importance of NLP science, still most of the South Asian languages are under developing phase. Sindhi language is one of them, which stands among the most ancient languages in the world. The Sindhi language has a great influence on the large community in Sindh province of Pakistan and some states of India and other countries. But unfortunately, it is at infant level in terms of computational processing, because it has not received such attention of language engineering community, due to its complex morphological structure and scarcity of language resources. Therefore, this study has been carried out in order to summarize the existing work on Sindhi Language Processing (SLP) and to explore future research opportunities, also some potential research problems. This paper will be helpful for the researchers in order to find all the information regarding SLP at one place in a unique way. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gao2017434,
author={Gao, K. and Zhang, S.-S. and Su, S. and Li, M.},
title={Modeling on evaluation object extraction in e-commerce corpus based on semantic feature},
journal={Proceedings of 2016 8th International Conference on Modelling, Identification and Control, ICMIC 2016},
year={2017},
pages={434-438},
doi={10.1109/ICMIC.2016.7804151},
art_number={7804151},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011290552&doi=10.1109%2fICMIC.2016.7804151&partnerID=40&md5=65db6606157eeccd05cb6708dd9e10d9},
abstract={As a newly shopping tool, electronic commerce has been drawing more and more attention of researchers. According to the characteristics of comments diversity, it is necessary to extract evaluation object which is an important component of sentiment information. This paper explores Conditional Random Field (CRF) to do evaluation objects extraction. After observing generally used features in sentiment extraction, this paper conclude all the features into four categories, i.e. word Segmentation, Part-of-speech Tagging (POS), Dependency Parsing, Semantic Dependency Parsing. What's more, focusing on the introduction of new feature semantic dependency is a very vital item in our research. In the experiment, we examine the various features and combinations in the extraction task performance, and make a detailed comparative study. The experimental results confirm that adding the feature of semantic dependency has better performance in terms of the evaluation object extraction. © 2016 University of MEDEA, Algeria.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhai2017245,
author={Zhai, Y. and Liu, L. and Song, W. and Du, C. and Zhao, X.},
title={The application of natural language processing in compiler principle system},
journal={Proceedings of 2017 International Conference on Progress in Informatics and Computing, PIC 2017},
year={2017},
pages={245-248},
doi={10.1109/PIC.2017.8359551},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048154138&doi=10.1109%2fPIC.2017.8359551&partnerID=40&md5=fb78099fe452eaf8aac978579c7b536b},
abstract={Compiling principle is an important course of computer science major, which mainly introduces general principles and basic methods of the construction of compiling programs mainly. Due to high demands of the logic analysis ability, the course bring abstract and unintelligible experience to many students. Thus it is quite difficult for students to master the main points of this course within the limited class time. Based on the requirement above, this paper mainly proposed a method of making use of natural language processing in the research and application of compiling process, which utilizes Maximum Probability Word Segmentation algorithm during the process of lexical analysis and syntax analysis, to offer more effective interface between human and computer. The proposed method can provide students with intuitive and profound knowledge concept in the process of learning how to compile, makes it easier and quicker for students to understand the principle of computer compiling. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tursun2016,
author={Tursun, E. and Ganguly, D. and Osman, T. and Yang, Y.-T. and Abdukerim, G. and Zhou, J.-L. and Liu, Q.},
title={A semisupervised tag-transition-based markovian model for Uyghur morphology analysis},
journal={ACM Transactions on Asian and Low-Resource Language Information Processing},
year={2016},
volume={16},
number={2},
doi={10.1145/2968410},
art_number={8},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997079132&doi=10.1145%2f2968410&partnerID=40&md5=db00c5c84f2228db1d13149001917e43},
abstract={Morphological analysis, which includes analysis of part-of-speech (POS) tagging, stemming, and morpheme segmentation, is one of the key components in natural language processing (NLP), particularly for agglutinative languages. In this article, we investigate the morphological analysis of the Uyghur language, which is the native language of the people in the Xinjiang Uyghur autonomous region of western China. Morphological analysis of Uyghur is challenging primarily because of factors such as (1) ambiguities arising due to the likelihood of association of a multiple number of POS tags with a word stem or a multiple number of functional tags with a word suffix, (2) ambiguous morpheme boundaries, and (3) complex morphopholonogy of the language. Further, the unavailability of a manually annotated training set in the Uyghur language for the purpose of word segmentation makes Uyghur morphological analysis more difficult. In our proposed work, we address these challenges by undertaking a semisupervised approach of learning a Markov model with the help of a manually constructed dictionary of "suffix to tag" mappings in order to predict the most likely tag transitions in the Uyghur morpheme sequence. Due to the linguistic characteristics of Uyghur, we incorporate a prior belief in our model for favoring word segmentations with a lower number of morpheme units. Empirical evaluation of our proposed model shows an accuracy of about 82%. We further improve the effectiveness of the tag transition model with an active learning paradigm. In particular, we manually investigated a subset of words for which the model prediction ambiguity was within the top 20%. Manually incorporating rules to handle these erroneous cases resulted in an overall accuracy of 93.81%. © 2016 ACM.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ye20166993,
author={Ye, Z. and Jia, Z. and Huang, J. and Yin, H.},
title={Part-of-speech tagging based on dictionary and statistical machine learning},
journal={Chinese Control Conference, CCC},
year={2016},
volume={2016-August},
pages={6993-6998},
doi={10.1109/ChiCC.2016.7554459},
art_number={7554459},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987912398&doi=10.1109%2fChiCC.2016.7554459&partnerID=40&md5=34dc9eaadb4113424692fc24a4f920f2},
abstract={Part-of-speech tagging is the basis of Natural Language Processing, and is widely used in information retrieval, text processing and machine translation fields. The traditional statistical machine learning methods of POS tagging rely on the high quality training data, but obtaining the training data is very time-consuming. The methods of POS tagging based on dictionaries ignore the context information, which lead to lower performance. This paper proposed a POS tagging approach which combines methods based on dictionaries and traditional statistical machine learning. The experimental results show that the approach not only can solve the problem that the training data are insufficient in statistical methods, but also can improve the performance of the methods based on dictionaries. The People's Daily corpus in January 1998 is used as testing data, and the accurate rate of POS tagging achieves 95.80%. For the ambiguity word POS tagging, the accuracy achieves 88%. © 2016 TCCT.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2016124,
author={Zhang, C. and He, S. and Gao, X.},
title={A word sense disambiguation system based on Bayesian model},
journal={Proceedings of 2015 4th International Conference on Computer Science and Network Technology, ICCSNT 2015},
year={2016},
pages={124-127},
doi={10.1109/ICCSNT.2015.7490720},
art_number={7490720},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979299389&doi=10.1109%2fICCSNT.2015.7490720&partnerID=40&md5=15ddb824faa10b6baa249d5d15533d20},
abstract={Research on word sense disambiguation (WSD) is of great importance in natural language processing fields. In this paper, a novel word sense disambiguation system is designed in which Bayesian theory is applied to determine correct sense of an ambiguous word. Morphology knowledge in word unit is mined to guide WSD process. Neighboring morphology knowledge of an ambiguous word is used as feature for constructing WSD classifier. Word segmentation tool is integrated into this system and browser/server (B/S) framework is adopted. Experimental results show that the performance of WSD system is good. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Puri2016,
author={Puri, S. and Singh, S.P.},
title={Sentence Detection and Extraction in machine printed imaged document using matching technique},
journal={2015 2nd International Conference on Recent Advances in Engineering and Computational Sciences, RAECS 2015},
year={2016},
doi={10.1109/RAECS.2015.7453382},
art_number={7453382},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966267386&doi=10.1109%2fRAECS.2015.7453382&partnerID=40&md5=7134580c1fb530b6aa5794445cbba254},
abstract={Sentence extraction is a new, challenging and critical step in the printed scanned imaged documents. In this paper, an efficient 4-layered Sentence Detection and Extraction System (SDES) model is proposed which is designed to detect and extract sentences from machine printed imaged document. Its internal details and architecture clearly show that how it processes an image to find out the underlying sentences. The basic idea is to first preprocess the imaged document for noise removal and skew correction, and then textual entities are detected and segmented at page, line and word levels. Firstly, the horizontal and vertical projection profiles are taken to segment and separate the lines and words. After skew correction, two stage Character Based and Word Based Leveled matching and testing are performed, which verify and identify the correct character and word by searching for similar textual characters and words in Character Set Storage (CSS) and Word Pseudo Thesaurus (WPT). If any word pattern is not matched and identified by WPT, then it is stored in the Unmatched Word Storage (UWS) for the future reference. Such testing and verification are used at two levels to increase the accuracy% of SDES, and thereby, reducing the errors. It increases the system performance greatly. Finally, all the sentences of imaged document are extracted. Experimental results are found at the word, character and sentence levels. Their accuracy% results are good which show the high system performance and efficiency. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Matsumoto2016473,
author={Matsumoto, K. and Yoshida, M. and Kita, K.},
title={Sensibility estimation method for youth slang by using sensibility co-occurrence feature vector obtained from microblog},
journal={Proceedings of 2015 IEEE International Conference on Computer and Communications, ICCC 2015},
year={2016},
pages={473-478},
doi={10.1109/CompComm.2015.7387618},
art_number={7387618},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963939887&doi=10.1109%2fCompComm.2015.7387618&partnerID=40&md5=4a1d165f66f914a0b90d4bd3b4765816},
abstract={Social networking sites such as Twitter provide more opportunities to express what people think or intend in short text. In short text, abbreviations such as "ASAP" or "joinus" and emoticons are often used. Because these expressions are not registered into the existing dictionaries, these are analyzed as unknown expressions. That can be a bottleneck for improving accuracy of reputation analysis in text mining. To use context for unknown word clustering is a major method, however, it usually requires word segmentation process and it has weakness for split errors of unknown expressions such as youth slang. In this paper, we proposed a method to obtain the appropriate context even though unknown expressions cause split errors and estimate sensibility expressed in the text. Because the dimensions of the obtained context vector were enormous, we also proposed a method to create a feature vector based on the co-occurrence of the sensibility words as simple expression with low dimension. As an evaluation experiment, the proposed method showed certain accuracy even with the small training data. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Khan2016192,
author={Khan, I.A. and Choi, J.-T.},
title={Lexicon-corpus Based Korean Unknown Foreign Word Extraction and Updating Using Syllable Identification},
journal={Procedia Engineering},
year={2016},
volume={154},
pages={192-198},
doi={10.1016/j.proeng.2016.07.445},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997830871&doi=10.1016%2fj.proeng.2016.07.445&partnerID=40&md5=ab520c5f80a1b76958b5ca824165bd28},
abstract={This paper presents an efficient text mining method focusing on extraction and updating of unknown words (unknown foreign words) to improve data classification and POS tags. Proposed methods can also help to improve the accuracy of mining frequent pattern and association rules from unstructured (textual) data. Many researches have been done by numerous scholars on estimation and segmentation for unknown words, but, they are limited to grammatical and linguistic rules with limited vocabulary. In our project we have consider the fact, that no language is free from the influence of foreign languages, especially, country like Korea where there is a rapid improvement in the area of culture and media and the frequent usage of these foreign languages, resulted in mixing up different languages, their style along with slangs and also abbreviated words in daily life and conversation. The main characteristic of our system is to find such unknown foreign words and update them to appropriate words, which depends on available information through dictionaries. We have also explained the essential natural language processing (NLP) tools used for data processing. Our proposed method used simple but efficient techniques, first it converts the data into structured form, using data preprocessing techniques. In this phase data passes through different stages, such as, cleaning, integration and selection of important data, and then it gets organized into databases structure for further analysis and processing. This database consists of different kinds of dictionaries, our system heavily based on dictionaries. We have manually created various kinds of dictionaries for different kinds of unknown foreign words processing and analysis with the help of our team members. Our proposed methods for discovering and updating foreign unknown word, first discovers the foreign word using morphological analysis with the help of automatically and manually created dictionaries, then suffix trimming and word segmentation, next our algorithm checks for its different written pattern using dictionaries according to its spelling and synonym word in native language (Korean) and also, updates the POS tags. We have tested on different collection of data from economics news, beauty & fashion and college student blogs, the results have shown great efficiency and improvement, and they were adequate enough to research further.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wilkinson20163086,
author={Wilkinson, A. and Zhao, T. and Black, A.W.},
title={Deriving phonetic transcriptions and discovering word segmentations for speech-to-speech translation in low-resource settings},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2016},
volume={08-12-September-2016},
pages={3086-3090},
doi={10.21437/Interspeech.2016-1319},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994229363&doi=10.21437%2fInterspeech.2016-1319&partnerID=40&md5=e60074ad5b17c3015a839fde03b67724},
abstract={We investigate speech-to-speech translation where one language does not have a well-defined written form. We use English-Spanish and Mandarin-English bitext corpora in order to provide both gold-standard text-based translations and experimental results for different levels of automatically derived symbolic representations from speech. We constrain our experiments such that the methods developed can be extended to low-resource languages. We derive different phonetic representations of the source texts in order to model the kinds of transcriptions that can be learned from low-resource-language speech data. We experiment with different methods of clustering the elements of the phonetic representations together into word-like units. We train MT models on the resulting texts, and report BLEU scores for the different representations and clustering methods in order to compare their effectiveness. Finally, we discuss our findings and suggest avenues for future research. Copyright © 2016 ISCA.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nivre20153,
author={Nivre, J.},
title={Towards a universal grammar for natural language processing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9041},
pages={3-16},
doi={10.1007/978-3-319-18111-0_1},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942673144&doi=10.1007%2f978-3-319-18111-0_1&partnerID=40&md5=0c36bdb69403326297326f303bf15cc2},
abstract={Universal Dependencies is a recent initiative to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. In this paper, I outline the motivation behind the initiative and explain how the basic design principles follow from these requirements. I then discuss the different components of the annotation standard, including principles for word segmentation, morphological annotation, and syntactic annotation. I conclude with some thoughts on the challenges that lie ahead. © Springer International Publishing Switzerland 2015.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Qian20151837,
author={Qian, T. and Zhang, Y. and Zhang, M. and Ren, Y. and Ji, D.},
title={A transition-based model for joint segmentation, POS-tagging and normalization},
journal={Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing},
year={2015},
pages={1837-1846},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959905712&partnerID=40&md5=00d6ac7fdc06bd2b83d3dfb4ec314f4a},
abstract={We propose a transition-based model for joint word segmentation, POS tagging and text normalization. Different from previous methods, the model can be trained on standard text corpora, overcoming the lack of annotated microblog corpora. To evaluate our model, we develop an annotated corpus based on microblogs. Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02%, compared to the traditional approach. © 2015 Association for Computational Linguistics.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Takahashi20151186,
author={Takahashi, F. and Mori, S.},
title={Keyboard logs as natural annotations for word segmentation},
journal={Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing},
year={2015},
pages={1186-1196},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959884109&partnerID=40&md5=b951aa277fd249c48fa0691e5b63d495},
abstract={In this paper we propose a framework to improve word segmentation accuracy using input method logs. An input method is software used to type sentences in languages which have far more characters than the number of keys on a keyboard. The main contributions of this paper are: 1) an input method server that proposes word candidates which are not included in the vocabulary, 2) a publicly usable input method that logs user behavior (like typing and selection of word candidates), and 3) a method for improving word segmentation by using these logs. We conducted word segmentation experiments on tweets from Twitter, and showed that our method improves accuracy in this domain. Our method itself is domain-independent and only needs logs from the target domain. © 2015 Association for Computational Linguistics.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kartbayev2015421,
author={Kartbayev, A.},
title={Refining Kazakh word alignment using simulation modeling methods for statistical machine translation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9362},
pages={421-427},
doi={10.1007/978-3-319-25207-0_38},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951267727&doi=10.1007%2f978-3-319-25207-0_38&partnerID=40&md5=acfd3ce218009c0298fc315946630f6c},
abstract={Word alignment play an important role in the training of statistical machine translation systems. We present a technique to refine word alignments at phrase level after the collection of sentences from the Kazakh-English parallel corpora. The estimation technique extracts the phrase pairs from the word alignment and then incorporates them into the translation system for further steps. Although it is a pretty important step in training procedure, an word alignment process often has practical concerns with agglutinative languages. We consider an approach, which is a step towards an improved statistical translation model that incorporates morphological information and has better translation performance. Our goal is to present a statistical model of the morphology dependent procedure, which was evaluated over the Kazakh-English language pair and has obtained an improved BLEU score over state-of-the-art models. © Springer International Publishing Switzerland 2015.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Weinman2014375,
author={Weinman, J.J. and Butler, Z. and Knoll, D. and Feild, J.},
title={Toward integrated scene text reading},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
year={2014},
volume={36},
number={2},
pages={375-387},
doi={10.1109/TPAMI.2013.126},
art_number={6549105},
note={cited By 47},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891621153&doi=10.1109%2fTPAMI.2013.126&partnerID=40&md5=aada740bcad8388e856a99389025870d},
abstract={The growth in digital camera usage combined with a worldly abundance of text has translated to a rich new era for a classic problem of pattern recognition, reading. While traditional document processing often faces challenges such as unusual fonts, noise, and unconstrained lexicons, scene text reading amplifies these challenges and introduces new ones such as motion blur, curved layouts, perspective projection, and occlusion among others. Reading scene text is a complex problem involving many details that must be handled effectively for robust, accurate results. In this work, we describe and evaluate a reading system that combines several pieces, using probabilistic methods for coarsely binarizing a given text region, identifying baselines, and jointly performing word and character segmentation during the recognition process. By using scene context to recognize several words together in a line of text, our system gives state-of-the-art performance on three difficult benchmark data sets. © 1979-2012 IEEE.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pate2014844,
author={Pate, J.K. and Johnson, M.},
title={Syllable weight encodes mostly the same information for english word segmentation as dictionary stress},
journal={EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
year={2014},
pages={844-853},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961296056&partnerID=40&md5=cf2aa54e54fbb32050bfd4d0c6349bd0},
abstract={Stress is a useful cue for English word segmentation. A wide range of computational models have found that stress cues enable a 2-10% improvement in segmentation accuracy, depending on the kind of model, by using input that has been annotated with stress using a pronouncing dictionary. However, stress is neither invariably produced nor unambiguously identifiable in real speech. Heavy syllables, i.e. those with long vowels or syllable codas, attract stress in English. We devise Adaptor Grammar word segmentation models that exploit either stress, or syllable weight, or both, and evaluate the utility of syllable weight as a cue to word boundaries. Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does. © 2014 Association for Computational Linguistics.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ludusan2014560,
author={Ludusan, B. and Versteegh, M. and Jansen, A. and Gravier, G. and Cao, X.-N. and Johnson, M. and Dupoux, E.},
title={Bridging the gap between speech technology and natural language processing: An evaluation toolbox for term discovery systems},
journal={Proceedings of the 9th International Conference on Language Resources and Evaluation, LREC 2014},
year={2014},
pages={560-567},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020779259&partnerID=40&md5=8d4e5fda6f19a9d80393dc250c6be7f7},
abstract={The unsupervised discovery of linguistic terms from either continuous phoneme transcriptions or from raw speech has seen an increasing interest in the past years both from a theoretical and a practical standpoint. Yet, there exists no common accepted evaluation method for the systems performing term discovery. Here, we propose such an evaluation toolbox, drawing ideas from both speech technology and natural language processing. We first transform the speech-based output into a symbolic representation and compute five types of evaluation metrics on this representation: the quality of acoustic matching, the quality of the clusters found, and the quality of the alignment with real words (type, token, and boundary scores). We tested our approach on two term discovery systems taking speech as input, and one using symbolic input. The latter was run using both the gold transcription and a transcription obtained from an automatic speech recognizer, in order to simulate the case when only imperfect symbolic information is available. The results obtained are analysed through the use of the proposed evaluation metrics and the implications of these metrics are discussed.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sun2014563,
author={Sun, X. and Li, W. and Wang, H. and Lu, Q.},
title={Feature-frequency-adaptive on-line training for fast and accurate natural language processing},
journal={Computational Linguistics},
year={2014},
volume={40},
number={3},
pages={563-586},
doi={10.1162/COLI_a_00193},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910067653&doi=10.1162%2fCOLI_a_00193&partnerID=40&md5=0e9182cc0ab48126232f79eee985559a},
abstract={Training speed and accuracy are two major concerns of large-scale natural language processing systems. Typically, we need to make a tradeoff between speed and accuracy. It is trivial to improve the training speed via sacrificing accuracy or to improve the accuracy via sacrificing speed. Nevertheless, it is nontrivial to improve the training speed and the accuracy at the same time, which is the target of this work. To reach this target, we present a new training method, featurefrequency-adaptive on-line training, for fast and accurate training of natural language processing systems. It is based on the core idea that higher frequency features should have a learning rate that decays faster. Theoretical analysis shows that the proposed method is convergent with a fast convergence rate. Experiments are conducted based on well-known benchmark tasks, including named entity recognition, word segmentation, phrase chunking, and sentiment analysis. These tasks consist of three structured classification tasks and one non-structured classification task, with binary features and real-valued features, respectively. Experimental results demonstrate that the proposed method is faster and at the same time more accurate than existing methods, achieving state-of-the-art scores on the tasks with different characteristics. © 2014 Association for Computational Linguistics.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhu201479,
author={Zhu, L. and Li, H. and Wang, S. and Li, C.},
title={Exploration and development of text knowledge extraction},
journal={Information Technology and Computer Application Engineering - Proceedings of the 2013 International Conference on Information Technology and Computer Application Engineering, ITCAE 2013},
year={2014},
pages={79-83},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900429048&partnerID=40&md5=2427896a84c7d70a080a58e189d138f1},
abstract={Text knowledge extraction technology has been applied in many fields, but few practices in the field of petroleum exploration and development. In this paper,we comprehensively utilize a statistical and natural language understanding technology to extract knowledge from the articles in the field of petroleum exploration and development. First of all, we get the key words and the core sentences containing article process model. Then after the word segmentation and phrase recognition, we use semantic templates to match and extract semantic information from the key sentences. The experimental results show that this method achieves 70% accuracy rate in keywords spotting and process model extraction. © 2014 Taylor & Francis Group, London.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gao201426,
author={Gao, H.-Y. and Nian, F.-X.},
title={User reviews based product feature mining of mobile phones in E-commerce},
journal={Beijing Ligong Daxue Xuebao/Transaction of Beijing Institute of Technology},
year={2014},
volume={34},
pages={26-30},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922612126&partnerID=40&md5=9dd8c8cbddd927f65a1f5b4dc4de37bc},
abstract={Product review mining aims to quickly extract useful information from massive comments published by users and adopt an intuitive way to help consumers make purchasing decisions. Fine-grained product feature mining is very important, however, the product characteristics semantics (upper and lower characteristics, synonymous features) analysis is inadequate on existing product reviews researches. Firstly, the ontology of mobile phone features was constructed based on mobile phone descriptions. Then crawling programs was employed to get product comments and followed by conducting words segmentation, part of speech tagging, getting rid of the repeats and other pretreatments. Using the Apriori algorithm, the appropriate product features from user's perspective were extracted. Combining with HowNet dictionary, semantic extension was carried to improve the ontology of product features, which will facilitate further accurate sentiment analysis of the product reviews. ©, 2014, Beijing Institute of Technology. All right reserved.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zeng2013190,
author={Zeng, L. and Li, F.},
title={A classification-based approach for implicit feature identification},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2013},
volume={8202 LNAI},
pages={190-202},
doi={10.1007/978-3-642-41491-6_18},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893031086&doi=10.1007%2f978-3-642-41491-6_18&partnerID=40&md5=77380a4f99a8805341450d35c070b926},
abstract={In recent years, sentiment analysis and opinion mining has grown to be one of the most active research areas. Most of the existing researches on feature-level opinion mining are dedicated to extract explicitly appeared features and opinion words. However, among the numerous kinds of reviews on the web, there are a significant number of reviews that contain only opinion words which imply some product features. The identification of such implicit features is still one of the most challenge tasks in opinion mining. In this paper, we propose a classification-based approach to deal with the task of implicit feature identification. Firstly, by exploiting the word segmentation, part-of-speech(POS) tagging and dependency parsing, a rule based method to extract the explicit feature-opinion pairs is presented. Secondly, the feature-opinion pairs for each opinion word are clustered and the training documents for each clustered feature-opinion pair are then constructed. Finally, the identification of implicit features is formulated into a classification-based feature selection. Experiments demonstrate that our approach outperforms the existing methods significantly. © Springer-Verlag 2013.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang20133039,
author={Wang, N. and Xu, L. and Li, L.Y. and Xu, L.X.},
title={Design and implementation of an automatic scoring subjective question system based on domain ontology},
journal={Advanced Materials Research},
year={2013},
volume={753-755},
pages={3039-3042},
doi={10.4028/www.scientific.net/AMR.753-755.3039},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884861075&doi=10.4028%2fwww.scientific.net%2fAMR.753-755.3039&partnerID=40&md5=e9bb74c4f7d2180b7130df66ae8c1139},
abstract={Automated assessment technology for subjective tests is one of the key techniques of exam systems. A model based on domain ontology is proposed in this paper, which can be used in exam systems to estimate subjective tests. After analysing the present research status of subjective automated assessment technology, the paper makes a study on the construction method of domain ontology by taking software engineering domain as an example. Semantic similarity calculation based on domain ontology is used for automatic assessment in this paper. The automatic assessment system can divide a sentence into a series of phrases by using the natural language processing technology and get the score by evaluating the semantic similarity of the student's answer. The experiments show that the results of the system which has certain valuable feasibility and applicability are credible and the scoring errors are acceptable. © (2013) Trans Tech Publications, Switzerland.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Goldberg2013121,
author={Goldberg, Y. and Elhadad, M.},
title={Word segmentation, unknown-word resolution, and morphological agreement in a Hebrew parsing system},
journal={Computational Linguistics},
year={2013},
volume={39},
number={1},
pages={121-160},
doi={10.1162/COLI_a_00137},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874618096&doi=10.1162%2fCOLI_a_00137&partnerID=40&md5=78e968695482a4747271d37e38b3ef3b},
abstract={We present a constituency parsing system for Modern Hebrew. The system is based on the PCFG-LA parsing method of Petrov et al. (2006), which is extended in various ways in order to accommodate the specificities of Hebrew as a morphologically rich language with a small treebank. We show that parsing performance can be enhanced by utilizing a language resource external to the treebank, specifically, a lexicon-based morphological analyzer. We present a computational model of interfacing the external lexicon and a treebank-based parser, also in the common case where the lexicon and the treebank follow different annotation schemes. We show that Hebrew word-segmentation and constituency-parsing can be performed jointly using CKY lattice parsing. Performing the tasks jointly is effective, and substantially outperforms a pipelinebased model. We suggest modeling grammatical agreement in a constituency-based parser as a filter mechanism that is orthogonal to the grammar, and present a concrete implementation of the method. Although the constituency parser does not make many agreement mistakes to begin with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make. These contributions extend outside of the scope of Hebrew processing, and are of general applicability to the NLP community. Hebrew is a specific case of a morphologically rich language, and ideas presented in this work are useful also for processing other languages, including English. The lattice-based parsingmethodology is useful in any case where the input is uncertain. Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant for any language with a small treebank. © 2013 Association for Computational Linguistics.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Namer201392,
author={Namer, F.},
title={A Rule-Based Morphosemantic Analyzer for French for a Fine-Grained Semantic Annotation of Texts},
journal={Communications in Computer and Information Science},
year={2013},
volume={380 CCIS},
pages={92-114},
doi={10.1007/978-3-642-40486-3_6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904608636&doi=10.1007%2f978-3-642-40486-3_6&partnerID=40&md5=39c06edc8d7ffbce4c228d77c841004d},
abstract={We describe DériF, a rule-based morphosemantic analyzer developed for French. Unlike existing word segmentation tools, DériF provides derived and compound words with various sorts of semantic information: (1) a definition, computed from both the base meaning and the specificities of the morphological rule; (2) lexical-semantic features, inferred from general linguistic properties of derivation rules; (3) lexical relations (synonymy, (co-)hyponymy) with other, morphologically unrelated, words belonging to the same analyzed corpus. © Springer-Verlag Berlin Heidelberg 2013.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Elsner201342,
author={Elsner, M. and Goldwater, S. and Feldman, N.H. and Wood, F.},
title={A joint learning model of word segmentation, lexical acquisition, and phonetic variability},
journal={EMNLP 2013 - 2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
year={2013},
pages={42-54},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926303216&partnerID=40&md5=1bb60c0df570ef1356d6b59e11aae487},
abstract={We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We define the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed first, our model is qualitatively more similar to human learners. On data with variable pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence. © 2013 Association for Computational Linguistics.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wu20121713,
author={Wu, X. and Fan, W. and Yu, Y.},
title={Sembler: Ensembling crowd sequential labeling for improved quality},
journal={Proceedings of the National Conference on Artificial Intelligence},
year={2012},
volume={2},
pages={1713-1719},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868274778&partnerID=40&md5=056a981811f122859f050ce2dfaa4ab1},
abstract={Many natural language processing tasks, such as named entity recognition (NER), part of speech (POS) tagging, word segmentation, and etc., can be formulated as sequential data labeling problems. Building a sound labeler requires very large number of correctly labeled training examples, which may not always be possible. On the other hand, crowdsourcing provides an inexpensive yet efficient alternative to collect manual sequential labeling from non-experts. However the quality of crowd labeling cannot be guaranteed, and three kinds of errors are typical: (1) incorrect annotations due to lack of expertise (e.g., labeling gene names from plain text requires corresponding domain knowledge); (2) ignored or omitted annotations due to carelessness or low confidence; (3) noisy annotations due to cheating or vandalism. To correct these mistakes, we present Sembler, a statistical model for ensembling crowd sequential labelings. Sembler considers three types of statistical information: (1) the majority agreement that proves the correctness of an annotation; (2) correct annotation that improves the credibility of the corresponding annotator; (3) correct annotation that enhances the correctness of other annotations which share similar linguistic or contextual features. We evaluate the proposed model on a real Twitter and a synthetical biological data set, and find that Sembler is particularly accurate when more than half of annotators make mistakes. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chang2012828,
author={Chang, S.-F. and Yu, L.-C.},
title={A multi-function MSN robot for campus information seeking},
journal={Advanced Science Letters},
year={2012},
volume={9},
pages={828-832},
doi={10.1166/asl.2012.2643},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862902958&doi=10.1166%2fasl.2012.2643&partnerID=40&md5=4582590a11bfce38b51c8e993aca82e6},
abstract={An MSN robot is an intelligent system that can chat with users to accomplish a specific task. In this paper, we present a framework to build an MSN robot that can chat with students to provide campus information. The MSN robot has two major functions; that is it can provide two kinds of information: curriculum information and living information. The curriculum information includes a road map of all courses so that students can register courses according to their interests and future plans. The living information includes the information of restaurants, houses, transportation, and scenic spots around the campus. The MSN robot consists of three components: word segmentation and POS tagging, natural language understanding, and response text generation. The chat between the robot and students starts with an input of free-text question about campus information. Once the question is received, the natural language understanding component determines the meaning of the input question using classification methods. Finally, the output component generates a text response using a set of predefined templates. The MSN robot is implemented using the Dot MSN, an open source package that can connect the MSN Messenger service. The experimental results show that among several classification methods the support vector machine (SVM) achieves the best performance in classifying users' input questions into curriculum information or living information. © 2012 American Scientific Publishers.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xue2011,
author={Xue, H. and Yang, Y. and Turghun, O. and Li, X. and Zhang, R.},
title={Uyghur word segmentation using a combination of rules and statistics},
journal={Advances in Information Sciences and Service Sciences},
year={2011},
volume={3},
number={11},
doi={10.4156/AISS.vol3.issue11.13},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855209817&doi=10.4156%2fAISS.vol3.issue11.13&partnerID=40&md5=97b20d57922d4f990a0eb87f5410062c},
abstract={Rich morphology of Uyghur produces a large number of words and leads to high out of vocabulary (OOV) rates that can cause many errors in Uyghur natural language processing (NLP). Morphological word segmentation is the very important component to overcome this problem caused by Uyghur morphology. This paper depicts some morphological rules by analyzing the universal structure of Uyghur words and presents a partly supervised word segmentation method. In this method, the suffix corpus was utilized to give all the possible morphological word segmentations, from which the optimal word segmentation is selected by the MAP-based model. In addition, cascaded language model was used to improve the accuracy of word segmentation. The test set composed of 5000 words was collected and segmented by hand. The experiment on this test set was given and experimental results show that the proposed method was more effective.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Laga201123,
author={Laga, T. and Zhao, X.},
title={Theoretical framework of Mongolian word segmentation specification for information processing},
journal={Proceedings - 2011 International Conference on Asian Language Processing, IALP 2011},
year={2011},
pages={23-25},
doi={10.1109/IALP.2011.45},
art_number={6121461},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862939676&doi=10.1109%2fIALP.2011.45&partnerID=40&md5=0d469ef24f9f1ba2fe286255d9ad9685},
abstract={The establishment of Contemporary Mongolian word segmentation specification for information processing has a great significance in the standardization of information processing, the compatibleness of different systems, the sharing of corpus, grammatical analysis, and POS tagging. The present paper studies the framework of Mongolian word segmentation including guidelines, formulating principles, styles, scopes of segmentation units, establishment foundation, structure of the specification and so on, and lays the theoretical foundation for this specification. © 2011 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hewlett201139,
author={Hewlett, D. and Cohen, P.},
title={Word segmentation as general chunking},
journal={CoNLL 2011 - Fifteenth Conference on Computational Natural Language Learning, Proceedings of the Conference},
year={2011},
pages={39-47},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862288892&partnerID=40&md5=26aaa7e1a5aecc23904240ec78ba7340},
abstract={During language acquisition, children learn to segment speech into phonemes, syllables, morphemes, and words. We examine word segmentation specifically, and explore the possibility that children might have general purpose chunking mechanisms to perform word segmentation. The Voting Experts (VE) and Bootstrapped Voting Experts (BVE) algorithms serve as computational models of this chunking ability. VE finds chunks by searching for a particular information-theoretic signature: low internal entropy and high boundary entropy. BVE adds to VE the ability to incorporate information about word boundaries previously found by the algorithm into future segmentations. We evaluate the general chunking model on phonemically encoded corpora of child-directed speech, and show that it is consistent with empirical results in the developmental literature. We argue that it offers a parsimonious alternative to special purpose linguistic models. © 2011 Association for Computational Linguistics.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hewlett2011540,
author={Hewlett, D. and Cohen, P.},
title={Fully unsupervised word segmentation with BVE and MDL},
journal={ACL-HLT 2011 - Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
year={2011},
volume={2},
pages={540-545},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859036614&partnerID=40&md5=3ec416230c3f540c6806480dbe8fb3a1},
abstract={Several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. However, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the Minimum Description Length (MDL) principle. Therefore, it is necessary to generate a set of candidate segmentations and select between them according to the MDL principle. We evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the Bootstrapped Voting Experts algorithm consistently outperforms other methods when paired with MDL. © 2011 Association for Computational Linguistics.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2011357,
author={Wang, K. and Thrasher, C. and Hsu, B.-J.},
title={Web scale NLP: A case study on URL word breaking},
journal={Proceedings of the 20th International Conference on World Wide Web, WWW 2011},
year={2011},
pages={357-366},
doi={10.1145/1963405.1963457},
note={cited By 32},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052123854&doi=10.1145%2f1963405.1963457&partnerID=40&md5=f0884c46e20c2b2cf89b5c9a1bb92dd2},
abstract={This paper uses the URL word breaking task as an example to elaborate what we identify as crucialin designingstatistical natural language processing (NLP) algorithmsfor Web scale applications: (1) rudimentary multilingual capabilities to cope with the global nature of the Web, (2) multi-style modeling to handle diverse language styles seen in the Web contents, (3) fast adaptation to keep pace with the dynamic changes of the Web, (4) minimal heuristic assumptions for generalizability and robustness, and (5) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost. We first show that the state-of-the-art word breaking techniquescan be unified and generalized under the Bayesian minimum risk (BMR) framework that, using a Web scale N-gram, can meet the first three requirements. We discuss how the existing techniques can be viewed asintroducing additional assumptions to the basic BMR framework, and describe a generic yet efficient implementation called word synchronous beam search.Testing the framework and its implementation on a series of large scale experiments reveals the following. First, the language style used to build the modelplays a critical role in the word breaking task, and the most suitable for the URL word breaking task appears to be that ofthe document title where the best performance is obtained. Models created from other language styles, such as from document body, anchor text, and even queries, exhibit varying degrees of mismatch. Although all styles benefit from increasing modeling power which, in our experiments, corresponds tothe use of a higher order N-gram, the gain is most recognizable for the title model. The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models, but are less effective and, in many cases,lead to poorer performance than the matched model with minimal assumptions. For the matched model based on document titles, an accuracy rate of 97.18% can already be achieved using simple trigram without any heuristics. Copyright © 2011 by the Association for Computing Machinery, Inc. (ACM).},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kesidis2011131,
author={Kesidis, A.L. and Galiotou, E. and Gatos, B. and Pratikakis, I.},
title={A word spotting framework for historical machine-printed documents},
journal={International Journal on Document Analysis and Recognition},
year={2011},
volume={14},
number={2},
pages={131-144},
doi={10.1007/s10032-010-0134-4},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957479567&doi=10.1007%2fs10032-010-0134-4&partnerID=40&md5=2de08ceded8dd7dcf2411d64ea992e53},
abstract={In this paper, we propose a word spotting framework for accessing the content of historical machine-printed documents without the use of an optical character recognition engine. A preprocessing step is performed in order to improve the quality of the document images, while word segmentation is accomplished with the use of two complementary segmentation methodologies. In the proposed methodology, synthetic word images are created from keywords, and these images are compared to all the words in the digitized documents. A user feedback process is used in order to refine the search procedure. The methodology has been evaluated in early Modern Greek documents printed during the seventeenth and eighteenth century. In order to improve the efficiency of accessing and search, natural language processing techniques have been addressed that comprise a morphological generator that enables searching in documents using only a base word-form for locating all the corresponding inflected word-forms and a synonym dictionary that further facilitates access to the semantic context of documents. © 2010 Springer-Verlag.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lehal2011136,
author={Lehal, G.S. and Saini, T.S.},
title={A transliteration based word segmentation system for Shahmukhi script},
journal={Communications in Computer and Information Science},
year={2011},
volume={139 CCIS},
pages={136-143},
doi={10.1007/978-3-642-19403-0_22},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952050316&doi=10.1007%2f978-3-642-19403-0_22&partnerID=40&md5=114db1d7c3acc0aad18224e7bf0bdb99},
abstract={Word Segmentation is an important prerequisite for almost all Natural Language Processing (NLP) applications. Since word is a fundamental unit of any language, almost every NLP system first needs to segment input text into a sequence of words before further processing. In this paper, Shahmukhi word segmentation has been discussed in detail. The presented word segmentation module is part of Shahmukhi-Gurmukhi transliteration system. Shahmukhi script is usually written without short vowels leading to ambiguity. Therefore, we have designed a novel approach for Shahmukhi word segmentation in which we used target Gurmukhi script lexical resources instead of Shahmukhi resources. We employ a combination of techniques to investigate an effective algorithm by applying syntactical analysis process using Shahmukhi Gurmukhi dictionary, writing system rules and statistical methods based on n-grams models. © 2011 Springer-Verlag.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Paul2011690,
author={Paul, M. and Finch, A. and Sumita, E.},
title={Integration of multiple bilingually-trained segmentation schemes into statistical machine translation},
journal={IEICE Transactions on Information and Systems},
year={2011},
volume={E94-D},
number={3},
pages={690-697},
doi={10.1587/transinf.E94.D.690},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952129828&doi=10.1587%2ftransinf.E94.D.690&partnerID=40&md5=f02f8345f7fbdde304c3c697b3f3d408},
abstract={This paper proposes an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches. The method can be applied to any language pair in which the source language is unsegmented and the target language segmentation is known. In the first step, an iterative bootstrap method is applied to learn multiple segmentation schemes that are consistent with the phrasal segmentations of an SMT system trained on the resegmented bitext. In the second step, multiple segmentation schemes are integrated into a single SMT system by characterizing the source language side and merging identical translation pairs of differently segmented SMT models. Experimental results translating five Asian languages into English revealed that the proposed method of integrating multiple segmentation schemes outperforms SMT models trained on any of the learned word segmentations and performs comparably to available monolingually built segmentation tools. © 2011 The Institute of Electronics, Information and Communication Engineers.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Weber2010,
author={Weber, C. and Handl, J.},
title={A base-form lexicon of content words for correct word segmentation and syntactic-semantic annotation},
journal={Semantic Approaches in Natural Language Processing - Proceedings of the Conference on Natural Language Processing 2010, KONVENS},
year={2010},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894110931&partnerID=40&md5=f6254b4cf0ee2665f59a6be8a53db773},
abstract={One issue in natural language processing is the interaction between a rule-based computational morphology and a syntactic-semantic analysis system. This is because derivational and compound word forms raise the question of how to deal with ambiguities caused by the rule-based analyser, and how to add additional information like valency to a derivational or compound word form if its valency frames differ from those of its root word. In this paper we propose a lexicon design addressing both of these issues. We evaluate our design in the context of a large-scale morphological analysis system for German in which the lexicon serves as an interface between morphology and syntax. In doing so, we aim at enriching the wellformed analysis results with additional information so that an adequate syntactic-semantic analysis can be ensured.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yuan2010,
author={Yuan, L.},
title={Improvement for the automatic part-of-speech tagging based on Hidden Markov Model},
journal={ICSPS 2010 - Proceedings of the 2010 2nd International Conference on Signal Processing Systems},
year={2010},
volume={1},
pages={V1744-V1747},
doi={10.1109/ICSPS.2010.5555259},
art_number={5555259},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957282060&doi=10.1109%2fICSPS.2010.5555259&partnerID=40&md5=0497a5691e41adad2300192f94dc2f6f},
abstract={In this paper, the Markov Family Models, a kind of statistical Models was firstly introduced. Under the assumption that the probability of a word depends both on its own tag and previous word, but its own tag and previous word are independent if the word is known, we simplify the Markov Family Model and use for part-of-speech tagging successfully. Experimental results show that this part-of-speech tagging method based on Markov Family Model has greatly improved the precision comparing the conventional POS tagging method based on Hidden Markov Model under the same testing conditions. The Markov Family Model is also very useful in other natural language processing technologies such as word segmentation, statistical parsing, text-to-speech, optical character recognition, etc. © 2010 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zheng2010181,
author={Zheng, X.-L. and Zhou, C.-L. and Zeng, H.-L.},
title={Song ci style automatic identification},
journal={Journal of Donghua University (English Edition)},
year={2010},
volume={27},
number={2},
pages={181-184},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649613712&partnerID=40&md5=16c5881f1157390f094abd8ef4e13de6},
abstract={To identify Song Ci style automatically, we put forward a novel stylistic text categorization approach based on words and their semantic in this paper. And a modified special word segmentation method, a new semantic relativity computing method based on HowNet along with the corresponding word sense disambiguation method are proposed to extract words and semantic features from Song Ci. Experiments are carried out and the results show that these methods are effective.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Parlak2009782,
author={Parlak, S. and Saraclar, M.},
title={Spoken information retrieval for turkish broadcast news},
journal={Proceedings - 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009},
year={2009},
pages={782-783},
doi={10.1145/1571941.1572126},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449149712&doi=10.1145%2f1571941.1572126&partnerID=40&md5=f15d01d8f71b2fbcf688a299ff1270e8},
abstract={Speech Retrieval systems utilize automatic speech recognition (ASR) to generate textual data for indexing. However, automatic transcriptions include errors, either because of out-of-vocabulary (OOV) words or due to ASR inaccuracy. In this work, we address spoken information retrieval in Turkish, a morphologically rich language where OOV rates are high. We apply several techniques, such as using subword units and indexing alternative hypotheses, to cope with the OOV problem and ASR inaccuracy. Experiments are performed on our Turkish Broadcast News (BN) Corpus which also incorporates a spoken IR collection. Results indicate that word segmentation is quite useful but the efficiency of indexing alternative hypotheses depends on retrieval type.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cho200929,
author={Cho, H.-C. and Lee, D.-G. and Lee, J.-T. and Stenetorp, P. and Tsujii, J. and Rim, H.-C.},
title={A novel word segmentation approach for written languages with word boundary markers},
journal={ACL-IJCNLP 2009 - Joint Conf. of the 47th Annual Meeting of the Association for Computational Linguistics and 4th Int. Joint Conf. on Natural Language Processing of the AFNLP, Proceedings of the Conf.},
year={2009},
pages={29-32},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859908122&partnerID=40&md5=90cbf81c86ca010ee0a13a41a0957027},
abstract={Most NLP applications work under the assumption that a user input is error-free; thus, word segmentation (WS) for written languages that use word boundary markers (WBMs), such as spaces, has been regarded as a trivial issue. However, noisy real-world texts, such as blogs, e-mails, and SMS, may contain spacing errors that require correction before further processing may take place. For the Korean language, many researchers have adopted a traditional WS approach, which eliminates all spaces in the user input and re-inserts proper word boundaries. Unfortunately, such an approach often exacerbates the word spacing quality for user input, which has few or no spacing errors; such is the case, because a perfect WS model does not exist. In this paper, we propose a novel WS method that takes into consideration the initial word spacing information of the user input. Our method generates a better output than the original user input, even if the user input has few spacing errors. Moreover, the proposed method significantly outperforms a state-of-the-art Korean WS model when the user input initially contains less than 10% spacing errors, and performs comparably for cases containing more spacing errors. We believe that the proposed method will be a very practical pre-processing module. © 2009 ACL and AFNLP.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2009,
title={ACL-IJCNLP 2009 - Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and 4th International Joint Conference on Natural Language Processing of the AFNLP, Proceedings of the Conference},
journal={ACL-IJCNLP 2009 - Joint Conf. of the 47th Annual Meeting of the Association for Computational Linguistics and 4th Int. Joint Conf. on Natural Language Processing of the AFNLP, Proceedings of the Conf.},
year={2009},
page_count={602},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859912182&partnerID=40&md5=9d5951e8fa79ccdda280a2b72d6d8aca},
abstract={The proceedings contain 174 papers. The topics discussed include: unsupervised argument identification for semantic role labeling; exploiting heterogeneous treebanks for parsing; cross language dependency parsing using a bilingual lexicon; topological field parsing of german; unsupervised multilingual grammar induction; reinforcement learning for mapping instructions to actions; learning semantic correspondences with less supervision; bayesian unsupervised word segmentation with nested pitman-yor language modeling; knowing the unseen: estimating vocabulary size over unseen samples; a ranking approach to stress prediction for letter-to-phoneme conversion; reducing the annotation effort for letter-to-phoneme conversion; revisiting pivot language approach for machine translation; and efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Gabay200861,
author={Gabay, D. and Ziv, B.E. and Elhadad, M.},
title={Using wikipedia links to construct word segmentation corpora},
journal={AAAI Workshop - Technical Report},
year={2008},
volume={WS-08-15},
pages={61-63},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449775446&partnerID=40&md5=ad7bc10a1c3ed07e582931513ab8ae88},
abstract={Tagged corpora are essential for evaluating and training natural language processing tools. The cost of constructing large enough manually tagged corpora is high, even when the annotation level is shallow. This article describes a simple method to automatically create a partially tagged corpus, using Wikipedia hyperlinks. The resulting corpus contains information about the correct segmentation of 523,599 non-consecutive words in 363,090 sentences. We used our method to construct a corpus of Modern Hebrew (which we have made available at http://www.cs.bgu.ac.il/-nlpproj). The method can also be applied to other languages where word segmentation is difficult to determine, such as East and South-East Asian languages. Copyright © 2008.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Qinglin2008,
author={Qinglin, G. and Kehe, W. and Wei, L.},
title={The research and realization about question answer system based on natural language processing},
journal={Second International Conference on Innovative Computing, Information and Control, ICICIC 2007},
year={2008},
doi={10.1109/ICICIC.2007.585},
art_number={4428144},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-39049090472&doi=10.1109%2fICICIC.2007.585&partnerID=40&md5=67b63faf00518c605e9d82d8eeb17f0f},
abstract={Automatic Question Answer System(QAS)is a kind of high-powered software system based on Internet. Its key technology is the interrelated technology based on natural language understanding, including the construction of knowledge base and corpus, the Word Segmentation and POS Tagging of text, the Grammatical Analysis and Semantic Analysis of sentences etc. This thesis dissertated mainly the denotation of knowledge-information based on semantic network in QAS, the stochastic syntax-parse model named LSF of knowledge-information in QAS, the structure and constitution of QAS. And the LSF model parameters were exercised; it proved that they are feasible. At the same time, through "the limited-domain QAS" which was exploited for banks by us, these technologies are proved effective andpropagable. © 2007 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guo200792,
author={Guo, Q. and Li, C.},
title={The research on the application of text clustering and natural language understanding in automatic abstracting},
journal={Proceedings - Fourth International Conference on Fuzzy Systems and Knowledge Discovery, FSKD 2007},
year={2007},
volume={4},
pages={92-96},
doi={10.1109/FSKD.2007.584},
art_number={4406353},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44049094329&doi=10.1109%2fFSKD.2007.584&partnerID=40&md5=1786ee4735dd963c58059a2f015c195a},
abstract={A method of realization of Automatic Abstracting based on Text Clustering and Natural Language Understanding is brought forward, aimed at overcoming shortages of some current methods. The method makes use of text Clustering and can realize Automatic Abstracting of multi- documents. The algorithm of twice Word Segmentation based on the Title and FirstSentences in Paragraphs is brought forward. Its precision and recall is above 95% For a specific domain on plastics, an Automatic Abstracting system named TCAAS is implemented. The precision and recall of multidocument's Automatic Abstracting is above 75% And experiments do prove that it is feasible to use the method to develop a domain Automatic Abstracting System, which is valuable for further study in more depth. © 2007 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang2007962,
author={Zhang, L. and Lu, R.-Z.},
title={Understanding speech utterances in mandarin dialogue system},
journal={WSEAS Transactions on Information Science and Applications},
year={2007},
volume={4},
number={5},
pages={962-967},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248332570&partnerID=40&md5=c838fc8d4a9a2cc1bce9422bb5a8ff3d},
abstract={In this paper, we present a mandarin spoken dialogue system - STRQS (Shanghai Traffic Route Querying System), which is used for querying best traffic route between any two locations in Shanghai. A series of language processing strategies is used to understand speech utterances. The understanding processing is done in three steps: First, word segmentation and part-of-speech tagging module splits the utterance into words and labels them with semantic categories. The second step is a robust partial parsing process. Parsing is based on Unification Grammar (UG). An augmented chart algorithm with feature computing is implemented. Finally, the parsed utterance is associated with a semantic interpreter by a frame module. Semantic based analysis method we developed can directly extract information from the output of a speech recognizer, which contains errors and ill-formed components. The testing results demonstrate the robustness of our approach.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Álvarez200735,
author={Álvarez, M.A.M. and Pombo, J.O. and Barcala Rodríguez, F.M. and Gil, J.G.},
title={Practical application of one-pass Viterbi algorithm in tokenization and part-of-speech tagging},
journal={International Conference Recent Advances in Natural Language Processing, RANLP},
year={2007},
volume={2007-January},
pages={35-40},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959058631&partnerID=40&md5=94a0c3d0a722f66f83f956b81baf2923},
abstract={Sentence word segmentation and Part-Of-Speech (POS) tagging are common preprocessing tasks for many Natural Language Processing (NLP) applications. This paper presents a practical application for POS tagging and segmentation disambiguation using an extension of the one-pass Viterbi algorithm called Viterbi-N. We introduce the internals of the developed system, which is based on lattices and a stochastic model built using second order Hidden Markov Models (HMMs). Also, we present the results of an evaluation process and the analysis of the error cases. The results achieved suggest that the Viterbi-N algorithm applied on lattices allows POS tagging and segmentation disambiguation to be accomplished in a common process. Although the tests were done for the Galician language, the solution proposed could be easily exported to other languages.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sornlertlamvanich20071565,
author={Sornlertlamvanich, V. and Charoenporn, T. and Tongchim, S. and Kruengkrai, C. and Isahara, H.},
title={Statistical-based approach to non-segmented language processing},
journal={IEICE Transactions on Information and Systems},
year={2007},
volume={E90-D},
number={10},
pages={1565-1573},
doi={10.1093/ietisy/e90-d.10.1565},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-68249162367&doi=10.1093%2fietisy%2fe90-d.10.1565&partnerID=40&md5=b975a7c339586b8e49372a82a28f30a5},
abstract={Several approaches have been studied to cope with the exceptional features of non-segmented languages. When there is no explicit information about the boundary of a word, segmenting an input text is a formidable task in language processing. Not only the contemporary word list, but also usages of the words have to be maintained to cover the use in the current texts. The accuracy and efficiency in higher processing do heavily rely on this word boundary identification task. In this paper, we introduce some statistical based approaches to tackle the problem due to the ambiguity in word segmentation. The word boundary identification problem is then defined as a part of others for performing the unified language processing in total. To exhibit the ability in conducting the unified language processing, we selectively study the tasks of language identification, word extraction, and dictionary-less search engine. Copyright © 2007 The Institute of Electronics, Information and Communication Engineers.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lee200728,
author={Lee, D.-G. and Rim, H.-C. and Yook, D.},
title={Automatic word spacing using probabilistic models based on character n-grams},
journal={IEEE Intelligent Systems},
year={2007},
volume={22},
number={1},
pages={28-35},
doi={10.1109/MIS.2007.4},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847611998&doi=10.1109%2fMIS.2007.4&partnerID=40&md5=f49f0e501e9b8b12dedfb8fee532c736},
abstract={Probabilistic models based on Hidden Markov models (HMM) for automatic word spacing that use characters n-grams, which is a sub-sequence of n characters in a given character sequence, are discussed. Automatic word spacing is a preprocessing techniques used for correcting boundaries between words in a sentence containing spacing errors. These model can be effectively applied to a natural language with a small character set, such as English, using character n-grams that are larger than trigrams. These models, which are language independent and can be effectively used for languages having word spacing, can also be used for word segmentation in the languages without explicit word spacing. These models, by generalizing the HMMs, can consider a broad context and estimate accurate probabilities.},
document_type={Article},
source={Scopus},
}

@ARTICLE{VanDalen2005211,
author={Van Dalen, R.C. and Wiggers, P. and Rothkrantz, L.J.M.},
title={Modelling lexical stress},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2005},
volume={3658 LNAI},
pages={211-218},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646048041&partnerID=40&md5=fbc7d621761b047f9ed7d44cfb94fb6a},
abstract={Human listeners use lexical stress for word segmentation and disambiguation. We look into using lexical stress for speech recognition by examining a Dutch-language corpus. We propose that different spectral features are needed for different phonemes and that, besides vowels, consonants should be taken into account. © Springer-Verlag Berlin Heidelberg 2005.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Guo2005705,
author={Guo, Q.-L. and Fan, X.-Z. and Liu, C.-A.},
title={Automatic abstracting based on text clustering and natural language understanding},
journal={Beijing Ligong Daxue Xuebao/Transaction of Beijing Institute of Technology},
year={2005},
volume={25},
number={8},
pages={705-709},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-26644433687&partnerID=40&md5=8d770e215c7f48cd82a08bb2b43150a2},
abstract={A method of realization of automatic abstracting based on text clustering and natural language understanding is brought forward, aimed at overcoming shortages of some current methods. The method makes use of text clustering and can realize automatic abstracting of multi-documents. The algorithm of twice word segmentation based on the title and first-sentences in paragraphs is brought forward. Its precision and recall is above 95%. For a specific domain on plastics, an automatic abstracting system is implemented. The precision and recall of multi-document's automatic abstracting is above 75%. And experiments do prove that it is feasible to use the method to develop a domain automatic abstracting system, that is valuable for further study in more depth.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chao200470,
author={Chao, C. and Zhengrong, S.},
title={A self-service digital virtual reference service system for a college libraries},
journal={Proceedings of the Eighth IASTED International Conference on Internet and Multimedia Systems and Applications},
year={2004},
pages={70-74},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-10444271801&partnerID=40&md5=b5b7d14732bf6a975e18daa028e97480},
abstract={In this article a virtual reference service system is described, which accomplishes several self-service reference functions with current digital techniques. The system consists of three knowledge databases: general FAQ database, academic FAQ database and academic resource navigation database, based on which the system is able to provide natural language reference service by word segmentation of natural language, to train the reference service according to reference choice feedbacks of readers, as well as to perfect its own knowledge database through the reference service librarians provided to readers.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kim2002320,
author={Kim, S.H. and Jeong, C.B. and Kwag, H.K. and Suen, C.Y.},
title={Word segmentation of printed text lines based on gap clustering and special symbol detection},
journal={Proceedings - International Conference on Pattern Recognition},
year={2002},
volume={16},
number={2},
pages={320-323},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751570122&partnerID=40&md5=ab405aaa302da18ac3a52e46e7eb577c},
abstract={This paper proposes a word segmentation method for machine-printed text lines. It utilizes gaps and special symbols as delimiters between words. A gap clustering technique is used to identify the gaps between words regardless of the gap-size variations among different document images. Next a special symbol detection technique is applied to find two types of special symbols lying between words. An experiment with 1.675 text lines in 100 different English and Korean documents shows that the proposed method achieves a high accuracy of word segmentation. © 2002 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Boyce2002325,
author={Boyce, B.R.},
title={In this issue},
journal={Journal of the American Society for Information Science and Technology},
year={2002},
volume={53},
number={5},
pages={325-326},
doi={10.1002/asi.10086},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036501429&doi=10.1002%2fasi.10086&partnerID=40&md5=13ba1ab38d4d3d34f7b72c7d06917260},
document_type={Editorial},
source={Scopus},
}

@ARTICLE{Kazakov2001121,
author={Kazakov, D. and Manandhar, S.},
title={Unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming},
journal={Machine Learning},
year={2001},
volume={43},
number={1-2},
pages={121-162},
doi={10.1023/A:1007629103294},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035312598&doi=10.1023%2fA%3a1007629103294&partnerID=40&md5=eaae5dc95f7c91cc97525afdf2bb2c17},
abstract={This article presents a combination of unsupervised and supervised learning techniques for the generation of word segmentation rules from a raw list of words. First, a language bias for word segmentation is introduced and a simple genetic algorithm is used in the search for a segmentation that corresponds to the best bias value. In the second phase, the words segmented by the genetic algorithm are used as an input for the first order decision list learner CLOG. The result is a set of first order rules which can be used for segmentation of unseen words. When applied on either the training data or unseen data, these rules produce segmentation which are linguistically meaningful, and to a large degree conforming to the annotation provided.},
document_type={Article},
source={Scopus},
}
