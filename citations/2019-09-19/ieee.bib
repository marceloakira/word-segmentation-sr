@INPROCEEDINGS{8359551,
author={Y. {Zhai} and L. {Liu} and W. {Song} and C. {Du} and X. {Zhao}},
booktitle={2017 International Conference on Progress in Informatics and Computing (PIC)},
title={The application of natural language processing in compiler principle system},
year={2017},
volume={},
number={},
pages={245-248},
abstract={Compiling principle is an important course of computer science major, which mainly introduces general principles and basic methods of the construction of compiling programs mainly. Due to high demands of the logic analysis ability, the course bring abstract and unintelligible experience to many students. Thus it is quite difficult for students to master the main points of this course within the limited class time. Based on the requirement above, this paper mainly proposed a method of making use of natural language processing in the research and application of compiling process, which utilizes Maximum Probability Word Segmentation algorithm during the process of lexical analysis and syntax analysis, to offer more effective interface between human and computer. The proposed method can provide students with intuitive and profound knowledge concept in the process of learning how to compile, makes it easier and quicker for students to understand the principle of computer compiling.},
keywords={computer science education;educational courses;natural language processing;probability;program compilers;important course;general principles;basic methods;compiling programs;logic analysis ability;natural language processing;compiling process;lexical analysis;syntax analysis;computer compiling;compiler principle system;compiling principle;computer science major;human computer interface;maximum probability word segmentation algorithm;natural language processing;compiling Principles;Maximum Probability Word Segmentation algorithm},
doi={10.1109/PIC.2017.8359551},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6559585,
author={A. {Srithirath} and P. {Seresangtakul}},
booktitle={2013 10th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology},
title={A hybrid approach to Lao word segmentation using longest syllable level matching with named entities recognition},
year={2013},
volume={},
number={},
pages={1-5},
abstract={The Lao language is written without words delimiter which makes it extremely difficult to process. The development of automatic word segmentation for natural language processing for the Lao language is an essential but challenging task. This paper proposes a longest syllable level match with named entities recognition approach for Lao word segmentation. Syllables were first extracted from the input text and then longest matching was applied. This is one of the techniques in the Dictionary Based approach with named entities recognition being used to combine them to form the words. The performance result obtained from this approach, in precision and recall, was 85.21% and 92.36%, respectively.},
keywords={dictionaries;natural language processing;pattern matching;Lao word segmentation;longest syllable level matching;Lao language;automatic word segmentation;natural language processing;named entities recognition approach;syllable extraction;dictionary based approach;Dictionaries;Indexes;Nickel;Educational institutions;Natural language processing;Lao word segmentation;tokenization;syllable extraction;longest matching;dictionary based;named entities recognition},
doi={10.1109/ECTICon.2013.6559585},
ISSN={},
month={May},}
@INPROCEEDINGS{4428144,
author={Q. {Guo} and K. {Wu} and W. {Li}},
booktitle={Second International Conference on Innovative Computing, Informatio and Control (ICICIC 2007)},
title={The Research and Realization about Question Answer System based on Natural Language Processing},
year={2007},
volume={},
number={},
pages={502-502},
abstract={Automatic Question Answer System (QAS) is a kind of high-powered software system based on Internet. Its key technology is the interrelated technology based on natural language understanding, including the construction of knowledge base and corpus, the Word Segmentation and POS Tagging of text, the Grammatical Analysis and Semantic Analysis of sentences etc. This thesis dissertated mainly the denotation of knowledge-information based on semantic network in QAS, the stochastic syntax-parse model named LSF of knowledge-information in QAS, the structure and constitution of QAS. And the LSF model parameters were exercised; it proved that they are feasible. At the same time, through "the limited-domain QAS" which was exploited for banks by us, these technologies are proved effective and propagable.},
keywords={computational linguistics;grammars;information retrieval;information retrieval systems;natural language processing;semantic networks;automatic question answering system;natural language processing;high-powered software system;Internet;word segmentation;text POS tagging;sentence grammatical analysis;sentence semantic analysis;semantic network;LSF stochastic syntax-parse model;information retrieval;Natural language processing;Natural languages;Information analysis;Internet;Constitution;Web server;Databases;Software systems;Tagging;Stochastic processes},
doi={10.1109/ICICIC.2007.585},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8629215,
author={D. {Tanaya} and M. {Adriani}},
booktitle={2018 International Conference on Asian Language Processing (IALP)},
title={Word Segmentation for Javanese Character Using Dictionary, SVM, and CRF},
year={2018},
volume={},
number={},
pages={240-243},
abstract={Word segmentation method based on dictionary for Javanese character still cannot overcome the problem of ambiguity, derivational word segmentation, and unknown words identification. We propose a new approach for Javanese character segmentation, i.e. a machine learning based method using CRF and SVM with a set feature of Javanese character's categorization and its combination with dictionary-based method. Through several experiments, it is proven that combination of dictionary-based method and CRF performs best than word segmentation using CRF, SVM, and traditional dictionary-based word segmentation.},
keywords={character recognition;dictionaries;image segmentation;learning (artificial intelligence);natural language processing;random processes;support vector machines;dictionary-based method;CRF;SVM;word segmentation method;derivational word segmentation;unknown words identification;Javanese character segmentation;dictionary-based word segmentation;Support vector machines;Machine learning algorithms;Dictionaries;Machine learning;Learning systems;Task analysis;Feature extraction;Javanese character;word segmentation;conditional random field;support vector machine},
doi={10.1109/IALP.2018.8629215},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4723042,
author={W. {Bo} and L. {Yunqing}},
booktitle={2008 International Conference on Computer Science and Software Engineering},
title={Research on the Design of the Ontology-Based Automatic Question Answering System},
year={2008},
volume={5},
number={},
pages={871-874},
abstract={Automatic question answering system is a hot issue in the field of natural language processing, and is playing an increasingly important role in the long-distance teaching through networks. This paper proposes an ontology-based automatic question answering system model, at first, build restricted area ontology, then take advantage of the accurate description of concept as well as the definition of the relationship of the concepts, to expand keywords and improve the accuracy and recall rates.},
keywords={distance learning;information retrieval;knowledge based systems;natural language processing;ontologies (artificial intelligence);teaching;ontology-based automatic question answering system;natural language processing;long-distance teaching;knowledge base;Ontologies;Education;Educational institutions;Artificial intelligence;Natural language processing;Information analysis;Statistical analysis;Natural languages;Computer science;Software engineering;automatic question answering system;word segmentation;ontology;knowledge base},
doi={10.1109/CSSE.2008.233},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6743944,
author={R. {Sun} and W. {Zhou} and Z. {Liu}},
booktitle={2013 6th International Congress on Image and Signal Processing (CISP)},
title={Using language rules to improve the performance of word segmentation},
year={2013},
volume={03},
number={},
pages={1665-1669},
abstract={Due to the disadvantage of the word segmentation tool in the aspect of the accuracy of word segmentation and speech tagging, it has harmful effects to the further research work. So this paper proposes a method based on the results of word segmentation to utilize language rules, which has been defined and described in Event Ontology, to verify and correct them. Experimental results show that compared with the method of only using word segmentation tool, the method of using language rules has a better performance.},
keywords={natural language processing;ontologies (artificial intelligence);text analysis;language rules;word segmentation tool;speech tagging;event ontology;Ontologies;Hidden Markov models;Accuracy;Tagging;Computers;Sun;Dictionaries;Language Rule;Word Segmentation;Performance;Event;Event Ontology},
doi={10.1109/CISP.2013.6743944},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5579805,
author={ {Aye Myat Mon} and {Soe Lai Phyue} and {Myint Myint Thein} and {Su Su Htay} and {Thinn Thinn Win}},
booktitle={2010 3rd International Conference on Advanced Computer Theory and Engineering(ICACTE)},
title={Analysis of Myanmar Word boundary and segmentation by using Statistical Approach},
year={2010},
volume={5},
number={},
pages={V5-233-V5-237},
abstract={This paper proposed a unified approach for Myanmar Word analysis using Finite State Automata (FSA), Rule Based Heuristic Approach and Statistical Approach. Myanmar has no inter-word space and it make the tokenizing task difficulties. Therefore, to recognize the word, we implement with FSA. Segmentation is a major problem because of no delimiter. If there were errors in segmentation, this will cause subsequence failure in further NLP processes. Segmentation is also an essential preprocessing task for Natural Language Processing, such as Machine Translation, Information Retrieval etc. In this system, the Rule Based Heuristic Approach and Statistical Approach are used with corpus based dictionary. Evaluation results showed that the method is very effective for the Myanmar language.},
keywords={finite state machines;natural language processing;statistical analysis;word processing;word boundary;word segmentation;statistical approach;finite state automata;rule based heuristic approach;natural language processing;corpus based dictionary;Myanmar language;Entropy;Merging;Natural Language Processing;FSA;Segmentation;Syllable Merging;Statistical approach},
doi={10.1109/ICACTE.2010.5579805},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6121461,
author={T. {Laga} and X. {Zhao}},
booktitle={2011 International Conference on Asian Language Processing},
title={Theoretical Framework of Mongolian Word Segmentation Specification for Information Processing},
year={2011},
volume={},
number={},
pages={23-25},
abstract={The establishment of Contemporary Mongolian word segmentation specification for information processing has a great significance in the standardization of information processing, the compatibleness of different systems, the sharing of corpus, grammatical analysis, and POS tagging. The present paper studies the framework of Mongolian word segmentation including guidelines, formulating principles, styles, scopes of segmentation units, establishment foundation, structure of the specification and so on, and lays the theoretical foundation for this specification.},
keywords={formal specification;natural language processing;text analysis;word processing;Mongolian word segmentation specification;information processing;corpus sharing;grammatical analysis;POS tagging;Information processing;Guidelines;Pragmatics;Educational institutions;Compounds;Grammar;Contemporary Mongolian word segmentation specification for information processing;theoretical framework;guidelines},
doi={10.1109/IALP.2011.45},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7333852,
author={E. {Kavallieratou}},
booktitle={2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
title={Word segmentation using Wigner-Ville distribution},
year={2015},
volume={},
number={},
pages={701-705},
abstract={In this paper, a novel technique for Word-segmentation is presented, based on Wigner-Ville distribution. The technique does not require training, while it is adapted to the writing style of the document image. Evaluation is performed on a standard dataset and the results are promising.},
keywords={document image processing;image segmentation;natural language processing;word segmentation;Wigner-Ville distribution;writing style;document image;standard dataset;Frequency modulation;word segmentation;Wigner-Ville distribution;Document image},
doi={10.1109/ICDAR.2015.7333852},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8629163,
author={C. {Ma} and J. {Yang}},
booktitle={2018 International Conference on Asian Language Processing (IALP)},
title={Burmese Word Segmentation Method and Implementation Based on CRF},
year={2018},
volume={},
number={},
pages={340-343},
abstract={Burmese belongs to the languages whose writing system has no delimiters to mark word boundaries. However, related works on Burmese word segmentation are still at the initial stage. This paper aims to fill the blank by employing CRF model to the task. The performance of the CRF method is evaluated with confidence, precision of segmentation. We prepared an experimental database of 5,000 sentences, which were manually segmented by Burmese experts. After the 6-fold cross-validation of the experimental data set, the experimental results show that the average confidence level of the CRF method is 93.4%, which is greater than the threshold, and the average value of the F1 is 93.0%. Therefore, the CRF segmentation method satisfies the requirements for developing a Burmese speech synthesis system.},
keywords={natural language processing;speech synthesis;statistical analysis;word processing;CRF model;experimental database;experimental data;CRF segmentation method;Burmese speech synthesis system;Burmese word segmentation method;Burmese language;Encoding;Writing;Machine learning;Speech synthesis;Compounds;Tagging;Information science;Burmese language;word segmentation;CRF;confidence},
doi={10.1109/IALP.2018.8629163},
ISSN={},
month={Nov},}
@ARTICLE{7188527,
author={H. {Wang} and X. {Han} and L. {Liu} and W. {Song} and M. {Yuan}},
journal={China Communications},
title={An improved unsupervised approach to word segmentation},
year={2015},
volume={12},
number={7},
pages={82-95},
abstract={ESA is an unsupervised approach to word segmentation previously proposed by Wang, which is an iterative process consisting of three phases: Evaluation, Selection and Adjustment. In this article, we propose ExESA, the extension of ESA. In ExESA, the original approach is extended to a 2-pass process and the ratio of different word lengths is introduced as the third type of information combined with cohesion and separation. A maximum strategy is adopted to determine the best segmentation of a character sequence in the phrase of Selection. Besides, in Adjustment, ExESA re-evaluates separation information and individual information to overcome the overestimation frequencies. Additionally, a smoothing algorithm is applied to alleviate sparseness. The experiment results show that ExESA can further improve the performance and is time-saving by properly utilizing more information from un-annotated corpora. Moreover, the parameters of ExESA can be predicted by a set of empirical formulae or combined with the minimum description length principle.},
keywords={iterative methods;natural language processing;smoothing methods;text analysis;unsupervised learning;improved unsupervised approach;word segmentation;iterative process;ExESA;word length;cohesion;character sequence segmentation;separation information;overestimation frequency;smoothing algorithm;minimum description length principle;Entropy;Smoothing methods;Length measurement;Frequency measurement;Uncertainty;Prediction algorithms;Accuracy;word segmentation;character sequence;smoothing algorithm;maximum strategy},
doi={10.1109/CC.2015.7188527},
ISSN={},
month={July},}
@ARTICLE{8811494,
author={Q. {Xie} and X. {Zhou} and J. {Wang} and X. {Gao} and X. {Chen} and C. {Liu}},
journal={IEEE Access},
title={Matching Real-World Facilities to Building Information Modeling Data Using Natural Language Processing},
year={2019},
volume={7},
number={},
pages={119465-119475},
abstract={Building Information Modeling (BIM) is a promising technology for building informatics. Currently, an increasing number of applications adopt BIM to improve the building operations and facility management. In these applications, matching real-world facilities to the corresponding BIM items is a fundamental yet challenging task. This study addresses this issue using Natural Language Processing. Firstly, a novel BIM hierarchy tree (HiTree) is proposed to model the original spatial structure relationships of a BIM. Then, the locations of facilities are extracted from natural language through processes of word segmentation, keyword extraction, and semantic disambiguation. Thirdly, an algorithm that matches real-world facilities to the BIM data is developed using the HiTree and the extracted locations. Finally, a concrete case for a 35,000 m2 library is presented to verify the effectiveness of the proposed solution. BIM has become a common paradigm in the construction industry, and our scheme can facilitate more applications of BIM in building operations and facility management. One of the most representative applications is integrating the BIM data and information within IoT (Internet of Things) system intelligently by matching the BIM data to real-world facilities.},
keywords={Facilities management;Natural language processing;Semantics;Data mining;Architecture;Building information modeling (BIM);facility;natural language processing (NLP);facility management},
doi={10.1109/ACCESS.2019.2937219},
ISSN={},
month={},}
@INPROCEEDINGS{7490720,
author={ {Chunxiang Zhang} and {Shan He} and {Xueyao Gao}},
booktitle={2015 4th International Conference on Computer Science and Network Technology (ICCSNT)},
title={A word sense disambiguation system based on bayesian model},
year={2015},
volume={01},
number={},
pages={124-127},
abstract={Research on word sense disambiguation (WSD) is of great importance in natural language processing fields. In this paper, a novel word sense disambiguation system is designed in which bayesian theory is applied to determine correct sense of an ambiguous word. Morphology knowledge in word unit is mined to guide WSD process. Neighboring morphology knowledge of an ambiguous word is used as feature for constructing WSD classifier. Word segmentation tool is integrated into this system and browser/server (B/S) framework is adopted. Experimental results show that the performance of WSD system is good.},
keywords={Bayes methods;data mining;Internet;natural language processing;online front-ends;pattern classification;word sense disambiguation;natural language processing fields;Bayesian model;word unit;morphology knowledge mining;WSD process;ambiguous word;WSD classifier;word segmentation tool;browser-server framework;B/S framework;WSD system;Semantics;Feature extraction;Bayes methods;Computational modeling;Vocabulary;Unified modeling language;Browsers;word sense disambiguation;morphology knowledge;word segmentation tool;browser/server},
doi={10.1109/ICCSNT.2015.7490720},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5555259,
author={L. {Yuan}},
booktitle={2010 2nd International Conference on Signal Processing Systems},
title={Improvement for the automatic part-of-speech tagging based on hidden Markov model},
year={2010},
volume={1},
number={},
pages={V1-744-V1-747},
abstract={In this paper, the Markov Family Models, a kind of statistical Models was firstly introduced. Under the assumption that the probability of a word depends both on its own tag and previous word, but its own tag and previous word are independent if the word is known, we simplify the Markov Family Model and use for part-of-speech tagging successfully. Experimental results show that this part-of-speech tagging method based on Markov Family Model has greatly improved the precision comparing the conventional POS tagging method based on Hidden Markov Model under the same testing conditions. The Markov Family Model is also very useful in other natural language processing technologies such as word segmentation, statistical parsing, text-to-speech, optical character recognition, etc.},
keywords={hidden Markov models;natural language processing;probability;speech processing;statistics;automatic part-of-speech tagging;hidden Markov model;statistical models;probability;natural language processing;Hidden Markov models;Markov processes;Tagging;Biological system modeling;Training;Natural language processing;Markov Family model;Part-of-Speech tagging;Hidden Markov model;Viterbi algorithm},
doi={10.1109/ICSPS.2010.5555259},
ISSN={},
month={July},}
@INPROCEEDINGS{7976613,
author={X. {Li} and F. {He}},
booktitle={2016 8th International Conference on Information Technology in Medicine and Education (ITME)},
title={Word Segmentation in Japanese for Construction of Scientific Japanese Corpus},
year={2016},
volume={},
number={},
pages={864-868},
abstract={Japanese word segmentation in the specialized field is the difficulty of building the Scientific Japanese Corpus. This paper deeply studied the basic problem of the word segmentation. They are ambiguity and words without login. In this paper we tried to use the ART network and immune principle to solve the problem of Japanese word segmentation, more than 90% of the words are correctly separated by our method.},
keywords={natural language processing;text analysis;word processing;scientific Japanese corpus;Japanese word segmentation;ART network;Neurons;Immune system;Subspace constraints;Dictionaries;Adaptive systems;Pathogens;Vocabulary;Japanese Corpus;Word Gegmentation;ARTNetwork;Adaptive Network},
doi={10.1109/ITME.2016.0199},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6720529,
author={S. {Somsap} and P. {Seresangtakul}},
booktitle={2013 International Conference on Control, Automation and Information Sciences (ICCAIS)},
title={Isarn Dharma word segmentation},
year={2013},
volume={},
number={},
pages={53-57},
abstract={This paper presents Isarn Dhama word segmentation based on the Isarn Dharma writing system and dictionary. In this study, input text is segmented into sequences of Isarn Dharma Character Clusters (IDCCs). Each IDCC represents a group of inseparable Isarn Dharma characters based on the Isarn Dharma writing system. The sequence of IDCCs will be considered as input in order to look for the most suitable segmentation word from the dictionary using the IDCC longest matching algorithm. Grouping rules were then used to group adjacent remaining IDCCs that do not match an Isarn word in the dictionary. In order to evaluate the efficiency of the proposed technique, Isarn literature, Jataka, legend and Buddha foretell were used as the testing data to test the proposed system; comparing with longest matching and a hybrid of the IDCC longest matching. The experiment results showed that the F-measures are 80.15%, 85.06% and 86.07% for the longest matching, the IDCC longest matching algorithm, and the proposed method, respectively.},
keywords={dictionaries;natural language processing;pattern clustering;text analysis;grouping rules;efficiency evaluation;Isarn literature;Jataka;Buddha foretell;testing data;F-measures;legend foretell;IDCC longest-matching algorithm;inseparable Isarn Dharma characters;IDCC sequences;Isarn Dharma character cluster sequences;input text segmentation;dictionary;Isarn Dharma writing system;Isarn Dharma word segmentation;Dictionaries;Clustering algorithms;Educational institutions;Computer science;Accuracy;Information technology},
doi={10.1109/ICCAIS.2013.6720529},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4410418,
author={T. {Fukuda} and M. {Izumi} and T. {Miura}},
booktitle={19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)},
title={Word Segmentation Using Domain Knowledge Based on Conditional Random Fields},
year={2007},
volume={2},
number={},
pages={436-439},
abstract={In this investigation, we propose an experimental approach for word segmentation in Japanese under domain-dependent situation. We apply Conditional Random Fields (CRF) to our issue. CRF learns several probabilistic parameters from training data with specific feature functions dependent on domains. Here we propose how to define domain specific feature functions.},
keywords={learning (artificial intelligence);natural language processing;probability;random processes;text analysis;Japanese word segmentation;conditional random field;probabilistic parameter;training data;domain specific feature function;domain knowledge;text processing;Natural languages;Artificial intelligence;Training data;Statistics;Stochastic processes;Pattern analysis;Speech;Dictionaries},
doi={10.1109/ICTAI.2007.93},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5423133,
author={K. K. {Zin} and N. L. {Thein}},
booktitle={2009 International Conference on the Current Trends in Information Technology (CTIT)},
title={Part of speech Tagging for Myanmar using Hidden Markov Model},
year={2009},
volume={},
number={},
pages={1-6},
abstract={Part-Of-Speech (POS) Tagging is the process of assigning the words with their categories that best suits the definition of the word as well as the context of the sentence in which it is used. In this paper, we describe a machine learning algorithm for Myanmar Tagging using a corpus-based approach. In order to tag Myanmar language, we need to take part word segmentation, part of speech tagging using HMM and several Tag-sets. Thus, this paper deals with a combination of supervised and un-supervised learning which use pre-tagged and untagged corpus respectively. To assign to each word with the correct tag, we describe Supervised POS Tagging by using the class labels in terms of predictor features on manually tagged corpus and also describe Unsupervised POS Tagging for automatically training without using a manually tagged corpus. By experiments, the best configuration is investigated on different amount of training data and the accuracy is 97.56%.},
keywords={hidden Markov models;natural language processing;speech processing;unsupervised learning;hidden Markov model;part of speech tagging;machine learning algorithm;Myanmar Tagging;corpus based approach;Myanmar language;word segmentation;HMM;unsupervised learning;pretagged corpus;untagged corpus;unsupervised POS tagging;Tagging;Hidden Markov models;Natural languages;Speech processing;Machine learning algorithms;Training data;Natural language processing;White spaces;Speech analysis;Testing},
doi={10.1109/CTIT.2009.5423133},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7916560,
author={W. A. {Jamro}},
booktitle={2017 International Conference on Innovations in Electrical Engineering and Computational Technologies (ICIEECT)},
title={Sindhi Language Processing: A survey},
year={2017},
volume={},
number={},
pages={1-8},
abstract={In this era of information technology, natural language processing (NLP) has become volatile field because of digital reliance of today's communities. The growth of Internet usage bringing the communities, cultures and languages online. In this regard much of the work has been done of the European and east Asian languages, in the result these languages have reached mature level in terms of computational processing. Despite the great importance of NLP science, still most of the South Asian languages are under developing phase. Sindhi language is one of them, which stands among the most ancient languages in the world. The Sindhi language has a great influence on the large community in Sindh province of Pakistan and some states of India and other countries. But unfortunately, it is at infant level in terms of computational processing, because it has not received such attention of language engineering community, due to its complex morphological structure and scarcity of language resources. Therefore, this study has been carried out in order to summarize the existing work on Sindhi Language Processing (SLP) and to explore future research opportunities, also some potential research problems. This paper will be helpful for the researchers in order to find all the information regarding SLP at one place in a unique way.},
keywords={natural language processing;word processing;Sindhi language processing;natural language processing;Internet usage;European languages;east Asian languages;computational processing;NLP science;South Asian languages;Sindh province;Pakistan;India;language engineering community;morphological structure;language resources;SLP;Compounds;Tagging;Writing;Tools;Morphology;Optical character recognition software;Sindhi Language Processing;SLP;Morphological Analysis;Word Segmentation;Parts-of-Speech Tagging;Diacritization;technique and approaches},
doi={10.1109/ICIEECT.2017.7916560},
ISSN={},
month={April},}
@INPROCEEDINGS{6424202,
author={F. {Stahlberg} and T. {Schlippe} and S. {Vogel} and T. {Schultz}},
booktitle={2012 IEEE Spoken Language Technology Workshop (SLT)},
title={Word segmentation through cross-lingual word-to-phoneme alignment},
year={2012},
volume={},
number={},
pages={85-90},
abstract={We present our new alignment model Model 3P for cross-lingual word-to-phoneme alignment, and show that unsupervised learning of word segmentation is more accurate when information of another language is used. Word segmentation with cross-lingual information is highly relevant to bootstrap pronunciation dictionaries from audio data for Automatic Speech Recognition, bypass the written form in Speech-to-Speech Translation or build the vocabulary of an unseen language, particularly in the context of under-resourced languages. Using Model 3P for the alignment between English words and Spanish phonemes outperforms a state-of-the-art monolingual word segmentation approach [1] on the BTEC corpus [2] by up to 42% absolute in F-Score on the phoneme level and a GIZA++ alignment based on IBM Model 3 by up to 17%.},
keywords={natural language processing;speech recognition;unsupervised learning;word segmentation;cross lingual word-to-phoneme alignment;unsupervised learning;cross lingual information;bootstrap pronunciation dictionaries;automatic speech recognition;speech-to-speech translation;English words;Spanish phonemes;Hidden Markov models;Error analysis;Grammar;Dictionaries;Vocabulary;Training data;Vectors;alignment model;word segmentation;under-resourced language;speech-to-speech translation},
doi={10.1109/SLT.2012.6424202},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7440524,
author={W. {Sukfong} and S. {Nakkrasae} and P. {Panpipat}},
booktitle={2016 8th International Conference on Knowledge and Smart Technology (KST)},
title={Motor insurance knowledge management in AIML format using tree structured dictionary mechanism and conceptual graph},
year={2016},
volume={},
number={},
pages={158-163},
abstract={Nowadays, information providers of car insurance through the site are not effective enough for consumers needs. Thus, this paper presents a motor insurance knowledge management based on Artificial Intelligence Markup Language (AIML). In the part of knowledge management, tree structure dictionary mechanism method is used for word segmentation and the conversion technique to correct grammatical sentences (Pattern Normalization) utilizes conceptual graphs. Motor insurance enquiry intelligent system via mobile devices is then implemented. It is analyzed and designed by using object-oriented approach with UML (Unified Modeling Language). The result shows that the developed system can be used for answer questions and works more efficiently.},
keywords={automobiles;insurance data processing;knowledge based systems;knowledge management;natural language processing;tree data structures;Unified Modeling Language;motor insurance knowledge management;AIML format;tree structured dictionary mechanism;conceptual graph;car insurance information providers;Artificial Intelligence Markup Language;word segmentation;conversion technique;grammatical sentence correction;pattern normalization;motor insurance enquiry intelligent system;mobile devices;object-oriented approach;UML;Unified Modeling Language;Insurance;Knowledge based systems;Dictionaries;Knowledge management;Markup languages;Unified modeling language;Word Segmentation;Dictionary Mechanism;Pattern Normalization;Conceptual Graphs;AIML (Artificial Intelligence Markup Language);CORPUS;Object Oriented Analysis and Design;UML},
doi={10.1109/KST.2016.7440524},
ISSN={},
month={Feb},}
@INPROCEEDINGS{5565148,
author={A. {Agarwal} and A. {Jain} and N. {Prakash} and S. S. {Agrawal}},
booktitle={2010 3rd International Conference on Computer Science and Information Technology},
title={Word based emotion conversion in Hindi language},
year={2010},
volume={9},
number={},
pages={419-423},
abstract={The main system to communicate with computers is the use of natural language in text form. To add naturalness and intelligibility, the speech form is becoming an important method to communicate with computers and other machines. Human-machine and human-robot dialogues in the next generation will be conquered by natural speech, which is fully impulsive and thus obsessed by emotion. Emotion adds expressiveness to the natural language speech. There is a great zeal to research in this field. In this paper we have proposed an algorithm for word based emotion conversion of neutral speech into emotional speech like `happy' and `sad' for Hindi language. This emotion conversion algorithm is based on the segmentation of the spoken utterance into words and the pitch differences of these words between different emotions. The segmentation of words in the spoken utterance is done using another algorithm of word boundary detection, which gives details of start of the word, end of the word and the no. of words in a sentence. This algorithm of word boundary detection is devised for Hindi language and is based on two main prosodic features `pitch' and `intensity'.},
keywords={emotion recognition;human computer interaction;human-robot interaction;natural language processing;speech intelligibility;speech processing;word based emotion conversion;Hindi language;speech intelligibility;speech form;human-machine dialogue;human-robot dialogue;natural speech;natural language speech;neutral speech;emotional speech;emotion conversion algorithm;spoken utterance;pitch differences;word segmentation;word boundary detection;Databases;Word boundary detection;Emotion conversion;Prosody;Pitch;Intensity},
doi={10.1109/ICCSIT.2010.5565148},
ISSN={},
month={July},}
@INPROCEEDINGS{7083900,
author={S. {Lushanthan} and A. R. {Weerasinghe} and D. L. {Herath}},
booktitle={2014 14th International Conference on Advances in ICT for Emerging Regions (ICTer)},
title={Morphological analyzer and generator for Tamil Language},
year={2014},
volume={},
number={},
pages={190-196},
abstract={Morphological analysis is an essential component in Natural Language Processing (NLP) applications ranging from spell checker to machine translation. When performing a morphological analysis it leads to segmentation of a word into morphemes, combined with an analysis of the attachments of these morphemes. In English language the complexity of the formation of words is not much higher compared with Indic languages. Hence, Tamil language too does have its complexities when building up a NLP application. The morphemes in the language, the rules how these morphemes are connected and the changes occur when they attach together are the important factors that need to be considered when building up a Morphological Analyzer for any language. Our “Morphological Analyzer and Generator for Tamil Language” will be generating the word forms of a stem/ root, given a particular context and at the same time, a surface form in Tamil language should get analyzed into its proper context. This model tries to cover only the nouns and verbs in the Tamil language. This paper illustrates how the lexicon and the orthographic rules of Tamil language have been written as regular expressions using only finite state operations and how this approach has been implemented to develop a morphological analyzer/generator. This model is built using the Xerox toolkit, which uses “Two-level Morphology”, and almost 2000 noun stems and 96 verb stems have been incorporated into the network. A noun stem now produces about 40 different forms and a verb stem produces up to 240 forms. We have also defined our own transliteration scheme for this purpose.},
keywords={finite state machines;language translation;natural language processing;morphological analyzer;morphological generator;Tamil language;natural language processing;NLP;spell checker;machine translation;word segmentation;morphemes;lexicon;orthographic rules;regular expressions;finite state operations;Xerox toolkit;two-level morphology;transliteration scheme;Tamil Morphological Analyzer and Generator;Morphology;Finite State Transducer;Regular Expressions},
doi={10.1109/ICTER.2014.7083900},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5313753,
author={G. {Zhang} and Y. {Gao} and D. {Ji} and X. {Ren}},
booktitle={2009 International Conference on Natural Language Processing and Knowledge Engineering},
title={Research on Katakana phrase translation based on bi-directional integration},
year={2009},
volume={},
number={},
pages={1-6},
abstract={In order to solve the problem of Katakana reduced to English in Japanese-English translation, we employ the phrase-based statistical machine translation model to perform Katakana phrase (or word) translation from Japanese to English. The Katakana phrase is segmented into words by CRF, and then Japanese-English and English-Japanese bi-directional integration translation is carried out on those segmented results. The translated results of all the segmented words are comprehensively scored to obtain the best English phrase translation result. The experimental results indicate that the Katakana phrase translation precision reaches 76%, effectively addresses the problem of the Katakana reduced to English.},
keywords={computational linguistics;language translation;natural language processing;statistical analysis;text analysis;Katakana phrase translation;bidirectional integration;Japanese-English translation;phrase-based statistical machine translation;Katakana word translation;word segmentation;Bidirectional control;Knowledge engineering;Aerospace engineering;Dictionaries;Terminology;Natural languages;Katakana;Bi-directional Integration;Phrase-based Statistical Machine Translation},
doi={10.1109/NLPKE.2009.5313753},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7939534,
author={S. {Gupta}},
booktitle={2016 International Conference on Recent Advances and Innovations in Engineering (ICRAIE)},
title={Efficient malicious domain detection using word segmentation and BM pattern matching},
year={2016},
volume={},
number={},
pages={1-6},
abstract={On the World Wide Web, the malicious links are highly problematic in the dissemination channels as a source code to the malware broadcasting. These suspicious malicious links gives full access to the web attackers as an instrument of web pages on internet. It is easily affected by the results of attackers on the system of victim where system is utilized easily for performing the cyber-attacks such as stealing the financial credentials, phishing-spamming, hacking and many more such web attacks. The developed system must be accurate and fast enough to detect these types of such cyber-attacks by observing the ability to find new developed malicious URLs or malicious source code contents. It is the critical task to detect the malicious contents in network of the web pages over the World Wide Web. The various malicious cyber-attacks like spamming, code phishing are done by using the malicious URLs to mount these types of cyber-attacks. Internet unlawful activities are found in Malicious Web sites as cornerstone of the Malicious URLs. The main threat is to identify these attacks so that the suspicious URLs can be easily resolved as malicious URLs along with its source code of the web pages. In this paper, a method has been proposed which is highly useful in the field of World Wide Web networking of domains for detecting the malicious URLs by using BM (Boyer-Moore) [1] string pattern matching algorithm based on word segmentation approach [2]. The nature of attacks is identified as a malicious URL or source code of the web sites questioned on the World Wide Web. The proposed approach is based on the real time system for getting suspicious URL from the DNS server followed by the detection on the basis of word segmentation of source code. The discriminative features of this system are verified by using the proposed method which gives a variety of properties including text and link in the source code as highly powerful and novel approach in the detection of suspicious URLs.},
keywords={Internet;natural language processing;real-time systems;security of data;string matching;Web sites;malicious domain detection;word segmentation;World Wide Web;dissemination channels;malware broadcasting;suspicious malicious links;Web attackers;Web pages;cyber-attacks;malicious URL;malicious source code contents;malicious content detection;Internet unlawful activities;malicious Web sites;BM string pattern matching algorithm;Boyer-Moore algorithm;real time system;DNS server;Uniform resource locators;Web pages;Pattern matching;Databases;Servers;Algorithm design and analysis;BM (Boyer-Moore) Pattern;Malicious Web Pages;Classification Module;Web-Based Attacks},
doi={10.1109/ICRAIE.2016.7939534},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6204792,
author={E. {Şahin} and H. {Adıgüzel} and P. {Duygulu} and M. {Kalpaklı}},
booktitle={2012 20th Signal Processing and Communications Applications Conference (SIU)},
title={OTAP ottoman archives internet interface},
year={2012},
volume={},
number={},
pages={1-2},
abstract={Within Ottoman Text Archive Project a web interface to aid in uploading, binarization, line and word segmentation, labeling, recognition and testing of the Ottoman Turkish texts has been developed. It became possible to retrieve expert knowledge of scholars working with Ottoman archives through this interface, and apply this knowledge in developing further technologies in transliteration of historical manuscripts.},
keywords={expert systems;information retrieval systems;Internet;natural language interfaces;natural language processing;text analysis;OTAP ottoman;Internet interface;ottoman text archive project;Web interface;text uploading;text binarization;line segmentation;word segmentation;text labeling;text recognition;text testing;Ottoman Turkish texts;expert knowledge;Ottoman archives;transliteration;historical manuscripts;Internet;Abstracts;Text recognition;Testing;Educational institutions;Radio access networks;Transform coding},
doi={10.1109/SIU.2012.6204792},
ISSN={},
month={April},}
@INPROCEEDINGS{6376786,
author={Y. {Dai} and X. {Ren}},
booktitle={2012 International Conference on Audio, Language and Image Processing},
title={A hybrid method to segment words},
year={2012},
volume={},
number={},
pages={1131-1134},
abstract={Word segmentation is the foundations of machine translation, text classification and information searching. A method is proposed which combines word segmentation based on dictionary with reverse maximum matching and word segmentation based on statistic with suffix array. The input texts are segmented using the reserve maximum matching method based on dictionary, and a two-way suffix arrays are constructed, longest common prefix are computed, candidate words are filtered out by setting the threshold, the candidate words are filtered using mutual information in order to the true words. The texts that are ambiguity are filtered using information entropy. It is showed that the accuracy of word segmentation may achieve above 97% in the experiment.},
keywords={language translation;natural language processing;pattern classification;text analysis;hybrid method;word segmentation;machine translation;text classification;information searching;suffix array;input texts;common prefix;information entropy;Arrays;Dictionaries;Accuracy;Information filters;Sorting;Matched filters},
doi={10.1109/ICALIP.2012.6376786},
ISSN={},
month={July},}
@INPROCEEDINGS{7453382,
author={S. {Puri} and S. P. {Singh}},
booktitle={2015 2nd International Conference on Recent Advances in Engineering Computational Sciences (RAECS)},
title={Sentence Detection and Extraction in machine printed imaged document using matching technique},
year={2015},
volume={},
number={},
pages={1-6},
abstract={Sentence extraction is a new, challenging and critical step in the printed scanned imaged documents. In this paper, an efficient 4-layered Sentence Detection and Extraction System (SDES) model is proposed which is designed to detect and extract sentences from machine printed imaged document. Its internal details and architecture clearly show that how it processes an image to find out the underlying sentences. The basic idea is to first preprocess the imaged document for noise removal and skew correction, and then textual entities are detected and segmented at page, line and word levels. Firstly, the horizontal and vertical projection profiles are taken to segment and separate the lines and words. After skew correction, two stage Character Based and Word Based Leveled matching and testing are performed, which verify and identify the correct character and word by searching for similar textual characters and words in Character Set Storage (CSS) and Word Pseudo Thesaurus (WPT). If any word pattern is not matched and identified by WPT, then it is stored in the Unmatched Word Storage (UWS) for the future reference. Such testing and verification are used at two levels to increase the accuracy% of SDES, and thereby, reducing the errors. It increases the system performance greatly. Finally, all the sentences of imaged document are extracted. Experimental results are found at the word, character and sentence levels. Their accuracy% results are good which show the high system performance and efficiency.},
keywords={document image processing;feature extraction;image denoising;image matching;natural language processing;machine printed imaged document;matching technique;4-layered sentence detection and extraction system;SDES model;noise removal;skew correction;character based leveled matching;word based leveled matching;character set storage;CSS;word pseudo thesaurus;WPT;unmatched word storage;UWS;Image segmentation;Feature extraction;Layout;Data mining;Testing;Cascading style sheets;Hidden Markov models;projection profile;line segmentation;word segmentation;character recognition;storage bins;sentence extraction;natural language processing},
doi={10.1109/RAECS.2015.7453382},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8704816,
author={L. {Mzamo} and A. {Helberg} and S. {Bosch}},
booktitle={2019 Southern African Universities Power Engineering Conference/Robotics and Mechatronics/Pattern Recognition Association of South Africa (SAUPEC/RobMech/PRASA)},
title={Towards an unsupervised morphological segmenter for isiXhosa},
year={2019},
volume={},
number={},
pages={166-170},
abstract={In this paper, branching entropy techniques and isiXhosa language heuristics are adapted to develop unsupervised morphological segmenters for isiXhosa. An overview of isiXhosa segmentation issues is given, followed by a discussion on previous work in automated segmentation, and segmentation of isiXhosa in particular. Two unsupervised isiXhosa segmenters are presented and compared to a random minimum baseline and Morfessor-Baseline, a standard in unsupervised word segmentation. Morfessor-Baseline outperforms both isiXhosa segmenters at 79.10% boundary identification accuracy. The IsiXhosa Branching Entropy Segmenter (XBES) performance varies depending on the segmentation mode used, with a maximum of 73.39%. The IsiXhosa Heuristic Maximum Likelihood Segmenter (XHMLS) achieves 72.42%. The study suggests that unsupervised isiXhosa morphological segmentation is feasible with better optimization of the current attempts.},
keywords={entropy;image segmentation;maximum likelihood estimation;natural language processing;unsupervised learning;segmentation mode;unsupervised isiXhosa morphological segmentation;unsupervised morphological segmenter;isiXhosa language heuristics;isiXhosa segmentation issues;automated segmentation;unsupervised isiXhosa segmenters;random minimum baseline;Morfessor-Baseline;unsupervised word segmentation;IsiXhosa branching entropy segmenter performance;IsiXhosa heuristic maximum likelihood segmenter;natural language processing;unsupervised machine learning;morphological segmentation;isiXhosa},
doi={10.1109/RoboMech.2019.8704816},
ISSN={},
month={Jan},}
@ARTICLE{5175425,
author={G. {Tambouratzis}},
journal={IEEE Transactions on Evolutionary Computation},
title={Using an Ant Colony Metaheuristic to Optimize Automatic Word Segmentation for Ancient Greek},
year={2009},
volume={13},
number={4},
pages={742-753},
abstract={Given a text or collection of texts involving unconstrained language, a basic task in a multitude of applications is the identification of stems and endings for each word form, which is termed morphological analysis. In this paper, the use of an ant colony optimization (ACO) metaheuristic is proposed for a linguistic task that involves the automated morphological segmentation of Ancient Greek word forms into stem and ending. The task of morphological analysis is essential for implementing text-processing applications such as semantic analysis and information retrieval. The difficulty of the morphological analysis task differs depending on the language chosen, being hardest in the case of highly-inflectional languages, where each stem may be associated with a large number of different endings. In this paper, focus is placed on the morphological analysis of ancient Greek, which has been shown to be a particularly hard task. To perform this task, a system for the automated morphological processing has been proposed, which implements the morphological analysis of words by coupling an iterative pattern-recognition algorithm with a modest amount of linguistic knowledge, expressed via a set of interactions associated with weights. In an earlier version of the system, these weights were determined by combining the input from specialized scientists with a lengthy manual optimization process. In this paper, the ACO metaheuristic is applied to the task of defining near-optimal system weights using an automated process based on a set of training data. The experiments performed indicate that the segmentation quality achieved by ACO is equivalent to or in several cases substantially higher than that achieved using manually optimized weights.},
keywords={iterative methods;linguistics;natural language processing;optimisation;pattern recognition;text analysis;ant colony metaheuristic;optimize automatic word segmentation;ancient Greek;morphological analysis;semantic analysis;information retrieval;text processing application;highly-inflectional languages;automated morphological processing;iterative pattern-recognition algorithm;linguistic knowledge;manual optimization process;near-optimal system;segmentation quality;Ant colony optimization;Information analysis;Information retrieval;Data mining;Pattern analysis;Performance analysis;Algorithm design and analysis;Iterative algorithms;Training data;Text processing;Ancient Greek;ant colony optimization (ACO) metaheuristic;automated morphological analysis;heuristic function;text processing},
doi={10.1109/TEVC.2009.2014363},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8639515,
author={A. {Thual} and C. {Dancette} and J. {Karadayi} and J. {Benjumea} and E. {Dupoux}},
booktitle={2018 IEEE Spoken Language Technology Workshop (SLT)},
title={A K-Nearest Neighbours Approach To Unsupervised Spoken Term Discovery},
year={2018},
volume={},
number={},
pages={491-497},
abstract={The following topics are dealt with: speech recognition; neural nets; speech processing; learning (artificial intelligence); natural language processing; recurrent neural nets; speaker recognition; speech synthesis; feature extraction; and text analysis.},
keywords={natural language processing;speech processing;speech recognition;neural nets;speech processing;learning (artificial intelligence);natural language processing;recurrent neural nets;speaker recognition;speech synthesis;feature extraction;text analysis;Pipelines;Acoustics;Approximation algorithms;Indexes;Feature extraction;Measurement;Clustering methods;spoken term discovery;word discovery;unsupervised;word segmentation},
doi={10.1109/SLT.2018.8639515},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8691202,
author={T. {Zhao} and L. {Li} and Y. {Xie} and Y. {Lv}},
booktitle={2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems (CCIS)},
title={Data-driven Risk Assessment for Peer-to-Peer Network Lending Agencies},
year={2018},
volume={},
number={},
pages={799-803},
abstract={With the rapid development of Peer-to-Peer(P2P) network lending in the financial field, more data of lending agencies have appeared. P2P agencies also have problems such as absconded with ill-gotten gains and out of business. Therefore, it is necessary to assess their risks based on P2P company data. This paper proposes a framework of Data-driven Risk Assessment for P2P(DRAP2P) network lending agencies based on unstructured natural language data. First, use the natural language processing technology, such as word segmentation, keyword, LDA topic model, word2vec and doc2vec, to process and extract features of company profile which reflect its business status. Then, seven machine learning classifiers and three deep learning models are used for analysis. Since keywords show good performance in machine learning models, we improve Convolutional Neural Network(CNN) with keywords and propose two CNN+Keyword models, namely CNN+Keyword(static+BP) and CNN+Keyword (Expand word embedding). Experiments have shown that CNN+Keyword(static+BP) can achieve the best performance. Finally, we use the method of meta-learning to integrate CNN+Keyword(static+BP) and logistic regression classifier to further strengthen the performance.},
keywords={convolutional neural nets;feature extraction;financial data processing;learning (artificial intelligence);natural language processing;pattern classification;peer-to-peer computing;regression analysis;convolutional neural network;data-driven risk assessment;peer-to-peer network;machine learning classifiers;DRAP2P network lending agencies;peer-to-peer network lending agencies;logistic regression classifier;financial field;doc2vec;word2vec;natural language processing technology;unstructured natural language data;CNN+Keyword models;machine learning models;deep learning models;Data-driven;P2P risk assessment;Machine learning;Deep learning;Meta-learning},
doi={10.1109/CCIS.2018.8691202},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4620976,
author={ {Hsien-Chang Wang} and {Yueh-Chin Chan}},
booktitle={2008 International Conference on Machine Learning and Cybernetics},
title={On the abstraction and presentation of multi-source knowledge},
year={2008},
volume={6},
number={},
pages={3307-3309},
abstract={This paper proposed a knowledge abstraction and presentation system by information gathered Internet web pages. Documents gathered from different Websites are first segmented into different paragraphs according to their topics. The linguistic processing such as word segmentation, word tagging and word frequency evaluation are applied to these corpora first. Then two types of similarities are calculated in our study: the paragraph-based and sentence-based similarity.},
keywords={abstracting;information retrieval;Internet;natural language processing;word processing;multi-source knowledge;knowledge abstraction;knowledge presentation;Internet web pages;linguistic processing;word segmentation;word tagging;word frequency evaluation;paragraph-based similarity;sentence-based similarity;mean opinion score evaluation;Web pages;Knowledge engineering;Internet;Information filters;Data mining;Information filtering;Machine learning},
doi={10.1109/ICMLC.2008.4620976},
ISSN={},
month={July},}
@INPROCEEDINGS{5395511,
author={V. K. {Koppula} and N. {Atul} and U. {Garain}},
booktitle={2009 Second International Conference on Emerging Trends in Engineering Technology},
title={Robust Text Line, Word And Character Extraction from Telugu Document Image},
year={2009},
volume={},
number={},
pages={269-272},
abstract={Designing an OCR system for Indian languages in general is more complex than those of European languages due the linguistic complexity. Efforts are on the way for the development of efficient OCR systems for Indian languages, especially for Telugu, a popular South Indian language. In this paper, we proposed a method for reliable extraction of text line, word and character from document images of Telugu scripts. In the text line segmentation, first we establish the relationship between the connected components and then cluster the connected components of a line using vertical spatial relation and nearest neighbor algorithm. In word segmentation, the space between two adjacent characters is computed and clustered into word space and character space. Consonant and vowel modifiers are segregated from the word image and segment the characters.},
keywords={document image processing;feature extraction;image segmentation;natural language processing;optical character recognition;text analysis;word extraction;character extraction;Telugu document image;OCR system;Indian languages;linguistic complexity;text line segmentation;vertical spatial relation;nearest neighbor algorithm;word segmentation;text line extraction;Robustness;Image segmentation;Optical character recognition software;Nearest neighbor searches;Clustering algorithms;Carbon capture and storage;Educational institutions;Computational Intelligence Society;Computer vision;Character recognition},
doi={10.1109/ICETET.2009.196},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5607428,
author={R. {Zhang} and Q. {Zeng} and S. {Feng}},
booktitle={2010 IEEE 2nd Symposium on Web Society},
title={Data query using short domain question in natural language},
year={2010},
volume={},
number={},
pages={351-354},
abstract={This paper presents the application of understanding of short domain question in natural language to data query. A domain dictionary can be obtained through extracting schema of all tables and short text data from database by ODBC API. The word segmentation tool, IK Analyzer, which is extended with the obtained domain dictionary, is used to segment the short text questions. From the segmentation results, the keywords are extracted to obtain query target and query requirement of the question and to generate a SQL statement for data query. The method proposed in this paper can be applied to question-answering system based on database.},
keywords={dictionaries;information retrieval systems;natural language processing;query processing;SQL;word processing;data query;short domain question;natural language;domain dictionary;ODBC API;word segmentation tool;IK analyzer;short text questions;query target;query requirement;SQL statement;question-answering system;Databases;Natural languages;Dictionaries;Data mining;Books;Arrays;Vocabulary},
doi={10.1109/SWS.2010.5607428},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6299310,
author={H. {Yamamoto} and M. {Tanaka} and Y. {Kondo}},
booktitle={2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing},
title={Diachronic Corpus and Linguistic Space: New Methods for the Analysis of Language Change},
year={2012},
volume={},
number={},
pages={381-384},
abstract={The project, design and development of the diachronic corpus of Japanese began in 2009 at the Department of Corpus Study, the National Institute of Japanese Language and Linguistics, Japan (NINJAL), as a collaborative research project by linguists and literature scholars of NINJAL and the University of Oxford. Its focus is on collecting representative Japanese literary works and classical documents from the tenth century to the nineteenth century. We are currently working on the development of the prototype version of the diachronic Japanese corpus: i.e. selection of materials, digitization of texts, addition of alternative texts (containing different orthography) to original texts, compilation of a basic thesaurus that differentiates between different spellings, and word segmentation. This paper addresses the discussion of the basic concepts encountered during our work on the project: synchronic and diachronic analysis, which led us to the design of a serial comparison model which allows us to examine language change between documents or literary works with respect to time.},
keywords={linguistics;natural language processing;research and development;text analysis;linguistic space;language change analysis;national institute of Japanese language and linguistics;NINJAL;collaborative research project;University of Oxford;representative Japanese literary works;classical documents;diachronic Japanese corpus;thesaurus;word segmentation;diachronic analysis;synchronic analysis;Pragmatics;Analytical models;Educational institutions;Thesauri;Collaboration;Materials;History;diachronic;synchronic;language change;history of Japanese;serial comparison model;differential of lexical component},
doi={10.1109/SNPD.2012.104},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7230422,
author={M. {Ablimit} and A. {Hamdulla} and A. {Pattar}},
booktitle={2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)},
title={Reducing morpho-phonetic confusion in sub-word based Uyghur ASR},
year={2015},
volume={},
number={},
pages={348-352},
abstract={Sub-word units like morphemes are selected as the lexicon for highly inflectional languages, as they can provide better coverage and a smaller vocabulary size. However, short units shrink the context of statistical models, prone to morpho-phonetic changes, and not always outperform the word based model. When sequence of units are merged or split, unit boundaries are phonetically harmonized in the speech which reflects as the morpho-phonetic changes in the text. This paper investigates morpho-phonetic confusions in the sub-word segmentation of Uyghur text, and phonetic reasons which affect automatic speech recognition (ASR) accuracy. An optimal lexicon set is obtained by comparing ASR results of different layers of lexica, which avoids phonetic confusions in the frequently misrecognized morpheme sequences. This optimal lexicon, which is obtained totally from a HMM based acoustic model, outperformed all the baseline linguistic units. And when all these units are directly incorporated a deep neural network (DNN) based acoustic model, without changing the training corpora and language models, the optimal lexicon not only drastically improved the ASR accuracy but also outperformed other units as a proof of the generality of our approach. Experimental results demonstrate that the optimal lexicon obtained by reducing morpho-phonetic confusions exhibits better ASR accuracy and robustness.},
keywords={hidden Markov models;natural language processing;neural nets;speech processing;speech recognition;morpho-phonetic confusion;Uyghur ASR;morpheme;inflectional language;vocabulary size;statistical model;morpho-phonetic change;sub-word segmentation;Uyghur text;phonetic reason;automatic speech recognition;ASR accuracy;optimal lexicon set;HMM based acoustic model;baseline linguistic unit;deep neural network;DNN based acoustic model;training corpora;language model;Hidden Markov models;Accuracy;Acoustics;Speech;Vocabulary;Training;Surface morphology;ASR;Uyghur;phonetic;morphology;DNN},
doi={10.1109/ChinaSIP.2015.7230422},
ISSN={},
month={July},}
@INPROCEEDINGS{6189683,
author={M. {Mishra} and V. K. {Mishra} and H. R. {Sharma}},
booktitle={2012 2nd National Conference on Computational Intelligence and Signal Processing (CISP)},
title={Leveraging knowledge based question answer technology to address user-interactive short domain question in natural language},
year={2012},
volume={},
number={},
pages={86-90},
abstract={With the rapid growth of the Internet and database technologies in recent years, question answering systems (QAS) have emerged as important applications. Although many QAS have been implemented, little work has been done on the development of a user-centered evaluation for QAS. User-centered evaluation is used to understand a user's needs and identify important dimensions and factors in the development of an information system in order to improve its acceptance. This paper presents the application of understanding of short domain question in natural language to data query. The word segmentation tool, IK Analyzer, which is extended with the obtained domain dictionary, is used to segment the short text questions. From the segmentation results, the keywords are extracted to obtain query target and query requirement of the question and to generate a SQL statement for data query. The method proposed in this paper can be applied to question-answering system based on database and to develop a user-centered evaluation model for QAS from the user's perspective for enhancing the user satisfaction and acceptance of QAS.},
keywords={database management systems;natural language processing;query processing;question answering (information retrieval);user interfaces;knowledge based question answer technology;user-interactive short domain question;natural language;Internet;database technology;user-centered evaluation;user need;information system;data query;word segmentation tool;domain dictionary;short text question segmentation;query target;query requirement;SQL statement;Structured Query Language;user satisfaction;user acceptance;Databases;Natural languages;Dictionaries;Data mining;Information systems;Analytical models;Data models;Evaluation;Measurement;Question answering system;User satisfaction;domain dictionary;segmentation tool},
doi={10.1109/NCCISP.2012.6189683},
ISSN={},
month={March},}
@INPROCEEDINGS{607161,
author={A. {Ito} and M. {Kohda}},
booktitle={Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96},
title={Language modeling by string pattern N-gram for Japanese speech recognition},
year={1996},
volume={1},
number={},
pages={490-493 vol.1},
abstract={This paper describes a new powerful statistical language model based on N-gram model for Japanese speech recognition. In English, a sentence is written word-by-word. On the other hand. A sentence in Japanese has no word boundary character. Therefore. A Japanese sentence requires word segmentation by morphemic analysis before the construction of word N-gram. We propose an N-gram based language model which requires no word segmentation. This model uses character string patterns as units of N-gram. The string patterns are chosen from the training text according to a statistical criterion. We carried out several experiments to compare perplexities of the proposed and the conventional models. which showed the advantage of our model. For many of the readers' interest, we applied this method to English text. As the result of a preliminary experiment, the proposed method got better performance than conventional word trigram.},
keywords={speech recognition;natural languages;string pattern N-gram;Japanese speech recognition;language modeling;statistical language model;word segmentation;morphemic analysis;conventional word trigram;Natural languages;Speech recognition;Speech analysis;Spread spectrum communication;Information analysis;Information retrieval;Natural language processing;Probability;Dictionaries;Testing},
doi={10.1109/ICSLP.1996.607161},
ISSN={},
month={Oct},}
@ARTICLE{6549105,
author={J. J. {Weinman} and Z. {Butler} and D. {Knoll} and J. {Feild}},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Toward Integrated Scene Text Reading},
year={2014},
volume={36},
number={2},
pages={375-387},
abstract={The growth in digital camera usage combined with a worldly abundance of text has translated to a rich new era for a classic problem of pattern recognition, reading. While traditional document processing often faces challenges such as unusual fonts, noise, and unconstrained lexicons, scene text reading amplifies these challenges and introduces new ones such as motion blur, curved layouts, perspective projection, and occlusion among others. Reading scene text is a complex problem involving many details that must be handled effectively for robust, accurate results. In this work, we describe and evaluate a reading system that combines several pieces, using probabilistic methods for coarsely binarizing a given text region, identifying baselines, and jointly performing word and character segmentation during the recognition process. By using scene context to recognize several words together in a line of text, our system gives state-of-the-art performance on three difficult benchmark data sets.},
keywords={document image processing;image motion analysis;image recognition;image segmentation;image sensors;probability;integrated scene text reading;digital camera usage;worldly text abundance;pattern recognition;document processing;unconstrained lexicons;motion blur;curved layouts;perspective projection;occlusion;probabilistic methods;character segmentation;word segmentation;Image segmentation;Character recognition;Text recognition;Probabilistic logic;Hidden Markov models;Noise;Robustness;Scene text recognition;cropped word recognition;character recognition;discriminative semi-Markov model;image binarization;skew detection;baseline estimation;text guidelines;word normalization;word segmentation;Algorithms;Artificial Intelligence;Data Interpretation, Statistical;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Natural Language Processing;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Systems Integration},
doi={10.1109/TPAMI.2013.126},
ISSN={},
month={Feb},}
@INPROCEEDINGS{6765459,
author={Y. {Noguchi} and M. {Kondo} and S. {Kogure} and T. {Konishi} and Y. {Itoh} and A. {Takagi} and H. {Asoh} and I. {Kobayashi}},
booktitle={2013 International Joint Conference on Awareness Science and Technology Ubi-Media Computing (iCAST 2013 UMEDIA 2013)},
title={Generating a variety of expressions from visual information and user-designated viewpoints},
year={2013},
volume={},
number={},
pages={322-328},
abstract={This paper reports the development and evaluation of a natural language generation system which generates a variety of language expressions from visual information taken by a CCD camera. The feature of this system is to generate a variety of language expressions from combinations of different syntactic structures and different sets of vocabulary, while managing the generation process based on the user-designated viewpoints. The system converts the visual information into a concept dependency structure using a semantic representation framework proposed by Takagi and Itoh. The system then transforms the structure and divides it into a set of words, deriving a word dependency structure, which is later arranged into a sentence. The transformation of a concept dependency structure and the variation in word segmentation allow the system to generate a variety of sentences from the same visual information. In this paper, we employ user-designated viewpoints to scenes containing more than one object. We designed the parameters of the user-designated viewpoints which enable the system to manage the generation process and to generate a variety of expressions. An evaluation has confirmed that the system generates certain variations according to parameter values set by the user. The variations include expressions referring to attribute values of the objects in the scenes and relative expressions denoting the relations between the targeted object and others.},
keywords={CCD image sensors;natural language processing;word processing;visual scene;word segmentation;word dependency structure;semantic representation framework;concept dependency structure;vocabulary;syntactic structure combination;CCD camera;natural language generation system;user-designated viewpoint;visual information;language expression generation;Semantics;Visualization;Natural languages;Image color analysis;Standards;Educational institutions;Prototypes;natural language generation;viewpoints;relative expressions;visual scene},
doi={10.1109/ICAwST.2013.6765459},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8286379,
author={N. D. {Londhe} and G. B. {Kshirsagar}},
booktitle={2017 International Conference on Communication and Signal Processing (ICCSP)},
title={Continuous speech recognition system for chhattisgarhi},
year={2017},
volume={},
number={},
pages={0365-0369},
abstract={The next step of isolated word recognition will be the sentence or continuous speech recognition. In this paper, an algorithm to estimates maximum posterior probabilities from input chhattisgarhi speech has proposed. To compute the accurate and precise maximum likelihoods, one need to extract the features from segmented words accurately. An implemented algorithm for automatic word segmentation gives up to 96.00% of average word boundary detection accuracy. Mel filter cepstral coefficients, delta cepstral and delta-delta cepstral speech characteristics have been extracted from the signals. Automatic segmented words from the speech have added and created a vocabulary. Further, 3-gram language model has implemented to resolve the ambiguity of similar pronunciation of the words. The last stage of the system is to compute maximum log likelihoods and maximum posterior probabilities of the word sequence. The experiments have been carried out on self-recorded 220 continuous chhattisgarhi speech, which consist of 2640 sentences and 330,000 words. We have used multilayer feedforward neural network (MLP) and Hidden Markov model (HMM) for maximum likelihood estimation (MLE). The details of experimental analysis have presented in the paper.},
keywords={cepstral analysis;feature extraction;feedforward neural nets;filtering theory;hidden Markov models;maximum likelihood estimation;multilayer perceptrons;natural language processing;probability;speech recognition;maximum posterior probabilities;continuous chhattisgarhi speech;feature extraction;Mel filter cepstral coefficients;delta cepstral speech characteristics;multilayer feedforward neural network;hidden Markov model;HMM;isolated word recognition;continuous speech recognition system;maximum likelihood estimation;word sequence;3-gram language model;delta-delta cepstral speech characteristics;average word boundary detection accuracy;automatic word segmentation;segmented words;precise maximum likelihoods;input chhattisgarhi speech;Speech recognition;Speech;Hidden Markov models;Mel frequency cepstral coefficient;Feature extraction;Continuous speech recognition;Posterior probabilities;maximum log-likelihoods;Automatic word segmentation;Speech characteristics;MLP;HMM},
doi={10.1109/ICCSP.2017.8286379},
ISSN={},
month={April},}
@INPROCEEDINGS{8682764,
author={M. {Vetter} and S. {Sakti} and S. {Nakamura}},
booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Cross-lingual Speech-based Tobi Label Generation Using Bidirectional Lstm},
year={2019},
volume={},
number={},
pages={6620-6624},
abstract={In this paper we investigate the automatic generation of ToBI-style prosody labels. The work is motivated by the idea of using prosodic information to facilitate the automatic lexicon discovery for unseen and under-resourced languages for which sufficient training data is not available. Specifically, the prosodic boundaries are meant to serve as additional top-down information in the word segmentation step. To this end we attempt to apply the trained Japanese models cross-lingually on a language not seen in training (English). We generate break index labels, using only the speech signal as input, with no additional information given at test time in the form of transcripts or prior word segmentations. The labels are generated using bidirectional LSTMs trained on spontaneous Japanese speech. We evaluate the quality of these labels using established metrics, with an F1 score of 0.55 for cross-lingual prosodic break detection (given a tolerance of 80 ms).},
keywords={natural language processing;speech recognition;speech synthesis;cross-lingual speech-based tobi label generation;ToBI-style prosody labels;prosodic boundaries;word segmentation step;speech signal;spontaneous Japanese speech;cross-lingual prosodic break detection;Japanese models;bidirectional LSTM;word segmentations;Training;Indexes;Task analysis;Data models;Neural networks;Speech recognition;Labeling;Prosody detection;ToBI label generation;cross-lingual speech processing;word segmentation},
doi={10.1109/ICASSP.2019.8682764},
ISSN={},
month={May},}
@INPROCEEDINGS{8269008,
author={H. {Kamper} and K. {Livescu} and S. {Goldwater}},
booktitle={2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
title={An embedded segmental K-means model for unsupervised segmentation and clustering of speech},
year={2017},
volume={},
number={},
pages={719-726},
abstract={Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most approaches lie at methodological extremes: some use probabilistic Bayesian models with convergence guarantees, while others opt for more efficient heuristic techniques. Despite competitive performance in previous work, the full Bayesian approach is difficult to scale to large speech corpora. We introduce an approximation to a recent Bayesian model that still has a clear objective function but improves efficiency by using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental K-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. We first compare ES-KMeans to previous approaches on common English and Xitsonga data sets (5 and 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being 5 times faster with fewer hyperparameters. However, its clusters are less pure than those of the other models. We then show that ES-KMeans scales to larger corpora by applying it to the 5 languages of the Zero Resource Speech Challenge 2017 (up to 45 hours), where it performs competitively compared to the challenge baseline.},
keywords={Bayes methods;linguistics;natural language processing;pattern clustering;speech processing;speech recognition;unsupervised learning;competitive performance;Bayesian approach;speech corpora;hard clustering;Bayesian inference;fixed-dimensional acoustic word embeddings;word segmentation;ES-KMeans scales;zero-resource speech processing;convergence guarantees;heuristic techniques;probabilistic Bayesian models;Xitsonga data sets;English data sets;time 5.0 hour;time 2.5 hour;time 45.0 hour;Speech;Bayes methods;Clustering algorithms;Acoustics;Standards;Speech processing;Probabilistic logic;Zero-resource speech processing;word segmentation;unsupervised learning;language acquisition},
doi={10.1109/ASRU.2017.8269008},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6473732,
author={Y. {Qin}},
booktitle={2012 International Conference on Asian Language Processing},
title={Machine Transliteration Based on Error-Driven Learning},
year={2012},
volume={},
number={},
pages={205-208},
abstract={Transliteration is a common translation method when named entities are introduced into another language. Direct orthographical mapping (DOM) approach is successfully applied in machine transliteration by segmenting a word according to syllables and then mapping them directly into target language without considering its pronunciation. The paper studies the performance of two-stage machine transliteration based on Conditional Random Fields. To reduce the amount of computation in model training, we propose an error-driven learning by dividing the training data into several groups and training the transliteration model step by step based on the error prediction data until the performance doesnât increase or the limitation of the computer. Experiments on data of NEWS2011 show that error-driven model training reduces computational complexity and saves the time of model training. Compared to the combining transliteration model, our transliteration system increases the accuracy of top-1 output with 0.06, reaching 0.652.},
keywords={computational complexity;information retrieval;language translation;learning (artificial intelligence);natural language processing;random processes;two-stage machine transliteration;error-driven learning;translation method;direct orthographical mapping;DOM;conditional random fields;error-driven model training;computational complexity reduction;cross-lingual information retrieval;word segmentation;Training;Computational modeling;Training data;Data models;Conferences;Accuracy;Testing;Machine transliteration;Conditional Random Fields;Model training;Error-driven},
doi={10.1109/IALP.2012.48},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7974624,
author={X. {Cong} and L. {Li}},
booktitle={2016 IEEE International Conference on Network Infrastructure and Digital Content (IC-NIDC)},
title={UGC quality evaluation based on meta-learning and content feature analysis},
year={2016},
volume={},
number={},
pages={495-499},
abstract={With the fast development of Social Networking Services, there has been increasingly vast amount of information published by massive network users. Given this information explosion, how to analyze the quality of User Generated Contents (UGC) automatically becomes a challenging task for researchers. To solve the problem, we need to build an effective UGC quality evaluation system. In the light of our experience, we believe that the textual content of UGC is the key factor for its quality. Hence, we focus on textual content based quality evaluation and classification instead of using UGC publishing related data, such as times being commented and forwarded in this paper. We extract various features of the textual contents based on natural language processing technologies firstly, such as word segmentation, keywords, topic model, sentence parsing, distributed word representation etc. Secondly, we build several base-learning classifiers with different features and different machine learning algorithms to assign UGC contents with four different quality labels. Then, we create the global meta-learning model based on these base classifiers to generate the final quality labels for UGC contents. We have also implemented a series of experiments based on realistic data collected from Tianya Forum and use 10-fold cross-validation to test the model. Results have shown that our proposed meta-learning model performs much better.},
keywords={feature extraction;learning (artificial intelligence);natural language processing;pattern classification;social networking (online);text analysis;UGC quality evaluation system;metalearning model;content feature analysis;social networking services;user generated contents;UGC textual content based quality evaluation;UGC textual content classification;feature extraction;natural language processing;base-learning classifiers;machine learning;Tianya Forum;Quality of service;User Generated Contents (UGC);meta-learning;feature analysis;quality evaluation;multiple classifier fusion},
doi={10.1109/ICNIDC.2016.7974624},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7333755,
author={M. {Javed} and P. {Nagabhushan} and B. B. {Chaudhuri}},
booktitle={2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
title={A direct approach for word and character segmentation in run-length compressed documents with an application to word spotting},
year={2015},
volume={},
number={},
pages={216-220},
abstract={Segmentation of a text document into lines, words and characters is an important objective in application like OCR and related analytics. However in today's scenario, the documents are compressed for archival and transmission efficiency. Text segmentation in compressed documents warrants decompression, and needs additional computing resources. In this backdrop, the paper proposes a method for text segmentation directly in run-length compressed, printed English text documents. Line segmentation is done using the projection profile technique. Further segmentation into words and characters is accomplished by tracing the white runs along the base region of the text line. During the process, a run based region growing technique is applied in the spatial neighborhood of the white runs to trace the vertical space between the characters. After detecting the character spaces in the entire text line, the decision of word space and character space is made by computing the average character space. Subsequently based on the spatial position of the detected words and characters, their respective compressed segments are extracted. The proposed algorithm is tested with 1083 compressed text lines, and F-measure of 97.93% and 92.86% respectively for word and character segmentation are obtained. Finally an application of word spotting is also presented.},
keywords={data compression;decision making;document image processing;image segmentation;natural language processing;optical character recognition;text detection;word processing;character segmentation;run-length compressed documents;word spotting;direct approach;word segmentation;text document segmentation;OCR;transmission efficiency;archival efficiency;compressed document warrant decompression;computing resources;text segmentation;printed English text documents;line segmentation;projection profile technique;run based region growing technique;character space detection;projection profile technique;word space decision;character space decision;compressed segment extraction;F-measure;Optical character recognition software;Image coding;Adaptation models;Compressed text document segmentation;compressed word segmentation;compressed character segmentation;word spotting in compressed domain},
doi={10.1109/ICDAR.2015.7333755},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8462002,
author={Y. {Wang} and H. {Lee} and L. {Lee}},
booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Segmental Audio Word2Vec: Representing Utterances as Sequences of Vectors with Applications in Spoken Term Detection},
year={2018},
volume={},
number={},
pages={6269-6273},
abstract={While Word2Vec represents words (in text) as vectors carrying semantic information, audio Word2Vec was shown to be able to represent signal segments of spoken words as vectors carrying phonetic structure information. Audio Word2Vec can be trained in an unsupervised way from an unlabeled corpus, except the word boundaries are needed. In this paper, we extend audio Word2Vec from word-level to utterance-level by proposing a new segmental audio Word2Vec, in which unsupervised spoken word boundary segmentation and audio Word2Vec are jointly learned and mutually enhanced, so an utterance can be directly represented as a sequence of vectors carrying phonetic structure information. This is achieved by a segmental sequence-to-sequence autoencoder (SSAE), in which a segmentation gate trained with reinforcement learning is inserted in the encoder. Experiments on English, Czech, French and German show very good performance in both unsupervised spoken word segmentation and spoken term detection applications (significantly better than frame-based DTW).},
keywords={audio signal processing;natural language processing;recurrent neural nets;speech processing;speech recognition;text analysis;unsupervised learning;vectors;segmental audio word2vec;representing utterances;vectors sequences;spoken term detection;semantic information;phonetic structure information;segmental sequence-to-sequence autoencoder;SSAE;reinforcement learning;natural language processing;unsupervised spoken word boundary segmentation;word-level;Logic gates;Decoding;Training;Erbium;Phonetics;Learning (artificial intelligence);Guidelines;recurrent neural network;autoencoder;reinforcement learning;policy gradient},
doi={10.1109/ICASSP.2018.8462002},
ISSN={},
month={April},}
@INPROCEEDINGS{7490128,
author={M. {Kulkarni} and S. S. {Karande} and S. {Lodha}},
booktitle={2016 12th IAPR Workshop on Document Analysis Systems (DAS)},
title={Unsupervised Word Clustering Using Deep Features},
year={2016},
volume={},
number={},
pages={263-268},
abstract={Digitization is crucial especially in the Indian context. OCR engines fail on Indian scripts mainly because character segmentation is non-trivial. Even word based recognition approaches suffer from the issues such as time degradations, word segmentation errors, font style/size variations. In this paper, we propose a deep learning architecture based approach for unsupervised word clustering. An edge responsive untrained Convolutional Neural Network (CNN) is used as a feature extractor. Graph connected component analysis is applied on the similarity graph computed from the word features. Our approach inherently detects similar shape patterns at word level and hence, it is language agnostic. We validated our approach against multiple state of art word matching techniques. Experimental results show that our approach significantly outperforms all of them on variety of data sets. In addition, the approach is observed to be robust to word segmentation errors, font style/size variations.},
keywords={convolution;document image processing;feature extraction;graph theory;image segmentation;learning (artificial intelligence);natural language processing;neural nets;optical character recognition;pattern clustering;unsupervised word clustering;deep features;digitization;OCR engines;Indian scripts;character segmentation;word based recognition;word segmentation errors;font style variations;font size variations;deep learning architecture;edge responsive untrained convolutional neural network;feature extraction;graph connected component analysis;similarity graph;Feature extraction;Convolution;Machine learning;Computer architecture;Neural networks;Object recognition;Training;Deep learning;word clustering},
doi={10.1109/DAS.2016.14},
ISSN={},
month={April},}
@INPROCEEDINGS{8461545,
author={L. {Ondel} and P. {Godard} and L. {Besacier} and E. {Larsen} and M. {Hasegawa-Johnson} and O. {Scharenborg} and E. {Dupoux} and L. {Burget} and F. {Yvon} and S. {Khudanpur}},
booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Bayesian Models for Unit Discovery on a Very Low Resource Language},
year={2018},
volume={},
number={},
pages={5939-5943},
abstract={Developing speech technologies for low-resource languages has become a very active research field over the last decade. Among others, Bayesian models have shown some promising results on artificial examples but still lack of in situ experiments. Our work applies state-of-the-art Bayesian models to unsupervised Acoustic Unit Discovery (AUD) in a real low-resource language scenario. We also show that Bayesian models can naturally integrate information from other resourceful languages by means of informative prior leading to more consistent discovered units. Finally, discovered acoustic units are used, either as the I-best sequence or as a lattice, to perform word segmentation. Word segmentation results show that this Bayesian approach clearly outperforms a Segmental-DTW baseline on the same corpus.},
keywords={acoustic signal processing;Bayes methods;natural language processing;speech processing;low-resource language scenario;resourceful languages;discovered acoustic units;Bayesian approach;speech technologies;low-resource languages;active research field;Bayesian Models;Very Low Resource Language;unsupervised acoustic unit discovery;AUD;word segmentation;Segmental-DTW baseline;Hidden Markov models;Bayes methods;Data models;Mel frequency cepstral coefficient;Lattices;Measurement;Acoustic Unit Discovery;Low-Resource ASR;Bayesian Model;Informative Prior},
doi={10.1109/ICASSP.2018.8461545},
ISSN={},
month={April},}
@INPROCEEDINGS{6108899,
author={K. K. {Sita} and S. {Suhasini} and Z. P. {Shaik Mohd.}},
booktitle={2011 International Conference on Image Information Processing},
title={Statistical estimation of emotions in speech notes by featured term analogy},
year={2011},
volume={},
number={},
pages={1-6},
abstract={Human Being is the only creature in the World who can express his emotions in various forms. Capturing the extent of emotions in a particular speech notes through quantification of verbal expressions is undoubtedly a challenging area to study. Either positive or negative, whatever be the emotions are in the notes, if we succeed in assessing the degree of positiveness or negativeness, the impact of the notes while addressing to the intended people can be pre-estimated easily. This paper presents a brief idea of how we can statistically estimate the emotional characteristics of a speech notes by analyzing the content in the speech notes through Information Extraction. Since it is mandatory for any sentence to have atleast a verb, we primarily concentrate on the verbs as featured terms in each sentence and there by statistically estimate the rigorousness of the verb on the entire speech note. The strength of the verb in its severity can be measured by comparing it with strong, medium, light corpus of emotions developed specially. This enables to judge the effectiveness of a speech notes in various emotions.},
keywords={emotion recognition;information retrieval;natural language processing;statistical analysis;word processing;statistical emotion estimation;speech note;term analogy feature;verbal expression quantification;information extraction;verb;Speech;Feature extraction;Tagging;Information processing;Speech processing;Estimation;Encoding;Emotion Estimator;Feature Term Identification (FTI);Inflectional morphemes;Normalization;POSTagging;Word Segmentation;Term Weight},
doi={10.1109/ICIIP.2011.6108899},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8313765,
author={K. {Thangairulappan} and K. {Mohan}},
booktitle={2017 Fourth International Conference on Image Information Processing (ICIIP)},
title={Efficient segmentation of printed tamil script into characters using projection and structure},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Segmenting text lines and touching characters remain a problem. But this paper segments the printed touching lines and characters of Tamil script into lines, words and characters. Standard horizontal projection and vertical projection methods cannot segment the touched lines and characters. The proposed method solves the problem of touching lines and touching characters of Tamil Script based on the structural properties of the characters and projection. The proposed method is implemented on different set of documents collected from different Tamil literary periodicals with different sizes and fonts. Experimental results are compared with the projection profile based technique and connected component labelling technique. Results shown that the proposed method segment the documents into lines, words and characters with good accuracy for the regular fonts with any size even though the lines and characters are of touching nature. This method can be applied in preparing OCR system and in document analysis and recognition system when the printed documents are with line and character overlapping.},
keywords={document image processing;image segmentation;natural language processing;optical character recognition;text analysis;text lines;touching characters;standard horizontal projection;vertical projection methods;touched lines;character overlapping;document segmentation;printed tamil script segmentation;printed touching lines;Tamil literary periodicals;projection profile based technique;connected component labelling technique;OCR system;document analysis;document recognition system;Image segmentation;Shape;Information processing;Character recognition;Smoothing methods;Computed tomography;Computer science;Tamil document;touching lines;line-segmentation;word-segmentation;touching-character segmentation;projection profile},
doi={10.1109/ICIIP.2017.8313765},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7036013,
author={E. {Bekbulatov} and A. {Kartbayev}},
booktitle={2014 IEEE 8th International Conference on Application of Information and Communication Technologies (AICT)},
title={A study of certain morphological structures of Kazakh and their impact on the machine translation quality},
year={2014},
volume={},
number={},
pages={1-5},
abstract={This paper describes a morphological analysis of the Kazakh language for Kazakh-English statistical machine translation through changing the compound words of Kazakh language, and explores the effect of using the modified input on translation quality with a large number of training sentences. Word alignment problem would become more serious for translation from morphologically rich language such as Kazakh to morphologically simple one such as English, due to the problem of data sparseness on translation word forms in many different morphological variants. We present our investigations on unsupervised Kazakh morphological segmentation over newspaper corpus and compare unsupervised segmentation against rule-based language processing tools. In our experiments, the results show that our proposed method can improve word alignment and translation quality.},
keywords={computational linguistics;language translation;natural language processing;word processing;morphological structure;machine translation quality;morphological analysis;Kazakh language;Kazakh-English statistical machine translation;compound word;training sentence;word alignment problem;data sparseness;translation word;unsupervised Kazakh morphological segmentation;newspaper corpus;unsupervised segmentation;rule-based language processing tool;Morphology;Training;Automata;Pragmatics;Instruments;Smoothing methods;computational linguistics;kazakh morphology;word segmentation;machine translation},
doi={10.1109/ICAICT.2014.7036013},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8462264,
author={S. {Bhati} and H. {Kamper} and K. {Sri Rama Murty}},
booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Phoneme Based Embedded Segmental K-Means for Unsupervised Term Discovery},
year={2018},
volume={},
number={},
pages={5169-5173},
abstract={Identifying and grouping the frequently occurring word-like patterns from raw acoustic waveforms is an important task in the zero resource speech processing. Embedded segmental K-means (ES-KMeans) discovers both the word boundaries and the word types from raw data. Starting from an initial set of subword boundaries, the ES-Kmeans iteratively eliminates some of the boundaries to arrive at frequently occurring longer word patterns. Notice that the initial word boundaries will not be adjusted during the process. As a result, the performance of the ES-Kmeans critically depends on the initial subword boundaries. Originally, syllable boundaries were used to initialize ES-Kmeans. In this paper, we propose to use a phoneme segmentation method that produces boundaries closer to true boundaries for ES-KMeans initialization. The use of shorter units increases the number of initial boundaries which leads to a significant increment in the computational complexity. To reduce the computational cost, we extract compact lower dimensional embeddings from an auto-encoder. The proposed algorithm is benchmarked on Zero Resource 2017 challenge, which consists of 70 hours of unlabeled data across three languages, viz. English, French, and Mandarin. The proposed algorithm outperforms the baseline system without any language-specific parameter tuning.},
keywords={computational complexity;natural language processing;pattern clustering;speech processing;speech recognition;unsupervised learning;unsupervised term discovery;frequently occurring word-like patterns;raw acoustic waveforms;zero resource speech processing;word types;initial subword boundaries;syllable boundaries;phoneme segmentation method;ES-KMeans initialization;compact lower dimensional embeddings;embedded segmental K-means;computational complexity;auto-encoder;language-specific parameter tuning;Clustering algorithms;Speech processing;Kernel;Mel frequency cepstral coefficient;System performance;Task analysis;Zero Resource speech processing;unsupervised learning;spoken term discovery;word segmentation},
doi={10.1109/ICASSP.2018.8462264},
ISSN={},
month={April},}
@ARTICLE{7467531,
author={A. {Taniguchi} and T. {Taniguchi} and T. {Inamura}},
journal={IEEE Transactions on Cognitive and Developmental Systems},
title={Spatial Concept Acquisition for a Mobile Robot That Integrates Self-Localization and Unsupervised Word Discovery From Spoken Sentences},
year={2016},
volume={8},
number={4},
pages={285-297},
abstract={In this paper, we propose a novel unsupervised learning method for the lexical acquisition of words related to places visited by robots, from human continuous speech signals. We address the problem of learning novel words by a robot that has no prior knowledge of these words except for a primitive acoustic model. Furthermore, we propose a method that allows a robot to effectively use the learned words and their meanings for self-localization tasks. The proposed method is nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates the generative model for self-localization and the unsupervised word segmentation in uttered sentences via latent variables related to the spatial concept. We implemented the proposed method SpCoA on SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile robot in a real environment. Further, we conducted experiments for evaluating the performance of SpCoA. The experimental results showed that SpCoA enabled the robot to acquire the names of places from speech sentences. They also revealed that the robot could effectively utilize the acquired spatial concepts and reduce the uncertainty in self-localization.},
keywords={Bayes methods;mobile robots;natural language processing;nonparametric statistics;sensor fusion;speech processing;uncertain systems;unsupervised learning;mobile robot;unsupervised word discovery;spoken sentences;unsupervised learning;lexical word acquisition;human continuous speech signals;learned words usage;self-localization tasks;nonparametric Bayesian spatial concept acquisition method;SpCoA;unsupervised word segmentation;uttered sentences;latent variables;SIGVerse;TurtleBot2;spatial concepts;uncertainty reduction;Robot sensing systems;Robot kinematics;Speech;Speech recognition;Vocabulary;Unsupervised learning;Learning place names;lexical acquisition;self-localization;spatial concept},
doi={10.1109/TCDS.2016.2565542},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6639221,
author={S. {Sitaram} and S. {Palkar} and Y. {Chen} and A. {Parlikar} and A. W. {Black}},
booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
title={Bootstrapping Text-to-Speech for speech processing in languages without an orthography},
year={2013},
volume={},
number={},
pages={7992-7996},
abstract={Speech synthesis technology has reached the stage where given a well-designed corpus of audio and accurate transcription an at least understandable synthesizer can be built without necessarily resorting to new innovations. However many languages do not have a well-defined writing system but such languages could still greatly benefit from speech systems. In this paper we consider the case where we have a (potentially large) single speaker database but have no transcriptions and no standardized way to write transcriptions. To address this scenario we propose a method that allows us to bootstrap synthetic voices purely from speech data. We use a novel combination of automatic speech recognition and automatic word segmentation for the bootstrapping. Our experimental results on speech corpora in two languages, English and German, show that synthetic voices that are built using this method are close to understandable. Our method is language-independent and can thus be used to build synthetic voices from a speech corpus in any new language.},
keywords={bootstrapping;natural language processing;speech processing;speech recognition;speech synthesis;German language;English language;automatic word segmentation;automatic speech recognition;speech data;text-to-speech bootstrapping;speech processing;speech synthesis technology;well-defined writing system;single speaker database;synthetic voices;Speech;Data models;Speech recognition;Decoding;Synthesizers;Speech processing;Speech Synthesis;Synthesis without Text;Languages without an Orthography},
doi={10.1109/ICASSP.2013.6639221},
ISSN={},
month={May},}
@ARTICLE{7451222,
author={J. {Nishihara} and T. {Nakamura} and T. {Nagai}},
journal={IEEE Transactions on Cognitive and Developmental Systems},
title={Online Algorithm for Robots to Learn Object Concepts and Language Model},
year={2017},
volume={9},
number={3},
pages={255-268},
abstract={Humans form concept of objects by classifying them into categories, and acquire language by simultaneously interacting with others. Thus, the meaning of a word can be learned by connecting a recognized word to its corresponding concept. We consider this ability important for robots to flexibly develop knowledge of language and concepts. In this paper, we propose an online algorithm for robots to acquire knowledge of natural language and learn object concepts. A robot learns the language model from word sequences, which are obtained by the segmentation of phoneme sequences provided by a user, by using unsupervised word segmentation each time it is provided with a new object. Moreover, the robot acquires object concepts using these word sequences as well as multimodal information obtained by observing objects. The crucial aspect of our model is the interdependence of words and concepts: there is a high probability that the same words will be uttered to describe objects in the same category. By taking this relationship into account, our proposed method enables robots to acquire a more accurate language model and object concepts online. Experimental results verify this.},
keywords={natural language processing;robot programming;unsupervised learning;word processing;online algorithm;robot learning;object concepts;language model;natural language knowledge;word sequences;phoneme sequence segmentation;unsupervised word segmentation;Speech recognition;Speech;Robot sensing systems;Visualization;Feedback loop;Speech processing;Language acquisition;multimodal categorization;multimodal latent Dirichlet allocation (MLDA);object concepts;online learning;unsupervised learning},
doi={10.1109/TCDS.2016.2552579},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6818181,
author={W. {Wang} and T. {Zhao} and C. {Zhang}},
booktitle={2013 Ninth International Conference on Natural Computation (ICNC)},
title={Bilingual seed lexicon adaptation for entity translation extraction},
year={2013},
volume={},
number={},
pages={1309-1313},
abstract={Bilingual seed lexicon, which is considered as a bridge between two languages, is one of the main resources used for entity translation extraction tasks from comparable corpora. However, little attention has been paid to this lexicon except its coverage. In fact, the quality of the seed lexicon is one of the key factors that affect the accuracy of entity translation extraction. In this paper, we propose a new self-adaptive model. We use a word segmentation technique to adapt segmented corpora and then propose two strategies of weight allocation and corresponding filter. Experiments demonstrate that our technique significantly outperforms the standard approach.},
keywords={language translation;linguistics;natural language processing;text analysis;bilingual seed lexicon adaptation;entity translation extraction;self-adaptive model;word segmentation technique;segmented corpora;weight allocation;Resource management;Context;Noise;Vectors;Correlation;Standards;Radio spectrum management;adaptation;seed lexicon;comparable corpora;entity translation extraction},
doi={10.1109/ICNC.2013.6818181},
ISSN={},
month={July},}
@INPROCEEDINGS{6385812,
author={T. {Araki} and T. {Nakamura} and T. {Nagai} and S. {Nagasaka} and T. {Taniguchi} and N. {Iwahashi}},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
title={Online learning of concepts and words using multimodal LDA and hierarchical Pitman-Yor Language Model},
year={2012},
volume={},
number={},
pages={1623-1630},
abstract={In this paper, we propose an online algorithm for multimodal categorization based on the autonomously acquired multimodal information and partial words given by human users. For multimodal concept formation, multimodal latent Dirichlet allocation (MLDA) using Gibbs sampling is extended to an online version. We introduce a particle filter, which significantly improve the performance of the online MLDA, to keep tracking good models among various models with different parameters. We also introduce an unsupervised word segmentation method based on hierarchical Pitman-Yor Language Model (HPYLM). Since the HPYLM requires no predefined lexicon, we can make the robot system that learns concepts and words in completely unsupervised manner. The proposed algorithms are implemented on a real robot and tested using real everyday objects to show the validity of the proposed system.},
keywords={educational robots;human-robot interaction;learning systems;natural language processing;online learning;multimodal LDA;hierarchical Pitman-Yor language model;online algorithm;multimodal categorization;multimodal information;partial words;multimodal concept formation;multimodal latent Dirichlet allocation;Gibbs sampling;particle filter;unsupervised word segmentation;predefined lexicon;robot system;Robot sensing systems;Humans;Vectors;Haptic interfaces;Data models;Predictive models},
doi={10.1109/IROS.2012.6385812},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4579373,
author={M. A. {Sattar} and K. {Mahmud} and H. {Arafat} and A. F. M. {Noor Uz Zaman}},
booktitle={2007 10th international conference on computer and information technology},
title={Segmenting bangla text for optical recognition},
year={2007},
volume={},
number={},
pages={1-6},
abstract={One of the important reasons for poor recognition rate in optical character recognition (OCR) system is the error in character segmentation. Existence of different type of characters in the scanned documents is a major problem to design an effective character segmentation procedure. In this paper, a new technique is presented for identification and segmentation of Bengali printed characters. This paper focuses on the segmentation of printed Bengali characters for efficient recognition of the characters. Our Line segmentation success rate is 99.7 % for 1000 lines, we have tested. Our Word segmentation success rate is 99.8 % for 4900 words tested. From the experiment we noticed that isolated characters fall into isolated group in 99.50 % cases. Most of the errors come from connected characters and characters having tau in front of them as segmenting tau we take the help of width. From the experiment we noticed that most of the errors came from components having multi-touching points between two characters.},
keywords={natural language processing;optical character recognition;text analysis;optical character recognition;character segmentation procedure;Bengali printed characters;line segmentation;word segmentation;text segmentation;Bangla OCR;Bangla Text segmentation;Bangla Language Processing},
doi={10.1109/ICCITECHN.2007.4579373},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8661146,
author={M. {Pourreza} and R. {Derakhshan} and H. {Fayyazi} and M. {Sabokrou}},
booktitle={2018 9th International Symposium on Telecommunications (IST)},
title={Sub-word based Persian OCR Using Auto-Encoder Features and Cascade Classifier},
year={2018},
volume={},
number={},
pages={481-485},
abstract={In Persian text, unlike English, the letters are stick together and their shapes varies depending on where they are in the word. This makes it difficult to distinguish letters in Persian OCR. One way to overcome this problem is to recognize the sub-words, not the letters. In this paper with the help of a complete sub-word image dictionary, a new approach for Persian OCR is presented. For sub-word image recognition, two cascade SVM classifiers that are trained with the features extracted by Auto-Encoder, are exploited. The extracted sub-word texts form the words based on the results of a pre-process word segmentation step. The resulted text is enhanced using a fast post-process algorithm which uses a word dictionary.},
keywords={feature extraction;image classification;image segmentation;natural language processing;optical character recognition;support vector machines;text analysis;pre-process word segmentation;sub-word based Persian OCR;feature extraction;word dictionary;extracted sub-word texts;cascade SVM classifiers;sub-word image recognition;complete sub-word image dictionary;Persian text;cascade classifier;auto-encoder features;Feature extraction;Optical character recognition software;Dictionaries;Support vector machines;Image segmentation;Image recognition;Clustering algorithms;Persian OCR;Sub-Word Image Dictionary;Auto-Encoder;Clustering;Cascade Classifier;Post-Process},
doi={10.1109/ISTEL.2018.8661146},
ISSN={},
month={Dec},}
@INPROCEEDINGS{953918,
author={Y. {Ishitani}},
booktitle={Proceedings of Sixth International Conference on Document Analysis and Recognition},
title={Model-based information extraction method tolerant of OCR errors for document images},
year={2001},
volume={},
number={},
pages={908-915},
abstract={A new method for information extraction from document images is proposed in this paper as the basis for a document reader which can extract required keywords and their logical relationship from various printed documents. Such documents obtained from OCR results may have not only unknown words and compound words, but also incorrect words due to OCR errors. To cope with OCR errors, the proposed method adopts robust keyword matching which searches for a string pattern from two dimensional OCR results consisting of a set of possible character candidates. This keyword matching uses a keyword dictionary that includes incorrect words with typical OCR errors and segments of words to deal with the above difficulties. After keyword matching, a global document matching is carried out between keyword matching results in an input document and document models which consist of keyword models and their logical relationship. This global matching determines the most suitable model for the input document and solves word segmentation problems accurately even if the document has unknown words, compound words, or incorrect words. Experimental results obtained for 100 documents show that the method is robust and effective for various document structures.},
keywords={information retrieval;optical character recognition;document image processing;image matching;dictionaries;image segmentation;string matching;model-based information extraction;OCR errors;document image processing;document reader;printed documents;keyword dictionary;global document matching;word segmentation;experimental results;unknown words;compound words;keyword matching;Data mining;Optical character recognition software;Robustness;Error analysis;Research and development;Pattern matching;Dictionaries;Natural language processing;Image segmentation;Information analysis},
doi={10.1109/ICDAR.2001.953918},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7451528,
author={A. {Kumar} and L. {Padró} and A. {Oliver}},
booktitle={2015 International Conference on Asian Language Processing (IALP)},
title={Unsupervised learning of agglutinated morphology using nested Pitman-Yor process based morpheme induction algorithm},
year={2015},
volume={},
number={},
pages={45-48},
abstract={In this paper we describe a method of morphologically segment highly agglutinating and inflectional languages from the Dravidian family. We use the nested Pitman-Yor process to segment long agglutinated words into their basic components, and use a corpus based morpheme induction algorithm to perform morpheme segmentation. We test our method on two languages, Malayalam and Kannada and compare the results with Morfessor-baseline.},
keywords={natural language processing;text analysis;unsupervised learning;unsupervised learning;agglutinated morphology;nested Pitman-Yor process-based morpheme induction algorithm;Dravidian family;long-agglutinated word segmentation;corpus-based morpheme induction algorithm;morpheme segmentation;Malayalam language;Kannada language;Morfessor-baseline;morphologically segmented highly-agglutinating-inflectional languages;Computational modeling;Morphology;Biological system modeling;Nested Pitman-Yor Process;Agglutinated mor-phology;Indian languages},
doi={10.1109/IALP.2015.7451528},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6037316,
author={T. {Armstrong} and S. {Antetomaso}},
booktitle={2011 IEEE International Conference on Development and Learning (ICDL)},
title={Unsupervised discovery of phoneme boundaries in multi-speaker continuous speech},
year={2011},
volume={2},
number={},
pages={1-5},
abstract={Children rapidly learn the inventory of phonemes used in their native tongues. Computational approaches to learning phoneme boundaries from speech data do not yet reach the level of human performance. We present an algorithm that operates on, qualitatively, similar data to those children receive: natural language utterances from multiple speakers. Our algorithm is unsupervised and discovers phoneme boundary positions in speech. The approach draws inspiration from the word and text segmentation literature. To demonstrate the efficacy of our algorithm on speech data, we present empirical results of our method using the TIMIT data set. Our method achieves F-measure scores in the 0.68 - 0.73 range for locating phoneme boundary positions.},
keywords={natural language processing;speech processing;unsupervised discovery;phoneme boundaries;multispeaker continuous speech;speech data;human performance;natural language utterances;multiple speakers;word segmentation;text segmentation;Manuals;Entropy;Feature extraction;Gold;Speech},
doi={10.1109/DEVLRN.2011.6037316},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7916723,
author={A. {Sheshasaayee} and {Angela Deepa. V.R}},
booktitle={2016 Online International Conference on Green Engineering and Technologies (IC-GET)},
title={Ascertaining the morphological components of Tamil language using unsupervised approach},
year={2016},
volume={},
number={},
pages={1-6},
abstract={The learning of morphological components in a natural language by means of segmentation of words to identify the prime chores of stems and affixes lead to effective morphological analysis. Morphological segmentation is an important step in the analysis of natural languages to identify its intricate properties. Remarkable approaches been used in order to construct an effective morphological segmentation framework to study the highly agglutinative Tamil language. This paper focus on unsupervised means of segmenting Tamil lexicons by using a novel algorithm across various parameters. The proposed work shows a promising result in favour to the identification of morphemes with their suffixes.},
keywords={natural language processing;unsupervised learning;morphological component learning;Tamil language;unsupervised approach;natural language analysis;word segmentation;effective morphological analysis;effective morphological segmentation framework;Tamil lexicon segmentation;morpheme identification;Pragmatics;Morphology;Computer science;Algorithm design and analysis;Computational modeling;Measurement;Morphological segmentation;unsupervised;machine translation;agglutinative;morphemes;suffix},
doi={10.1109/GET.2016.7916723},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6176832,
author={K. {Ghosh} and K. S. {Rao}},
booktitle={2012 National Conference on Communications (NCC)},
title={Subword based approach for grapheme-to-phoneme conversion in Bengali text-to-speech synthesis system},
year={2012},
volume={},
number={},
pages={1-5},
abstract={In this paper, we propose a subword based approach for grapheme-to-phoneme (G2P) conversion in a text-to-speech (TTS) synthesis system. The proposed method resolves the problems present in both the manual and rule-based approaches for G2P conversion. The subword method uses a segmentation procedure which chops a word into its main part (root word) and subword part (suffix). By proper segmentation of a word, this method efficiently offers insight into the basic morphological information of the word. The proposed method reduces the size of the pronunciation dictionary without affecting the vocabulary coverage. For the rule-based approach, the proposed method segments the word into two parts and predicts the pronunciation for both the word segments separately. The final pronunciation is achieved by concatenating pronunciations for both the word segments. The subword method improves the accuracy of the rule-based approach by resolving the ambiguity especially in case of the inflected or compound words.},
keywords={natural language processing;speech synthesis;text analysis;vocabulary;subword based approach;grapheme-to-phoneme conversion;Bengali text-to-speech synthesis system;G2P conversion;TTS synthesis system;manual based approach;rule-based approach;word segmentation;root word;morphological information;pronunciation dictionary;vocabulary coverage;pronunciation concatenation;inflected word;compound word;Dictionaries;Manuals;Speech;Accuracy;Databases;Compounds;Information technology;Subword method;stemming;grapheme-to-phoneme conversion;text-to-speech synthesis},
doi={10.1109/NCC.2012.6176832},
ISSN={},
month={Feb},}
@INPROCEEDINGS{7179088,
author={F. {Stahlberg} and T. {Schlippe} and S. {Vogel} and T. {Schultz}},
booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Cross-lingual lexical language discovery from audio data using multiple translations},
year={2015},
volume={},
number={},
pages={5823-5827},
abstract={Zero-resource Automatic Speech Recognition (ZR ASR) addresses target languages without given pronunciation dictionary, transcribed speech, and language model. Lexical discovery for ZR ASR aims to extract word-like chunks from speech. Lexical discovery benefits from the availability of written translations in another source language. In this paper, we improve lexical discovery even more by combining multiple source languages. We present a novel method for combining noisy word segmentations resulting in up to 11.2% relative F-score gain. When we extract word pronunciations from the combined segmentations to bootstrap an ASR system, we improve accuracy by 9.1% relative compared to the best system with only one translation, and by 50.1% compared to monolingual lexical discovery.},
keywords={natural language processing;speech recognition;cross lingual lexical language discovery;audio data;multiple language translation;zero resource automatic speech recognition;word-like chunks;lexical discovery;multiple source language;noisy word segmentation;Speech;Zirconium;Dictionaries;Acoustics;Automatic speech recognition;Computational modeling;Lexical language discovery;zero-resource automatic speech recognition;word-to-phoneme alignment;non-written languages},
doi={10.1109/ICASSP.2015.7179088},
ISSN={},
month={April},}
@INPROCEEDINGS{8389951,
author={A. {Mehta} and A. {Gor}},
booktitle={2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)},
title={Multifont multisize Gujarati OCR with style identification},
year={2017},
volume={},
number={},
pages={275-281},
abstract={In recent years, OCR (Optical Character Recognition) technology has been applied throughout the entire spectrum of industries, revolutionizing the document management process. Document Analysis (DA) is a pre-requisite in every OCR task. Detection of style is one of the most important issues in DA. Style identification is the cause for accuracy of the text recognition system in the OCR. An approach is introduced for multifont, multisize OCR with style identification for printed Gujarati script. Here, scanned image is segmented into lines and words followed by style identification (Bold, Italic, or Normal) on segmented words. Then characters are segmented from words. For identifying characters uniquely Histogram of Oriented Gradients (HOG) and Chain Code Histogram (CCH) are used. Individual classifiers are used for recognizing bold, italic and normal style characters using SVM classifier. At the end post processing is carried out to finalize the class label. Approach is tested on various documents of different fonts (LMG-Arun, Gujarati-saral, thesis fonts) and sizes (12, 14, and 16). Total 34 consonants(to), 3 vowels, 6 modifiers and combination of modifiers with consonant are consider for recognition purpose. Overall recognition accuracy of all types of document is comparable.},
keywords={character sets;document image processing;gradient methods;image classification;image segmentation;natural language processing;optical character recognition;support vector machines;text analysis;multisize Gujarati OCR;style identification;OCR technology;Optical Character Recognition;document management process;Document Analysis;OCR task;text recognition system;multifont OCR;multisize OCR;printed Gujarati script;word segmentation;normal style character recognition;italic style character recognition;bold style character recognition;support vector machines;Histogram of Oriented Gradients;HOG;Chain Code Histogram;CCH;Image segmentation;Histograms;Optical character recognition software;Character recognition;Feature extraction;Support vector machines;Data analysis;optical character recognition;style identification;stroke width;zone identification;orientation angle;histogram of oriented gradients;chain code histogram;one vs all SVM;post processing},
doi={10.1109/ICECDS.2017.8389951},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6754849,
author={S. A. {Angadi} and M. M. {Kodabagi}},
booktitle={2014 Fifth International Conference on Signal and Image Processing},
title={A Robust Segmentation Technique for Line, Word and Character Extraction from Kannada Text in Low Resolution Display Board Images},
year={2014},
volume={},
number={},
pages={42-49},
abstract={Reliable extraction/segmentation of text lines, words and characters is one of the very important steps for development of automated systems for understanding the text in low resolution display board images. In this paper, a new approach for segmentation of text lines, words and characters from Kannada text in low resolution display board images is presented. The proposed method uses projection profile features and on pixel distribution statistics for segmentation of text lines. The method also detects text lines containing consonant modifiers and merges them with corresponding text lines, and efficiently separates overlapped text lines as well. The character extraction process computes character boundaries using vertical profile features for extracting character images from every text line. Further, the word segmentation process uses k-means clustering to group inter character gaps into character and word cluster spaces, which are used to compute thresholds for extracting words. The method also takes care of variations in character and word gaps. The proposed methodology is evaluated on a data set of 1008 low resolution images of display boards containing Kannada text captured from 2 mega pixel cameras on mobile phones at various sizes 240x320, 600x800 and 900x1200. The method achieves text line segmentation accuracy of 97.17%, word segmentation accuracy of 97.54% and character extraction accuracy of 99.09%. The proposed method is tolerant to font variability, spacing variations between characters and words, absence of free segmentation path due to consonant and vowel modifiers, noise and other degradations. The experimentation with images containing overlapped text lines has given promising results.},
keywords={character recognition;feature extraction;image resolution;image segmentation;natural language processing;pattern clustering;statistics;text analysis;robust segmentation technique;line extraction;word extraction;character extraction;Kannada text;low resolution display board images;reliable extraction;pixel distribution statistics;vertical profile features;character image extraction;k-means clustering;Image segmentation;Feature extraction;Image resolution;Accuracy;Algorithm design and analysis;Vectors;Equations;Segmentation;K-Means Clustering;Projection Profile Features;Low Resolution Images;Display Boards},
doi={10.1109/ICSIP.2014.11},
ISSN={},
month={Jan},}
@INPROCEEDINGS{7019590,
author={T. {Shreekanth} and V. {Udayashankara}},
booktitle={2014 International Conference on Contemporary Computing and Informatics (IC3I)},
title={A two stage Braille Character segmentation approach for embossed double sided Hindi Devanagari Braille documents},
year={2014},
volume={},
number={},
pages={533-538},
abstract={The Optical Braille Character Recognition (OBR) system is in significant need in order to preserve the Braille documents to make them available in future for the large section of visually impaired people and also to make the bi-directional communication between the sighted people and the visually impaired people feasible. The recognition and transcribing the double sided Braille document into its corresponding natural text is a challenging task. This difficulty is due to the overlapping of the front side dots (Recto) with that of the back side dots (Verso) in the Inter-point Braille document. In such cases, the usual method of template matching to distinguish recto and verso dots is not efficient. In this paper a new system for double sided Braille dot recognition is proposed, which employs a two-stage highly efficient and an adaptive technique to differentiate the recto and verso dots from an inter-point Braille using the projection profile method. In this paper we present (i) a horizontal projection profile for Braille line segmentation, (ii) vertical projection profile for Braille word segmentation and (iii) Integration of horizontal and vertical projection profiles along with distance thresholding for Braille character segmentation. We demonstrate the effectiveness of this segmentation technique on a large dataset consisting of 754 words from Hindi Devanagari Braille documents with varying image resolution and with different word patterns. A recognition rate of 96.9% has been achieved.},
keywords={handicapped aids;image resolution;image segmentation;natural language processing;optical character recognition;two stage Braille character segmentation approach;embossed double sided Hindi Devanagari Braille documents;optical Braille character recognition system;OBR;bidirectional communication;sighted people;visually impaired people;natural text;front side dots;recto;back side dots;verso;interpoint Braille document;double sided Braille dot recognition;image resolution;Image segmentation;Character recognition;Informatics;Image resolution;Lighting;Optical character recognition software;Optical imaging;Hindi Devanagari Braille;OBR;Projection profile;Recto;Verso},
doi={10.1109/IC3I.2014.7019590},
ISSN={},
month={Nov},}
@ARTICLE{6717132,
author={M. {Doi} and H. {Lei}},
journal={Proceedings of the IEEE},
title={STARS: Word Processing for the Japanese Language [Scanning Our Past]},
year={2014},
volume={102},
number={2},
pages={222-228},
abstract={Toshiba started the automatic kanato-kanji conversion project in 1971. Kenichi Mori's research group solved the homonym problem using usage frequency information and interactive learning. In text editing, a user selects one of the homonyms displayed in the order of the usage frequency information. This information was based on the usage frequency in newspapers. A user's selection is automatically learned and used to update the usage frequency information: using the algorithm of last used homonym is the first out next time. Use of the last-in-first-out algorithm for selection of the homonym was crucial to the success of the conversion technology. In September 1978, Toshiba exhibited the JW-10, the first Japanese language word processor. During the 1990s, Japanese word processors became obsolete because of the growing popularity of PCs and word-processing software that incorporate kana-to-kanji conversion using automatic word segmentation. Programs like Microsoft Word and Justsystems' Ichitaro (1985) made this conversion a basic function of PCs. By 1999, major Japanese companies had exited the Japanese word-processor business.},
keywords={natural language processing;word processing;automatic kanato-kanji conversion project;homonym problem;usage frequency information;interactive learning;text editing;newspapers;user selection;last-in-first-out algorithm;Toshiba;JW-10;Japanese language word processor;PCs;word-processing software;kana-to-kanji conversion;automatic word segmentation;Microsoft Word;Justsystems' Ichitaro;Natural language processing;Japan;Text processing;History},
doi={10.1109/JPROC.2013.2295876},
ISSN={},
month={Feb},}
@INPROCEEDINGS{6645984,
author={},
booktitle={2013 International Conference on Asian Language Processing},
title={Table of contents},
year={2013},
volume={},
number={},
pages={v-x},
abstract={The following topics are dealt with: Asian language processing; discourse analysis; information extraction; information retrieval; linguistics; language study; machine translation; sentiment analysis; spoken language processing; syntactic analysis; semantic analysis; text classification; text clustering; word segmentation; and POS tagging.},
keywords={information retrieval;language translation;linguistics;natural language processing;pattern classification;pattern clustering;text analysis;Asian language processing;discourse analysis;information extraction;information retrieval;linguistics;language study;machine translation;sentiment analysis;spoken language processing;syntactic analysis;semantic analysis;text classification;text clustering;word segmentation;POS tagging},
doi={10.1109/IALP.2013.4},
ISSN={},
month={Aug},}
