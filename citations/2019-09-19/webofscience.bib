% Encoding: UTF-8
ï»¿
@article{ ISI:000470910200001,
Author = {Shivakumar, Prashanth Gurunath and Georgiou, Panayiotis},
Title = {{Confusion2Vec: towards enriching vector space word representations with
   representational ambiguities}},
Journal = {{PEERJ COMPUTER SCIENCE}},
Year = {{2019}},
Month = {{JUN 10}},
Abstract = {{Word vector representations are a crucial part of natural language
   processing (NLP) and human computer interaction. In this paper, we
   propose a novel word vector representation, Confusion2Vec, motivated
   from the human speech production and perception that encodes
   representational ambiguity. Humans employ both acoustic similarity cues
   and contextual cues to decode information and we focus on a model that
   incorporates both sources of information. The representational ambiguity
   of acoustics, which manifests itself in word confusions, is often
   resolved by both humans and machines through contextual cues. A range of
   representational ambiguities can emerge in various domains further to
   acoustic perception, such as morphological transformations, word
   segmentation, paraphrasing for NLP tasks like machine translation, etc.
   In this work, we present a case study in application to automatic speech
   recognition (ASR) task, where the word representational
   ambiguities/confusions are related to acoustic similarity. We present
   several techniques to train an acoustic perceptual similarity
   representation ambiguity. We term this Confusion2Vec and learn on
   unsupervised-generated data from ASR confusion networks or lattice-like
   structures. Appropriate evaluations for the Confusion2Vec are formulated
   for gauging acoustic similarity in addition to semantic-syntactic and
   word similarity evaluations. The Confusion2Vec is able to model word
   confusions efficiently, without compromising on the semantic-syntactic
   word relations, thus effectively enriching the word vector space with
   extra task relevant ambiguity information. We provide an intuitive
   exploration of the two-dimensional Confusion2Vec space using principal
   component analysis of the embedding and relate to semantic
   relationships, syntactic relationships, and acoustic relationships. We
   show through this that the new space preserves the semantic/syntactic
   relationships while robustly encoding acoustic similarities. The
   potential of the new vector representation and its ability in the
   utilization of uncertainty information associated with the lattice is
   demonstrated through small examples relating to the task of ASR error
   correction.}},
DOI = {{10.7717/peerj-cs.195}},
Article-Number = {{e195}},
ISSN = {{2376-5992}},
ResearcherID-Numbers = {{Georgiou, Panayiotis Panos/E-8387-2018}},
ORCID-Numbers = {{Georgiou, Panayiotis Panos/0000-0002-0790-7161}},
Unique-ID = {{ISI:000470910200001}},
}

@inproceedings{ ISI:000481622601073,
Author = {Liu, Di and Su, Jiangwen and Song, Lihua and Qiu, Zhen},
Book-Group-Author = {{IOP}},
Title = {{Application of Internet segmentation research based on Natural Language
   Processing technology in enterprise public opinion risk monitoring}},
Booktitle = {{2018 INTERNATIONAL SYMPOSIUM ON POWER ELECTRONICS AND CONTROL
   ENGINEERING (ISPECE 2018)}},
Series = {{Journal of Physics Conference Series}},
Year = {{2019}},
Volume = {{1187}},
Note = {{International Symposium on Power Electronics and Control Engineering
   (ISPECE), Xian Univ Technol, Xian, PEOPLES R CHINA, DEC 28-30, 2018}},
Abstract = {{With the advent of the mobile Internet era, the network has become a
   distribution center of various information such as media, entertainment,
   sports, economy, politics and so on. A large amount of information is
   generated and disappeared on the network every day. How to effectively
   extract and identify the relevant data, and judge and analyze them is an
   important part of the corporate public opinion control. This paper uses
   natural language processing technology to study the word segmentation of
   text information on the network, and applies it to the risk detection of
   corporate public opinion.}},
DOI = {{10.1088/1742-6596/1187/4/042007}},
Article-Number = {{042007}},
ISSN = {{1742-6588}},
EISSN = {{1742-6596}},
Unique-ID = {{ISI:000481622601073}},
}

@inproceedings{ ISI:000470071700124,
Author = {Shen, Hanji and Long, Chun and Wan, Wei and Li, Jun and Qin, Yakui and
   Fu, Yuhao and Song, Xiaofan},
Book-Group-Author = {{IEEE}},
Title = {{Log Layering Based on Natural Language Processing}},
Booktitle = {{2019 21ST INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY
   (ICACT): ICT FOR 4TH INDUSTRIAL REVOLUTION}},
Series = {{International Conference on Advanced Communication Technology}},
Year = {{2019}},
Pages = {{660-663}},
Note = {{21st International Conference on Advanced Communication Technology
   (ICACT), Pyeongchang, SOUTH KOREA, FEB 17-20, 2019}},
Organization = {{IEEE; IEEE Commun Soc; Global IT Res Inst; Natl Informat Soc Agcy; ETRI;
   Gangwon Convent \& Visitors Bur; Natl Univ, Inst Vietnam, Informat
   Technol; KICS; IEEK ComSoc; Korean Inst Informat Scientists \&
   Engineers; Open Standards \& Internet Assoc; Korea Inst Informat Secur
   \& Cryptol; IEEE Commun Soc Commun \& Informat Secur Tech Community;
   IEEE Commun Soc Optical Networking Tech Community; IEEE Commun Soc Int
   Conference Adv Commun Technol}},
Abstract = {{With the increasing number and variety of logs, the requirement of
   storage space is growing rapidly. Meantime, the speed and accuracy of
   querying in massive logs are becoming increasingly important. Although
   the well-built distributed storage technique solves the problem of mass
   storage and fast query, the cost is too high. As logs are created as the
   method to trace the historical operation, the requirement for query rate
   is not high. To balance the storage cost and query rate, this paper
   proposes a real-time log layering storage technique based on natural
   language processing. According to the characteristics of the log data,
   this technique is combined with the text language processing technique.
   It compresses the real-time log data effectively while considering the
   query efficiency. Firstly, the method extracts the feature of each log
   that flows in, which will be the type name of the log. Then, the method
   performs word segmentation on the log and encodes each word to store the
   key value pairs. Finally, the key value pairs of the log are stored in
   the memory, and the code of each log is stored in the database.
   Experiments show that this method can ensure the integrity of the data
   effectively, decompression time dropped to 40\%, compression rate down
   to 35\%.}},
ISSN = {{1738-9445}},
ISBN = {{979-11-88428-02-1}},
Unique-ID = {{ISI:000470071700124}},
}

@inproceedings{ ISI:000470064000028,
Author = {Mzamo, Lulamile and Helberg, Albert and Bosch, Sonja},
Book-Group-Author = {{IEEE}},
Title = {{Towards an unsupervised morphological segmenter for isiXhosa}},
Booktitle = {{2019 SOUTHERN AFRICAN UNIVERSITIES POWER ENGINEERING CONFERENCE/ROBOTICS
   AND MECHATRONICS/PATTERN RECOGNITION ASSOCIATION OF SOUTH AFRICA
   (SAUPEC/ROBMECH/PRASA)}},
Year = {{2019}},
Pages = {{166-170}},
Note = {{27th Southern African Universities Power Engineering Conference (SAUPEC)
   / 11th Robotics and Mechatronics Conference of South Africa (RobMech) /
   29th Annual Symposium of Pattern-Recognition-Association-of-South-Africa
   (PRASA), Cent Univ Technol, Bloemfontein, SOUTH AFRICA, JAN 28-30, 2019}},
Organization = {{IEEE; S African Inst Elect Engineers; Robot Assoc S Africa; Pattern
   Recognit Assoc S Africa; IEEE S Africa Sect; FESTO; ABB; Altair; Cobots;
   OPAL RT Technologies; HORNE Technologies}},
Abstract = {{In this paper, branching entropy techniques and isiXhosa language
   heuristics are adapted to develop unsupervised morphological segmenters
   for isiXhosa. An overview of isiXhosa segmentation issues is given,
   followed by a discussion on previous work in automated segmentation, and
   segmentation of isiXhosa in particular. Two unsupervised isiXhosa
   segmenters are presented and compared to a random minimum baseline and
   Morfessor-Baseline, a standard in unsupervised word segmentation.
   Morfessor-Baseline outperforms both isiXhosa segmenters at 79.10\%
   boundary identification accuracy. The IsiXhosa Branching Entropy
   Segmenter (XBES) performance varies depending on the segmentation mode
   used, with a maximum of 73.39\%. The IsiXhosa Heuristic Maximum
   Likelihood Segmenter (XHMLS) achieves 72.42\%. The study suggests that
   unsupervised isiXhosa morphological segmentation is feasible with better
   optimization of the current attempts.}},
ISBN = {{978-1-7281-0369-3}},
Unique-ID = {{ISI:000470064000028}},
}

@article{ ISI:000450597900037,
Author = {Zhang, Zeliang and Bi, Xinwen},
Title = {{Research and Experiment of Intelligent Natural Language Processing
   Algorithms}},
Journal = {{WIRELESS PERSONAL COMMUNICATIONS}},
Year = {{2018}},
Volume = {{102}},
Number = {{4}},
Pages = {{2927-2939}},
Month = {{OCT}},
Abstract = {{Natural language processing is mainly divided into two parts: speech
   processing and word processing. The level of word processing is mainly
   studied. Natural language processing is divided into lexical analysis,
   syntax analysis and semantic analysis. Aiming at the scope of the
   language ambiguity and thesaurus in the field of smart home, the maximal
   matching algorithm is used to segment the natural language. Then,
   through the way of template matching, semantic comprehension finally
   forms the code form that can control the home node. In the system
   applied in this paper, the speech is processed into words through the
   existing voice input function of the mobile terminal. Then, the control
   instruction is obtained through the language processing method. The
   processed data is communicated to the server via socket. The server
   sends the data to the home node through the Zigbee protocol. Finally,
   control of home appliances is achieved.}},
DOI = {{10.1007/s11277-018-5316-2}},
ISSN = {{0929-6212}},
EISSN = {{1572-834X}},
Unique-ID = {{ISI:000450597900037}},
}

@inproceedings{ ISI:000468838000153,
Author = {Zhao, Tianyuan and Li, Lei and Xie, Yang and Lv, Yue},
Book-Group-Author = {{IEEE}},
Title = {{Data-driven Risk Assessment for Peer-to-Peer Network Lending Agencies}},
Booktitle = {{PROCEEDINGS OF 2018 5TH IEEE INTERNATIONAL CONFERENCE ON CLOUD COMPUTING
   AND INTELLIGENCE SYSTEMS (CCIS)}},
Series = {{International Conference on Cloud Computing and Intelligence Systems}},
Year = {{2018}},
Pages = {{799-803}},
Note = {{5th IEEE International Conference on Cloud Computing and Intelligence
   Systems (CCIS), Nanjing, PEOPLES R CHINA, NOV 23-25, 2018}},
Organization = {{IEEE; IEEE Beijing Sect; Chinese Assoc Artificial Intelligence; Nanjing
   Univ Posts \& Telecommunicat; Shanghai Univ; Jiangsu Engineering Lab Big
   Data Anal \& Control Active Distribut Network; Nanjing Univ Sci \&
   Technol; Swinburne Univ Technol; Shanghai Key Lab Power Stn Automat
   Technol}},
Abstract = {{With the rapid development of Peer-to-Peer(P2P) network lending in the
   financial field, more data of lending agencies have appeared. P2P
   agencies also have problems such as absconded with ill-gotten gains and
   out of business. Therefore, it is necessary to assess their risks based
   on P2P company data. This paper proposes a framework of Data-driven Risk
   Assessment for P2P(DRAP2P) network lending agencies based on
   unstructured natural language data. First, use the natural language
   processing technology, such as word segmentation, keyword, LDA topic
   model, word2vec and doc2vec, to process and extract features of company
   profile which reflect its business status. Then, seven machine learning
   classifiers and three deep learning models are used for analysis. Since
   keywords show good performance in machine learning models, we improve
   Convolutional Neural Network(CNN) with keywords and propose two
   CNN+Keyword models, namely CNN+Keyword(static+BP) and CNN+Keyword
   (Expand word embedding). Experiments have shown that
   CNN+Keyword(static+BP) can achieve the best performance. Finally, we use
   the method of meta-learning to integrate CNN+Keyword(static+BP) and
   logistic regression classifier to further strengthen the performance.}},
ISSN = {{2376-5933}},
ISBN = {{978-1-5386-6005-8}},
Unique-ID = {{ISI:000468838000153}},
}

@inproceedings{ ISI:000405903400040,
Author = {Jamro, Wazir Ali},
Book-Group-Author = {{IEEE}},
Title = {{Sindhi Language Processing: A Survey}},
Booktitle = {{2017 INTERNATIONAL CONFERENCE ON INNOVATIONS IN ELECTRICAL ENGINEERING
   AND COMPUTATIONAL TECHNOLOGIES (ICIEECT)}},
Year = {{2017}},
Note = {{International Conference on Innovations in Electrical Engineering and
   Computational Technologies (ICIEECT), Karachi, PAKISTAN, APR 05-07, 2017}},
Organization = {{Indus Univ, Fac Engn Sci \& Technol; Inst Elect \& Elect Engineers;
   Pakistan Engn Council; NTC; IEEE Karachi Sect; LEJ Natl Sci Informat
   Ctr, Salimuzzaman Siddiqui Auditorium Int Ctr Chem \& Biol Sci, Higher
   Educ Commiss; K Elect; Natl Comp Educ \& Accreditat Council, Higher Educ
   Commiss; Higher Educ Commiss Islamabad; Pakistan Sci Fdn; VMWare; Habib
   Bank Ltd; Pakistan Telecommunicat Author}},
Abstract = {{In this era of information technology, natural language processing (NLP)
   has become volatile field because of digital reliance of today's
   communities. The growth of Internet usage bringing the communities,
   cultures and languages online. In this regard much of the work has been
   done of the European and east Asian languages, in the result these
   languages have reached mature level in terms of computational
   processing. Despite the great importance of NLP science, still most of
   the South Asian languages are under developing phase. Sindhi language is
   one of them, which stands among the most ancient languages in the world.
   The Sindhi language has a great influence on the large community in
   Sindh province of Pakistan and some states of India and other countries.
   But unfortunately, it is at infant level in terms of computational
   processing, because it has not received such attention of language
   engineering community, due to its complex morphological structure and
   scarcity of language resources. Therefore, this study has been carried
   out in order to summarize the existing work on Sindhi Language
   Processing (SLP) and to explore future research opportunities, also some
   potential research problems. This paper will be helpful for the
   researchers in order to find all the information regarding SLP at one
   place in a unique way.}},
ISBN = {{978-1-5090-3310-2}},
Unique-ID = {{ISI:000405903400040}},
}

@inproceedings{ ISI:000464102900048,
Author = {Zhai, Yujia and Liu, Lizhen and Song, Wei and Du, Chao and Zhao, Xinlei},
Editor = {{Xiao, L and Wang, Y}},
Title = {{The Application of Natural Language Processing in Compiler Principle
   System}},
Booktitle = {{PROCEEDINGS OF 2017 IEEE INTERNATIONAL CONFERENCE ON PROGRESS IN
   INFORMATICS AND COMPUTING (PIC 2017)}},
Series = {{Proceedings of the IEEE International Conference on Progress in
   Informatics and Computing}},
Year = {{2017}},
Pages = {{245-248}},
Note = {{5th IEEE International Conference on Progress in Informatics and
   Computing (PIC), Nanjing, PEOPLES R CHINA, DEC 15-17, 2017}},
Organization = {{IEEE; Nanjing Univ Sci \& Technol; Shanghai Univ Finance \& Econ; IEEE
   Beijing Sect}},
Abstract = {{Compiling principle is an important course of computer science major,
   which mainly introduces general principles and basic methods of the
   construction of compiling programs mainly. Due to high demands of the
   logic analysis ability, the course bring abstract and unintelligible
   experience to many students. Thus it is quite difficult for students to
   master the main points of this course within the limited class time.
   Based on the requirement above, this paper mainly proposed a method of
   making use of natural language processing in the research and
   application of compiling process, which utilizes Maximum Probability
   Word Segmentation algorithm during the process of lexical analysis and
   syntax analysis, to offer more effective interface between human and
   computer. The proposed method can provide students with intuitive and
   profound knowledge concept in the process of learning how to compile,
   makes it easier and quicker for students to understand the principle of
   computer compiling.}},
ISSN = {{2474-0209}},
ISBN = {{978-1-5386-1978-0}},
Unique-ID = {{ISI:000464102900048}},
}

@article{ ISI:000391439200001,
Author = {Tursun, Eziz and Ganguly, Debasis and Osman, Turghun and Yang, Ya-Ting
   and Abdukerim, Ghalip and Zhou, Jun-Lin and Liu, Qun},
Title = {{A Semisupervised Tag-Transition-Based Markovian Model for Uyghur
   Morphology Analysis}},
Journal = {{ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION
   PROCESSING}},
Year = {{2016}},
Volume = {{16}},
Number = {{2}},
Month = {{DEC}},
Abstract = {{Morphological analysis, which includes analysis of part-of-speech (POS)
   tagging, stemming, and morpheme segmentation, is one of the key
   components in natural language processing (NLP), particularly for
   agglutinative languages. In this article, we investigate the
   morphological analysis of the Uyghur language, which is the native
   language of the people in the Xinjiang Uyghur autonomous region of
   western China. Morphological analysis of Uyghur is challenging primarily
   because of factors such as (1) ambiguities arising due to the likelihood
   of association of a multiple number of POS tags with a word stem or a
   multiple number of functional tags with a word suffix, (2) ambiguous
   morpheme boundaries, and (3) complex morphopholonogy of the language.
   Further, the unavailability of a manually annotated training set in the
   Uyghur language for the purpose of word segmentation makes Uyghur
   morphological analysis more difficult. In our proposed work, we address
   these challenges by undertaking a semisupervised approach of learning a
   Markov model with the help of a manually constructed dictionary of
   ``suffix to tag{''} mappings in order to predict the most likely tag
   transitions in the Uyghur morpheme sequence. Due to the linguistic
   characteristics of Uyghur, we incorporate a prior belief in our model
   for favoring word segmentations with a lower number of morpheme units.
   Empirical evaluation of our proposed model shows an accuracy of about
   82\%. We further improve the effectiveness of the tag transition model
   with an active learning paradigm. In particular, we manually
   investigated a subset of words for which the model prediction ambiguity
   was within the top 20\%. Manually incorporating rules to handle these
   erroneous cases resulted in an overall accuracy of 93.81\%.}},
DOI = {{10.1145/2968410}},
Article-Number = {{8}},
ISSN = {{2375-4699}},
EISSN = {{2375-4702}},
Unique-ID = {{ISI:000391439200001}},
}

@inproceedings{ ISI:000400282203053,
Author = {Ye Zhonglin and Jia Zhen and Huang Junfu and Yin Hongfeng},
Editor = {{Chen, J and Zhao, Q}},
Title = {{Part-of-speech Tagging Based on Dictionary and Statistical Machine
   Learning}},
Booktitle = {{PROCEEDINGS OF THE 35TH CHINESE CONTROL CONFERENCE 2016}},
Series = {{Chinese Control Conference}},
Year = {{2016}},
Pages = {{6993-6998}},
Note = {{35th Chinese Control Conference (CCC), Chengdu, PEOPLES R CHINA, JUL
   27-29, 2016}},
Organization = {{Chinese Assoc Automat, Tech Comm Control Theory; Syst Engn Soc China; SW
   Jiaotong Univ; Chinese Acad Sci, Acad Math \& Syst Sci; China Soc Indu
   \& Appl Math; Univ Elect Sci \& Technol China; Sichuan Univ; Asian
   Control Assocn; IEEE Control Syst Soc; Inst Control Robot \& Syst;
   SocInstrument \& Control Engineers; Sichuan Soc Automat \& Instrument}},
Abstract = {{Part-of-speech tagging is the basis of Natural Language Processing, and
   is widely used in information retrieval, text processing and machine
   translation fields. The traditional statistical machine learning methods
   of POS tagging rely on the high quality training data, but obtaining the
   training data is very time-consuming. The methods of POS tagging based
   on dictionaries ignore the context information, which lead to lower
   performance. This paper proposed a POS tagging approach which combines
   methods based on dictionaries and traditional statistical machine
   learning. The experimental results show that the approach not only can
   solve the problem that the training data are insufficient in statistical
   methods, but also can improve the performance of the methods based on
   dictionaries. The People's Daily corpus in January 1998 is used as
   testing data, and the accurate rate of POS tagging achieves 95.80\%. For
   the ambiguity word POS tagging, the accuracy achieves 88\%.}},
ISSN = {{2161-2927}},
ISBN = {{978-9-8815-6391-0}},
Unique-ID = {{ISI:000400282203053}},
}

@inproceedings{ ISI:000385793200026,
Author = {Khan, Irfan Ajmal and Choi, Jin-Tak},
Editor = {{Kim, JH and Kim, HS and Yoo, DG and Jung, D and Song, CG}},
Title = {{LEXICON-CORPUS BASED KOREAN UNKNOWN FOREIGN WORD EXTRACTION AND UPDATING
   USING SYLLABLE IDENTIFICATION}},
Booktitle = {{12TH INTERNATIONAL CONFERENCE ON HYDROINFORMATICS (HIC 2016) - SMART
   WATER FOR THE FUTURE}},
Series = {{Procedia Engineering}},
Year = {{2016}},
Volume = {{154}},
Pages = {{192-198}},
Note = {{12th International Conference on Hydroinformatics (HIC) - Smart Water
   for the Future, SOUTH KOREA, AUG 21-26, 2016}},
Organization = {{Incheon Metropolitan Govt; Korea Tourism Org; Smart Water Grid Res Grp}},
Abstract = {{This paper presents an efficient text mining method focusing on
   extraction and updating of unknown words (unknown foreign words) to
   improve data classification and POS tags. Proposed methods can also help
   to improve the accuracy of mining frequent pattern and association rules
   from unstructured (textual) data. Many researches have been done by
   numerous scholars on estimation and segmentation for unknown words, but,
   they are limited to grammatical and linguistic rules with limited
   vocabulary. In our project we have consider the fact, that no language
   is free from the influence of foreign languages, especially, country
   like Korea where there is a rapid improvement in the area of culture and
   media and the frequent usage of these foreign languages, resulted in
   mixing up different languages, their style along with slangs and also
   abbreviated words in daily life and conversation. The main
   characteristic of our system is to find such unknown foreign words and
   update them to appropriate words, which depends on available information
   through dictionaries. We have also explained the essential natural
   language processing (NLP) tools used for data processing. Our proposed
   method used simple but efficient techniques, first it converts the data
   into structured form, using data preprocessing techniques. In this phase
   data passes through different stages, such as, cleaning, integration and
   selection of important data, and then it gets organized into databases
   structure for further analysis and processing. This database consists of
   different kinds of dictionaries, our system heavily based on
   dictionaries. We have manually created various kinds of dictionaries for
   different kinds of unknown foreign words processing and analysis with
   the help of our team members. Our proposed methods for discovering and
   updating foreign unknown word, first discovers the foreign word using
   morphological analysis with the help of automatically and manually
   created dictionaries, then suffix trimming and word segmentation, next
   our algorithm checks for its different written pattern using
   dictionaries according to its spelling and synonym word in native
   language (Korean) and also, updates the POS tags. We have tested on
   different collection of data from economics news, beauty \& fashion and
   college student blogs, the results have shown great efficiency and
   improvement, and they were adequate enough to research further. (C) 2016
   Published by Elsevier Ltd.}},
DOI = {{10.1016/j.proeng.2016.07.445}},
ISSN = {{1877-7058}},
Unique-ID = {{ISI:000385793200026}},
}

@inproceedings{ ISI:000466782100100,
Author = {Cong, Xiaoyue and Li, Lei},
Editor = {{Guo, J and Yang, J and Wang, W and Zhang, L and Gao, S and Ma, Z and Lu, J and Liu, Z}},
Title = {{UGC QUALITY EVALUATION BASED ON META-LEARNING AND CONTENT FEATURE
   ANALYSIS}},
Booktitle = {{PROCEEDINGS OF 2016 5TH IEEE INTERNATIONAL CONFERENCE ON NETWORK
   INFRASTRUCTURE AND DIGITAL CONTENT (IEEE IC-NIDC 2016)}},
Series = {{IEEE International Conference on Network Infrastructure and Digital
   Content}},
Year = {{2016}},
Pages = {{495-499}},
Note = {{5th IEEE International Conference on Network Infrastructure and Digital
   Content (IC-NIDC), Beijing Univ Posts \& Telecommunicat, Beijing,
   PEOPLES R CHINA, SEP 23-25, 2016}},
Organization = {{IEEE; IEEE Beijing Sect; Advanced Intelligence \& Network Serv 111
   Project China; Tohoku Univ, Global Ctr Excellence; Hanyang Univ, Fus IT
   Educ Future Innovat Leaders, BK21 Plus Program; Norwegian Univ Sci \&
   Technol; Aalborg Univ, Ctr Teleinfrastruktur; Chinese Assoc Artificial
   Intelligence; Hanyang Univ, Creat Educ Program Software, BK21 Plus
   Program; Beijing Nat Sci Fdn; Inst Engn \& Technol; Hanyang Univ}},
Abstract = {{With the fast development of Social Networking Services, there has been
   increasingly vast amount of infonnation published by massive network
   users. Given this information explosion, how to analyze the quality of
   User Generated Contents (UGC) automatically becomes a challenging task
   for researchers. To solve the problem, we need to build an effective UGC
   quality evaluation system. In the light of our experience, we believe
   that the textual content of UGC is the key factor for its quality.
   Hence, we focus on textual content based quality evaluation and
   classification instead of using UGC publishing related data, such as
   times being commented and forwarded in this paper. We extract various
   features of the textual contents based on natural language processing
   technologies firstly, such as word segmentation, keywords, topic model,
   sentence parsing, distributed word representation etc. Secondly, we
   build several base-learning classifiers with different features and
   different machine learning algorithms to assign UGC contents with four
   different quality labels. Then, we create the global meta-learning model
   based on these base classifiers to generate the final quality labels for
   UGC contents. We have also implemented a series of experiments based on
   realistic data collected from Tianya Forum and use 10-fold
   cross-validation to test the model. Results have shown that our proposed
   meta-learning model performs much better.}},
ISSN = {{2374-0272}},
ISBN = {{978-1-5090-1246-6}},
Unique-ID = {{ISI:000466782100100}},
}

@article{ ISI:000368651300005,
Author = {Khan, Irfan Ajmal and Choi, Jin-Tak},
Title = {{Efficient text mining method and simple tweaks for discovering and
   updating unknown foreign words and improving association rules
   extraction from textual data}},
Journal = {{ASIA LIFE SCIENCES}},
Year = {{2015}},
Number = {{12}},
Pages = {{71-88}},
Month = {{DEC}},
Abstract = {{A text mining method for discovering hidden knowledge from unstructured
   (textual) data and also extraction of frequent patterns, association
   rules and unknown foreign words are presented in this study. Association
   rule mining is an important data mining model studied widely by the
   database and data mining community. Although various association rules
   mining techniques have been successfully used for market basket analysis
   but very few has applied on textual data. In this paper we have explains
   the method of mining association rules on Korean textual data. We have
   also explained the essential natural language processing (NLP) tools
   used. First we have converted unstructured data into something
   structured by passing textual data through Data Preprocessing stage.
   This process cleans and integrates data, select relevant data then
   transforms into database along with useful information which can help
   algorithm improve the mining process. These database(s) consists of
   different kinds of dictionaries. Second we have created different types
   of dictionaries for different processing stages. Our foreign unknown
   word discovering and updating method first discover the foreign word
   using morphological analysis and unknown words and foreign unknown word
   dictionaries. Next is suffix trimming and word segmentation, then these
   foreign words were fed to a process, where it looks for its different
   written pattern using dictionaries according to its spelling and synonym
   word in native language (Korean). Next step is to update the POS tags
   using rule base POS tagging. Then data mining techniques are used to
   extract hidden patterns. These patterns are evaluated by specific rules
   until we get the valid and satisfactory result. We have tested on Korean
   news corpus and results have shown that it has worked well, and the
   results were adequate enough to further research.}},
ISSN = {{0117-3375}},
Unique-ID = {{ISI:000368651300005}},
}

@article{ ISI:000368651300052,
Author = {Khan, Irfan Ajmal and Seo, Ji-Hoon and Choi, Jin-Tak},
Title = {{Efficient text mining method and simple tweaks for discovering and
   updating unknown foreign words and improving association rules
   extraction from textual data}},
Journal = {{ASIA LIFE SCIENCES}},
Year = {{2015}},
Number = {{12}},
Pages = {{663-680}},
Month = {{DEC}},
Abstract = {{We present a text mining method for discovering hidden knowledge from
   unstructured (textual) data and also extraction of frequent patterns,
   association rules and unknown foreign words. Association rule mining is
   an important data mining model studied widely by the database and data
   mining community. Although various association rules mining techniques
   have been successfully used for market basket analysis but very few has
   applied on textual data. In this paper we have explains the method of
   mining association rules on Korean textual data. We have also explained
   the essential natural language processing (NLP) tools used. First we
   have converted unstructured data into something structured by passing
   textual data through Data Preprocessing stage. This process cleans and
   integrates data, select relevant data then transforms into database
   along with useful information which can help algorithm improve the
   mining process. These database(s) consists of different kinds of
   dictionaries. Second we have created different types of dictionaries for
   different processing stages. Our foreign unknown word discovering and
   updating method first discover the foreign word using morphological
   analysis and unknown words and foreign unknown word dictionaries. Next
   is suffix trimming and word segmentation, then these foreign words were
   fed to a process, where it looks for its different written pattern using
   dictionaries according to its spelling and synonym word in native
   language (Korean). Next step is to update the POS tags using rule base
   POS tagging. Then data mining techniques are used to extract hidden
   patterns. These patterns are evaluated by specific rules until we get
   the valid and satisfactory result. We have tested on Korean news corpus
   and results have shown that it has worked well, and the results were
   adequate enough to research further.}},
ISSN = {{0117-3375}},
Unique-ID = {{ISI:000368651300052}},
}

@inproceedings{ ISI:000380557800109,
Author = {Puri, Shalini and Singh, Satya Prakash},
Book-Group-Author = {{IEEE}},
Title = {{Sentence Detection and Extraction in Machine Printed Imaged Document
   using Matching Technique}},
Booktitle = {{2015 2nd International Conference on Recent Advances in Engineering \&
   Computational Sciences (RAECS)}},
Year = {{2015}},
Note = {{2nd International Conference on Recent Advances in Engineering,
   Chandigarh, INDIA, DEC 21-22, 2015}},
Abstract = {{Sentence extraction is a new, challenging and critical step in the
   printed scanned imaged documents. In this paper, an efficient 4-layered
   Sentence Detection and Extraction System (SDES) model is proposed which
   is designed to detect and extract sentences from machine printed imaged
   document. Its internal details and architecture clearly show that how it
   processes an image to find out the underlying sentences. The basic idea
   is to first preprocess the imaged document for noise removal and skew
   correction, and then textual entities are detected and segmented at
   page, line and word levels. Firstly, the horizontal and vertical
   projection profiles are taken to segment and separate the lines and
   words. After skew correction, two stage Character Based and Word Based
   Leveled matching and testing are performed, which verify and identify
   the correct character and word by searching for similar textual
   characters and words in Character Set Storage (CSS) and Word Pseudo
   Thesaurus (WPT). If any word pattern is not matched and identified by
   WPT, then it is stored in the Unmatched Word Storage (UWS) for the
   future reference. Such testing and verification are used at two levels
   to increase the accuracy\% of SDES, and thereby, reducing the errors. It
   increases the system performance greatly. Finally, all the sentences of
   imaged document are extracted. Experimental results are found at the
   word, character and sentence levels. Their accuracy\% results are good
   which show the high system performance and efficiency.}},
ISBN = {{978-1-4673-8253-3}},
Unique-ID = {{ISI:000380557800109}},
}

@inproceedings{ ISI:000362441400001,
Author = {Nivre, Joakim},
Editor = {{Gelbukh, A}},
Title = {{Towards a Universal Grammar for Natural Language Processing}},
Booktitle = {{COMPUTATIONAL LINGUISTICS AND INTELLIGENT TEXT PROCESSING (CICLING
   2015), PT I}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9041}},
Pages = {{3-16}},
Note = {{16th Annual Conference on Intelligent Text Processing and Computational
   Linguistics (CICLing), Nile Univ, Cairo, EGYPT, APR 14-20, 2015}},
Abstract = {{Universal Dependencies is a recent initiative to develop
   cross-linguistically consistent treebank annotation for many languages,
   with the goal of facilitating multilingual parser development,
   cross-lingual learning, and parsing research from a language typology
   perspective. In this paper, I outline the motivation behind the
   initiative and explain how the basic design principles follow from these
   requirements. I then discuss the different components of the annotation
   standard, including principles for word segmentation, morphological
   annotation, and syntactic annotation. I conclude with some thoughts on
   the challenges that lie ahead.}},
DOI = {{10.1007/978-3-319-18111-0\_1}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-18111-0; 978-3-319-18110-3}},
Unique-ID = {{ISI:000362441400001}},
}

@inproceedings{ ISI:000393314500026,
Author = {Zhang, Chunxiang and He, Shan and Gao, Xueyao},
Book-Group-Author = {{IEEE}},
Title = {{A Word Sense Disambiguation System Based on Bayesian Model}},
Booktitle = {{PROCEEDINGS OF 2015 4TH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND
   NETWORK TECHNOLOGY (ICCSNT 2015)}},
Year = {{2015}},
Pages = {{124-127}},
Note = {{4th International Conference on Computer Science and Network Technology
   (ICCSNT), Harbin, PEOPLES R CHINA, DEC 19-20, 2015}},
Organization = {{Heilongjiang Univ; Dalian Jiaotong Univ; NE Normal Univ; Shaanxi Normal
   Univ; Harbin Inst Technol; IEEE; IEEE Harbin Sect}},
Abstract = {{Research on word sense disambiguation (WSD) is of great importance in
   natural language processing fields. In this paper, a novel word sense
   disambiguation system is designed in which bayesian theory is applied to
   determine correct sense of an ambiguous word. Morphology knowledge in
   word unit is mined to guide WSD process. Neighboring morphology
   knowledge of an ambiguous word is used as feature for constructing WSD
   classifier. Word segmentation tool is integrated into this system and
   browser/server (B/S) framework is adopted. Experimental results show
   that the performance of WSD system is good.}},
ISBN = {{978-1-4673-8173-4}},
Unique-ID = {{ISI:000393314500026}},
}

@article{ ISI:000341843500004,
Author = {Sun, Xu and Li, Wenjie and Wang, Houfeng and Lu, Qin},
Title = {{Feature-Frequency-Adaptive On-line Training for Fast and Accurate
   Natural Language Processing}},
Journal = {{COMPUTATIONAL LINGUISTICS}},
Year = {{2014}},
Volume = {{40}},
Number = {{3}},
Pages = {{563-586}},
Month = {{SEP}},
Abstract = {{Training speed and accuracy are two major concerns of large-scale
   natural language processing systems. Typically, we need to make a
   tradeoff between speed and accuracy. It is trivial to improve the
   training speed via sacrificing accuracy or to improve the accuracy via
   sacrificing speed. Nevertheless, it is nontrivial to improve the
   training speed and the accuracy at the same time, which is the target of
   this work. To reach this target, we present a new training method,
   feature-frequency-adaptive on-line training, for fast and accurate
   training of natural language processing systems. It is based on the core
   idea that higher frequency features should have a learning rate that
   decays faster. Theoretical analysis shows that the proposed method is
   convergent with a fast convergence rate. Experiments are conducted based
   on well-known benchmark tasks, including named entity recognition, word
   segmentation, phrase chunking, and sentiment analysis. These tasks
   consist of three structured classification tasks and one non-structured
   classification task, with binary features and real-valued features,
   respectively. Experimental results demonstrate that the proposed method
   is faster and at the same time more accurate than existing methods,
   achieving state-of-the-art scores on the tasks with different
   characteristics.}},
DOI = {{10.1162/COLI\_a\_00193}},
ISSN = {{0891-2017}},
EISSN = {{1530-9312}},
ORCID-Numbers = {{Li, Wenjie/0000-0002-7360-8864
   Lu, Qin/0000-0002-9092-2476}},
Unique-ID = {{ISI:000341843500004}},
}

@inproceedings{ ISI:000355611002026,
Author = {Ludusan, Bogdan and Versteegh, Maarten and Jansen, Aren and Gravier,
   Guillaume and Cao, Xuan-Nga and Johnson, Mark and Dupoux, Emmanuel},
Editor = {{Calzolari, N and Choukri, K and Declerck, T and Loftsson, H and Maegaard, B and Mariani, J and Moreno, A and Odijk, J and Piperidis, S}},
Title = {{Bridging the gap between speech technology and natural language
   processing: an evaluation toolbox for term discovery systems}},
Booktitle = {{LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND
   EVALUATION}},
Year = {{2014}},
Pages = {{560-567}},
Note = {{9th International Conference on Language Resources and Evaluation
   (LREC), Reykjavik, ICELAND, MAY 26-31, 2014}},
Organization = {{Holmes Semant Solut; European Media Lab GmBH; EML; VoiceBox
   Technologies; KDICTIONARIES}},
Abstract = {{The unsupervised discovery of linguistic terms from either continuous
   phoneme transcriptions or from raw speech has seen an increasing
   interest in the past years both from a theoretical and a practical
   standpoint. Yet, there exists no common accepted evaluation method for
   the systems performing term discovery. Here, we propose such an
   evaluation toolbox, drawing ideas from both speech technology and
   natural language processing. We first transform the speech-based output
   into a symbolic representation and compute five types of evaluation
   metrics on this representation: the quality of acoustic matching, the
   quality of the clusters found, and the quality of the alignment with
   real words (type, token, and boundary scores). We tested our approach on
   two term discovery systems taking speech as input, and one using
   symbolic input. The latter was run using both the gold transcription and
   a transcription obtained from an automatic speech recognizer, in order
   to simulate the case when only imperfect symbolic information is
   available. The results obtained are analysed through the use of the
   proposed evaluation metrics and the implications of these metrics are
   discussed.}},
ISBN = {{978-2-9517408-8-4}},
ORCID-Numbers = {{Dupoux, Emmanuel/0000-0002-7814-2952}},
Unique-ID = {{ISI:000355611002026}},
}

@article{ ISI:000315648000007,
Author = {Goldberg, Yoav and Elhadad, Michael},
Title = {{Word Segmentation, Unknown-word Resolution, and Morphological Agreement
   in a Hebrew Parsing System}},
Journal = {{COMPUTATIONAL LINGUISTICS}},
Year = {{2013}},
Volume = {{39}},
Number = {{1}},
Pages = {{121-160}},
Month = {{MAR}},
Abstract = {{We present a constituency parsing system for Modern Hebrew. The system
   is based on the PCFG-LA parsing method of Petrov et al. (2006), which is
   extended in various ways in order to accommodate the specificities of
   Hebrew as a morphologically rich language with a small treebank. We show
   that parsing performance can be enhanced by utilizing a language
   resource external to the treebank, specifically, a lexicon-based
   morphological analyzer. We present a computational model of interfacing
   the external lexicon and a treebank-based parser, also in the common
   case where the lexicon and the treebank follow different annotation
   schemes. We show that Hebrew word-segmentation and constituency-parsing
   can be performed jointly using CKY lattice parsing. Performing the tasks
   jointly is effective, and substantially outperforms a pipeline-based
   model. We suggest modeling grammatical agreement in a constituency-based
   parser as a filter mechanism that is orthogonal to the grammar, and
   present a concrete implementation of the method. Although the
   constituency parser does not make many agreement mistakes to begin with,
   the filter mechanism is effective in fixing the agreement mistakes that
   the parser does make.
   These contributions extend outside of the scope of Hebrew processing,
   and are of general applicability to the NLP community. Hebrew is a
   specific case of a morphologically rich language, and ideas presented in
   this work are useful also for processing other languages, including
   English. The lattice-based parsing methodology is useful in any case
   where the input is uncertain. Extending the lexical coverage of a
   treebank-derived parser using an external lexicon is relevant for any
   language with a small treebank.}},
DOI = {{10.1162/COLI\_a\_00137}},
ISSN = {{0891-2017}},
EISSN = {{1530-9312}},
ORCID-Numbers = {{Elhadad, Michael/0000-0002-5629-2351}},
Unique-ID = {{ISI:000315648000007}},
}

@inproceedings{ ISI:000336944701290,
Author = {Wang, Na and Xu, Lin and Li, Liyao and Xu, Luxiong},
Editor = {{Sang, X and Kim, YH}},
Title = {{Design And Implementation of an Automatic Scoring Subjective Question
   System Based on Domain Ontology}},
Booktitle = {{MATERIALS PROCESSING AND MANUFACTURING III, PTS 1-4}},
Series = {{Advanced Materials Research}},
Year = {{2013}},
Volume = {{753-755}},
Pages = {{3039-3042}},
Note = {{3rd International Conference on Advanced Engineering Materials and
   Technology (AEMT 2013), Zhangjiajie, PEOPLES R CHINA, MAY 11-12, 2013}},
Organization = {{Hebei Prov Key Lab Inorgan Nonmetall Mat; Korea Maritime Univ}},
Abstract = {{Automated assessment technology for subjective tests is one of the key
   techniques of exam systems. A model based on domain ontology is proposed
   in this paper, which can be used in exam systems to estimate subjective
   tests. After analysing the present research status of subjective
   automated assessment technology, the paper makes a study on the
   construction method of domain ontology by taking software engineering
   domain as an example. Semantic similarity calculation based on domain
   ontology is used for automatic assessment in this paper. The automatic
   assessment system can divide a sentence into a series of phrases by
   using the natural language processing technology and get the score by
   evaluating the semantic similarity of the student's answer. The
   experiments show that the results of the system which has certain
   valuable feasibility and applicability are credible and the scoring
   errors are acceptable.}},
DOI = {{10.4028/www.scientific.net/AMR.753-755.3039}},
ISSN = {{1022-6680}},
ISBN = {{978-3-03785-764-9}},
Unique-ID = {{ISI:000336944701290}},
}

@inproceedings{ ISI:000329078800006,
Author = {Namer, Fiammetta},
Editor = {{Mahlow, C and Piotrowski, M}},
Title = {{A Rule-Based Morphosemantic Analyzer for French for a Fine-Grained
   Semantic Annotation of Texts}},
Booktitle = {{SYSTEMS AND FRAMEWORKS FOR COMPUTATIONAL MORPHOLOGY}},
Series = {{Communications in Computer and Information Science}},
Year = {{2013}},
Volume = {{380}},
Pages = {{92-114}},
Note = {{3rd International Workshop on Systems and Frameworks for Computational
   Morphology, Humboldt Univ, Berlin, GERMANY, SEP 06, 2013}},
Organization = {{German Soc Computat Linguist \& Language Technol; Humboldt Univ}},
Abstract = {{We describe DeriF, a rule-based morphosemantic analyzer developed for
   French. Unlike existing word segmentation tools, DeriF provides derived
   and compound words with various sorts of semantic information: (1) a
   definition, computed from both the base meaning and the specificities of
   the morphological rule; (2) lexical-semantic features, inferred from
   general linguistic properties of derivation rules; (3) lexical relations
   (synonymy, (co-)hyponymy) with other, morphologically unrelated, words
   belonging to the same analyzed corpus.}},
ISSN = {{1865-0929}},
EISSN = {{1865-0937}},
ISBN = {{978-3-642-40485-6}},
Unique-ID = {{ISI:000329078800006}},
}

@inproceedings{ ISI:000326049800121,
Author = {Srithirath, Arounyadeth and Seresangtakul, Pusadee},
Book-Group-Author = {{IEEE}},
Title = {{A Hybrid Approach to Lao Word Segmentation using Longest Syllable Level
   Matching with Named Entities Recognition}},
Booktitle = {{2013 10TH INTERNATIONAL CONFERENCE ON ELECTRICAL
   ENGINEERING/ELECTRONICS, COMPUTER, TELECOMMUNICATIONS AND INFORMATION
   TECHNOLOGY (ECTI-CON)}},
Year = {{2013}},
Note = {{10th International Conference on Electrical Engineering/Electronics,
   Computer, Telecommunications and Information Technology (ECTI-CON),
   Krabi, THAILAND, MAY 15-17, 2013}},
Organization = {{IEEE; IEEE Thailand Sect}},
Abstract = {{The Lao language is written without words delimiter which makes it
   extremely difficult to process. The development of automatic word
   segmentation for natural language processing for the Lao language is an
   essential but challenging task. This paper proposes a longest syllable
   level match with named entities recognition approach for Lao word
   segmentation. Syllables were first extracted from the input text and
   then longest matching was applied. This is one of the techniques in the
   Dictionary Based approach with named entities recognition being used to
   combine them to form the words. The performance result obtained from
   this approach, in precision and recall, was 85.21\% and 92.36\%,
   respectively.}},
ISBN = {{978-1-4799-0546-1; 978-1-4799-0545-4}},
Unique-ID = {{ISI:000326049800121}},
}

@article{ ISI:000290801100004,
Author = {Kesidis, A. L. and Galiotou, E. and Gatos, B. and Pratikakis, I.},
Title = {{A word spotting framework for historical machine-printed documents}},
Journal = {{INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION}},
Year = {{2011}},
Volume = {{14}},
Number = {{2, SI}},
Pages = {{131-144}},
Month = {{JUN}},
Abstract = {{In this paper, we propose a word spotting framework for accessing the
   content of historical machine-printed documents without the use of an
   optical character recognition engine. A preprocessing step is performed
   in order to improve the quality of the document images, while word
   segmentation is accomplished with the use of two complementary
   segmentation methodologies. In the proposed methodology, synthetic word
   images are created from keywords, and these images are compared to all
   the words in the digitized documents. A user feedback process is used in
   order to refine the search procedure. The methodology has been evaluated
   in early Modern Greek documents printed during the seventeenth and
   eighteenth century. In order to improve the efficiency of accessing and
   search, natural language processing techniques have been addressed that
   comprise a morphological generator that enables searching in documents
   using only a base word-form for locating all the corresponding inflected
   word-forms and a synonym dictionary that further facilitates access to
   the semantic context of documents.}},
DOI = {{10.1007/s10032-010-0134-4}},
ISSN = {{1433-2833}},
EISSN = {{1433-2825}},
Unique-ID = {{ISI:000290801100004}},
}

@inproceedings{ ISI:000306397600022,
Author = {Lehal, Gurpreet Singh and Saini, Tejinder Singh},
Editor = {{Singh, C and Lehal, GS and Sengupta, J and Sharma, DV and Goyal, V}},
Title = {{A Transliteration Based Word Segmentation System for Shahmukhi Script}},
Booktitle = {{INFORMATION SYSTEMS FOR INDIAN LANGUAGES}},
Series = {{Communications in Computer and Information Science}},
Year = {{2011}},
Volume = {{139}},
Pages = {{136-143}},
Note = {{International Conference on Infirmation Systems for Indian Languages
   (ICISIL 2011), Patiala, INDIA, MAR 09-11, 2011}},
Abstract = {{Word Segmentation is an important prerequisite for almost all Natural
   Language Processing (NLP) applications. Since word is a fundamental unit
   of any language, almost every NLP system first needs to segment input
   text into a sequence of words before further processing. In this paper,
   Shahmukhi word segmentation has been discussed in detail. The presented
   word segmentation module is part of Shahmukhi-Gurmukhi transliteration
   system. Shahmukhi script is usually written without short vowels leading
   to ambiguity. Therefore, we have designed a novel approach for Shahmukhi
   word segmentation in which we used target Gurmukhi script lexical
   resources instead of Shahmukhi resources. We employ a combination of
   techniques to investigate an effective algorithm by applying syntactical
   analysis process using Shahmukhi Gurmukhi dictionary, writing system
   rules and statistical methods based on n-grams models.}},
ISSN = {{1865-0929}},
ISBN = {{978-3-642-19402-3}},
Unique-ID = {{ISI:000306397600022}},
}

@inproceedings{ ISI:000257568400001,
Author = {Xiao, Guozheng},
Editor = {{He, YX and Xiao, GZ and Sun, MS}},
Title = {{Constructing verb synsets for language reasoning based on
   synset-allolexeme theory}},
Booktitle = {{RECENT ADVANCE OF CHINESE COMPUTING TECHNOLOGIES}},
Year = {{2007}},
Pages = {{3-10}},
Note = {{7th International Conference of Chinese Computing (ICCC 2007), Wuhan,
   PEOPLES R CHINA, OCT 13-15, 2007}},
Organization = {{Wuhan Univ, Ctr Study Language \& Informat; Chinese \& Oriental Language
   Informat Proc Soc Singapore; Soc Chinese Informat Proc China}},
Abstract = {{Natural language processing is marching toward a new goal: automatic
   semantic analysis, after word segmentation, part-of-speech tagging and
   syntactic analysis have been almost achieved. Separating and describing
   verb senses and constructing synsets are important tasks in reaching the
   goal. This paper introduces a new theory in constructing synsets:
   synset-allolexeme theory as well as corresponding syntactic-semantic
   component analysis by depicting semantic lexemes and semantic varieties
   of the original meaning of verbs.}},
ISBN = {{978-981-08-0099-4}},
Unique-ID = {{ISI:000257568400001}},
}

@Comment{jabref-meta: databaseType:bibtex;}
