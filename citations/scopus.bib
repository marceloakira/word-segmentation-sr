Scopus
EXPORT DATE: 10 September 2019

@ARTICLE{Patel2019437,
author={Patel, R.N. and Pimpale, P.B. and Sasikumar, M.},
title={Machine translation in Indian languages: Challenges and resolution},
journal={Journal of Intelligent Systems},
year={2019},
volume={28},
number={3},
pages={437-445},
doi={10.1515/jisys-2018-0014},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053124049&doi=10.1515%2fjisys-2018-0014&partnerID=40&md5=5b1e11decdb5614448ceda0e259a43a4},
affiliation={KBCS Division, Centre for Development of Advanced Computing, Gulmohar Cross Road No. 9, Opp Juhu Shopping Center, Juhu, Mumbai, 400049, India},
abstract={English to Indian language machine translation poses the challenge of structural and morphological divergence. This paper describes English to Indian language statistical machine translation using preordering and suffix separation. The preordering uses rules to transfer the structure of the source sentences prior to training and translation. This syntactic restructuring helps statistical machine translation to tackle the structural divergence and hence provides better translation quality. The suffix separation is used to tackle the morphological divergence between English and highly agglutinative Indian languages. We demonstrate that the use of preordering and suffix separation helps in improving the quality of English to Indian language machine translation. © 2019 Walter de Gruyter GmbH, Berlin/Boston.},
author_keywords={reordering;  Statistical machine translation;  suffix and compound splitting;  transliteration},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li2018175,
author={Li, J. and Du, Q. and Shi, K. and He, Y. and Wang, X. and Xu, J.},
title={Helpful or not? an investigation on the feasibility of identifier splitting via CNN-BiLSTM-CRF},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
year={2018},
volume={2018-July},
pages={175-181},
doi={10.18293/SEKE2018-167},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056829823&doi=10.18293%2fSEKE2018-167&partnerID=40&md5=da7e9e1bd5ebdb234108fb0e7e22f2f9},
affiliation={School of Software Engineering, Tongji University, China; Software Engineering RandD Centre, Jishi Building, Tongji University, China; Smart City Labotary, Jishi Building, Tongji University, China; Shanghai Research and Development Center, Baidu Inc, China},
abstract={We recently introduced a new technique to handle source code identifier splitting. The proposed technique, denoted as CNN-BiLSTM-CRF[a neural network composed of a convolutional neural network(CNN), bidirectional long short-Term memory networks(BiLSTM) and conditional random fields(CRFs)] enables us to obtain a model that splits identifiers correctly and effectively. This technique combines the use of a CNN layer with the mature BiLSTM-CRF model. The experimental results indicate that CNN-BiLSTM-CRF delivers outstanding performance on all four of the evaluation oracles. More importantly, we endeavored to provide insight into the practical feasibility of this technique by considering the aspects of generality, data size in demand and construction cost, etc. Finally, we reasoned out that CNN-BiLSTM-CRF should be helpful and improvable for identifier splitting in practical works in terms of the accuracy and feasibility. This was validated by multifaceted experiments. Index Terms-identifier splitting, source code mining, program comprehension, CNN, BiLSTM-CRF, feasibility investigation. © 2018 Universitat zu Koln. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sugisaki2018141,
author={Sugisaki, K. and Tuggener, D.},
title={German compound splitting using the compound productivity of morphemes},
journal={KONVENS 2018 - Conference on Natural Language Processing / Die Konferenz zur Verarbeitung Naturlicher Sprache},
year={2018},
pages={141-147},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064207728&partnerID=40&md5=9ad88e6eff661a660d155b41ada1fbb9},
affiliation={German Department, University of Zurich, Schönberggasse 9, Zurich, 8001, Switzerland; School of Engineering, Zurich University of Applied Sciences, Steinberggasse 13, Winterthur, 8400, Switzerland},
abstract={In this work, we present a novel compound splitting method for German by capturing the compound productivity of morphemes. We use a giga web corpus to create a lexicon and decompose noun compounds by computing the probabilities of compound elements as bound and free morphemes. Furthermore, we provide a uniformed evaluation of several unsupervised approaches and morphological analysers for the task. Our method achieved a high F1 score of 0.92, which was a comparable result to state-of-the-art methods. © KONVENS 2018 - Conference on Natural Language Processing / Die Konferenz zur Verarbeitung Naturlicher Sprache.All right reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jagfeld201758,
author={Jagfeld, G. and Ziering, P. and Van Der Plas, L.},
title={Evaluating compound splitters extrinsically with textual entailment},
journal={ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
year={2017},
volume={2},
pages={58-63},
doi={10.18653/v1/P17-2010},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040556164&doi=10.18653%2fv1%2fP17-2010&partnerID=40&md5=abad0047ea69c94b560a54599233b45f},
affiliation={Institute for Natural Language Processing, University of Stuttgart, Germany; Institute of Linguistics, University of Malta, Malta},
abstract={Traditionally, compound splitters are evaluated intrinsically on gold-standard data or extrinsically on the task of statistical machine translation. We explore a novel way for the extrinsic evaluation of compound splitters, namely recognizing textual entailment. Compound splitting has great potential for this novel task that is both transparent and well-defined. Moreover, we show that it addresses certain aspects that are either ignored in intrinsic evaluations or compensated for by task-internal mechanisms in statistical machine translation. We show significant improvements using different compound splitting methods on a German textual entailment dataset. © 2017 Association for Computational Linguistics.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2017,
title={ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
journal={ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
year={2017},
volume={1},
page_count={2189},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040944171&partnerID=40&md5=32afcce8baaf4dd269f848c5e1f2df4d},
abstract={The proceedings contain 297 papers. The topics discussed include: adversarial multi-task learning for text classification; neural end-to-end learning for computational argumentation mining; neural symbolic machines: learning semantic parsers on freebase with weak supervision; morph-fitting: fine-tuning word vector spaces with simple language-specific rules; lexical features in coreference resolution: to be used with caution; alternative objective functions for training MT evaluation metrics; a principled framework for evaluating summarizers: comparing models of summary quality against human judgments; vector space models for evaluating semantic fluency in autism; evaluating compound splitters extrinsically with textual entailment; and an analysis of action recognition datasets for language and vision tasks.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{NoAuthor2017,
title={ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
journal={ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
year={2017},
volume={2},
page_count={2873},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040563381&partnerID=40&md5=f45eb642350433696b687c9df0520cb6},
abstract={The proceedings contain 297 papers. The topics discussed include: adversarial multi-task learning for text classification; neural end-to-end learning for computational argumentation mining; neural symbolic machines: learning semantic parsers on freebase with weak supervision; morph-fitting: fine-tuning word vector spaces with simple language-specific rules; lexical features in coreference resolution: to be used with caution; alternative objective functions for training MT evaluation metrics; a principled framework for evaluating summarizers: comparing models of summary quality against human judgments; vector space models for evaluating semantic fluency in autism; evaluating compound splitters extrinsically with textual entailment; and an analysis of action recognition datasets for language and vision tasks.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Khatiwada2016,
author={Khatiwada, S. and Kelly, M. and Mahmoud, A.},
title={STAC: A tool for Static Textual Analysis of Code},
journal={IEEE International Conference on Program Comprehension},
year={2016},
volume={2016-July},
doi={10.1109/ICPC.2016.7503746},
art_number={7503746},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979735899&doi=10.1109%2fICPC.2016.7503746&partnerID=40&md5=e2527d1a6b7dde72270740ca4502929d},
affiliation={Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, LA  70808, United States},
abstract={Static textual analysis techniques have been recently applied to process and synthesize source code. The underlying tenet is that important information is embedded in code identifiers and internal code comments. Such information can be analyzed to provide automatic aid for several software engineering activities. To facilitate this line of work, we present STAC, a tool for supporting Static Textual Analysis of Code. STAC is designed as a light-weight stand-alone tool that provides a practical one-stop solution for code indexing. Code indexing is the process of extracting important textual information from source code. Accurate indexing has been found to significantly influence the performance of code retrieval and analysis methods. STAC provides features for extracting and processing textual patterns found in Java, C++, and C# code artifacts. These features include identifier splitting, stemming, lemmatization, and spell-checking. STAC is also provided as an API to help researchers to integrate basic code indexing features into their code. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Riedl2016617,
author={Riedl, M. and Biemann, C.},
title={Unsupervised compound splitting with distributional semantics rivals supervised methods},
journal={2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
year={2016},
pages={617-622},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994120942&partnerID=40&md5=907e2881fa0d72e3de877cd94f519d2d},
affiliation={Language Technology, Computer Science Department, Technische Universität Darmstadt, Hochschulstrasse 10, Darmstadt, D-64289, Germany},
abstract={In this paper we present a word decompounding method that is based on distributional semantics. Our method does not require any linguistic knowledge and is initialized using a large monolingual corpus. The core idea of our approach is that parts of compounds (like "candle" and "stick") are semantically similar to the entire compound, which helps to exclude spurious splits (like "candles" and "tick"). We report results for German and Dutch: For German, our unsupervised method comes on par with the performance of a rule-based and a supervised method and significantly outperforms two unsupervised baselines. For Dutch, our method performs only slightly below a rule-based optimized compound splitter. ©2016 Association for Computational Linguistics.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shishkova201642,
author={Shishkova, A. and Chernyak, E.},
title={Annotated suffix tree method for German compound splitting},
journal={CEUR Workshop Proceedings},
year={2016},
volume={1886},
pages={42-47},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029175537&partnerID=40&md5=c9ad928c2c810005449090bf909e5a0a},
affiliation={National Research University, Higher School of Economics, Moscow, Russian Federation},
abstract={The paper presents an unsupervised and knowledge-free approach to compound splitting. Although the research is focused on German compounds, the method is expected to be extensible to other compounding languages. The approach is based on the annotated suffix tree (AST) method proposed and modified by Mirkin et al. To the best of our knowledge, annotated suffix trees have not yet been used for compound splitting. The main idea of the approach is to match all the substrings of a word (suffixes and prefixes separately) against an AST, determining the longest and sufficiently frequent substring to perform a candidate split. A simplification considers only the suffixes (or prefixes) and splits a word at the beginning of the selected suffix (the longest and sufficiently frequent one). The results are evaluated by precision and recall.},
author_keywords={Annotated suffix tree;  Compound splitting;  German language},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ziering2016644,
author={Ziering, P. and Van Der Plas, L.},
title={Towards unsupervised and language-independent compound splitting using inflectional morphological transformations},
journal={2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
year={2016},
pages={644-653},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994128765&partnerID=40&md5=5b0b9b0941575eb4aac9f0edfea61654},
affiliation={Institute for Natural Language Processing, University of Stuttgart, Germany; Institute of Linguistics, University of Malta, Malta},
abstract={In this paper, we address the task of language-independent, knowledge-lean and unsupervised compound splitting, which is an essential component for many natural language processing tasks such as machine translation. Previous methods on statistical compound splitting either include language-specific knowledge (e.g., linking elements) or rely on parallel data, which results in limited applicability. We aim to overcome these limitations by learning compounding morphology from inflectional information derived from lemmatized monolingual corpora. In experiments for Germanic languages, we show that our approach significantly outperforms language-dependent stateof-the-art methods in finding the correct split point and that word inflection is a good approximation for compounding morphology. ©2016 Association for Computational Linguistics.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2016,
title={CEUR Workshop Proceedings},
journal={CEUR Workshop Proceedings},
year={2016},
volume={1886},
page_count={141},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029174464&partnerID=40&md5=a158b39ec293bf3538d7d84aff05054b},
abstract={The proceedings contain 16 papers. The topics discussed include: using a hybrid algorithm for lemmatization of a diachronic corpus; classification of e-commerce websites by product categories; automatic generation of lexical exercises; evaluation of distributional compositional operations on collocations through semantic similarity; identification of singleton mentions in Russian; annotated suffix tree method for German compound splitting; building NLP pipeline for Russian with a handful of linguistic knowledge; Morphchecker for nonstandard data: a tool for morphological error correction in learner corpora; formal concept lattices as semantic maps; syntactic annotation for a Hittite corpus: problems and principles; lexis meets meter: attraction of lexical units in Russian verse; dynamics of core of language vocabulary; big-data-augmented approach to emerging technologies identification: case of agriculture and food sector; quantum logic and natural language processing; and transforming RuThes thesaurus to generate Russian WordNet.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Hirschmann20163199,
author={Hirschmann, F. and Nam, J. and Fürnkranz, J.},
title={What makes word-level neural machine translation hard: A case study on English-German translation},
journal={COLING 2016 - 26th International Conference on Computational Linguistics, Proceedings of COLING 2016: Technical Papers},
year={2016},
pages={3199-3208},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039147994&partnerID=40&md5=92b1d72c2067f0a3005d477c56d828af},
affiliation={Knowledge Engineering Group, Technische Universität Darmstadt, Darmstadt, Germany},
abstract={Traditional machine translation systems often require heavy feature engineering and the combination of multiple techniques for solving different subproblems. In recent years, several end-to-end learning architectures based on recurrent neural networks have been proposed. Unlike traditional systems, Neural Machine Translation (NMT) systems learn the parameters of the model and require only minimal preprocessing. Memory and time constraints allow to take only a fixed number of words into account, which leads to the out-of-vocabulary (OOV) problem. In this work, we analyze why the OOV problem arises and why it is considered a serious problem in German. We study the effectiveness of compound word splitters for alleviating the OOV problem, resulting in a 2.5+ BLEU points improvement over a baseline on the WMT' 14 German-to-English translation task. For English-to-German translation, we use target-side compound splitting through a special syntax during training that allows the model to merge compound words and gain 0.2 BLEU points. © 1963-2018 ACL.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Carvalho2015117,
author={Carvalho, N.R. and Almeida, J.J. and Henriques, P.R. and Varanda, M.J.},
title={From source code identifiers to natural language terms},
journal={Journal of Systems and Software},
year={2015},
volume={100},
pages={117-128},
doi={10.1016/j.jss.2014.10.013},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919460270&doi=10.1016%2fj.jss.2014.10.013&partnerID=40&md5=b2f690f4be9ae86e85985c8c428a0953},
affiliation={Department of Informatics, University of Minho, Campus de Gualtar, Braga, 4710-057, Portugal; Polytechnic Institute of Bragança, Campus de Santa Apolónia, Bragança, 5300-253, Portugal},
abstract={Program comprehension techniques often explore program identifiers, to infer knowledge about programs. The relevance of source code identifiers as one relevant source of information about programs is already established in the literature, as well as their direct impact on future comprehension tasks. Most programming languages enforce some constrains on identifiers strings (e.g., white spaces or commas are not allowed). Also, programmers often use word combinations and abbreviations, to devise strings that represent single, or multiple, domain concepts in order to increase programming linguistic efficiency (convey more semantics writing less). These strings do not always use explicit marks to distinguish the terms used (e.g., CamelCase or underscores), so techniques often referred as hard splitting are not enough. This paper introduces Lingua::IdSplitter a dictionary based algorithm for splitting and expanding strings that compose multi-term identifiers. It explores the use of general programming and abbreviations dictionaries, but also a custom dictionary automatically generated from software natural language content, prone to include application domain terms and specific abbreviations. This approach was applied to two software packages, written in C, achieving a f-measure of around 90% for correctly splitting and expanding identifiers. A comparison with current state-of-the-art approaches is also presented. © 2014 Elsevier Inc.},
author_keywords={Identifier splitting;  Natural language processing;  Program comprehension},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bretschneider2015207,
author={Bretschneider, C. and Zillner, S.},
title={Semantic splitting of German medical compounds},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9302},
pages={207-215},
doi={10.1007/978-3-319-24033-6_24},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951778429&doi=10.1007%2f978-3-319-24033-6_24&partnerID=40&md5=b80f85ba2c958dce5bb50b1ba2869b01},
affiliation={Siemens AG, Corporate Technology, Munich, Germany; Center for Information and Language Processing, University Munich, Munich, Germany; School of International Business and Entrepreneurship, Steinbeis University, Berlin, Germany},
abstract={Compounding is widespread in highly inflectional languages with a quarter of all nouns created by composition. In our field of study, the German medical language, the amount of compounds significantly outnumbers this figure with 64%. Thus, their correct splitting is a high-impact preprocessing step for any NLP-based application. In this work we address two challenges of medical decomposition: First, we introduce the consideration of unknown constituents in order to split compounds that were not recognized as such so far. Second, our approach builds on the corpus-based approach of Koehn and Knight and adds semantic knowledge from domain ontologies to increase the accuracy during disambiguation of the various split options. Using this first-of-a-kind semantic approach in a study on decomposition of German medical compounds, we outperform the existing approaches by far. © Springer International Publishing Switzerland 2015.},
author_keywords={Compound splitting;  Medical NLP;  Ontology;  Semantics},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hill20141754,
author={Hill, E. and Binkley, D. and Lawrie, D. and Pollock, L. and Vijay-Shanker, K.},
title={An empirical study of identifier splitting techniques},
journal={Empirical Software Engineering},
year={2014},
volume={19},
number={6},
pages={1754-1780},
doi={10.1007/s10664-013-9261-0},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909993591&doi=10.1007%2fs10664-013-9261-0&partnerID=40&md5=4caa900c8b8977abd2599ad8fc9ee126},
affiliation={Department of Computer Science, Montclair State University, Montclair, NJ  07043, United States; Department of Computer Science, Loyola University Maryland, Baltimore, MD  21210, United States; Department of Computer and Information Sciences, University of Delaware, Newark, DE  19716, United States},
abstract={Researchers have shown that program analyses that drive software development and maintenance tools supporting search, traceability and other tasks can benefit from leveraging the natural language information found in identifiers and comments. Accurate natural language information depends on correctly splitting the identifiers into their component words and abbreviations. While conventions such as camel-casing can ease this task, conventions are not well-defined in certain situations and may be modified to improve readability, thus making automatic splitting more challenging. This paper describes an empirical study of state-of-the-art identifier splitting techniques and the construction of a publicly available oracle to evaluate identifier splitting algorithms. In addition to comparing current approaches, the results help to guide future development and evaluation of improved identifier splitting approaches. © 2013, Springer Science+Business Media New York.},
author_keywords={Identifier names;  Program comprehension;  Software engineering tools;  Source code text analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Guerrouj20141706,
author={Guerrouj, L. and Di Penta, M. and Guéhéneuc, Y.-G. and Antoniol, G.},
title={An experimental investigation on the effects of context on source code identifiers splitting and expansion},
journal={Empirical Software Engineering},
year={2014},
volume={19},
number={6},
pages={1706-1753},
doi={10.1007/s10664-013-9260-1},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910005434&doi=10.1007%2fs10664-013-9260-1&partnerID=40&md5=35d6ab1cf7d4a2406a37b234777286ef},
affiliation={SOCCER Lab., DGIGL, École Polytechnique de Montréal, QC, Canada; University of Sannio, Sannio, Italy},
abstract={Recent and past studies indicate that source code lexicon plays an important role in program comprehension. Developers often compose source code identifiers with abbreviated words and acronyms, and do not always use consistent mechanisms and explicit separators when creating identifiers. Such choices and inconsistencies impede the work of developers that must understand identifiers by decomposing them into their component terms, and mapping them onto dictionary, application or domain words. When software documentation is scarce, outdated or simply not available, developers must therefore use the available contextual information to understand the source code. This paper aims at investigating how developers split and expand source code identifiers, and, specifically, the extent to which different kinds of contextual information could support such a task. In particular, we consider (i) an internal context consisting of the content of functions and source code files in which the identifiers are located, and (ii) an external context involving external documentation. We conducted a family of two experiments with 63 participants, including bachelor, master, Ph.D. students, and post-docs. We randomly sampled a set of 50 identifiers from a corpus of open source C programs and we asked participants to split and expand them with the availability (or not) of internal and external contexts. We report evidence on the usefulness of contextual information for identifier splitting and acronym/abbreviation expansion. We observe that the source code files are more helpful than just looking at function source code, and that the application-level contextual information does not help any further. The availability of external sources of information only helps in some circumstances. Also, in some cases, we observe that participants better expanded acronyms than abbreviations, although in most cases both exhibit the same level of accuracy. Finally, results indicated that the knowledge of English plays a significant effect in identifier splitting/expansion. The obtained results confirm the conjecture that contextual information is useful in program comprehension, including when developers split and expand identifiers to understand them. We hypothesize that the integration of identifier splitting and expansion tools with IDE could help to improve developers’ productivity. © 2013, Springer Science+Business Media New York.},
author_keywords={Identifier splitting and expansion;  Program understanding;  Task context},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Fishel2014159,
author={Fishel, M. and Sennrich, R.},
title={Handling technical OOVs in SMT},
journal={Proceedings of the 17th Annual Conference of the European Association for Machine Translation, EAMT 2014},
year={2014},
pages={159-162},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000716797&partnerID=40&md5=3634713662e062110d558fbc30f94db4},
affiliation={Institute of Computational Linguistics, University of Zurich, Binzmühlestr. 14, Zürich, CH-8050, Switzerland},
abstract={We present a project on machine translation of software help desk tickets, a highly technical text domain. The main source of translation errors were out-of-vocabulary tokens (OOVs), most of which were either in-domain German compounds or technical token sequences that must be preserved verbatim in the output. We describe our efforts on compound splitting and treatment of non-translatable tokens, which lead to a significant translation quality gain. © 2014 The authors.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pecina2014165,
author={Pecina, P. and Dušek, O. and Goeuriot, L. and Hajič, J. and Hlaváčová, J. and Jones, G.J.F. and Kelly, L. and Leveling, J. and Mareček, D. and Novák, M. and Popel, M. and Rosa, R. and Tamchyna, A. and Urešová, Z.},
title={Adaptation of machine translation for multilingual information retrieval in the medical domain},
journal={Artificial Intelligence in Medicine},
year={2014},
volume={61},
number={3},
pages={165-185},
doi={10.1016/j.artmed.2014.01.004},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904321749&doi=10.1016%2fj.artmed.2014.01.004&partnerID=40&md5=0f3963e3d9b30a067ad27552815ea4a4},
affiliation={Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University in Prague, Malostranské nám. 25, 118 00 Prague 1, Czech Republic; CNGL Centre for Global Intelligent Content, School of Computing, Dublin City University, Glasnevin, Dublin 9, Ireland},
abstract={Objective: We investigate machine translation (MT) of user search queries in the context of cross-lingual information retrieval (IR) in the medical domain. The main focus is on techniques to adapt MT to increase translation quality; however, we also explore MT adaptation to improve effectiveness of cross-lingual IR. Methods and data: Our MT system is Moses, a state-of-the-art phrase-based statistical machine translation system. The IR system is based on the BM25 retrieval model implemented in the Lucene search engine. The MT techniques employed in this work include in-domain training and tuning, intelligent training data selection, optimization of phrase table configuration, compound splitting, and exploiting synonyms as translation variants. The IR methods include morphological normalization and using multiple translation variants for query expansion. The experiments are performed and thoroughly evaluated on three language pairs: Czech-English, German-English, and French-English. MT quality is evaluated on data sets created within the Khresmoi project and IR effectiveness is tested on the CLEF eHealth 2013 data sets. Results: The search query translation results achieved in our experiments are outstanding - our systems outperform not only our strong baselines, but also Google Translate and Microsoft Bing Translator in direct comparison carried out on all the language pairs. The baseline BLEU scores increased from 26.59 to 41.45 for Czech-English, from 23.03 to 40.82 for German-English, and from 32.67 to 40.82 for French-English. This is a 55% improvement on average. In terms of the IR performance on this particular test collection, a significant improvement over the baseline is achieved only for French-English. For Czech-English and German-English, the increased MT quality does not lead to better IR results. Conclusions: Most of the MT techniques employed in our experiments improve MT of medical search queries. Especially the intelligent training data selection proves to be very successful for domain adaptation of MT. Certain improvements are also obtained from German compound splitting on the source language side. Translation quality, however, does not appear to correlate with the IR performance - better translation does not necessarily yield better retrieval. We discuss in detail the contribution of the individual techniques and state-of-the-art features and provide future research directions. © 2014 Elsevier B.V.},
author_keywords={Compound splitting;  Cross-language information retrieval;  Domain adaptation of statistical machine translation;  Intelligent training data selection for machine translation;  Medical query translation;  Statistical machine translation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wołk2014107,
author={Wołk, K. and Marasek, K.},
title={Real-time statistical speech translation},
journal={Advances in Intelligent Systems and Computing},
year={2014},
volume={275 AISC},
number={VOLUME 1},
pages={107-113},
doi={10.1007/978-3-319-05951-8_11},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904602811&doi=10.1007%2f978-3-319-05951-8_11&partnerID=40&md5=d9d3d16efc7102d26081575e8d464b8b},
affiliation={Department of Multimedia, Polish Japanese Institute of Information Technology, Koszykowa 86, 02-008 Warsaw, Poland},
abstract={This research investigates the Statistical Machine Translation approaches to translate speech in real time automatically. Such systems can be used in a pipeline with speech recognition and synthesis software in order to produce a real-time voice communication system between foreigners. We obtained three main data sets from spoken proceedings that represent three different types of human speech. TED, Europarl, and OPUS parallel text corpora were used as the basis for training of language models, for developmental tuning and testing of the translation system. We also conducted experiments involving part of speech tagging, compound splitting, linear language model interpolation, TrueCasing and morphosyntactic analysis. We evaluated the effects of variety of data preparations on the translation results using the BLEU, NIST, METEOR and TER metrics and tried to give answer which metric is most suitable for PL-EN language pair. © Springer International Publishing Switzerland 2014.},
author_keywords={Knowledge-free learning;  Machine learning;  Machine translation;  NLP;  Speech translation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Binkley2013401,
author={Binkley, D. and Lawrie, D. and Pollock, L. and Hill, E. and Vijay-Shanker, K.},
title={A dataset for evaluating identifier splitters},
journal={IEEE International Working Conference on Mining Software Repositories},
year={2013},
pages={401-404},
doi={10.1109/MSR.2013.6624055},
art_number={6624055},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889056852&doi=10.1109%2fMSR.2013.6624055&partnerID=40&md5=20c554dc45e942680416227bea5a5bdf},
affiliation={Loyola University Maryland, Baltimore, MD 21210, United States; University of Delaware, Newark, DE 19716, United States; Montclair State University, Montclair, NJ, 07043, United States},
abstract={Software engineering and evolution techniques have recently started to exploit the natural language information in source code. A key step in doing so is splitting identifiers into their constituent words. While simple in concept, identifier splitting raises several challenging issues, leading to a range of splitting techniques. Consequently, the research community would benefit from a dataset (i.e., a gold set) that facilitates comparative studies of identifier splitting techniques. A gold set of 2,663 split identifiers was constructed from 8,522 individual human splitting judgements and can be obtained from www.cs.loyola.edu/∼binkley/ludiso. This set's construction and observations aimed at its effective use are described. © 2013 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kpodjedo20131090,
author={Kpodjedo, S. and Ricca, F. and Galinier, P. and Antoniol, G. and Gueheneuc, Y.-G.},
title={Madmatch: Many-to-many approximate diagram matching for design comparison},
journal={IEEE Transactions on Software Engineering},
year={2013},
volume={39},
number={8},
pages={1090-1111},
doi={10.1109/TSE.2013.9},
art_number={6568862},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881038069&doi=10.1109%2fTSE.2013.9&partnerID=40&md5=9a548cfb32b924a02c660b079f2e1e6b},
affiliation={Ecole Polytechnique of Montreal, DGIGL, 2900 - Edouard Montpetit, Montreal, QC H3C 3A7, Canada; Universita di Genova, Genova 16126, Italy},
abstract={Matching algorithms play a fundamental role in many important but difficult software engineering activities, especially design evolution analysis and model comparison. We present MADMatch, a fast and scalable many-to-many approximate diagram matching approach based on an error-tolerant graph matching (ETGM) formulation. Diagrams are represented as graphs, costs are assigned to possible differences between two given graphs, and the goal is to retrieve the cheapest matching. We address the resulting optimization problem with a tabu search enhanced by the novel use of lexical and structural information. Through several case studies with different types of diagrams and tasks, we show that our generic approach obtains better results than dedicated state-of-the-art algorithms, such as AURA, PLTSDiff, or UMLDiff, on the exact same datasets used to introduce (and evaluate) these algorithms. © 1976-2012 IEEE.},
author_keywords={approximate graph matching;  Diagram differencing;  identifier splitting;  search-based software engineering},
document_type={Article},
source={Scopus},
}

@ARTICLE{Guerrouj2013575,
author={Guerrouj, L. and Di Penta, M. and Antoniol, G. and Guéh́eneuc, Y.-G.},
title={TIDIER: An identifier splitting approach using speech recognition techniques},
journal={Journal of software: Evolution and Process},
year={2013},
volume={25},
number={6},
pages={575-599},
doi={10.1002/smr.539},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883654687&doi=10.1002%2fsmr.539&partnerID=40&md5=79227717373f850527ad7601246a17bb},
affiliation={DGIGL/P TIDEJ Team/SOCCER Lab., École Polytechnique de Montréal, Québec, Canada; RCOST - Department of Engineering, University of Sannio, Italy},
abstract={The software engineering literature reports empirical evidence on the relation between various characteristics of a software system and its quality. Among other factors, recent studies have shown that a proper choice of identifiers influences understandability and maintainability. Indeed, identifiers are developers' main source of information and guide their cognitive processes during program comprehension when high-level documentation is scarce or outdated and when source code is not sufficiently commented. This paper proposes a novel approach to recognize words composing source code identifiers. The approach is based on an adaptation of Dynamic Time Warping used to recognize words in continuous speech. The approach overcomes the limitations of existing identifier-splitting approaches when naming conventions (e.g., Camel Case) are not used or when identifiers contain abbreviations. We apply the approach on a sample of more than 1000 identifiers extracted from 340 C programs and compare its results with a simple Camel Case splitter and with an implementation of an alternative identifier splitting approach, Samurai. Results indicate the capability of the novel approach: (i) to outperform the alternative ones, when using a dictionary augmented with domain knowledge or a contextual dictionary and (ii) to expand 48% of a set of selected abbreviations into dictionary words. Copyright © 2011 John Wiley & Sons, Ltd.},
author_keywords={Identifier splitting;  Linguistic analysis;  Program comprehension},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Srinivasan20121113,
author={Srinivasan, S. and Bhattacharya, S. and Chakraborty, R.},
title={Segmenting web-domains and hashtags using length specific models},
journal={ACM International Conference Proceeding Series},
year={2012},
pages={1113-1122},
doi={10.1145/2396761.2398410},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871061019&doi=10.1145%2f2396761.2398410&partnerID=40&md5=4a4ef08843f61ac76f3dbc0b0b0a71a5},
affiliation={Yahoo SDC, Bangalore, India; Yahoo Labs., Bangalore, India; Indian Statistical Institute, Kolkata, India},
abstract={Segmentation of a string of English language characters into a sequence of words has many applications. Here, we study two applications in the internet domain. First application is the web domain segmentation which is crucial for monetization of broken URLs. Secondly, we propose and study a novel application of twitter hashtag segmentation for increasing recall on twitter searches. Existing methods for word segmentation use unsupervised language models. We find that when using multiple corpora, the joint probability model from multiple corpora performs significantly better than the individual corpora. Motivated by this, we propose weighted joint probability model, with weights specific to each corpus. We propose to train the weights in a supervised manner using max-margin methods. The supervised probability models improve segmentation accuracy over joint probability models. Finally, we observe that length of segments is an important parameter for word segmentation, and incorporate length-specific weights into our model. The length specific models further improve segmentation accuracy over supervised probability models. For all models proposed here, inference problem can be solved using the dynamic programming algorithm. We test our methods on five different datasets, two from web domains data, and three from news headlines data from an LDC dataset. The supervised length specific models show significant improvements over unsupervised single corpus and joint probability models. Cross-testing between the datasets confirm that supervised probability models trained on all datasets, and length specific models trained on news headlines data, generalize well. Segmentation of hashtags result in significant improvement in recall on searches for twitter trends. © 2012 ACM.},
author_keywords={compound splitting;  hashtag segmentation;  structured learning;  web domain segmentation;  word segmentation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guerrouj2012103,
author={Guerrouj, L. and Galinier, P. and Guéhéneuc, Y.-G. and Antoniol, G. and Di Penta, M.},
title={TRIS: A fast and accurate identifiers splitting and expansion algorithm},
journal={Proceedings - Working Conference on Reverse Engineering, WCRE},
year={2012},
pages={103-112},
doi={10.1109/WCRE.2012.20},
art_number={6385106},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872297690&doi=10.1109%2fWCRE.2012.20&partnerID=40&md5=cde8887fb5e9ca9bd4f353e3c9bd9ad8},
affiliation={DGIGL, École Polytechnique de Montréal, Canada; Dept. of Engineering, University of Sannio, Italy},
abstract={Understanding source code identifiers, by identifying words composing them, is a necessary step for many program comprehension, reverse engineering, or redocumentation tasks. To this aim, researchers have proposed several identifier splitting and expansion approaches such as Samurai, TIDIER and more recently GenTest. The ultimate goal of such approaches is to help disambiguating conceptual information encoded in compound (or abbreviated) identifiers. This paper presents TRIS, TRee-based Identifier Splitter, a two-phases approach to split and expand program identifiers. First, TRIS pre-compiles transformed dictionary words into a tree representation, associating a cost to each transformation. In a second phase, it maps the identifier splitting/expansion problem into a minimization problem, i.e., the search of the shortest path (optimal split/expansion) in a weighted graph. We apply TRIS to a sample of 974 identifiers extracted from JHotDraw, 3,085 from Lynx, and to a sample of 489 identifiers extracted from 340 C programs. Also, we compare TRIS with GenTest on a set of 2,663 mixed Java, C and C++ identifiers. We report evidence that TRIS split (and expansion) is more accurate than state-of-the-art approaches and that it is also efficient in terms of computation time. © 2012 IEEE.},
author_keywords={Identifier Splitting/Expansion;  Linguistic Analysis;  Optimal Path;  Program Comprehension;  Weighted Acyclic Graph},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2012,
title={Proceedings - 19th Working Conference on Reverse Engineering, WCRE 2012},
journal={Proceedings - Working Conference on Reverse Engineering, WCRE},
year={2012},
page_count={526},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872332293&partnerID=40&md5=94138d1316529b8539e10feb7abee73c},
abstract={The proceedings contain 55 papers. The topics discussed include: assuring software quality by code smell detection; structured binary editing with a CFG transformation algebra; Astra: bottom-up construction of structured artifact repositories; detection and recovery of functions and their arguments in a retargetable decompiler; towards static analysis of virtualization-obfuscated binaries; understanding android fragmentation with topic analysis of vendor-specific bugs; using network analysis for recommendation of central software classes; TRIS: a fast and accurate identifiers splitting and expansion algorithm; using bug report similarity to enhance bug localisation; SCAN: an approach to label and relate execution trace segments; feature location in a collection of product variants; reverse engineering iOS mobile applications; and precise detection of uninitialized variables using dynamic analysis - extending to aggregate and vector types.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Binkley2012588,
author={Binkley, D. and Lawrie, D. and Uehlinger, C.},
title={Vocabulary normalization improves IR-based concept location},
journal={IEEE International Conference on Software Maintenance, ICSM},
year={2012},
pages={588-591},
doi={10.1109/ICSM.2012.6405328},
art_number={6405328},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873184808&doi=10.1109%2fICSM.2012.6405328&partnerID=40&md5=e62cf648b2b0e48402cf9e76791ac74b},
affiliation={Computer Science Department, Loyola University Maryland, Baltimore, United States},
abstract={Tool support is crucial to modern software development, evolution, and maintenance. Early tools reused the static analysis performed by the compiler. These were followed by dynamic analysis tools and more recently tools that exploit natural language. This later class has the advantage that it can incorporate not only the code, but artifacts from all phases of software construction and its subsequent evolution. Unfortunately, the natural language found in source code often uses a vocabulary different from that used in other software artifacts and thus increases the vocabulary mismatch problem. This problem exists because many natural-language tools imported from Information Retrieval (IR) and Natural Language Processing (NLP) implicitly assume the use of a single natural language vocabulary. Vocabulary normalization, which goes well beyond simple identifier splitting, brings the vocabulary of the source into line with other artifacts. Consequently, it is expected to improve the performance of existing and future IR and NLP based tools. As a case study, an experiment with an LSI-based feature locator is replicated. Normalization universally improves performance. For the tersest queries, this improvement is over 180% (p < 0.0001). © 2012 IEEE.},
author_keywords={concept location;  information retrieval;  vocabulary normalization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sureka20121,
author={Sureka, A.},
title={Source code identifier splitting using yahoo image and web search engine},
journal={Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
year={2012},
pages={1-8},
doi={10.1145/2384416.2384417},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869760971&doi=10.1145%2f2384416.2384417&partnerID=40&md5=042160e97cea76d60ed1709ec697fb7f},
affiliation={Indraprastha Institute of Information Technology, Delhi IIIT-D, New Delhi, India},
abstract={Source-code or program identifiers are sequence of characters consisting of one or more tokens representing domain concepts. Splitting or tokenizing identifiers that does not contain explicit markers or clues such as came-casing or using underscore as a token separatoris a technically challenging problem. In this paper, we present a technique for automatic tokenization and splitting of source-code identifiers using Yahoo web search and image search similarity distance. We present an algorithm that decides the split position based on various factors such as conceptual correlations and semantic relatedness between the left and right splits strings of a given identifier, popularity of the token and its length. The number of hits or search results returned by the web and image search engine serves as a proxy to measures such as term popularity and correlation. We perform a series of experiments to validate the proposed approach and present performance results.},
author_keywords={Identifier Splitting;  Identifier Tokenization;  Mining Software Repositories;  Program Comprehension;  Yahoo Similarity Distance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2012,
title={Proceedings of the 1st International Workshop on Software Mining, SoftwareMining-2012 - Held in Conjunction with the 18th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD-2012},
journal={Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
year={2012},
page_count={35},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869851232&partnerID=40&md5=32e139e50c5b76a74c5d787072b9caf5},
abstract={The proceedings contain 4 papers. The topics discussed include: source code identifier splitting using yahoo image and web search engine; software systems through complex networks science: review, analysis and applications; labeled topic detection of open source software from mining mass textual project profiles; and rank-directed layout of UML class diagrams.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Biggers2012164,
author={Biggers, L.R.},
title={The effects of identifier retention and stop word removal on a latent Dirichlet allocation based feature location technique},
journal={Proceedings of the Annual Southeast Conference},
year={2012},
pages={164-169},
doi={10.1145/2184512.2184551},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862667183&doi=10.1145%2f2184512.2184551&partnerID=40&md5=596eddec20f80762f4b8b0c7f84166e1},
affiliation={Department of Computer Science, University of Alabama, Tuscaloosa, AL 35487-0290, United States},
abstract={Feature location, an important task in program comprehension, occurs when the developer identifies the source code entity or entities responsible for implementing a functionality. Researchers have applied static analysis techniques to multiple software maintenance tasks, including feature localization. Static analysis techniques operate on a document corpus. Configuration and preprocessing decisions are required to build a suitable source code corpus for a static analysis technique. Currently, there is little guidance in the software engineering literature for making such configuration decisions. This paper focuses on two preprocessing methods for source code corpora, identifier splitting and stop word lists. We experiment on three open source Java test suites, i.e. Mylyn 1.0.1, Rhino 1.5R5, and Rhino 1.6R5. Our results indicate that identifier splitting and stop word list decisions do not significantly affect the performance of the LDA based feature location technique. © 2012 ACM.},
author_keywords={feature location;  information retrieval;  program comprehension;  software evolution and maintenance;  static analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Henrich2011420,
author={Henrich, V. and Hinrichs, E.},
title={Determining immediate constituents of compounds in GermaNet},
journal={International Conference Recent Advances in Natural Language Processing, RANLP},
year={2011},
pages={420-426},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866877956&partnerID=40&md5=4e50f1d779c887840937a10cea5dcd74},
affiliation={University of Tübingen, Germany},
abstract={In order to be able to systematically link compounds in GermaNet to their constituent parts, compound splitting needs to be applied recursively and has to identify the immediate constituents at each level of analysis. Existing tools for compound splitting for German only offer an analysis of all component parts of a compound at once without any grouping of subconstituents. Thus, existing tools for splitting compounds were adapted to overcome this issue. Algorithms combining three heterogeneous kinds of compound splitters are developed to achieve better results. The best overall result with an accuracy of 92.42% is achieved by a hybrid combined compound splitter that takes into account all knowledge provided by the individual compound splitters, and in addition some domain knowledge about German derivation morphology and compounding.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2011357,
author={Wang, K. and Thrasher, C. and Hsu, B.-J.},
title={Web scale NLP: A case study on URL word breaking},
journal={Proceedings of the 20th International Conference on World Wide Web, WWW 2011},
year={2011},
pages={357-366},
doi={10.1145/1963405.1963457},
note={cited By 32},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052123854&doi=10.1145%2f1963405.1963457&partnerID=40&md5=f0884c46e20c2b2cf89b5c9a1bb92dd2},
affiliation={Microsoft Research, ISRC, One Microsoft Way, Redmond, WA 98052, United States},
abstract={This paper uses the URL word breaking task as an example to elaborate what we identify as crucialin designingstatistical natural language processing (NLP) algorithmsfor Web scale applications: (1) rudimentary multilingual capabilities to cope with the global nature of the Web, (2) multi-style modeling to handle diverse language styles seen in the Web contents, (3) fast adaptation to keep pace with the dynamic changes of the Web, (4) minimal heuristic assumptions for generalizability and robustness, and (5) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost. We first show that the state-of-the-art word breaking techniquescan be unified and generalized under the Bayesian minimum risk (BMR) framework that, using a Web scale N-gram, can meet the first three requirements. We discuss how the existing techniques can be viewed asintroducing additional assumptions to the basic BMR framework, and describe a generic yet efficient implementation called word synchronous beam search.Testing the framework and its implementation on a series of large scale experiments reveals the following. First, the language style used to build the modelplays a critical role in the word breaking task, and the most suitable for the URL word breaking task appears to be that ofthe document title where the best performance is obtained. Models created from other language styles, such as from document body, anchor text, and even queries, exhibit varying degrees of mismatch. Although all styles benefit from increasing modeling power which, in our experiments, corresponds tothe use of a higher order N-gram, the gain is most recognizable for the title model. The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models, but are less effective and, in many cases,lead to poorer performance than the matched model with minimal assumptions. For the matched model based on document titles, an accuracy rate of 97.18% can already be achieved using simple trigram without any heuristics. Copyright © 2011 by the Association for Computing Machinery, Inc. (ACM).},
author_keywords={Compound splitting;  Multi-style language model;  URL segmentation;  Web scale word breaking;  Word segmentation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dit201111,
author={Dit, B. and Guerrouj, L. and Poshyvanyk, D. and Antoniol, G.},
title={Can better identifier splitting techniques help feature location?},
journal={IEEE International Conference on Program Comprehension},
year={2011},
pages={11-20},
doi={10.1109/ICPC.2011.47},
art_number={5970159},
note={cited By 48},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052408614&doi=10.1109%2fICPC.2011.47&partnerID=40&md5=c445c67d85da6e8e3c2ad216b5250e62},
affiliation={Department of Computer Science, College of William and Mary, Williamsburg, VA, United States; Department of Computer Science Engineering, École Polytechnique de Montréal, Québec, QC, Canada},
abstract={The paper presents an exploratory study of two feature location techniques utilizing three strategies for splitting identifiers: Camel Case, Samurai and manual splitting of identifiers. The main research question that we ask in this study is if we had a perfect technique for splitting identifiers, would it still help improve accuracy of feature location techniques applied in different scenarios and settings? In order to answer this research question we investigate two feature location techniques, one based on Information Retrieval and the other one based on the combination of Information Retrieval and dynamic analysis, for locating bugs and features using various configurations of preprocessing strategies on two open-source systems, Rhino and jEdit. The results of an extensive empirical evaluation reveal that feature location techniques using Information Retrieval can benefit from better preprocessing algorithms in some cases, and that their improvement in effectiveness while using manual splitting over state-of-the-art approaches is statistically significant in those cases. However, the results for feature location technique using the combination of Information Retrieval and dynamic analysis do not show any improvement while using manual splitting, indicating that any preprocessing technique will suffice if execution data is available. Overall, our findings outline potential benefits of putting additional research efforts into defining more sophisticated source code preprocessing techniques as they can still be useful in situations where execution information cannot be easily collected. © 2011 IEEE.},
author_keywords={dynamic analysis;  feature location;  identifier splitting algorithms;  information retrieval},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kumar201057,
author={Kumar, A. and Mittal, V. and Kulkarni, A.},
title={Sanskrit compound processor},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6465 LNAI},
pages={57-69},
doi={10.1007/978-3-642-17528-2_5},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651063736&doi=10.1007%2f978-3-642-17528-2_5&partnerID=40&md5=ba8a099b10cf2e63862a66f42bd24e2f},
affiliation={Department of Sanskrit Studies, University of Hyderabad, India; Language Technologies Research Centre, IIIT, Hyderabad, India},
abstract={Sanskrit is very rich in compound formation. Typically a compound does not code the relation between its components explicitly. To understand the meaning of a compound, it is necessary to identify its components, discover the relations between them and finally generate a paraphrase of the compound. In this paper, we discuss the automatic segmentation and type identification of a compound using simple statistics that results from the manually annotated data. © 2010 Springer-Verlag Berlin Heidelberg.},
author_keywords={Optimality Theory;  Sanskrit Compound Splitter;  Sanskrit Compound Type Identifier;  Sanskrit Compounds},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guerrouj2010301,
author={Guerrouj, L.},
title={Automatic derivation of concepts based on the analysis of source code identifiers},
journal={Proceedings - Working Conference on Reverse Engineering, WCRE},
year={2010},
pages={301-304},
doi={10.1109/WCRE.2010.45},
art_number={5645490},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650655941&doi=10.1109%2fWCRE.2010.45&partnerID=40&md5=085e4056d0a775ee451b7bed2ae95111},
affiliation={DGIGL - SOCCER Lab., Ptidej Team, Ecole Polytechnique de Montréal, QC, Canada},
abstract={The existing software engineering literature has empirically shown that a proper choice of identifiers influences software understandability and maintainability. Indeed, identifiers are developers' main up-to-date source of information and guide their cognitive processes during program understanding when the high-level documentation is scarce or outdated and when the source code is not sufficiently commented. Deriving domain terms from identifiers using high-level and domain concepts is not an easy task when naming conventions (e.g., Camel Case) are not used or strictly followed and-or when these words have been abbreviated or otherwise transformed. Our thesis is to develop an approach that overcomes the shortcomings of the existing approaches and maps identifiers to domain concepts even in the absence of naming conventions and-or the presence of abbreviations. Our approach uses a thesaurus of words and abbreviations to map terms or transformed words composing identifiers to dictionary words. It relies on an oracle that we manually build for the validation of our results. To evaluate our technique, we apply it to derive concepts from identifiers of different systems and open source projects. We also enrich it by the use of domain knowledge and context-aware dictionaries to analyze how sensitive are its performances to the use of contextual information and specialized knowledge. © 2010 IEEE.},
author_keywords={Identifier splitting;  Linguistic analysis;  Program comprehension;  Software quality},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zeman2010216,
author={Zeman, D.},
title={Using TectoMT as a preprocessing tool for phrase-based statistical machine translation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6231 LNAI},
pages={216-223},
doi={10.1007/978-3-642-15760-8_28},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049263442&doi=10.1007%2f978-3-642-15760-8_28&partnerID=40&md5=d8df80e3e3bbfd901f36029213a636fe},
affiliation={Univerzita Karlova v Praze, ÚFAL, Malostranské náměstí 25, 11800 Prague, Czech Republic},
abstract={We present a systematic comparison of preprocessing techniques for two language pairs: English-Czech and English-Hindi. The two target languages, although both belonging to the Indo-European language family, show significant differences in morphology, syntax and word order. We describe how TectoMT, a successful framework for analysis and generation of language, can be used as preprocessor for a phrase-based MT system.We compare the two language pairs and the optimal sets of source-language transformations applied to them. The following transformations are examples of possible preprocessing steps: lemmatization; retokenization, compound splitting; removing/adding words lacking counterparts in the other language; phrase reordering to resemble the target word order; marking syntactic functions. TectoMT, as well as all other tools and data sets we use, are freely available on the Web. © 2010 Springer-Verlag Berlin Heidelberg.},
author_keywords={phrase-based translation;  preprocessing;  reordering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Enslen200971,
author={Enslen, E. and Hill, E. and Pollock, L. and Vijay-Shanker, K.},
title={Mining source code to automatically split identifiers for software analysis},
journal={Proceedings of the 2009 6th IEEE International Working Conference on Mining Software Repositories, MSR 2009},
year={2009},
pages={71-80},
doi={10.1109/MSR.2009.5069482},
art_number={5069482},
note={cited By 102},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349897886&doi=10.1109%2fMSR.2009.5069482&partnerID=40&md5=d807de27d697935bc4b1aa42bcb2c29a},
affiliation={Department of Computer and Information Sciences, University of Delaware, Newark, DE 19716, United States},
abstract={Automated software engineering tools (e.g., program search, concern location, code reuse, quality assessment, etc.) increasingly rely on natural language information from comments and identifiers in code. The first step in analyzing words from identifiers requires splitting identifiers into their constituent words. Unlike natural languages, where space and punctuation are used to delineate words, identifiers cannot contain spaces. One common way to split identifiers is to follow programming language naming conventions. For example, Java programmers often use camel case, where words are delineated by uppercase letters or non-alphabetic characters. However, programmers also create identifiers by concatenating sequences of words together with no discernible delineation, which poses challenges to automatic identifier splitting. In this paper, we present an algorithm to automatically split identifiers into sequences of words by mining word frequencies in source code. With these word frequencies, our identifier splitter uses a scoring technique to automatically select the most appropriate partitioning for an identifier. In an evaluation of over 8000 identifiers from open source Java programs, our Samurai approach outperforms the existing state of the art techniques. © 2009 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bordag2008881,
author={Bordag, S.},
title={Unsupervised and Knowledge-free morpheme segmentation and analysis},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2008},
volume={5152 LNCS},
pages={881-891},
doi={10.1007/978-3-540-85760-0-113},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349825872&doi=10.1007%2f978-3-540-85760-0-113&partnerID=40&md5=6281e6daae5eb412cf1b5e3754629d23},
affiliation={Natural Language Processing Department, University of Leipzig, Germany},
abstract={This paper presents a revised version of an unsupervised and knowledge-free morpheme boundary detection algorithm based on letter successor variety (LSV) and a trie classifier [1]. Additional knowledge about relatedness of the found morphs is obtained from a morphemic analysis based on contextual similarity. For the boundary detection the challenge of increasing recall of found morphs while retaining a high precision is tackled by adding a compound splitter, iterating the LSV analysis and dividing the trie classifier into two distinctly applied clasifiers. The result is a significantly improved overall performance and a decreased reliance on corpus size. Further possible improvements and analyses are discussed. © 2008 Springer-Verlag Berlin Heidelberg.},
author_keywords={Distributed similarity;  Letter successor variety;  Morpheme analysis;  Morpheme boundary detection},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stymne2008182,
author={Stymne, S. and Holmqvist, M.},
title={Processing of Swedish compounds for phrase-based statistical machine translation},
journal={Proceedings of the 12th European Association for Machine Translation Conference, EAMT 2008},
year={2008},
pages={182-191},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857535918&partnerID=40&md5=11ef39e004bb3623703d713985652303},
affiliation={Department of Computer and Information Science, Linköping University, Sweden},
abstract={We investigated the effects of processing Swedish compounds for phrase-based SMT between Swedish and English. Compounds were split in a pre-processing step using an unsupervised empirical method. After translation into Swedish, compounds were merged, using a novel merging algorithm. We investigated two ways of handling compound parts, by marking them as compound parts or by normalizing them to a canonical form. We found that compound splitting did improve translation into Swedish, according to automatic metrics. For translation into English the results were not consistent across automatic metrics. However, error analysis of compound translation showed a small improvement in the systems that used splitting. The number of untranslated words in the English output was reduced by 50%.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Stymne2008464,
author={Stymne, S.},
title={German compounds in factored statistical machine translation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2008},
volume={5221 LNAI},
pages={464-475},
doi={10.1007/978-3-540-85287-2_44},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-52149110825&doi=10.1007%2f978-3-540-85287-2_44&partnerID=40&md5=a1f4fbab019cd70b45de69094f44a5e7},
affiliation={Department of Computer and Information Science, Linköping University, Sweden},
abstract={An empirical method for splitting German compounds is explored by varying it in a number of ways to investigate the consequences for factored statistical machine translation between English and German in both directions. Compound splitting is incorporated into translation in a preprocessing step, performed on training data and on German translation input. For translation into German, compounds are merged based on part-of-speech in a postprocessing step. Compound parts are marked, to separate them from ordinary words. Translation quality is improved in both translation directions and the number of untranslated words in the English output is reduced. Different versions of the splitting algorithm performs best in the two different translation directions. © 2008 Springer-Verlag Berlin Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pedersen200775,
author={Pedersen, B.S.},
title={Using shallow linguistic analysis to improve search on Danish compounds},
journal={Natural Language Engineering},
year={2007},
volume={13},
number={1},
pages={75-90},
doi={10.1017/S1351324906004256},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847289690&doi=10.1017%2fS1351324906004256&partnerID=40&md5=b20e47119434e911c56c063423f9c5b1},
affiliation={Center for Sprogteknologi, University of Copenhagen, Njalsgade 80, DK-2300 Copenhagen S, Denmark},
abstract={In this paper we focus on a specific search-related query expansion topic, namely search on Danish compounds and expansion to some of their synonymous phrases. Compounds constitute a specific issue in search, in particular in languages where they are written in one word, as is the case for Danish and the other Scandinavian languages. For such languages, expansion of the query compound into separate lemmas is a way of finding the often frequent alternative synonymous phrases in which the content of a compound can also be expressed. However, it is crucial to note that the number of irrelevant hits is generally very high when using this expansion strategy. The aim of this paper is therefore to examine how we can obtain better search results on split compounds, partly by looking at the internal structure of the original compound, partly by analyzing the context in which the split compound occurs. In this context, we pursue two hypotheses: (1) that some categories of compounds are more likely to have synonymous 'split' counterparts than others; and (2) that search results where both the search words (obtained by splitting the compound) occur in the same noun phrase, are more likely to contain a synonymous phrase to the original compound query. The search results from 410 enhanced compound queries are used as a test bed for our experiments. On these search results, we perform a shallow linguistic analysis and introduce a new, linguistically based threshold for retrieved hits. The results obtained by using this strategy demonstrate that compound splitting combined with a shallow linguistic analysis focusing on the argument structure of the compound head as well as on the recognition of NPs, can improve search by substantially bringing down the number of irrelevant hits. © 2006 Cambridge University Press.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bordag2007,
author={Bordag, S.},
title={Unsupervised and knowledge-free morpheme segmentation and analysis},
journal={CEUR Workshop Proceedings},
year={2007},
volume={1173},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921960169&partnerID=40&md5=af8fc25f3c4af981dbe962e6e760ad24},
affiliation={University of Leipzig, Germany},
abstract={This paper presents a revised version of an unsupervised and knowledge-free morpheme boundary detection algorithm based on letter successor variety (LSV) and a trie classifier [5]. Additionally a morphemic analysis based on contextual similarity provides knowledge about relatedness of the found morphs. For the boundary detection the challenge of increasing recall of found morphs while retaining a high precision is tackled by adding a compound splitter, iterating the LSV analysis and dividing the trie classifier into two distinctly applied clasifiers. The result is a significantly improved overall performance and a decreased reliance on corpus size. Further possible improvements and analyses are discussed.},
author_keywords={Distributed similarity;  Letter successor variety;  Morpheme analysis;  Morpheme boundary detection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ahlgren200781,
author={Ahlgren, P. and Kekäläinen, J.},
title={Indexing strategies for Swedish full text retrieval under different user scenarios},
journal={Information Processing and Management},
year={2007},
volume={43},
number={1},
pages={81-102},
doi={10.1016/j.ipm.2006.03.003},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748311879&doi=10.1016%2fj.ipm.2006.03.003&partnerID=40&md5=db68e318ded198f53c49414be315f238},
affiliation={University College of Borås, Swedish School, Library and Information Science, 501 90 Borås, Sweden; University of Tampere, Department of Information Studies, Kanslerinrinne 1, 33014 Tampere, Finland},
abstract={This paper deals with Swedish full text retrieval and the problem of morphological variation of query terms in the document database. The effects of combination of indexing strategies with query terms on retrieval effectiveness were studied. Three of five tested combinations involved indexing strategies that used conflation, in the form of normalization. Further, two of these three combinations used indexing strategies that employed compound splitting. Normalization and compound splitting were performed by SWETWOL, a morphological analyzer for the Swedish language. A fourth combination attempted to group related terms by right hand truncation of query terms. The four combinations were compared to each other and to a baseline combination, where no attempt was made to counteract the problem of morphological variation of query terms in the document database. The five combinations were evaluated under six different user scenarios, where each scenario simulated a certain user type. The four alternative combinations outperformed the baseline, for each user scenario. The truncation combination had the best performance under each user scenario. The main conclusion of the paper is that normalization and right hand truncation (performed by a search expert) enhanced retrieval effectiveness in comparison to the baseline. The performance of the three combinations of indexing strategies with query terms based on normalization was not far below the performance of the truncation combination. © 2006 Elsevier Ltd. All rights reserved.},
author_keywords={Base word form index;  Discounted cumulated gain;  Indexing strategy;  Inflected word form index;  Truncation;  User scenario},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ahlgren2006681,
author={Ahlgren, P. and Kekäläinen, J.},
title={Swedish full text retrieval: Effectiveness of different combinations of indexing strategies with query terms},
journal={Information Retrieval},
year={2006},
volume={9},
number={6},
pages={681-697},
doi={10.1007/s10791-006-9009-1},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749254069&doi=10.1007%2fs10791-006-9009-1&partnerID=40&md5=d74c4096df819a32e61f01451d6f1a9b},
affiliation={Swedish School of Library and Information Science, University College of Borås, 501 90 Borås, Sweden; Department of Information Studies, 33014 Tampere, Finland},
abstract={In this paper, which treats Swedish full text retrieval, the problem of morphological variation of query terms in the document database is studied. The Swedish CLEF 2003 test collection was used, and the effects of combination of indexing strategies with query terms on retrieval effectiveness were studied. Four of the seven tested combinations involved indexing strategies that used normalization, a form of conflation. All of these four combinations employed compound splitting, both during indexing and at query phase. SWETWOL, a morphological analyzer for the Swedish language, was used for normalization and compound splitting. A fifth combination used stemming, while a sixth attempted to group related terms by right hand truncation of query terms. The truncation was performed by a search expert. These six combinations were compared to each other and to a baseline combination, where no attempt was made to counteract the problem of morphological variation of query terms in the document database. Both the truncation combination, the four combinations based on normalization and the stemming combination outperformed the baseline. Truncation had the best performance. The main conclusion of the paper is that truncation, normalization and stemming enhanced retrieval effectiveness in comparison to the baseline. Further, normalization and stemming were not far below truncation. © Springer Science+Business Media, LLC 2006.},
author_keywords={Indexing strategies;  Morphological analysis;  Stemming;  Swedish;  Truncation},
document_type={Review},
source={Scopus},
}

@ARTICLE{Popović2006616,
author={Popović, M. and Stein, D. and Ney, H.},
title={Statistical machine translation of german compound words},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2006},
volume={4139 LNAI},
pages={616-624},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749674758&partnerID=40&md5=5a2ba01a1897f9b2a062138b3e9f05d3},
affiliation={Informatik VI, Computer Science Department, RWTH Aachen University, Ahornstrasse 55, 52056 Aachen, Germany},
abstract={German compound words pose special problems to statistical machine translation systems: the occurence of each of the components in the training data is not sufficient for successful translation. Even if the compound itself has been seen during training, the system may not be capable of translating it properly into two or more words. If German is the target language, the system might generate only separated components or may not be capable of choosing the correct compound. In this work, we investigate and compare different strategies for the treatment of German compound words in statistical machine translation systems. For translation from German, we compare linguistic-based and corpusbased compound splitting. For translation into German, we investigate splitting and rejoining German compounds, as well as joining English potential components. Additionaly, we investigate word alignments enhanced with knowledge about the splitting points of German compounds. The translation quality is consistently improved by all methods for both translation directions. © Springer-Verlag Berlin Heidelberg 2006.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kettunen2005476,
author={Kettunen, K. and Kunttu, T. and Järvelin, K.},
title={To stem or lemmatize a highly inflectional language in a probabilistic IR environment?},
journal={Journal of Documentation},
year={2005},
volume={61},
number={4},
pages={476-496},
doi={10.1108/00220410510607480},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-23744456817&doi=10.1108%2f00220410510607480&partnerID=40&md5=1a5faad125a6089080e9339a2715ac21},
affiliation={Department of Information Studies, University of Tampere, Tampere, Finland},
abstract={Purpose - To show that stem generation compares well with lemmatization as a morphological tool for a highly inflectional language for IR purposes in a best-match retrieval system. Design/methodology/approach - Effects of three different morphological methods - lemmatization, stemming and stem production - for Finnish are compared in a probabilistic IR environment (INQUERY). Evaluation is done using a four-point relevance scale which is partitioned differently in different test settings. Findings - Results show that stem production, a lighter method than morphological lemmatization, compares well with lemmatization in a best-match IR environment. Differences in performance between stem production and lemmatization are small and they are not statistically significant in most of the tested settings. It is also shown that hitherto a rather neglected method of morphological processing for Finnish, stemming, performs reasonably well although the stemmer used - a Porter stemmer implementation - is far from optimal for a morphologically complex language like Finnish. In another series of tests, the effects of compound splitting and derivational expansion of queries are tested. Practical implications - Usefulness of morphological lemmatization and stem generation for IR purposes can be estimated with many factors. On the average P-R level they seem to behave very close to each other in a probabilistic IR system. Thus, the choice of the used method with highly inflectional languages needs to be estimated along other dimensions too. Originality/value - Results are achieved using Finnish as an example of a highly inflectional language. The results are of interest for anyone who is interested in processing of morphological variation of a highly inflected language for IR purposes. © Emerald Group Publishing Limited.},
author_keywords={Information research;  Languages},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kojima2004367,
author={Kojima, Y. and Itoh, H. and Mano, H. and Ogawa, Y.},
title={Ricoh at CLEF 2003},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={3237},
pages={367-372},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-24944574936&partnerID=40&md5=0dd2d85c0b086f439347bed6ff3b692e},
affiliation={Software RandD Group, RICOH CO., Ltd., 1-1-17 Koishikawa, Bunkyo-ku, Tokyo 112-0002, Japan},
abstract={This paper describes RICOH's participation in the Monolingual Information Retrieval tasks of the Cross-Language Evaluation Forum (CLEF) 2003. We applied our system using the same kind of stemmer, the same options and different parameters to five European languages and compared the results for each langauge. Although the overall performance of the system was reasonable, there were two problems. The first was the lack of a compound splitter for German and the second was the failure of query expansion when there were few relevant documents. © Springer-Verlag 2004.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cöster2004337,
author={Cöster, R. and Sahlgren, M. and Karlgren, J.},
title={Selective compound splitting of swedish queries for boolean combinations of truncated terms},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={3237},
pages={337-344},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048871540&partnerID=40&md5=f5b2b1834f7cad7abe59755b8a103d03},
affiliation={Swedish Institute of Computer Science, SICS, Box 1263, SE-164 29 Kista, Sweden},
abstract={In languages that use compound words such as Swedish, it is often neccessary to split compound words when indexing documents or queries. One of the problems is that it is difficult to find constituents that express a concept similar to that expressed by the compound. The approach taken here is to expand a query with the leading constituents of the compound words. Every query term is truncated so as to increase recall by hopefully finding other compounds with the leading constituent as prefix. This approach increases recall in a rather uncontrolled way, so we use a Boolean quorum-level search method to rank documents both according to a tf-idf factor but also to the number of matching Boolean combinations. The Boolean combinations performed relatively well, taking into consideration that the queries were very short (maximum of five search terms). Also included in this paper are the results of two other methods we are currently working on in our lab; one for re-ranking search results on the basis of stylistic analysis of documents, and one for dimensionality reduction using Random Indexing. © Springer-Verlag 2004.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hollink200433,
author={Hollink, V. and Kamps, J. and Monz, C. and De Rijke, M.},
title={Monolingual document retrieval for European languages},
journal={Information Retrieval},
year={2004},
volume={7},
number={1-2},
pages={33-52},
note={cited By 54},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-3843105784&partnerID=40&md5=f9c951e94a8a59c020fd5a0c4b3bbf46},
affiliation={Lang. and Inference Technology Group, ILLC, University of Amsterdam, Nieuwe Achtergracht 166, 1018 WV, Amsterdam, Netherlands; Social Science Informatics (SWI), Department of Psychology, University of Amsterdam, Amsterdam, Netherlands},
abstract={Recent years have witnessed considerable advances in information retrieval for European languages other than English. We give an overview of commonly used techniques and we analyze them with respect to their impact on retrieval effectiveness. The techniques considered range from linguistically motivated techniques, such as morphological normalization and compound splitting, to knowledge-free approaches, such as n-gram indexing. Evaluations are carried out against data from the CLEF campaign, covering eight European languages. Our results show that for many of these languages a modicum of linguistic techniques may lead to improvements in retrieval effectiveness, as can the use of language independent techniques.},
author_keywords={Cross-lingual information retrieval;  European languages;  Monolingual document retrieval;  Morphological normalization;  Tokenization},
document_type={Review},
source={Scopus},
}

@ARTICLE{NoAuthor20041,
title={4th Workshop of the Cross-Language Evaluation Forum, CLEF 2003},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={3237},
pages={1-701},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943327424&partnerID=40&md5=0c20883da0670dabd600422a87a09de5},
abstract={The proceedings contain 65 papers. The special focus in this conference is on Ad-hoc Text Retrieval Tracks, Monolingual Experiments, Domain-Specific Document Retrieval, Interactive Cross-Language Retrieval and Cross-Language Question Answering. The topics include: Analysis of the reliability of the multilingual topic set for the cross language evaluation forum; the impact of word normalization methods and merging strategies on multilingual IR; combining query translation and document translation in cross-language retrieval; multilingual information retrieval using open, transparent resources in clef 2003; monolingual, bilingual and multilingual information retrieval; language-dependent and language-independent approaches to cross-lingual text retrieval; multilingual retrieval experiments with MIMOR at the university of Hildesheim; concept-based searching and merging for multilingual information retrieval; automatically generated phrases and relevance feedback for improving CLIR; merging results by predicted retrieval effectiveness; miracle approaches to multilingual information retrieval; experiments to evaluate probabilistic models for automatic stemmer generation and query word translation; regular sound changes for cross-language information retrieval; experiments with machine translation for monolingual, bilingual and multilingual retrieval; comparing weighting models for monolingual information retrieval; pruning texts with nlp and expanding queries with an ontology; report on clef-2003 monolingual tracks; selective compound splitting of Swedish queries for Boolean combinations of truncated terms; experiments with self organizing maps in clef 2003; a data-compression approach to the monolingual girt task; natural language access to the girt4 data and translation selection and document selection.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Ordelman2003225,
author={Ordelman, R. and Van Hessen, A. and De Jong, F.},
title={Compound decomposition in Dutch large vocabulary speech recognition},
journal={EUROSPEECH 2003 - 8th European Conference on Speech Communication and Technology},
year={2003},
pages={225-228},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009170957&partnerID=40&md5=9373eec435a757ed1ac5b17f6d94fcd8},
affiliation={Department of Computer Science, University of Twente, Netherlands},
abstract={This paper addresses compound splitting for Dutch in the context of broadcast news transcription. Language models were created using original text versions and text versions that were decomposed using a data-driven compound splitting algorithm. Language model performances were compared in terms of outof- vocabulary rates and word error rates in a real-world broadcast news transcription task. It was concluded that compound splitting does improve ASR performance. Best results were obtained when frequent compounds were not decomposed.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cöster2003,
author={Cöster, R. and Sahlgren, M. and Karlgren, J.},
title={Selective compound splitting of Swedish queries for Boolean combinations of truncated terms},
journal={CEUR Workshop Proceedings},
year={2003},
volume={1169},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978080613&partnerID=40&md5=9f2649a19fbda7e2c22704b89df5b266},
affiliation={Swedish Institute of Computer Science, SICS, Box 1263, Kista, SE-16429, Sweden},
abstract={Swedish is a compounding language, and therefore it is important to split compound words so that useful word constituents can be found. One of the problems is that it is difficult to find constituents that express a concept similar to that expressed by the compound. The approach taken in this paper is to look at how the leading constituent of the compound word can be used to expand a search query. The constituent was added to the original query, while still keeping the compound. Every word was then truncated so as to increase recall by hopefully finding other compounds with the leading constituent as prefix. Since this approach increase recall in a rather uncontrolled way, we also used a Boolean quorum-level type of query combination so that documents were ranked according to both the tf-idf factor but also to the number of matching Boolean combinations. The Boolean combinations performed relatively well, taken into consideration that the queries were very short (maximum five search terms). Also included in this paper are the results of two other methods we are currently working on in our lab; one for re-ranking search results on the basis of stylistic analysis of documents, and one for dimensionality reduction using Random Indexing.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Monz2001,
author={Monz, C. and De Rijke, M.},
title={The university of Amsterdam at CLEF 2001},
journal={CEUR Workshop Proceedings},
year={2001},
volume={1167},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921953688&partnerID=40&md5=85d483b1ce4dbd8f4a9881095044adb8},
affiliation={Institute for Logic, Language and Computation (ILLC), University of Amsterdam, Amsterdam, 1018 TV, Netherlands},
abstract={This paper describes the official runs of our team for CLEF-2001. We took part in the monolingual task, for Dutch, German, and Italian. The focus of our experiments was on the effects of morphological analyses such as stemming and compound splitting on retrieval effectiveness. Confirming earlier reports on retrieval in compound splitting languages such as Dutch and German, we found improvements to be around 25% for German and as much as 55% for Dutch. For Italian, lexicon-based stemming resulted in gains of up to 25%. Copyright © 2001 for the individual papers by the papers' authors.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hedlund2001147,
author={Hedlund, T. and Pirkola, A. and Järvelin, K.},
title={Aspects of Swedish morphology and semantics from the perspective of mono- and cross-language information retrieval},
journal={Information Processing and Management},
year={2001},
volume={37},
number={1},
pages={147-161},
doi={10.1016/S0306-4573(00)00024-8},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035152106&doi=10.1016%2fS0306-4573%2800%2900024-8&partnerID=40&md5=7a27a4aaed911888c61fa3e179606b73},
affiliation={Department of Information Studies, Univ. Tampere, PO Box 607, FIN-33101, Tampere, Finland},
abstract={This paper analyzes the features of the Swedish language from the viewpoint of mono- and cross-language information retrieval (CLIR). The study was motivated by the fact that Swedish is known poorly from the IR perspective. This paper shows that Swedish has unique features, in particular gender features, the use of fogemorphemes in the formation of compound words, and a high frequency of homographic words. Especially in dictionary-based CLIR, correct word normalization and compound splitting are essential. It was shown in this study, however, that publicly available morphological analysis tools used for normalization and compound splitting have pitfalls that might decrease the effectiveness of IR and CLIR. A comparative study was performed to test the degree of lexical ambiguity in Swedish, Finnish and English. The results suggest that part-of-speech tagging might be useful in Swedish IR due to the high frequency of homographic words.},
document_type={Article},
source={Scopus},
}

@ARTICLE{deVries2001149,
author={de Vries, A.P.},
title={A poor man’s approach to CLEF},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2001},
volume={2069},
pages={149-155},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947322423&partnerID=40&md5=b31734990ae64c19871251c93bb3c20f},
affiliation={CWI, Amsterdam, Netherlands; University of Twente, Enschede, Netherlands},
abstract={The primary goal of our participation in CLEF is to acquire experience with supporting cross-lingual retrieval. We submitted runs for all four target languages, but our main interest has been in the bilingual Dutch to English runs. We investigated whether we can obtain a reasonable performance without expensive (but high quality) resources; we have used only ‘off-the-shelf’, freely available tools for stopping, stemming, compound-splitting (only for Dutch) and translation. Although our results are encouraging, we must conclude that a poor man’s approach should not expect to result in rich men’s retrieval results. © Springer-Verlag Berlin Heidelberg 2001.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{VanHoudt20002345,
author={Van Houdt, B. and Blondia, C.},
title={Analysis of an identifier splitting algorithm combined with polling (ISAP) for contention resolution in a wireless access network},
journal={IEEE Journal on Selected Areas in Communications},
year={2000},
volume={18},
number={11},
pages={2345-2355},
doi={10.1109/49.895039},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034313661&doi=10.1109%2f49.895039&partnerID=40&md5=f161824d2f7fbf5fbea52208e852968e},
affiliation={University of Antwerp, Dept. of Math. and Computer Science, Perf. Anal. Telecommunication S., Universiteitsplein, 1, B-2610 Antwerp, Belgium},
abstract={In this paper, a contention resolution scheme for an uplink contention channel in a wireless access network is presented. The scheme consists of a tree algorithm, namely the identifier splitting algorithm (ISA), combined with a polling scheme. Initially, ISA is used, but at a certain level of the tree, the scheme switches to polling of the stations. This scheme is further enhanced by skipping a few levels in the tree when starting the algorithm (both in a static and a dynamic way) and by allowing multiple instants simultaneously. An analytical model of the system and its variants leads to the evaluation of its performance, by means of the delay density function and the throughput characteristics. This model is used to investigate the influence of the packet arrival rate, the instant at which the ISA scheme switches to polling, the starting level of the ISA scheme, and the use of multiple instances on the mean delay, the delay quantiles, and the throughput.},
document_type={Article},
source={Scopus},
}

@ARTICLE{VanHoudt200091,
author={Van Houdt, B. and Blondia, C.},
title={Performance evaluation of the identifier splitting algorithm with polling in wireless ATM networks},
journal={International Journal of Wireless Information Networks},
year={2000},
volume={7},
number={2},
pages={91-103},
doi={10.1023/A:1009531503222},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034171489&doi=10.1023%2fA%3a1009531503222&partnerID=40&md5=689222302516911242640a02682795f9},
affiliation={University of Antwerp, Dept. of Math. and Computer Science, Perf. Anal. Telecommunication S., Universiteitsplein, 1, B-2610 Antwerp, Belgium},
abstract={This paper presents a performance analysis of the Identifier Splitting Algorithm combined with polling, a contention resolution scheme used to inform the Base Station about the bandwidth needs of the Mobile Station in a wireless ATM network. An analytical model leads to the evaluation of performance parameters which determine the throughput and the access delay of the algorithm for different system parameters. This analysis is used to investigate the influence of the system parameters on the performance, from which guidelines for parameter tuning can be derived.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{VanHoudt199914,
author={Van Houdt, B. and Blondia, C. and Casals, O. and Garcia, J.},
title={Packet level performance characteristics of a MAC protocol for wireless ATM LANs},
journal={Conference on Local Computer Networks},
year={1999},
pages={14-23},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033336183&partnerID=40&md5=81ecadb6949074e1086de2efc46cb9ca},
affiliation={Univ of Antwerp, Belgium},
abstract={This paper determines packet level performance measures of a MAC protocol for a wireless ATM local area network. A key characteristic of the MAC protocol is the Identifier Splitting Algorithm with Polling, a contention resolution scheme used to inform the Base Station about the bandwidth needs of a Mobile Station when no piggybacking can be used. We consider higher layer packets that are generated at the Mobile Station and investigate the influence of the traffic characteristics of the packet arrival process on the efficiency of the protocol and on the delay that packets experience to access the shared medium.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{VanHoudt199910,
author={Van Houdt, B. and Blondia, C. and Casals, O. and Garcia, J. and Vazquez, D.},
title={Performance evaluation of a MAC protocol for wireless ATM networks supporting the ATM service categories},
journal={Proceedings of the 2nd ACM International Workshop on Wireless Mobile Multimedia, WOWMOM 1999},
year={1999},
pages={10-17},
doi={10.1145/313256.313268},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029391820&doi=10.1145%2f313256.313268&partnerID=40&md5=c6ef3e8a4fa86a5aa32ab722acfedfe0},
affiliation={University of Antwerp, Dept. Mathematics and Computer Science, Antwerp, B 2610, Belgium; Polytechnic University of Catalunya, Computer Architecture Department, Barcelona, E 08034, Spain},
abstract={This paper presents a Medium Access Control (MAC) protocol for broadband wireless LANs based on the ATM transfer mode, together with the evaluation of its performance in terms of throughput and access delay. Important characteristics of the MAC protocol are the way information between the Mobile Stations (MS) and Base Station (BS) is exchanged and the algorithm used to allocate the bandwidth in order to support the service categories. The performance is heavily influenced by the way the BS is informed about the bandwidth needs of the MSs. In order to obtain an efficient system, a contention resolution scheme based on an Identifier Splitting Algorithm combined with polling is proposed for that purpose, in case no piggybacking can be used. A detailed analytical evaluation, both on cell level and higher layer packet level, is performed, leading to an assessment of the efficiency and the access delay of the system.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kraaij1998605,
author={Kraaij, W. and Pohlmann, R.},
title={Comparing the effect of syntactic vs. statistical phrase indexing strategies for Dutch},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1998},
volume={1513},
pages={605-616},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945190539&partnerID=40&md5=63384c07b04912a56b313e6b019a4cb3},
affiliation={Institute of Applied Physics, Netherlands Organisation for Applied Scientific Research (TNO), Delft, Netherlands; Utrecht Institute of Linguistics OTS, Utrecht University, Utrecht, Netherlands},
abstract={In this paper we describe the results of experiments contrasting syntactic phrase indexing with statistical phrase indexing for Dutch texts. Our results showed that we at least need a compound split- ting algorithm for good quality retrieval for Dutch texts. If we then add either syntactic or statistical phrases, performance generally improves, but this effect is never statistically significant. If we compare syntactic vs. statistical phrase indexing, syntactic phrases are slightly superior to statistical phrases, particularly at high precision. At higher recall levels syntactic and statistical phrases are equally effective. However, since a compound splitting algorithm requires a dictionary and knowledge about constraints on compound formation, a purely non-linguistic indexing strategy, with or without phrases, does not seem to be very effective for Dutch. © Springer-Verlag Berlin Heidelberg 1998.},
document_type={Conference Paper},
source={Scopus},
}
